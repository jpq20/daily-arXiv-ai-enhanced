<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出了一种简单轻量级的方法来识别大语言模型中编码特定技能的神经元，通过将神经元激活与外部标签和模型置信度等辅助指标相关联，在复杂多技能场景中揭示可解释的任务特定行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各种任务中表现出卓越能力，但其内部机制仍然不透明。本文旨在开发一种方法来隔离编码特定技能的神经元，以增强模型的可解释性。

Method: 基于先前通过软提示训练识别"技能神经元"的工作，该方法扩展到涉及多个技能的复杂场景。通过将神经元激活与外部标签和模型自身置信度等辅助指标相关联，无需手动标记聚合即可揭示可解释的任务特定行为。

Result: 在开放文本生成和自然语言推理等任务上进行了实证验证，证明该方法能够检测到不仅驱动已知技能，还能揭示BigBench算术推理中先前未识别的捷径的神经元。

Conclusion: 该方法提供了一种简单有效的途径来理解大语言模型的内部工作机制，特别是在复杂多技能场景中识别关键神经元和潜在推理捷径。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [2] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 本研究探讨了在大型语言模型预训练中使用多种元数据类型（如文档质量指标）来加速训练的方法，发现细粒度元数据编码信息更有效，并提出了元数据追加和可学习元标记等新方法。


<details>
  <summary>Details</summary>
Motivation: 先前研究仅发现URLs作为元数据对预训练有益，但其他元数据类型是否带来更大效益尚不明确，因此需要系统研究多种元数据对LLM预训练的影响。

Method: 研究了多种元数据类型，引入元数据追加作为辅助任务，使用可学习元标记配合掩码损失，并通过探针分析潜在表示来理解元数据如何影响学习。

Result: 发现细粒度文档质量指标等元数据也能加速预训练，元数据追加和可学习元标记方法都能提高训练效率，元数据通过诱导质量感知的潜在结构来加速学习。

Conclusion: 研究结果为整合元数据以改进LLM预训练的效率和效果提供了实用指南，细粒度元数据和适当的元数据使用方法能显著提升训练性能。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [3] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 研究探讨捷克母语者对AI生成诗歌与人类创作诗歌的识别能力和审美评价，发现参与者无法有效区分AI与人类诗歌，且存在作者偏见影响审美评价。


<details>
  <summary>Details</summary>
Motivation: 当前大多数关于AI生成诗歌的研究集中在英语上，本研究旨在探索在形态复杂、资源较少的捷克语中，AI生成诗歌的感知和审美评价。

Method: 让捷克母语者识别和评价AI生成与人类创作的诗歌，使用逻辑回归模型分析识别准确性与审美评价之间的关系。

Result: 参与者对作者身份的识别准确率仅为45.8%，处于随机水平；审美评价显示强烈的作者偏见，当认为诗歌是AI生成时评分较低，但实际上AI诗歌评分与人类诗歌相当或更高。

Conclusion: AI能够在捷克语等形态复杂的低资源语言中生成令人信服的诗歌，读者的作者信念与诗歌的审美评价密切相关。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [4] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix是一个去中心化的多智能体合成数据生成框架，通过分布式队列传递序列化消息来替代中心化协调器，显著提升了数据生成吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体合成框架通常依赖中心化协调器，存在可扩展性瓶颈，或是针对特定领域硬编码，灵活性受限。

Method: 采用点对点设计，将控制和数据流表示为通过分布式队列传递的序列化消息，计算密集型操作由分布式服务处理，基于Ray构建。

Result: 在多种合成场景中，Matrix在相同硬件资源下实现了2-15倍的数据生成吞吐量提升，且不牺牲输出质量。

Conclusion: Matrix提供了一个模块化、可配置的去中心化框架，能够轻松适应广泛的数据生成工作流，显著提升了多智能体合成数据的效率和可扩展性。

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [5] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文通过系统评估大语言模型在不同任务难度下的泛化能力，发现跨难度泛化通常有限，训练数据中需要包含多种难度级别的样本才能获得更好的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究对于在简单或困难数据上训练能否带来更好结果存在争议，且不清楚这种增益是否体现在简单或困难的测试数据上。本文旨在系统评估LLMs在不同难度任务上的泛化能力。

Method: 使用数千种不同LLMs的输出和项目反应理论（IRT）对六个数据集中的示例进行难度排序，完全基于LLMs的能力来确定难度评级，排除人类主观判断。

Result: 跨难度泛化通常有限；在简单或困难数据上训练都无法在完整的难度范围内实现一致的改进。

Conclusion: 在LLMs的训练和评估数据中包含多种难度级别非常重要，在难度方面走捷径是有风险的。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在8-puzzle任务中的规划和状态推理能力，发现即使有反馈机制，模型仍存在内部状态表示脆弱和启发式规划能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在许多基准测试中表现出色，但其规划和状态推理能力尚不明确，需要直接评估这些能力。

Method: 使用8-puzzle任务测试四种模型，采用零样本、思维链和算法思维等提示策略，并引入分层纠正反馈和外部移动验证器。

Result: 反馈机制对某些模型-提示组合有改善，但成功运行通常冗长且计算昂贵。即使有外部移动验证器，所有模型都无法解决任何谜题。

Conclusion: 当前LLMs在规划方面存在显著限制，需要开发维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [7] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文提出了一个将系统动力学和结构方程建模结合到统一数学框架中的方法，用于支持负责任AI/ML的发展。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型在解决未解决问题时可能放大人类偏见，需要更丰富的因果模型来指导负责任AI/ML的发展，但不同方法基于不同假设难以整合。

Method: 将系统动力学和结构方程建模整合到一个共同的数学框架中，用于从分布生成系统、开发方法并比较结果。

Result: 建立了一个统一的数学框架，能够支持系统动力学在数据科学和AI/ML应用中的认识论基础。

Conclusion: 该框架有助于克服不同方法整合的障碍，为负责任AI/ML的发展提供更系统的理论支持。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 本文通过降维方法提取、处理和可视化基于Transformer的语言模型的潜在状态几何结构，揭示了注意力机制和MLP组件在中间层的明显分离模式，以及GPT-2位置嵌入的高维螺旋结构等新发现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现出色，但其内部机制难以解释。本文旨在通过系统分析Transformer内部结构，支持可复现的可解释性研究。

Method: 在Transformer块中的多个点捕获层间激活，通过主成分分析(PCA)和均匀流形逼近(UMAP)进行系统分析，在GPT-2和LLaMa模型上进行实验。

Result: 发现了注意力机制和MLP组件输出在中间层的明显分离模式，识别了初始序列位置潜在状态的高范数特征，可视化了潜在状态的层间演化，展示了GPT-2位置嵌入的高维螺旋结构和LLaMa的序列级几何模式。

Conclusion: 该方法支持对Transformer内部机制的系统分析，为可解释性研究提供了新的可视化工具和见解，代码已开源供进一步研究使用。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [9] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 研究发现算法效率提升主要依赖于计算规模，而非传统假设的独立改进。LSTM到Transformer的转换贡献了大部分效率增益，而小规模模型的算法进展远低于预期。


<details>
  <summary>Details</summary>
Motivation: 旨在解释2012-2023年间AI训练FLOP效率22,000倍提升的差距，发现现有小规模消融实验只能解释不到10倍增益，存在显著效率缺口。

Method: 通过扩展实验比较LSTM和Transformer的规模依赖效率差异，分析计算最优扩展定律的指数差异，并进行实验外推和文献估计。

Result: 成功解释了6,930倍效率增益，其中规模依赖的LSTM到Transformer转换贡献了主要部分，而其他许多创新在扩展方面差异很小。

Conclusion: 算法效率增益与计算规模密切相关，小模型算法进展比预期慢得多，效率衡量具有强烈的参考依赖性。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [10] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 研究发现深层Vision Transformers性能不如浅层模型，存在Cliff-Plateau-Climb三阶段模式。更好的性能与[CLS]令牌边缘化、补丁令牌分布式共识相关，信息混洗指数显示深层模型信息扩散增加但任务性能未提升。


<details>
  <summary>Details</summary>
Motivation: 解决深层Vision Transformers性能不如浅层模型的反常现象，探索表示随深度演化的规律。

Method: 对ViT-S、ViT-B和ViT-L在ImageNet上进行系统实证分析，使用信息混洗指数量化信息混合模式。

Result: 发现三阶段演化模式，深层模型信息-任务权衡出现更晚，额外层与信息扩散增加相关而非性能提升。

Conclusion: Transformer架构可能从精心校准的深度中获益更多，而非简单增加参数数量，信息混洗指数为模型诊断和设计提供指导。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [11] [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)
*Daniel R. Jiang,Jalaj Bhandari,Yukai Yang,Rémi Munos,Tyler Lu*

Main category: cs.LG

TL;DR: 本文提出了一种将多轮对话强化学习问题转化为单轮RLHF问题的迭代PPO方法，通过将学习到的多轮Q函数作为单轮奖励模型，利用现成的单轮RLHF工具实现稳定的多轮对话优化。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型在多轮对话中的表现面临挑战，特别是在目标导向场景中，存在奖励稀疏、规划与生成不匹配等问题。

Method: 提出迭代PPO算法，将多轮RL问题转化为序列单轮RLHF问题，使用多轮Q函数作为单轮奖励模型，交替进行Q函数拟合和策略改进。

Result: 该方法结合了在线和离线方法的优势，既能保持在线更新的适应性，又能获得离线训练的稳定性，且易于实现。

Conclusion: 迭代PPO为多轮对话优化提供了一种实用且稳定的解决方案，直接利用现有单轮RLHF工具，在在线和离线方法之间找到了平衡点。

Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

</details>


### [12] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 本文质疑电信AI训练中所有样本同等重要的假设，提出基于样本梯度分析的重要性框架，在保持性能的同时减少数据需求和计算开销，推动可持续AI发展。


<details>
  <summary>Details</summary>
Motivation: 电信AI应用中数据量大、噪声多、标注成本高，但传统方法假设所有训练样本同等重要，导致计算和能源效率低下。需要更智能的样本选择方法来提高效率。

Method: 通过跨epoch的样本级梯度分析识别影响力和冗余模式，提出样本重要性框架，选择性优先处理有影响力的数据，减少计算量。

Result: 在三个真实世界电信数据集上的实验表明，该方法在保持性能的同时显著减少了数据需求和计算开销。

Conclusion: 样本重要性框架能够在不影响准确性的前提下优化计算和能源使用，为电信领域的可持续AI发展提供了有效解决方案。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>
