<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 43]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Women's Health Benchmark for Large Language Models](https://arxiv.org/abs/2512.17028)
*Victoria-Elisabeth Gruber,Razvan Marinescu,Diego Fajardo,Amin H. Nassar,Christopher Arkfeld,Alexandria Ludlow,Shama Patel,Mehrnoosh Samaei,Valerie Klug,Anna Huber,Marcel Gühner,Albert Botta i Orfila,Irene Lagoja,Kimya Tarr,Haleigh Larson,Mary Beth Howard*

Main category: cs.CL

TL;DR: 该研究创建了首个女性健康基准测试WHB，评估了13个先进大语言模型在女性健康领域的表现，发现模型失败率约60%，存在严重准确性缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型成为数百万人获取健康信息的主要来源，其在女性健康领域的准确性尚未得到充分检验，需要专门的评估基准来揭示潜在风险。

Method: 研究者开发了女性健康基准测试WHB，包含96个经过严格验证的模型测试项，涵盖5个医学专业领域、3种查询类型和8种错误类型，用于系统评估LLM表现。

Result: 评估13个先进LLM显示约60%的失败率，表现因专业领域和错误类型差异显著；所有模型在"错过紧急情况"指标上表现普遍较差，而GPT-5等新模型在避免不当建议方面有显著改进。

Conclusion: 当前AI聊天机器人尚不能提供可靠的女性健康建议，需要进一步改进模型训练和验证，以确保在关键医疗领域的准确性和安全性。

Abstract: As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with "missed urgency" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.

</details>


### [2] [Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL](https://arxiv.org/abs/2512.17053)
*Khushboo Thaker,Yony Bresler*

Main category: cs.CL

TL;DR: Struct-SQL：一种新颖的知识蒸馏框架，使用结构化推理表示（查询执行计划）来训练小型语言模型，在Text-to-SQL任务上比非结构化CoT蒸馏基线提升8.1%


<details>
  <summary>Details</summary>
Motivation: 企业级Text-to-SQL系统面临成本、安全性和性能的三难困境。当前解决方案迫使企业在昂贵的大型语言模型和性能低下的小型语言模型之间做出选择。改进小型模型的现有方法依赖于从大型模型蒸馏非结构化思维链，但这种方法存在固有的模糊性。

Method: 提出Struct-SQL知识蒸馏框架，采用查询执行计划作为结构化推理蓝图，训练小型语言模型模仿强大大型语言模型。使用结构化思维链而非非结构化思维链进行蒸馏。

Result: 使用结构化CoT蒸馏的小型模型比非结构化CoT蒸馏基线绝对提升了8.1%。详细错误分析显示，性能提升的关键因素是语法错误显著减少。

Conclusion: 使用结构化逻辑蓝图教导模型进行推理，对于小型语言模型可靠生成SQL是有益的。结构化推理表示提供了更清晰、更可靠的教学信号。

Abstract: Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.

</details>


### [3] [XLM: A Python package for non-autoregressive language models](https://arxiv.org/abs/2512.17065)
*Dhruvesh Patel,Durga Prasad Maram,Sai Sreenivas Chintha,Benjamin Rozonoyer,Andrew McCallum*

Main category: cs.CL

TL;DR: XLM是一个用于快速实现小型非自回归语言模型的Python包，旨在解决该领域缺乏标准化工具和难以进行系统比较的问题。


<details>
  <summary>Details</summary>
Motivation: 非自回归文本生成在通用语言建模领域重新受到关注，但缺乏像自回归语言建模那样的标准化训练和推理库。现有非自回归语言模型的实现大多是定制化的，难以进行系统比较，且每个模型都需要自己的数据整理、损失函数和预测逻辑，难以复用通用组件。

Method: 开发了XLM Python包，专注于快速实现小型非自回归语言模型，同时提供配套的xlm-models包，包含一系列预训练的小型模型供研究社区使用。

Result: 成功开发了XLM包，代码已在GitHub上开源（https://github.com/dhruvdcoder/xlm-core），为研究社区提供了标准化的非自回归语言建模工具和预训练模型。

Conclusion: XLM包解决了非自回归语言建模领域缺乏标准化工具的问题，通过提供统一的实现框架和预训练模型，促进了该领域的研究和比较。

Abstract: In recent years, there has been a resurgence of interest in non-autoregressive text generation in the context of general language modeling. Unlike the well-established autoregressive language modeling paradigm, which has a plethora of standard training and inference libraries, implementations of non-autoregressive language modeling have largely been bespoke making it difficult to perform systematic comparisons of different methods. Moreover, each non-autoregressive language model typically requires it own data collation, loss, and prediction logic, making it challenging to reuse common components. In this work, we present the XLM python package, which is designed to make implementing small non-autoregressive language models faster with a secondary goal of providing a suite of small pre-trained models (through a companion xlm-models package) that can be used by the research community. The code is available at https://github.com/dhruvdcoder/xlm-core.

</details>


### [4] [Perturb Your Data: Paraphrase-Guided Training Data Watermarking](https://arxiv.org/abs/2512.17075)
*Pranav Shetty,Mirazul Haque,Petr Babkin,Zhiqiang Ma,Xiaomo Liu,Manuela Veloso*

Main category: cs.CL

TL;DR: SPECTRA是一种用于检测LLM训练数据的数字水印方法，即使水印数据仅占训练语料的0.001%以下也能可靠检测


<details>
  <summary>Details</summary>
Motivation: 随着LLM在从互联网抓取的大规模文本语料上进行训练，训练数据检测对于执行版权和数据许可变得至关重要

Method: 通过使用LLM对文本进行释义，根据单独的评分模型为每个释义分配分数，选择分数与原始文本最接近的释义以避免分布偏移，通过比较可疑模型与评分模型的标记概率来检测是否使用了水印数据进行训练

Result: SPECTRA在检测训练数据与非训练数据时实现了超过9个数量级的p值差距，优于所有测试基线

Conclusion: SPECTRA为数据所有者提供了一种可扩展的、在发布前部署的水印方案，即使在大规模LLM训练中也能存活

Abstract: Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.

</details>


### [5] [When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation](https://arxiv.org/abs/2512.17083)
*Michael H. Coen*

Main category: cs.CL

TL;DR: 论文提出对话主题分割的新评估框架，强调边界密度和段落连贯性比传统精确匹配F1更重要，发现现有基准的性能差异主要源于标注粒度不匹配而非模型质量改进。


<details>
  <summary>Details</summary>
Motivation: 对话主题分割对摘要、检索、记忆管理和对话连续性至关重要，但现有评估方法仍主要依赖严格的边界匹配和F1指标。随着LLM对话系统依赖分割管理超出固定上下文窗口的对话历史，传统评估方法已不适应现代需求，无法反映实际应用效果。

Method: 提出新的对话主题分割评估目标，将边界密度和段落连贯性作为主要标准，同时使用窗口容忍F1（W-F1）。在八个对话数据集（任务导向、开放域、会议风格和合成交互）上评估多种结构不同的分割策略，通过分离边界评分和边界选择来操作化评估框架。

Result: 跨数据集实证评估显示：1）现有基准报告的性能差异主要由标注粒度不匹配和稀疏边界标签驱动，而非模型质量差异；2）许多报告的改进源于评估伪影而非边界检测的真正改进；3）观察到高段落连贯性与相对于稀疏标签的极端过度分割并存，导致误导性的低精确匹配F1分数。

Conclusion: 对话主题分割应理解为选择适当粒度而非预测单一正确边界集。通过明确分离边界评分和边界选择，可以更准确地评估分割质量，边界密度和段落连贯性应成为主要评估标准，传统精确匹配F1指标存在误导性。

Abstract: Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of prior work, evaluation practice in dialogue topic segmentation remains dominated by strict boundary matching and F1-based metrics, even as modern LLM-based conversational systems increasingly rely on segmentation to manage conversation history beyond the model's fixed context window, where unstructured context accumulation degrades efficiency and coherence.
  This paper introduces an evaluation objective for dialogue topic segmentation that treats boundary density and segment coherence as primary criteria, alongside window-tolerant F1 (W-F1). Through extensive cross-dataset empirical evaluation, we show that reported performance differences across dialogue segmentation benchmarks are driven not by model quality, but by annotation granularity mismatches and sparse boundary labels. This indicates that many reported improvements arise from evaluation artifacts rather than improved boundary detection.
  We evaluated multiple, structurally distinct dialogue segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Across these settings, we observe high segment coherence combined with extreme oversegmentation relative to sparse labels, producing misleadingly low exact-match F1 scores. We show that topic segmentation is best understood as selecting an appropriate granularity rather than predicting a single correct boundary set. We operationalize this view by explicitly separating boundary scoring from boundary selection.

</details>


### [6] [Enhancing Long Document Long Form Summarisation with Self-Planning](https://arxiv.org/abs/2512.17179)
*Xiaotang Du,Rohit Saxena,Laura Perez-Beltrachini,Pasquale Minervini,Ivan Titov*

Main category: cs.CL

TL;DR: 提出了一种基于高亮引导的长文本摘要生成方法，利用句子级信息作为内容规划来提升摘要的可追溯性和忠实度。


<details>
  <summary>Details</summary>
Motivation: 解决长文本摘要中生成摘要的忠实度和可追溯性问题，特别是在信息密集的长文档中，传统方法难以保持事实一致性。

Method: 采用高亮引导的生成框架，通过自规划方法识别重要内容，然后基于规划生成摘要。探索了端到端和两阶段两种变体，发现两阶段流水线在长且信息密集的文档上表现更好。

Result: 在长文本摘要数据集上的实验表明，该方法能持续改善事实一致性，同时保持相关性和整体质量。在GovReport数据集上，最佳方法将ROUGE-L提高了4.1分，SummaC分数提升了约35%。定性分析显示高亮引导的摘要有助于保留重要细节，生成更准确和深入的跨领域摘要。

Conclusion: 高亮引导的摘要生成方法能有效提升长文本摘要的事实一致性和可追溯性，两阶段流水线在信息密集的长文档中表现优异，为长文本摘要任务提供了有前景的解决方案。

Abstract: We introduce a novel approach for long context summarisation, highlight-guided generation, that leverages sentence-level information as a content plan to improve the traceability and faithfulness of generated summaries. Our framework applies self-planning methods to identify important content and then generates a summary conditioned on the plan. We explore both an end-to-end and two-stage variants of the approach, finding that the two-stage pipeline performs better on long and information-dense documents. Experiments on long-form summarisation datasets demonstrate that our method consistently improves factual consistency while preserving relevance and overall quality. On GovReport, our best approach has improved ROUGE-L by 4.1 points and achieves about 35% gains in SummaC scores. Qualitative analysis shows that highlight-guided summarisation helps preserve important details, leading to more accurate and insightful summaries across domains.

</details>


### [7] [Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding](https://arxiv.org/abs/2512.17220)
*Yuqing Li,Jiangnan Li,Zheng Lin,Ziyan Zhou,Junjie Wu,Weiping Wang,Jie Zhou,Mo Yu*

Main category: cs.CL

TL;DR: MiA-RAG通过构建全局语义表征（mindscape）来增强RAG系统对长文本的理解能力，使检索和生成过程都能基于整体上下文进行，从而提升长文本任务的表现。


<details>
  <summary>Details</summary>
Motivation: 人类理解长文本时依赖整体语义表征，而当前RAG系统缺乏这种全局上下文指导，因此在长文本任务中表现不佳。心理学中的"Mindscape-Aware Capability"揭示了人类如何通过全局视图组织知识、解释新信息并整合分散证据。

Method: 提出Mindscape-Aware RAG (MiA-RAG)，通过分层摘要构建全局语义表征（mindscape），并以此条件化检索和生成过程。该方法使检索器能形成丰富的查询嵌入，生成器能在连贯的全局上下文中对检索到的证据进行推理。

Result: 在多样化的长文本和双语基准测试中，MiA-RAG在基于证据的理解和全局意义构建任务上持续超越基线方法。进一步分析表明，该方法能将局部细节与连贯的全局表征对齐，实现更类似人类的长文本检索和推理。

Conclusion: MiA-RAG是首个为基于LLM的RAG系统提供显式全局上下文感知的方法，通过构建mindscape来指导检索和生成过程，使系统能够更接近人类的方式理解和处理长文本内容。

Abstract: Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.

</details>


### [8] [Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition](https://arxiv.org/abs/2512.17247)
*Zahra Rahmani,Hossein Sameti*

Main category: cs.CL

TL;DR: 提出了一种针对波斯语等低资源语言的噪声鲁棒ASR纠错框架，通过多假设融合和噪声感知建模，在混合噪声测试集上将词错误率从31.10%降低到24.84%。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在噪声环境下性能显著下降，对于波斯语等低资源语言尤为严重。即使是Whisper等最先进模型在不同信噪比下也难以保持准确性。

Method: 提出噪声敏感的ASR纠错框架，结合多假设和噪声感知建模。使用噪声波斯语音生成5个最佳假设，引入误差级别噪声作为表示，捕捉假设间的语义和标记级分歧，量化噪声引起的语言失真。评估三种模型：基础LLaMA-2-7B、仅文本假设微调变体、以及集成ELN嵌入的噪声条件模型。

Result: 在混合噪声测试集上，提出的微调+ELN模型将词错误率从原始Whisper的31.10%降低到24.84%，显著优于仅文本微调基线的30.79%。原始LLaMA-2-7B模型将WER增加到64.58%，表明其无法独立纠正波斯语错误。

Conclusion: 结合多假设和噪声感知嵌入的方法在噪声现实场景中对于鲁棒的波斯语ASR是有效的，ELN条件模型实现了词错误率的显著降低。

Abstract: Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\% (Raw Whisper) to 24.84\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.

</details>


### [9] [Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience](https://arxiv.org/abs/2512.17260)
*Jiangjie Chen,Wenxiang Chen,Jiacheng Du,Jinyi Hu,Zhicheng Jiang,Allan Jie,Xiaoran Jin,Xing Jin,Chenggang Li,Wenlei Shi,Zhihong Wang,Mingxuan Wang,Chenrui Wei,Shufa Wei,Huajian Xin,Fan Yang,Weihao Gao,Zheng Yuan,Tianyang Zhan,Zeyu Zheng,Tianxi Zhou,Thomas Hanwen Zhu*

Main category: cs.CL

TL;DR: Seed-Prover 1.5是一个通过大规模智能体强化学习训练的形式定理证明模型，结合高效测试时扩展工作流，显著提升了形式定理证明的能力和效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在形式语言（如Lean）中进行定理证明仍然具有挑战性且计算成本高昂，特别是在处理本科及以上水平的数学问题时。需要开发更高效、更强大的形式定理证明系统。

Method: 1. 通过大规模智能体强化学习训练模型，让模型在与Lean等工具的持续交互中积累经验；2. 开发高效的测试时扩展工作流，利用自然语言证明的最新进展，弥合自然语言和形式语言之间的差距。

Result: 1. 在计算预算较小的情况下优于最先进方法；2. 解决了PutnamBench（本科水平）的88%、Fate-H（研究生水平）的80%、Fate-X（博士水平）的33%问题；3. 在9小时内解决了Putnam 2025的12道题中的11道。

Conclusion: 通过高质量形式反馈驱动的经验学习扩展，对形式数学推理的未来具有巨大潜力。Seed-Prover 1.5展示了智能体强化学习和测试时扩展工作流在形式定理证明中的有效性。

Abstract: Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \textbf{88\% of PutnamBench} (undergraduate-level), \textbf{80\% of Fate-H} (graduate-level), and \textbf{33\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.

</details>


### [10] [Subjective Question Generation and Answer Evaluation using NLP](https://arxiv.org/abs/2512.17289)
*G. M. Refatul Islam,Safwan Shaheer,Yaseen Nur,Mohammad Rafid Hamid*

Main category: cs.CL

TL;DR: 该研究旨在开发一个自动化系统，用于从文本输入生成主观问题并评估答案，以帮助教师评估学生作业并增强学生的学习体验。


<details>
  <summary>Details</summary>
Motivation: 虽然客观问题生成已有较多研究，但自动化主观问题生成和答案评估仍处于进展阶段。现有系统需要改进，以帮助教师评估学生作业，并让学生能够在阅读文章或书籍章节后进行自我评估，从而提升学习体验。

Method: 研究计划改进当前的NLP模型或创建新的模型，专门用于从文本输入中自动化生成主观问题并评估答案。

Result: 论文摘要中未提供具体实验结果，但研究目标是开发一个能够有效生成主观问题并评估答案的自动化系统。

Conclusion: 该研究旨在填补自动化主观问题生成和答案评估领域的空白，通过改进或创建新的NLP模型，为教育领域提供有价值的工具，帮助教师和学生提高教学和学习效率。

Abstract: Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.

</details>


### [11] [Governance-Aware Hybrid Fine-Tuning for Multilingual Large Language Models](https://arxiv.org/abs/2512.17344)
*Haomin Qi,Chengbo Huang,Zihan Dai,Yunkai Gao*

Main category: cs.CL

TL;DR: 提出了一种面向治理的混合微调框架，用于大语言模型的多语言低资源适应，结合梯度对齐的低秩更新与结构化正交变换，并通过轻量级数据治理步骤提升性能。


<details>
  <summary>Details</summary>
Motivation: 针对多语言、低资源环境下大语言模型适应的问题，需要在有限计算预算下实现准确性、校准性和跨语言平衡，同时保持训练稳定性和成本效益。

Method: 核心算法结合梯度对齐的低秩更新与结构化正交变换，通过层间混合引入选定子层的酉约束以稳定深度优化，同时配合轻量级无标签数据治理步骤（语言识别、近重复去除、质量过滤）。

Result: 在XNLI和FLORES数据集上，混合方法相比强PEFT基线获得一致提升，保持方向平衡并改善概率校准；对轻量级正字法变体更具鲁棒性；简单治理步骤带来累加性收益；训练足迹测量显示适度开销和有利的成本-质量边界。

Conclusion: 混合和酉PEFT与实用数据治理相结合，为资源高效的多语言适应提供了稳定且可访问的路径。

Abstract: We present a governance-aware hybrid fine-tuning framework for multilingual, low-resource adaptation of large language models. The core algorithm combines gradient-aligned low-rank updates with structured orthogonal transformations through layer-wise mixing and introduces unitary constraints in selected sub-layers to stabilize deep optimization. In tandem with lightweight, label-free data governance steps, including language identification, near-duplicate removal, and quality filtering, the framework targets accuracy, calibration, and cross-language parity under tight compute budgets. Across XNLI and FLORES, the hybrid approach delivers consistent gains over strong PEFT baselines while maintaining directional balance and improving probability calibration, as shown in Tables II and III. It is more resilient to lightweight orthographic variants, as shown in Table IV, and benefits additively from simple governance steps, as shown in Table V. Training footprint measurements indicate modest overhead and a favorable cost-quality frontier, as shown in Table VI and Figure 2. Together, these results show that hybrid and unitary PEFT provide a stable and accessible path to resource-efficient multilingual adaptation when paired with practical data governance.

</details>


### [12] [Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers](https://arxiv.org/abs/2512.17351)
*Zeyuan Allen-Zhu*

Main category: cs.CL

TL;DR: 论文提出CANON LAYERS架构组件，通过受控合成预训练任务评估模型核心能力，发现这些轻量级组件能显著提升推理深度、广度等能力，使弱架构达到SOTA水平


<details>
  <summary>Details</summary>
Motivation: 在学术规模预训练（如13亿参数，1000亿token）中，语言模型的架构差异难以评估，结果常被噪声和随机性主导。需要一种方法来隔离和评估模型的核心能力

Method: 引入受控合成预训练任务框架，在该框架中发现CANON LAYERS——轻量级架构组件，计算相邻token表示的加权和，可无缝集成到Transformer、线性注意力、状态空间模型等序列架构中

Result: 展示了12个关键结果：Canon层能将推理深度提升2倍，增强推理广度、知识操作等能力；使弱架构如NoPE达到RoPE水平，线性注意力达到Mamba2/GDN等SOTA线性模型水平；通过合成任务和真实学术规模预训练验证

Conclusion: 合成预训练框架为隔离学术规模下常被掩盖的核心模型能力提供了经济、有原则的路径；借助无限高质量数据，甚至可以预测未来架构在训练流程改进后的行为，解锁更深层次的推理和分层推断能力

Abstract: Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term "canon" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.
  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by $2\times$), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.

</details>


### [13] [UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models](https://arxiv.org/abs/2512.17385)
*Jiajun Wu,Jian Yang,Wei Zhang,Lin Jing,Yuqing Ma,Ensheng Shi,Yuchi Ma,Zhoujun Li,Xianglong Liu*

Main category: cs.CL

TL;DR: IPC是一种无监督代码生成框架，通过探测LLM内部知识来训练代码生成模型，无需外部标注数据


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码生成严重依赖有监督训练，需要大量标注数据或未标注代码片段，这些数据获取成本高且难以大规模获得

Method: 提出IPC框架，通过问题空间探测、测试理解探测、解决方案空间探测、知识整合与强化来探测LLM内部知识和置信度模式，利用自一致性机制和基于表示的质量估计识别可靠代码候选，训练UCoder模型

Result: 在多个代码基准测试中验证，无监督方法能达到与有监督方法竞争的性能，同时显著减少对标注数据和计算资源的依赖

Conclusion: 模型内部状态包含丰富的代码质量和正确性信号，合理利用这些信号可实现有效的无监督代码生成学习，为资源受限场景下训练代码LLM开辟新方向

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.

</details>


### [14] [Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection](https://arxiv.org/abs/2512.17630)
*Menna Elgabry,Ali Hamdi*

Main category: cs.CL

TL;DR: 本文提出了一种基于置信度加权和可信度感知的集成框架，用于文本情感检测。该方法结合了架构多样的小型Transformer模型，通过双重加权投票机制整合全局可信度和局部置信度，在DAIR-AI数据集上取得了93.5%的宏F1分数，超越了大型LLMs。


<details>
  <summary>Details</summary>
Motivation: 传统集成方法通常依赖同质架构，而本文受孔多塞陪审团定理启发，旨在通过结合架构多样的小型LLMs来提升情感检测性能，同时保持误差多样性并利用各模型的独特偏差。

Method: 1. 结合五种架构不同的小型Transformer模型：BERT、RoBERTa、DistilBERT、DeBERTa和ELECTRA，每个模型都针对情感分类进行完全微调；2. 最小化参数收敛以保持误差多样性；3. 采用双重加权投票机制，整合全局可信度（验证F1分数）和局部置信度（实例级概率）来动态加权模型贡献。

Result: 在DAIR-AI数据集上，该方法取得了93.5%的宏F1分数，超越了包括Falcon、Mistral、Qwen和Phi在内的最先进基准模型，即使在任务特定的低秩适应（LoRA）后也显著优于这些大型LLMs。总参数仅595M，比高达7B参数的模型更参数高效和鲁棒。

Conclusion: 精心设计的小型微调模型集成可以在情感检测等专业NLP任务中超越更大的LLMs，证明了参数效率与性能之间的平衡可以通过智能集成策略实现。

Abstract: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.

</details>


### [15] [Linear Personality Probing and Steering in LLMs: A Big Five Study](https://arxiv.org/abs/2512.17639)
*Michel Frising,Daniel Balcells*

Main category: cs.CL

TL;DR: 本文研究了使用线性方向对齐大五人格特质来探测和引导大语言模型行为的方法，发现线性方向在人格检测中有效，但在引导能力上受上下文影响较大。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出独特且一致的人格特质，这对信任和参与度有重要影响。虽然人格框架是表征和控制LLM行为的宝贵工具，但当前方法要么成本高昂（后训练），要么脆弱（提示工程）。线性方向的探测和引导成为一种廉价高效的替代方案。

Method: 使用Llama 3.3 70B模型生成406个虚构角色描述及其大五人格特质分数。通过提示模型这些描述和Alpaca问卷问题，采样随人格特质变化的隐藏激活。使用线性回归学习激活空间中每层的方向，测试其在探测和引导模型行为方面的有效性。

Result: 线性方向对齐特质分数在人格检测中是有效的探针，但其引导能力强烈依赖于上下文：在强制选择任务中产生可靠效果，但在开放式生成或提示中存在额外上下文时影响有限。

Conclusion: 线性方向可用于有效探测LLM的人格特质，但其引导能力受任务类型和上下文影响，在特定情境下有效，在复杂或开放式任务中效果有限。

Abstract: Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.

</details>


### [16] [Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems](https://arxiv.org/abs/2512.17648)
*Marco Gaido,Sara Papi,Mauro Cettolo,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 论文提出了simulstream框架，这是首个专门用于流式语音到文本翻译系统统一评估和演示的开源框架，解决了现有工具在长音频处理、输出修订和演示功能方面的不足。


<details>
  <summary>Details</summary>
Motivation: 流式语音到文本翻译需要实时处理输入语音，面临严格的延迟约束。现有评估工具SimulEval已不再维护，不支持输出修订，且仅适用于短片段模拟而非长音频流，缺乏演示功能。

Method: 开发了simulstream框架，专门针对长音频流处理设计，支持增量解码和重翻译方法，提供统一的质量和延迟评估标准，并包含交互式Web界面用于系统演示。

Result: simulstream成为首个专门用于流式语音到文本翻译的开源评估框架，能够比较不同方法（增量解码和重翻译）的质量和延迟表现，并提供演示功能。

Conclusion: simulstream框架解决了流式语音到文本翻译领域缺乏统一评估和演示工具的问题，为研究人员提供了更全面、实用的评估平台，特别适合长音频流处理场景。

Abstract: Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.

</details>


### [17] [Peeking Into The Future For Contextual Biasing](https://arxiv.org/abs/2512.17657)
*Ramaneswaran Selvakumar,Cindy Tseng,Eesung Kim,Vijendra Raj Apsingekar,Yun Tang*

Main category: cs.CL

TL;DR: 该论文提出了一种用于注意力编码器解码器模型的上下文偏置方法，通过同时预测多个未来token来提升罕见命名实体的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 端到端自动语音识别模型在通用转录方面表现出色，但在识别罕见或未见过的命名实体（如联系人姓名、地点）方面存在困难，而这些实体对于虚拟助手等下游应用至关重要。

Method: 提出了一种基于候选命名实体列表的上下文偏置方法，通过同时预测多个未来token，使模型能够"窥视未来"并对实体列表中的候选实体进行评分。该方法直接利用多token预测logits，无需额外的实体编码器或交叉注意力层，显著降低了架构复杂性。

Result: 在Librispeech数据集上的实验表明，与基线AED模型相比，该方法在命名实体词错误率方面实现了高达50.34%的相对改进。

Conclusion: 该方法通过多token预测实现了有效的上下文偏置，显著提升了命名实体识别性能，同时保持了模型架构的简洁性。

Abstract: While end-to-end (E2E) automatic speech recognition (ASR) models excel at general transcription, they struggle to recognize rare or unseen named entities (e.g., contact names, locations), which are critical for downstream applications like virtual assistants. In this paper, we propose a contextual biasing method for attention based encoder decoder (AED) models using a list of candidate named entities. Instead of predicting only the next token, we simultaneously predict multiple future tokens, enabling the model to "peek into the future" and score potential candidate entities in the entity list. Moreover, our approach leverages the multi-token prediction logits directly without requiring additional entity encoders or cross-attention layers, significantly reducing architectural complexity. Experiments on Librispeech demonstrate that our approach achieves up to 50.34% relative improvement in named entity word error rate compared to the baseline AED model.

</details>


### [18] [When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content](https://arxiv.org/abs/2512.17738)
*Lydia Nishimwe,Benoît Sagot,Rachel Bawden*

Main category: cs.CL

TL;DR: 该研究探讨用户生成内容翻译中的非标准语言处理问题，分析了四种UGC数据集的人类翻译指南，提出了非标准现象分类和翻译操作框架，并通过LLM案例研究展示了翻译评分对UGC特定提示的敏感性。


<details>
  <summary>Details</summary>
Motivation: 用户生成内容包含大量非标准语言现象（拼写错误、俚语、字符重复、表情符号等），这使得评估UGC翻译变得特别困难，因为"好"的翻译取决于输出所需的标准化程度。

Method: 研究分析了四个UGC数据集的人类翻译指南，推导出十二种非标准现象分类和五种翻译操作（标准化、复制、转移、省略、审查）。通过大型语言模型的案例研究，测试了翻译评分对明确UGC翻译指令提示的敏感性。

Result: 分析揭示了UGC处理方式的显著差异，导致参考翻译中存在不同程度的标准化。LLM翻译评分对包含明确UGC翻译指令的提示高度敏感，当提示与数据集指南一致时评分会提高。

Conclusion: 当保留UGC风格很重要时，公平评估需要模型和指标都能理解翻译指南。研究呼吁在数据集创建时制定清晰指南，并开发可控的、指南感知的UGC翻译评估框架。

Abstract: User-generated content (UGC) is characterised by frequent use of non-standard language, from spelling errors to expressive choices such as slang, character repetitions, and emojis. This makes evaluating UGC translation particularly challenging: what counts as a "good" translation depends on the level of standardness desired in the output. To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). Our analysis reveals notable differences in how UGC is treated, resulting in a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), we show that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that they improve when these align with the dataset's guidelines. We argue that when preserving UGC style is important, fair evaluation requires both models and metrics to be aware of translation guidelines. Finally, we call for clear guidelines during dataset creation and for the development of controllable, guideline-aware evaluation frameworks for UGC translation.

</details>


### [19] [Affect, Body, Cognition, Demographics, and Emotion: The ABCDE of Text Features for Computational Affective Science](https://arxiv.org/abs/2512.17752)
*Jan Philip Wahle,Krishnapriya Vishnubhotla,Bela Gipp,Saif M. Mohammad*

Main category: cs.CL

TL;DR: ABCDE数据集是一个包含4亿多条文本话语的大规模语料库，标注了情感、身体、认知、人口统计和情绪等特征，旨在促进计算情感科学和计算社会科学研究。


<details>
  <summary>Details</summary>
Motivation: 当前计算情感科学和计算社会科学研究面临的主要障碍是：虽然存在许多用于文本标注的资源和算法，但发现、访问和使用这些资源仍然很困难，特别是对于计算机科学领域之外的研究者。这限制了跨学科研究的开展。

Method: 作者构建了ABCDE数据集，收集了超过4亿条来自社交媒体、博客、书籍和AI生成来源的文本话语，并对这些文本进行了广泛的特征标注，包括情感、身体、认知、人口统计和情绪等相关特征。

Result: 成功创建了一个大规模、多源、多特征标注的文本数据集（ABCDE），包含超过4亿条文本话语，涵盖了情感科学、社会科学研究所需的各种特征维度。

Conclusion: ABCDE数据集为跨学科研究提供了重要资源，能够促进情感科学、认知科学、数字人文、社会学、政治学和计算语言学等多个领域的研究，降低了非计算机科学背景研究者使用标注数据的门槛。

Abstract: Work in Computational Affective Science and Computational Social Science explores a wide variety of research questions about people, emotions, behavior, and health. Such work often relies on language data that is first labeled with relevant information, such as the use of emotion words or the age of the speaker. Although many resources and algorithms exist to enable this type of labeling, discovering, accessing, and using them remains a substantial impediment, particularly for practitioners outside of computer science. Here, we present the ABCDE dataset (Affect, Body, Cognition, Demographics, and Emotion), a large-scale collection of over 400 million text utterances drawn from social media, blogs, books, and AI-generated sources. The dataset is annotated with a wide range of features relevant to computational affective and social science. ABCDE facilitates interdisciplinary research across numerous fields, including affective science, cognitive science, the digital humanities, sociology, political science, and computational linguistics.

</details>


### [20] [AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora](https://arxiv.org/abs/2512.17756)
*Zhihan Zhou,Daqian Shi,Rui Song,Lida Shi,Xiaolei Diao,Hao Xu*

Main category: cs.CL

TL;DR: AncientBench：首个专门评估大语言模型对古代汉字（特别是出土文献）理解能力的基准测试，包含字形、读音、意义、上下文四个维度的十个任务。


<details>
  <summary>Details</summary>
Motivation: 现有中文基准测试主要针对现代汉语和传世文献的古汉语，缺乏对出土文献古汉字理解能力的评估。古代文本理解对考古学和中国历史文明研究至关重要，需要专门的评估工具。

Method: 构建AncientBench基准测试，分为四个维度（字形理解、读音理解、意义理解、上下文理解），包含十个任务（部首、声旁、同音字、完形填空、翻译等）。邀请考古研究人员进行实验评估，提出古代模型作为基线，并对当前表现最佳的大语言模型进行广泛实验。

Result: 实验结果显示大语言模型在古代文本场景中具有巨大潜力，但与人类专家仍存在差距。基准测试为评估模型对古代汉字的理解能力提供了全面框架。

Conclusion: AncientBench填补了现有基准测试的空白，将促进大语言模型在考古学和古汉语领域的发展与应用，推动古代文本理解技术的进步。

Abstract: Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.

</details>


### [21] [Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity](https://arxiv.org/abs/2512.17769)
*Tanjim Taharat Aurpa,Farzana Akter,Md. Mehedi Hasan,Shakil Ahmed,Shifat Ara Rafiq,Fatema Khan*

Main category: cs.CL

TL;DR: 本文针对孟加拉语医疗实体识别任务，提出了一种新颖的Multi-BERT集成方法，在缺乏标注数据的低资源语言环境下取得了89.58%的最高准确率，比单层BERT模型提升了11.80%。


<details>
  <summary>Details</summary>
Motivation: 医疗实体识别在医学领域自动化系统开发中至关重要，但目前研究主要集中在英语等资源丰富的语言，而孟加拉语等低资源语言的相关研究严重不足，缺乏高质量的标注数据集。

Method: 首先评估了BERT、DistilBERT、ELECTRA和RoBERTa等Transformer模型，然后提出了一种新颖的Multi-BERT集成方法，并专门为孟加拉语医疗实体识别任务开发了一个高质量的数据集。

Result: Multi-BERT集成方法在所有基线模型中表现最佳，达到了89.58%的最高准确率，相比单层BERT模型提升了11.80%的准确率，证明了该方法在低资源语言医疗实体识别任务中的有效性。

Conclusion: Multi-BERT集成模型在孟加拉语医疗实体识别任务中展现出巨大潜力，为低资源语言的医疗自然语言处理研究奠定了基础，并推动了该领域的进一步发展。

Abstract: Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.

</details>


### [22] [DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports](https://arxiv.org/abs/2512.17776)
*Janghoon Han,Heegyu Kim,Changho Lee,Dahm Lee,Min Hyung Park,Hosung Song,Stanley Jungkyu Choi,Moontae Lee,Honglak Lee*

Main category: cs.CL

TL;DR: DEER是一个用于评估专家级深度研究报告的基准，包含50个跨13个领域的报告写作任务，提供专家评估分类体系和细粒度评分标准，并引入文档级事实核查架构来验证报告中所有声明。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型能够生成专家级报告，但现有评估基准存在不足：缺乏系统性的专家报告标准、过度依赖LLM评估无法捕捉需要专家判断的问题、源验证通常只覆盖有限的部分引用声明而非整个报告的事实可靠性。

Method: 1) 构建包含50个报告写作任务的基准，覆盖13个领域；2) 开发专家评估分类体系（7个维度，25个子维度），细化为130个评分标准；3) 提供任务特定的专家指导，帮助LLM评估者更一致地评估报告质量；4) 提出文档级事实核查架构，提取并验证报告中所有声明（包括引用和未引用的），量化外部证据质量。

Result: DEER与人类专家判断高度相关，能够提供系统优势和弱点的可解释诊断。该基准能够更全面地评估专家级研究报告的质量和事实可靠性。

Conclusion: DEER基准通过系统化的评估框架和全面的文档级事实核查，解决了现有评估方法在专家级深度研究报告评估中的局限性，为LLM生成的研究报告提供了更可靠、更全面的评估工具。

Abstract: As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.

</details>


### [23] [ShareChat: A Dataset of Chatbot Conversations in the Wild](https://arxiv.org/abs/2512.17843)
*Yueru Yan,Tuc Nguyen,Bo Su,Melissa Lieffers,Thai Le*

Main category: cs.CL

TL;DR: ShareChat是一个大规模跨平台语料库，包含来自5个主要AI平台的14.2万对话和66万轮次，保留了平台原生特性如推理痕迹、来源链接和代码工件，覆盖101种语言，时间跨度从2023年4月到2025年10月。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集将大语言模型视为通用文本生成器，剥离了塑造用户交互的界面上下文。不同LLM平台具有独特的界面设计和功能，但现有数据集缺乏这些平台特定的交互特征。

Method: 从ChatGPT、Claude、Gemini、Perplexity和Grok五个主要平台收集公开分享的URL，构建包含142,808个对话和超过660,000轮次的大规模跨平台语料库。数据集保留了平台原生特性，包括推理痕迹、来源链接和代码工件，覆盖101种语言，时间跨度从2023年4月到2025年10月。

Result: ShareChat数据集提供了比先前数据集更长的上下文窗口和更大的交互深度。通过三个代表性分析展示了数据集的多方面实用性：(1)分析对话完整性以衡量用户意图满意度；(2)评估内容生成中的来源引用行为；(3)进行时间分析以跟踪演化使用模式。

Conclusion: ShareChat为理解真实世界中用户与LLM聊天机器人的交互提供了重要且及时的资源，填补了现有数据集在保留平台原生特性和跨平台比较方面的空白。

Abstract: While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [24] [On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues](https://arxiv.org/abs/2512.17060)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 本文提出了一种基于交互分析理论的多智能体系统，通过将每个智能体划分为父母、成人和儿童三种自我状态，并结合信息检索机制，增强LLM智能体的心理深度和行为真实性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在模拟人类行为时缺乏心理深度和一致性，只能提供直接或统计上可能的答案，而忽略了真实人类互动中的深层目标、情感冲突和动机。特别是在社会、政治和心理研究领域，需要更真实地模拟群体动态和社会行为。

Method: 提出基于交互分析理论的多智能体系统，每个智能体被划分为三种自我状态：父母、成人和儿童。这些自我状态作为独立的知识结构，拥有各自的视角和推理风格。系统还集成了信息检索机制，使智能体能够从向量存储中检索相关上下文信息。

Result: 通过模拟对话场景的消融实验，比较了有信息检索和无信息检索的智能体。结果显示该架构表现良好，为探索心理基础结构如何丰富智能体行为开辟了新方向。

Conclusion: 本文贡献在于将交互分析理论与上下文信息检索相结合，提出了一种能够增强基于LLM的多智能体模拟真实性的智能体架构。

Abstract: LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.

</details>


### [25] [MAPPO-LCR: Multi-Agent Policy Optimization with Local Cooperation Reward in Spatial Public Goods Games](https://arxiv.org/abs/2512.17187)
*Zhaoqilin Yang,Axin Xiang,Kedi Yang,Tianjun Liu,Youliang Tian*

Main category: cs.MA

TL;DR: 该研究首次将多智能体近端策略优化(MAPPO)引入空间公共物品博弈，提出MAPPO-LCR方法，通过集中式评论家和局部合作奖励解决传统方法在大型交互群体中难以处理收益耦合和非平稳性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有空间公共物品博弈研究主要依赖演化更新规则或基于价值的强化学习方法，这些方法难以有效表示大型交互群体中的收益耦合和非平稳性。传统PPO方法将智能体视为独立学习者，忽略了群体交互中的内在耦合关系。

Method: 提出MAPPO-LCR方法：1) 使用集中式评论家评估联合策略配置，解决收益耦合问题；2) 引入局部合作奖励，使策略更新与周围合作密度对齐，而不改变原始博弈结构；3) 保持分散执行的同时，在训练期间实现群体层面的价值估计。

Result: 大量模拟实验表明，MAPPO-LCR能够稳定地促进合作涌现，并在不同增强因子下实现可靠收敛。统计分析进一步证实了MAPPO在空间公共物品博弈中相对于PPO的学习优势。

Conclusion: MAPPO-LCR框架成功解决了空间公共物品博弈中的收益耦合和非平稳性问题，通过集中式评论家和局部合作奖励机制，为研究群体合作行为提供了有效的多智能体强化学习方法。

Abstract: Spatial public goods games model collective dilemmas where individual payoffs depend on population-level strategy configurations. Most existing studies rely on evolutionary update rules or value-based reinforcement learning methods. These approaches struggle to represent payoff coupling and non-stationarity in large interacting populations. This work introduces Multi-Agent Proximal Policy Optimization (MAPPO) into spatial public goods games for the first time. In these games, individual returns are intrinsically coupled through overlapping group interactions. Proximal Policy Optimization (PPO) treats agents as independent learners and ignores this coupling during value estimation. MAPPO addresses this limitation through a centralized critic that evaluates joint strategy configurations. To study neighborhood-level cooperation signals under this framework, we propose MAPPO with Local Cooperation Reward, termed MAPPO-LCR. The local cooperation reward aligns policy updates with surrounding cooperative density without altering the original game structure. MAPPO-LCR preserves decentralized execution while enabling population-level value estimation during training. Extensive simulations demonstrate stable cooperation emergence and reliable convergence across enhancement factors. Statistical analyses further confirm the learning advantage of MAPPO over PPO in spatial public goods games.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: Dion2是一种简化Muon优化器计算的方法，通过采样矩阵的行或列来减少正交化步骤的计算和通信开销


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽然具有强大的经验性能和理论基础，但其正交化步骤的超线性计算成本随着规模增加而带来越来越大的开销。现有方法尝试减少进入正交化步骤的矩阵大小，但需要更简单有效的解决方案。

Method: Dion2通过在每个迭代中选择矩阵的一部分行或列，仅对这些选中的部分进行正交化。这种采样过程使更新变得稀疏，从而减少计算和通信成本。

Result: 该方法显著降低了Muon优化器的计算和通信开销，提高了算法的可扩展性。

Conclusion: Dion2提供了一种比先前方法更简单的矩阵缩减方法，通过稀疏化更新有效解决了Muon优化器在大规模应用中的计算瓶颈问题。

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [27] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 开发低成本双模式神经肌肉控制系统，结合EEG和EMG信号实现假肢的多自由度实时控制，总成本约240美元，适合资源匮乏地区使用。


<details>
  <summary>Details</summary>
Motivation: 传统低成本上肢假肢缺乏直观控制系统，限制了截肢者的功能性和可及性，特别是在资源匮乏地区。需要开发经济实惠且直观的控制方案。

Method: 使用NeuroSky MindWave Mobile 2采集EEG信号，通过ThinkGear蓝牙传输到ESP32微控制器，采用轻量级分类模型检测眨眼事件控制手部开合。同时使用MyoWare 2.0传感器采集EMG信号，通过阈值检测实现肘部控制，设置三个激活带和稳定性机制。

Result: 成功构建了功能原型，总成本约240美元，EEG控制四个手指舵机，EMG控制两个肘部舵机，实现了多自由度实时控制。

Conclusion: 该系统展示了低成本、生物直观的假肢控制可行路径，适合资源匮乏地区和全球健康应用。未来工作包括3D打印外壳、减少EMG延迟和改进舵机扭矩。

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [28] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 基于XGBoost的轻量级梯度提升框架，利用METAR地面观测数据和物理引导特征工程，实现低能见度和降水事件的短期预测，在11个国际机场的评估中显著优于传统TAF预报。


<details>
  <summary>Details</summary>
Motivation: 当前航空气象业务依赖计算密集的数值天气预报和人工发布的TAF产品，存在保守偏差和时间分辨率有限的问题，需要更高效、准确的短期预测方法保障航空安全和运行效率。

Method: 采用XGBoost梯度提升框架，仅使用METAR地面观测数据，通过基于热力学原理的物理引导特征工程增强特征表示，在2000-2024年历史数据上进行训练和评估。

Result: 在11个代表不同气候区的国际机场（包括SCEL、KJFK、KORD、KDEN、SBGR、VIDP）的盲测评估中，模型在3小时战术预测时段实现了显著更高的检测率，召回率提升2.5-4.0倍，同时减少了误报。SHAP分析显示模型能够隐式重建局地物理驱动因子。

Conclusion: 该轻量级框架成功捕捉了局地物理过程，无需人工配置，在战术预测时段优于传统TAF预报，为航空气象业务提供了可解释、可操作的短期预测解决方案。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [29] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 论文提出turn-PPO方法，针对多轮任务改进PPO算法，通过回合级MDP替代传统的token级MDP，在WebShop和Sokoban数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练交互式LLM智能体方面具有优势，但现有的GRPO算法在多轮任务中，特别是需要长时程推理的场景下存在局限性。需要更稳定有效的优势估计策略来处理多轮设置。

Method: 首先探索PPO作为GRPO的替代方案，发现PPO更鲁棒。然后提出turn-PPO变体，采用回合级MDP（马尔可夫决策过程）公式，而不是常用的token级MDP，专门针对多轮场景优化。

Result: 在WebShop和Sokoban数据集上的实验结果表明，turn-PPO方法无论是否包含长推理组件都表现出有效性，证明了该方法在多轮任务中的优越性。

Conclusion: turn-PPO通过回合级MDP公式改进了PPO算法在多轮任务中的表现，为训练交互式LLM智能体提供了更稳定有效的强化学习方法，特别是在需要长时程推理的场景下。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [30] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 论文提出SFBD-OMNI框架，利用大量噪声样本进行分布恢复，通过单边熵最优传输和EM算法处理任意测量模型，少量干净样本可显著提升恢复效果。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，获取完全观测样本成本高昂甚至不可行，而部分噪声观测相对容易收集。需要开发能够利用大量噪声样本恢复真实分布的方法。

Method: 将分布恢复任务构建为单边熵最优传输问题，采用EM类算法求解。提出SFBD-OMNI框架，基于桥模型将噪声样本分布映射到真实分布，推广了随机前向-反向反卷积方法以处理任意测量模型。

Result: 实验在基准数据集和多样化测量设置中展示了显著的定性和定量性能提升。提供了测试准则来判断真实分布是否可恢复，并证明在不可恢复情况下，少量干净样本可使分布基本可恢复。

Conclusion: SFBD-OMNI框架能够有效利用大量噪声样本进行分布恢复，处理任意测量模型，少量干净样本可显著提升恢复效果，为实际应用提供了实用解决方案。

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [31] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 本文证明了k最近邻分类器在完全可分度量空间中的弱普遍一致性、强Lebesgue-Besicovitch微分性质与Nagata意义下的σ有限维性三者之间的等价关系，填补了最后一个未证明的蕴含关系(1)⇒(3)。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是建立k最近邻分类器的普遍一致性与度量空间几何性质之间的完整等价关系。之前已有部分结果：(2)⇔(3)由Preiss宣布，(3)⇒(2)由Assouad和Quentin de Gromard证明，(2)⇒(1)由Cérou和Guyader证明，但关键的(1)⇒(3)尚未证明，这是该系列论文中提出的猜想。

Method: 作者采用数学证明的方法，通过分析k最近邻分类器的弱普遍一致性条件，推导出度量空间必须满足Nagata意义下的σ有限维性。论文还修正了该系列第二篇文章中的错误主张。

Result: 成功证明了(1)⇒(3)的蕴含关系，即如果k最近邻分类器在完全可分度量空间X中具有弱普遍一致性，那么X必须是Nagata意义下的σ有限维空间。这完成了三个条件之间的完整等价证明。

Conclusion: 本文最终建立了k最近邻分类器的普遍一致性、强Lebesgue-Besicovitch微分性质和Nagata σ有限维性三者之间的完整等价关系，解决了该领域的一个重要猜想，并修正了先前工作中的错误。

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [32] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 提出BEAMoE方法，通过低秩补偿实现带宽高效的自适应混合专家模型，在保持精度的同时减少推理时的I/O带宽需求


<details>
  <summary>Details</summary>
Motivation: 混合专家模型通过稀疏激活扩展容量，但给内存和带宽带来压力。现有的卸载方法虽然缓解了GPU内存问题，但token级路由导致不规则传输，使推理受限于I/O。静态均匀量化减少流量但会降低精度，特别是在激进压缩时忽略了专家异质性。

Method: 提出带宽高效的自适应混合专家模型，通过低秩补偿实现。方法使用路由器引导的精度恢复，采用预计算的低秩补偿器。在推理时，传输紧凑的低秩因子给每个token的Top-n专家，并应用补偿，同时保持其他专家为低比特。该方法与GPU和GPU-NDP系统的卸载集成。

Result: 该方法提供了优越的带宽-精度权衡，并提高了吞吐量。在GPU和GPU-NDP系统上集成卸载后，实现了更好的性能。

Conclusion: 提出的BEAMoE方法通过低秩补偿机制，有效解决了混合专家模型在推理时的带宽瓶颈问题，在保持模型精度的同时显著减少了I/O需求，为大规模MoE模型的部署提供了实用解决方案。

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [33] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 该研究通过在训练中引入故意包含错误的推理轨迹，提升了语言模型在数学推理中的错误检测和恢复能力，而不影响标准问题解决性能。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维提示在大型语言模型的数学推理中很关键，但模型对早期错误很脆弱：单个算术错误或不合理推断通常会传播到最终错误答案。研究者想探索是否可以通过训练模型处理有缺陷的推理轨迹来提升其错误检测和恢复能力。

Method: 使用MATH-lighteval竞赛级问题，生成包含单一控制错误（计算错误或推理错误）的链式思维前缀，使用GRPO对Qwen3-4B模型进行微调，采用二元最终答案奖励机制。

Result: Mixed-CoT-RL模型在干净问题上与标准RL表现相当（41% vs 41%），但在包含错误推理的问题上显著优于标准RL（24% vs 19%）。仅使用干净数据训练的RL会降低鲁棒性（19% vs 20%）。混合训练（包含推理错误和计算错误）效果最好。

Conclusion: 在训练中暴露有缺陷的推理轨迹可以改善模型的错误恢复行为而不牺牲准确性，这为提升LLMs数学推理的鲁棒性提供了一条可行路径。

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [34] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: 提出融合强化学习和MPC规划的新方法，通过自适应采样机制提升规划性能和数据效率


<details>
  <summary>Details</summary>
Motivation: 解决具有层次结构的规划问题，通过融合强化学习和MPC规划的优势，提升规划方法的鲁棒性和适应性

Method: 将强化学习与MPPI规划紧密耦合，利用RL动作指导MPPI采样器，自适应聚合MPPI样本来改进价值估计，形成自适应探索过程

Result: 在赛车驾驶、改进的Acrobot和添加障碍物的Lunar Lander等多个领域验证，相比现有方法成功率提升最高72%，收敛速度提升2.1倍

Conclusion: 该方法能够处理复杂规划问题，易于适应不同应用场景，在数据效率和整体性能方面表现优异

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [35] [UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data](https://arxiv.org/abs/2512.17100)
*Justin Li,Efe Sencan,Jasper Zheng Duan,Vitus J. Leung,Stephan Tsaur,Ayse K. Coskun*

Main category: cs.LG

TL;DR: UniCoMTE是一个模型无关的框架，用于为多元时间序列分类器生成反事实解释，通过修改输入样本评估对模型预测的影响，在ECG分类任务中表现出比现有方法更好的可理解性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在复杂时间序列数据分类中表现出色，但其黑盒性质限制了在高风险领域（如医疗健康）中的信任和采用，需要提高模型的可解释性。

Method: 提出UniCoMTE框架，通过修改输入样本并评估其对模型预测的影响来识别影响预测的关键时间特征。该框架与多种模型架构兼容，可直接处理原始时间序列输入。

Result: 在ECG时间序列分类器上评估，UniCoMTE生成的解释比LIME和SHAP等现有方法更简洁、稳定且与人类认知一致。医学专家问卷评估显示其具有更好的临床实用性。

Conclusion: UniCoMTE通过将模型预测与有意义的信号模式联系起来，提升了深度学习模型在现实世界时间序列应用中的可解释性，为高风险领域的模型信任建立提供了有效工具。

Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.

</details>


### [36] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: Atom是一个在移动设备上高效执行视频-语言多阶段管道的系统，通过模块分解和重用消除冗余模型加载，实现并行执行，显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 当前视频-语言模型在移动设备上执行多阶段管道时存在挑战，包括冗余模型加载和碎片化执行，导致效率低下

Method: 将十亿参数模型分解为可重用模块（如视觉编码器和语言解码器），在不同子任务（字幕生成、推理、索引等）中重用这些模块，实现并行执行

Result: 在商用智能手机上，Atom比非重用基线快27-33%，性能下降很小（检索任务Recall@1下降≤2.3，字幕生成CIDEr下降≤1.5）

Conclusion: Atom为边缘设备上的高效视频-语言理解提供了一个实用、可扩展的方法

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [37] [Bridging Training and Merging Through Momentum-Aware Optimization](https://arxiv.org/abs/2512.17109)
*Alireza Moayedikia,Alicia Troncoso*

Main category: cs.LG

TL;DR: 提出统一框架，在训练过程中维护因子化的动量和曲率统计信息，然后重用这些信息进行几何感知的模型组合，消除冗余计算。


<details>
  <summary>Details</summary>
Motivation: 当前工作流程在训练期间计算曲率信息后丢弃，然后在模型合并时重新计算类似信息，浪费计算资源并丢弃有价值的轨迹数据。

Method: 引入统一框架，在训练过程中维护因子化的动量和曲率统计信息，然后重用这些信息进行几何感知的模型组合，无需后验Fisher计算。

Result: 在自然语言理解基准测试中，曲率感知参数选择在所有稀疏度水平上都优于仅基于幅度的基线，多任务合并优于强基线，框架具有秩不变收敛性和超参数鲁棒性。

Conclusion: 通过将优化轨迹视为可重用资产而非丢弃，该方法消除了冗余计算，同时实现了更原则性的模型组合。

Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.

</details>


### [38] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: 首个针对古尼泊尔语的端到端手写文本识别系统，采用编码器-解码器架构和数据中心化技术，在字符错误率上达到4.9%


<details>
  <summary>Details</summary>
Motivation: 古尼泊尔语作为一种历史重要但资源匮乏的语言，缺乏有效的手写文本识别系统，需要开发专门的处理方法

Method: 采用行级转录方法，系统探索编码器-解码器架构和数据中心化技术，实现端到端识别管道

Result: 最佳模型字符错误率(CER)达到4.9%，并分析了解码策略和标记级混淆以理解模型行为和错误模式

Conclusion: 成功开发了首个古尼泊尔语手写文本识别系统，虽然评估数据集保密，但开源了训练代码和配置以支持低资源历史文字研究

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [39] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 该研究评估了CheXagent模型在胸部X光图像检索中处理否定语句的能力，通过微调方法改善了CLIP模型对否定语句的理解，同时分析了模型内部行为变化。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型如CLIP在医学影像任务中应用广泛，但存在处理否定语句能力不足的问题，这在医学诊断场景中尤为关键，可能影响模型的可靠性。

Method: 评估Stanford AIMI CheXagent模型在使用含否定和不含否定提示时的胸部X光图像检索能力，基于先前工作的方法进行微调改进，并通过token归因、t-SNE投影和注意力头消融分析模型内部行为。

Result: 研究显示微调后CLIP模型处理否定语句的能力有所改善，但正面提示评估的准确性略有下降；通过内部行为分析揭示了微调方法如何重塑文本编码器对否定临床语言的理解。

Conclusion: 该工作有助于更好地理解CLIP模型的内部行为，通过临床相关语言改进其对否定语句的处理能力，从而提高医学AI设备的可靠性。

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [40] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: GPA是一种改进的优化算法，通过解耦Nesterov方法中的插值常数，实现每步平滑平均，解决了单工作器DiLoCo和Schedule-Free算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决单工作器DiLoCo和Schedule-Free等基于平均的优化器的关键限制，包括DiLoCo的两循环结构增加内存需求和超参数数量，以及Schedule-Free的显式平均策略。

Method: 提出广义原始平均(GPA)方法，通过解耦Nesterov原始平均公式中的插值常数，实现每步平滑平均迭代，无需两循环结构，减少内存开销至单个额外缓冲区。

Result: 在Llama-160M模型上，GPA相比基线(AdamW)验证损失达到速度提升24.22%；在ImageNet ViT任务中，小批量和大批量设置下分别获得12%和27%的速度提升；理论证明GPA能匹配或超过原始优化器的收敛保证。

Conclusion: GPA通过解耦插值常数实现平滑平均，在保持理论收敛保证的同时，显著提升了优化性能，简化了超参数调优，并减少了内存开销。

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [41] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 该研究系统比较了五种时间序列预测模型在不同时间尺度（分钟、小时、天）和空间聚合水平（单个充电站到城市级）下对电动汽车充电需求的预测效果，使用了四个真实世界数据集。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及带来的电网管理挑战，预测充电需求成为重要研究问题。现有研究缺乏对不同时间尺度、空间聚合水平和多种城市环境的系统比较。

Method: 研究采用五种时间序列预测模型（传统统计方法、机器学习和深度学习方法），在四个公开真实数据集上评估短期（分钟级）、中期（小时级）和长期（天级）预测，以及从单个充电站到区域和城市级的空间聚合水平。

Result: 研究结果分别报告了四个数据集的预测性能，首次系统评估了电动汽车充电需求预测在如此广泛的时间尺度和空间聚合水平下的表现。

Conclusion: 这是首个使用多个真实世界数据集，系统评估电动汽车充电需求预测在不同时间尺度和空间聚合水平下效果的研究，为电网管理和充电基础设施规划提供了重要参考。

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [42] [SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS](https://arxiv.org/abs/2512.17262)
*Suraj Kumar,Arvind Kumar,Soumi Chattopadhyay*

Main category: cs.LG

TL;DR: SHARP-QoS是一个用于联合QoS预测的统一框架，通过双机制提取层次特征、自适应特征共享和EMA损失平衡策略，解决现有方法中负迁移和表示学习不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的QoS数据极其稀疏、噪声大且具有层次依赖关系，现有方法要么单独预测每个QoS参数导致计算成本高、泛化能力差，要么采用共享架构但存在负迁移问题（由于QoS参数数值范围不一致导致损失缩放）和表示学习不足，导致准确性下降。

Method: SHARP-QoS包含三个核心组件：1）通过双机制在庞加莱球中使用双曲卷积提取QoS和上下文结构的层次特征；2）自适应特征共享机制，允许信息丰富的QoS和上下文信号之间进行特征交换，采用门控特征融合模块支持结构和共享表示之间的动态特征选择；3）基于EMA的损失平衡策略，实现稳定的联合优化，缓解负迁移问题。

Result: 在包含两个、三个和四个QoS参数的三个数据集上的评估表明，SHARP-QoS在单任务和多任务基线方法中表现最优。广泛研究表明，该模型有效解决了稀疏性、对异常值的鲁棒性和冷启动等主要挑战，同时保持适度的计算开销。

Conclusion: SHARP-QoS通过层次特征提取、自适应特征共享和损失平衡策略，实现了可靠的联合QoS预测，能够处理现实世界中稀疏、噪声大且具有层次依赖关系的QoS数据，为面向服务的计算提供了有效的解决方案。

Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.

</details>


### [43] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 本文提出了一种信息论度量R-EMID来量化角色扮演模型在分布偏移下的性能退化，并开发了协同进化强化学习框架来提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 角色扮演模型在实际应用中广泛使用，但在真实部署时性能会下降。这种退化可归因于用户、角色和对话组合的分布偏移。现有方法如LLM-as-a-judge无法提供细粒度的诊断，缺乏形式化框架来表征RPM的泛化行为。

Method: 1. 引入信息论度量R-EMID（推理有效互信息差异）来以可解释的方式测量RPM性能退化；2. 推导R-EMID的上界来预测RPM的最坏情况泛化性能；3. 提出协同进化强化学习框架，自适应地建模用户、角色和对话上下文之间的连接，从而增强对话响应生成概率的估计。

Result: 评估了各种RPM的泛化性能，发现用户偏移在所有偏移中风险最高，而强化学习是增强RPM泛化最有效的方法。R-EMID能够量化不同分布偏移对模型性能的影响。

Conclusion: 提出的R-EMID度量提供了角色扮演模型泛化性能的量化评估框架，协同进化强化学习方法能够有效提升模型对分布偏移的鲁棒性，为RPM的实际部署提供了理论指导和实用工具。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [44] [MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics](https://arxiv.org/abs/2512.17273)
*Farinaz Mostajeran,Aruzhan Tleubek,Salah A Faroughi*

Main category: cs.LG

TL;DR: MINPO是一个统一的神经框架，用于建模由长程空间相互作用和/或长期时间记忆引起的非局部动力学，通过学习非局部算子及其逆并重构解场，有效处理各种积分-微分方程。


<details>
  <summary>Details</summary>
Motivation: 许多物理系统表现出由积分-微分方程描述的非局部时空行为。传统方法需要重复计算卷积积分，成本随核复杂度和维度快速增加。现有神经求解器可以加速特定计算，但不能泛化到不同的非局部结构。

Method: MINPO使用KANs或MLPs作为编码器，直接学习非局部算子及其逆的神经表示，然后显式重构未知解场。学习过程通过轻量级非局部一致性损失项来确保学习到的算子与重构解之间的一致性。

Result: 与经典技术和基于MLPs的先进神经策略（A-PINN、fPINN）及其KAN变体（A-PIKAN、fPIKAN）相比，MINPO在准确性方面表现出色，并在处理（i）多种核类型，（ii）不同核维度，以及（iii）重复核积分计算带来的大量计算需求方面展现出鲁棒性。

Conclusion: MINPO超越了特定问题公式，为受非局部算子控制的系统提供了一个统一框架，能够自然捕获并有效解决由广泛谱系的IDEs及其子集（包括分数阶PDEs）支配的非局部时空依赖关系。

Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.

</details>


### [45] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: MATCH-AD是一个用于阿尔茨海默病诊断的半监督学习框架，通过结合深度表示学习、图标签传播和最优输运理论，在只有三分之一数据有标签的情况下实现近乎完美的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病诊断面临临床评估昂贵且侵入性的挑战，导致神经影像数据集中只有少量样本有真实标签，需要开发能够利用大量未标注数据的诊断方法。

Method: 提出MATCH-AD框架，整合深度表示学习、基于图的标签传播和最优输运理论（Wasserstein距离），利用神经影像数据的流形结构将诊断信息从有限标注样本传播到大量未标注样本。

Result: 在近5000名受试者的NACC数据集上评估，尽管只有不到三分之一的样本有真实标签，但实现了近乎完美的诊断准确率，Kappa系数显示几乎完全一致，显著优于所有基线方法。

Conclusion: 该框架证明了原则性半监督学习能够释放全球积累的部分标注神经影像数据的诊断潜力，大幅减少标注负担，同时保持适合临床部署的准确性。

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [46] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: 研究发现上下文学习可分解为任务图式和绑定两个独立机制，通过激活修补实验验证了它们的可分离性，揭示了先验知识与图式依赖的权衡关系，并发现该机制在包括Mamba在内的多种架构中普遍存在。


<details>
  <summary>Details</summary>
Motivation: 传统研究将上下文学习视为单一机制（检索式、梯度下降式或纯贝叶斯式），缺乏对内部工作机制的因果机制验证。本研究旨在通过实验验证上下文学习是否可分解为可分离的神经机制。

Method: 使用激活修补实验，在9个模型（来自7个Transformer家族加上Mamba，参数规模370M-13B）上进行测试。通过晚期MLP修补实现任务图式100%转移，通过残差流修补实现绑定62%转移，验证机制可分离性。

Result: 1. 双重分离：任务图式通过晚期MLP修补100%转移，绑定通过残差流修补62%转移，证明机制可分离
2. 先验-图式权衡：图式依赖与先验知识负相关（Spearman rho = -0.596, p < 0.001）
3. 架构普适性：该机制在所有测试架构（包括非Transformer的Mamba）中都存在
4. 瓶颈是注意力而非输出：先验知识干扰通过注意力误路由（72.7%近因偏差）而非直接输出竞争（0%）

Conclusion: 上下文学习由任务图式和绑定两个可分离的神经机制组成，为双过程理论提供因果证据。模型在先验知识缺失时依赖任务图式，而先验知识干扰主要通过注意力机制而非输出竞争。这一发现对提示工程有实际意义：可靠的图式转移可减少新任务所需演示，先验感知设计可缓解高先验场景下38%的绑定失败率。

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [47] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: 提出自适应剪枝算法减少ST-GNN在边缘计算中的通信开销，同时引入SEPA新指标评估交通事件预测能力


<details>
  <summary>Details</summary>
Motivation: ST-GNN在智能交通系统中处理分布式传感器数据时，由于相邻边缘节点间重复传输重叠特征导致通信开销巨大，需要降低通信成本同时保持预测准确性

Method: 提出自适应剪枝算法动态过滤冗余邻居特征，基于近期模型性能调整剪枝率；引入SEPA指标专门评估交通减速和恢复事件的预测能力；在在线半分散设置中与传统FL、无服务器FL和Gossip Learning对比

Result: 自适应剪枝算法在所有在线半分散设置中显著降低通信成本的同时保持预测准确性；SEPA指标揭示了空间连接性在预测动态不规则交通中的真正价值

Conclusion: 通信成本可以在不损害对关键交通事件响应能力的情况下显著降低，自适应剪枝算法有效平衡了通信效率和预测性能

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [48] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 论文揭示了奖励模型和LLM-as-a-Judge系统存在一个关键漏洞：低困惑度的控制token序列可以翻转模型的二元判断，导致高误报率，这对RLHF、DPO等后训练流程构成实际风险。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和LLM-as-a-Judge系统是现代后训练流程（如RLHF、DPO、RLAIF）的核心组件，它们提供标量反馈和二元决策来指导模型选择和强化学习微调。然而，这些评判系统存在一个反复出现的脆弱性，可能被利用进行奖励攻击，影响模型训练和评估的可靠性。

Method: 提出了AdvJudge-Zero方法，利用模型的下一token分布和束搜索探索从头发现多样化的控制token序列。分析显示这些控制token诱导的隐藏状态扰动集中在低维"软模式"中，与评判模型的拒绝方向反对齐。

Result: 实验表明，这些控制token在大型开源权重和专用评判模型对数学和推理基准测试的错误答案进行评分时，会导致极高的误报率。最后，通过在少量控制token增强的示例上进行LoRA对抗训练，可以显著减少这些误报，同时保持评估质量。

Conclusion: 评判系统存在被低困惑度控制token序列攻击的漏洞，这对后训练流程构成实际风险。通过对抗训练可以有效缓解这一问题，提高评判系统的鲁棒性。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [49] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 提出基于多智能体强化学习的电力市场模型，用于分析长期市场机制对电力系统脱碳的影响，应用于意大利电力系统案例研究。


<details>
  <summary>Details</summary>
Motivation: 电力系统是实现碳中和经济转型的关键，长期电力市场机制（包括拍卖、支持计划等政策工具）对塑造发电结构至关重要。目前缺乏先进工具支持政策制定者设计、测试和评估长期市场。

Method: 开发多智能体强化学习模型，采用独立近端策略优化算法，模拟利润最大化的发电公司在批发电力市场中的投资决策。通过广泛的超参数搜索确保分散训练产生符合竞争行为的结果。

Result: 模型应用于意大利电力系统简化版本，测试不同竞争水平、市场设计和政策情景。结果显示市场设计对电力部门脱碳和避免价格波动具有关键作用。

Conclusion: 该框架能够评估多种政策和市场机制同时交互的长期电力市场，市场参与者能够响应和适应脱碳路径，为政策制定提供有力工具。

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [50] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 提出LLM-SGA框架和ARHOCD检测器，通过识别文本对抗攻击的关键不变性来增强泛化能力，并采用集成学习、动态权重分配和对抗训练来提高对抗条件下的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台存在大量有害内容，机器学习检测模型容易受到对抗攻击，现有研究在同时实现泛化能力和准确性方面存在挑战，需要新的方法来增强对抗鲁棒性。

Method: 1. 提出LLM-SGA框架，识别文本对抗攻击的关键不变性；2. 实例化ARHOCD检测器，包含三个创新组件：多基检测器集成、基于可预测性和检测器能力的动态权重分配方法（贝叶斯推理更新）、迭代优化基检测器和权重分配器的对抗训练策略。

Result: 在仇恨言论、谣言和极端主义内容三个数据集上评估，ARHOCD在对抗条件下表现出强大的泛化能力并提高了检测准确率。

Conclusion: ARHOCD通过创新的框架设计和组件实现了对抗鲁棒性的显著提升，为社交媒体有害内容检测提供了有效的解决方案，克服了现有研究在泛化能力和准确性之间的权衡问题。

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [51] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: 提出Write-Gated KV机制，通过学习预测token效用，在写入KV缓存前进行筛选，减少46-57%内存使用，提升长上下文推理效率


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理受限于二次注意力复杂度和线性KV缓存增长，现有方法通过后处理选择或淘汰忽略了根本低效问题：不加区分地写入持久内存

Method: 将KV缓存管理形式化为包含KV准入、选择和淘汰三个原语的因果系统，通过Write-Gated KV机制学习预测token效用，在写入前筛选，维护紧凑全局缓存和滑动本地缓存

Result: 减少46-57%内存使用，在Llama模型上实现3.03-3.45倍预填充加速和1.89-2.56倍解码加速，精度损失可忽略，兼容FlashAttention和分页KV系统

Conclusion: 学习"写什么"是高效长上下文推理的原则性和实用方法，Write-Gated KV通过早期过滤低效用状态有效管理KV缓存

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [52] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: SafeBench-Seq是一个用于蛋白质序列危害筛查的基准测试和基线分类器，基于公开数据构建，采用同源性聚类评估，在普通CPU上运行，仅发布元数据而不分发危险序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质设计基础模型存在生物安全风险，但社区缺乏简单、可重复的序列级危害筛查基准，需要在同源性控制下评估且能在普通CPU上运行。

Method: 使用公开数据（SafeProtein危害序列和UniProt良性序列），提取可解释特征（全局物理化学描述符和氨基酸组成），在≤40%同源性下聚类数据集，进行聚类级保留评估，采用校准分类器并评估概率质量。

Result: 随机分割会显著高估模型鲁棒性；校准线性模型表现出较好的校准性；树集成方法保持稍高的Brier/ECE分数；基准测试仅需CPU、可重复且仅发布元数据。

Conclusion: SafeBench-Seq为蛋白质序列危害筛查提供了一个简单、可重复、仅需元数据的基准测试框架，支持在同源性控制下进行严格评估，而不需要分发危险序列。

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [53] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 该论文提出了一个统计工具箱，用于在医学影像应用中系统分析机器学习模型在不同患者亚组中的性能差异，解决样本量差异、多重比较校正等统计挑战。


<details>
  <summary>Details</summary>
Motivation: 当前分析机器学习模型在不同患者亚组中的性能差异已成为标准做法，但进行统计学严谨的分析面临诸多挑战：需要选择合适的性能指标以适应不同样本量和基础率，需要确定指标不确定性并进行多重比较校正，以及在交叉分析中需要从组合众多的亚组中找到最"有趣"的亚组。

Method: 作者开发了一个统计工具箱，专门设计用于医学影像应用。该工具箱提供了系统的方法来处理上述统计挑战，包括：选择适合不同样本量和基础率的性能指标、确定指标不确定性、进行多重比较校正、在交叉分析中识别重要亚组的机制。

Result: 该工具箱在两个案例研究中得到验证：1）在ISIC2020数据集上的皮肤病变恶性分类；2）在MIMIC-CXR数据集上的胸部X射线疾病分类。这些案例展示了工具箱在实际医学影像应用中的有效性。

Conclusion: 该统计工具箱使从业者能够轻松而严谨地评估其模型在不同亚组中的潜在性能差异，特别适用于医学影像应用，有助于识别重要的模型失败模式。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [54] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: GreedySnake是一种新的SSD卸载训练系统，采用垂直调度策略，相比现有水平调度系统，在小批量训练时获得更高吞吐量，更接近屋顶线模型预测的理想场景。


<details>
  <summary>Details</summary>
Motivation: SSD卸载训练为LLM训练提供了一种经济实用的方法。现有系统使用水平调度（按顺序执行微批次），而GreedySnake旨在通过新的调度策略提高训练吞吐量，特别是在小批量情况下。

Method: GreedySnake采用垂直调度策略，在执行完一个层的所有微批次后再进入下一层。此外，系统将部分优化步骤与下一次迭代的前向传播重叠，以进一步缓解I/O瓶颈。

Result: 在A100 GPU上的实验结果显示，GreedySnake相比ZeRO-Infinity实现了显著的训练吞吐量提升：GPT-65B在1GPU上提升1.96倍，4GPU上提升1.93倍；GPT-175B在1GPU上提升2.53倍。

Conclusion: GreedySnake通过垂直调度和优化步骤重叠技术，有效提高了SSD卸载训练的吞吐量，使系统更接近屋顶线模型的理想性能，为大规模语言模型训练提供了更高效的解决方案。

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [55] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: TRAPO是一种混合训练框架，通过交错使用监督微调(SFT)和强化学习(RL)，解决了传统两阶段训练中的不一致问题，在数学推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统SFT然后RL的两阶段训练存在关键不一致性：SFT强制刚性模仿，抑制探索并导致遗忘，限制了RL的改进潜力。需要更高效的训练框架来统一外部监督和自我探索。

Method: 提出TRAPO框架，在每个训练实例中交错SFT和RL：在专家前缀上优化SFT损失，在模型自身补全上优化RL损失。引入信任区域SFT(TrSFT)来稳定训练，最小化信任区域内的前向KL散度，在区域外衰减优化。还包含自适应前缀选择机制，根据测量效用分配专家指导。

Result: 在五个数学推理基准测试中，TRAPO一致超越了标准SFT、RL、SFT-then-RL流程以及最近的最先进方法，为推理增强的LLMs建立了强大的新范式。

Conclusion: TRAPO通过统一监督学习和强化学习，解决了传统两阶段训练的不一致问题，为提升大语言模型的复杂推理能力提供了更有效的训练框架。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [56] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: YOTO是一个端到端的单细胞转录组基因选择框架，通过联合学习离散基因子集和预测任务，实现稀疏、可解释的特征选择，无需额外下游分类器训练。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞转录组特征选择方法多为多阶段流程或依赖事后特征归因，导致选择与预测任务弱耦合，需要更紧密的端到端解决方案。

Method: 提出YOTO框架，在单个可微分架构中联合识别离散基因子集并执行预测，通过稀疏性约束确保只有选定基因参与推理，采用多任务学习设计共享相关任务间的表征。

Result: 在两个代表性单细胞RNA-seq数据集上评估，YOTO始终优于现有最先进基线方法，展示了更好的预测性能和更紧凑、有意义的基因子集。

Conclusion: 稀疏、端到端、多任务的基因子集选择能提升预测性能并产生紧凑且有意义的基因子集，推动了生物标志物发现和单细胞分析的发展。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [57] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: 论文提出协作式前向-前向算法，通过引入层间协作机制改进原始前向-前向算法，解决层间隔离问题，提升深度架构中的收敛效率和表征协调性。


<details>
  <summary>Details</summary>
Motivation: 传统前向-前向算法存在层间隔离问题，各层独立优化"goodness"函数，缺乏集体学习动态，限制了表征协调和深度架构的收敛效率。

Method: 提出协作式前向-前向学习框架，包含两种协作范式：固定协作和自适应协作。固定协作采用恒定层间耦合，自适应协作使用可学习的协作参数。协作goodness函数整合所有层的加权贡献，实现协调的特征学习。

Result: 在MNIST和Fashion-MNIST数据集上的评估显示，协作式前向-前向算法相比基线前向-前向实现有显著性能提升。

Conclusion: 层间协作是前向-前向学习的重要改进方向，具有在神经形态计算架构和能源受限AI系统中的直接应用价值。

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [58] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: 本文提出了贝叶斯优化的新变体，针对具有解耦黑盒约束的问题，改进了知识梯度采集函数，能够智能选择评估相关约束而非全部约束。


<details>
  <summary>Details</summary>
Motivation: 在昂贵的全局黑盒优化问题中，许多约束在最优解处可能只有少数是起作用的，但现有方法通常需要评估所有约束，这造成了不必要的计算成本。因此需要开发能够智能选择评估相关约束的优化方法。

Method: 提出了贝叶斯优化的新变体，基于知识梯度采集函数，专门针对解耦黑盒约束问题设计。该方法能够识别哪些约束可能在最优解处起作用，从而只评估相关约束而非全部约束。

Result: 通过实证基准测试，这些新方法在性能上超越了现有最先进的方法，证明了其在处理解耦约束优化问题上的优越性。

Conclusion: 提出的贝叶斯优化变体能够有效处理解耦黑盒约束问题，通过智能选择评估相关约束，显著提高了优化效率，为昂贵黑盒优化问题提供了更有效的解决方案。

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [59] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: 本文提出了一种基于Wasserstein-Fisher-Rao几何的采样方法，通过引入显式修正项和加权随机微分方程来改进传统扩散模型在非凸/多模态分布上的采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于分数的扩散模型在处理非凸或多模态分布时，混合速率会指数级恶化。虽然这些方法在强对数凹分布上有指数收敛保证，但实际生成建模任务常涉及高度非对数凹的目标分布，因此需要开发能改善探索能力的采样方案。

Method: 利用信息几何工具，通过Wasserstein-Fisher-Rao几何增强基于扩散的采样器，引入受控的质量重加权机制。通过显式修正项实现重加权，并使用Feynman-Kac表示通过加权随机微分方程实现。

Result: 提出了基于WFR的采样动力学框架，为未来的理论和算法发展奠定了基础。该方法旨在改善非凸/多模态分布下的采样效率。

Conclusion: Wasserstein-Fisher-Rao几何为增强扩散采样器提供了有前景的途径，通过耦合样本空间中的传输与概率测度空间上的垂直（反应）动力学，可以改善非凸分布下的探索能力。本研究为WFR-based采样动力学提供了初步但严格的研究。

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [60] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: SRPL框架在真实世界自动驾驶场景中验证有效，能改善奖励-安全权衡，提升成功率并降低成本，但对策略优化器和数据集分布敏感，且在跨数据集泛化中表现更好。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习在自动驾驶中面临性能优化与安全要求之间的根本矛盾，SRPL框架通过预测未来约束违规模型来解决这一挑战，但此前主要在受控环境中验证，需要研究其在真实世界自动驾驶场景中的有效性。

Method: 使用Waymo Open Motion Dataset和NuPlan数据集进行系统实验，评估SRPL框架在真实世界自动驾驶场景中的表现，包括奖励-安全权衡、成功率、成本降低等指标，并进行零样本跨数据集评估验证泛化能力。

Result: SRPL能显著改善奖励-安全权衡，成功率提升效果量r=0.65-0.86，成本降低效果量r=0.70-0.83（p<0.05），但其有效性依赖于底层策略优化器和数据集分布；预测安全表示能提升对观测噪声的鲁棒性，且在跨数据集评估中SRPL增强的智能体表现出更好的泛化能力。

Conclusion: 预测安全表示在自动驾驶安全强化学习中具有潜力，能增强智能体的安全性和泛化能力，但实际应用效果受多种因素影响，需要根据具体场景和优化器进行适配。

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [61] [Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models](https://arxiv.org/abs/2512.17592)
*Arthur Guijt,Dirk Thierens,Ellen Kerkhof,Jan Wiersma,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: 该研究探讨了在数据分散且无法共享的医疗领域，如何通过异步协作和模型缝合技术来提升深度学习性能，相比联邦学习更灵活。


<details>
  <summary>Details</summary>
Motivation: 在医疗等数据分散且隐私敏感的领域，传统联邦学习需要同步训练和权重交换，存在实施难度。研究探索仅共享已训练模型的异步协作方式是否可行。

Method: 采用多目标视角，将各方数据性能独立评估。提出使用缝合层技术，在独立训练模型的中间表示层之间添加连接层来合并模型，而非简单的模型集成。

Result: 仅使用单方数据训练时，在自身数据上性能相似，但在其他方数据上表现较差。模型集成能提升泛化但牺牲各方自身数据性能。缝合层技术能在保持泛化优势的同时，恢复各方自身数据的竞争性性能。

Conclusion: 异步协作通过模型缝合技术可以实现竞争性结果，为数据分散且无法共享的领域提供了比联邦学习更灵活的解决方案。

Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.

</details>


### [62] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 该论文提出了一种系统分析算法在噪声、扰动和与其他动态系统互连环境下收敛性的统一框架，通过Lyapunov理论量化扰动影响，并应用于分布式学习、机器学习泛化、隐私保护等多个领域。


<details>
  <summary>Details</summary>
Motivation: 随着算法在复杂物理、社会和工程系统中的广泛应用，它们经常暴露于扰动、噪声以及与其他动态系统的互连环境中。传统算法分析通常假设算法在孤立环境下运行，缺乏对实际环境中扰动影响的系统性分析框架。

Method: 利用逆Lyapunov定理，推导出量化扰动影响的关键不等式，将孤立环境下算法的收敛保证扩展到存在扰动的场景，建立稳定性边界和收敛速率分析的理论框架。

Result: 建立了算法在扰动环境下收敛性的统一分析工具，能够量化扰动对算法性能的影响，并在分布式学习的通信约束、机器学习泛化的敏感性、隐私保护的噪声注入等多个应用场景中验证了该框架的有效性。

Conclusion: 该研究为分析算法在噪声、扰动和系统互连环境下的性能提供了统一的数学工具，填补了传统算法分析与实际应用环境之间的理论空白，具有广泛的适用性和实用价值。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [63] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文提出了一种名为"多谐级联"的深度机器学习架构，通过多谐样条包序列实现任意复杂度非线性函数的逼近，同时保持全局光滑性和概率解释，并提出了基于全局线性系统的替代梯度下降的训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法通常基于梯度下降优化，可能面临收敛问题、过拟合风险，且难以保持理论一致性和概率解释。本文旨在开发一种既能逼近复杂非线性函数，又能保持全局光滑性和概率解释的深度学习架构。

Method: 提出"多谐级联"架构：由多谐样条包序列组成，每层严格从随机函数理论和无差别原理推导而来。训练方法采用替代梯度下降的全局线性系统求解：在每个批次上，针对固定"节点星座"处的函数值求解单个全局线性系统，实现所有层的同步更新。

Result: 该方法具有以下优势：1) 保持各层的概率解释和与原始模型的理论一致性；2) 计算可简化为高效的2D矩阵操作，适合GPU加速；3) 在MNIST数据集上展示了快速学习且不过拟合的性能。

Conclusion: 多谐级联架构提供了一种理论严谨的深度学习替代方案，既能逼近复杂非线性函数，又能保持全局光滑性和概率解释。其基于全局线性系统的训练方法避免了传统梯度下降的局限性，在计算效率和防止过拟合方面表现出优势。

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [64] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: 本文首次为联邦SARSA算法（FedSARSA）在异构环境下的收敛性提供了理论分析，建立了样本和通信复杂度边界，并证明了FedSARSA能够实现相对于智能体数量的线性加速。


<details>
  <summary>Details</summary>
Motivation: 研究联邦强化学习中的SARSA算法在异构环境下的收敛性能，填补该领域理论分析的空白，量化异构性对算法性能的影响。

Method: 提出了FedSARSA算法，结合线性函数逼近和本地训练；开发了新的多步误差展开分析技术；建立了在局部转移和奖励都存在异构性情况下的收敛保证。

Result: 首次获得了FedSARSA在异构环境下的样本和通信复杂度边界；证明了算法能够实现相对于智能体数量的线性加速（除马尔可夫采样带来的高阶项外）；数值实验验证了理论发现。

Conclusion: FedSARSA在异构联邦强化学习环境中具有理论保证的收敛性能，能够有效利用多个智能体实现加速学习，为联邦强化学习的实际应用提供了理论基础。

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>


### [65] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: LaLoRA：一种基于拉普拉斯近化的权重空间正则化方法，通过约束高曲率方向的参数更新来缓解LoRA微调中的灾难性遗忘问题，在保持轻量化的同时改善学习-遗忘权衡。


<details>
  <summary>Details</summary>
Motivation: LoRA等参数高效微调方法虽然能快速将大模型适配到下游任务，但会导致模型遗忘原有的领域知识（灾难性遗忘）。需要一种方法在保持轻量化的同时保护模型先验知识。

Method: LaLoRA将拉普拉斯近似应用于LoRA权重，通过估计模型对每个参数的置信度，约束高曲率方向的参数更新。仅对LoRA权重应用拉普拉斯近似以保持轻量化，通过正则化强度直接控制学习-遗忘权衡。

Result: 在Llama模型数学推理微调实验中，LaLoRA改善了学习-遗忘权衡，正则化强度可直接控制该权衡。进一步探索了不同损失景观曲率近似方法、拉普拉斯近似所用数据的影响，并分析了超参数鲁棒性。

Conclusion: LaLoRA通过拉普拉斯近似正则化LoRA权重，有效缓解了参数高效微调中的灾难性遗忘问题，在保持轻量化的同时实现了更好的学习-遗忘权衡控制。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [66] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出可校准消歧损失（CDL），解决多示例部分标签学习中的校准问题，提升分类准确性和校准性能


<details>
  <summary>Details</summary>
Motivation: 现有MIPL方法存在校准性能差的问题，影响分类器可靠性，需要同时提升分类准确性和校准性能

Method: 提出可校准消歧损失（CDL），包含两种实现：基于候选标签集概率的校准，以及整合候选和非候选标签集概率的校准

Result: CDL能无缝集成到现有MIPL和PLL框架中，理论分析证明其下界和正则化特性优于传统消歧损失

Conclusion: 在基准和真实数据集上的实验表明，CDL显著提升了分类和校准性能

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [67] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: 本文研究发现ID嵌入和模态特征在序列推荐中具有互补性，提出了一种简单的集成方法，无需复杂融合架构即可超越现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前序列推荐模型对ID嵌入和模态特征的互补性缺乏深入理解。一些工作完全用模态特征替代ID嵌入，另一些则采用复杂的多阶段训练或对齐架构来联合使用两者。本文旨在填补这一理解空白。

Method: 首先研究ID和基于文本的序列推荐模型的互补性，然后提出新方法：通过独立训练保持ID-文本互补性，再通过简单的集成策略利用这种互补性。

Result: 研究表明ID和文本模型确实学习到了互补信号。提出的简单集成方法在性能上超越了多个竞争性序列推荐基线模型。

Conclusion: 要实现最先进的序列推荐性能，ID和文本特征都是必要的，但不需要复杂的融合架构。简单的集成策略就能有效利用两者的互补性。

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>


### [68] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 提出一种基于正则化随机傅里叶特征(RRFF)和有限元重建映射(RRFF-FEM)的算子学习方法，用于处理含噪声数据，通过学生t分布随机特征和频率加权Tikhonov正则化抑制高频噪声，在保持精度的同时减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 核基算子学习方法虽然能提供精确的理论保证且需要较少训练数据，但在大规模训练集上计算成本过高，且对噪声敏感。需要一种既能处理噪声又能保持计算效率的方法。

Method: 提出RRFF方法：使用多元学生t分布生成随机特征，结合频率加权Tikhonov正则化抑制高频噪声。当特征数N与训练样本数m满足N∝m log m时，系统保持良好条件数。还提出RRFF-FEM方法，结合有限元重建映射。

Result: 建立了随机特征矩阵极端奇异值的高概率界限，证明了系统良好条件性。在多个PDE基准问题（对流、Burgers、达西流、Helmholtz、Navier-Stokes、结构力学）上的实验表明，RRFF和RRFF-FEM对噪声鲁棒，相比无正则化随机特征模型性能提升且训练时间减少，与核方法和神经算子方法相比保持竞争力。

Conclusion: RRFF和RRFF-FEM方法为从含噪声数据中学习算子提供了一种计算高效且鲁棒的框架，在保持精度的同时显著减少训练时间，特别适用于大规模PDE问题。

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [69] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑框架的实体集扩展图方法，通过局部推理任务实现高效导航，避免完全构建大型扩展图。


<details>
  <summary>Details</summary>
Motivation: 传统的线性实体集扩展方法无法揭示知识资源中丰富的分类结构，而完全构建扩展图在实际场景中可能不切实际。

Method: 采用基于逻辑的扩展图框架，定义推理任务来检查两个元组在图中是否属于可比较、不可比较或相同的节点，通过限制输入或实体描述实现高效实现。

Result: 在现实假设下（如限制输入或实体描述），这些推理任务可以高效实现，支持局部增量导航扩展图，无需完全构建整个图。

Conclusion: 提出的方法支持扩展图的局部增量导航，使基于知识库的分类实体集扩展在实际应用中变得可行，无需完全构建大型图结构。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [70] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能(SGI)的操作定义，基于实践探究模型(PIM)，并创建了包含1000多个跨学科样本的SGI-Bench基准，用于评估大语言模型在科学研究任务中的表现，同时提出了测试时强化学习(TTRL)方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI有所进展，但缺乏一个统一的科学通用智能(SGI)框架，即能够自主构思、调查和跨科学领域推理的能力。需要建立操作化的SGI定义和系统性评估基准。

Method: 1. 基于实践探究模型(PIM: 审议、构思、行动、感知)定义SGI操作框架；2. 创建SGI-Bench基准，包含1000多个专家策划的跨学科样本，源自《科学》杂志的125个重大问题；3. 提出测试时强化学习(TTRL)，在推理时优化检索增强的新颖性奖励；4. 评估现有大语言模型在四个科学家对齐任务上的表现：深度研究、想法生成、干/湿实验、实验推理。

Result: 评估结果显示多个差距：深度研究任务中精确匹配率低(10-20%)，尽管步骤级对齐较好；生成的想法缺乏可行性和细节；干实验中代码可执行性高但执行结果准确性低；湿实验协议序列保真度低；多模态比较推理存在持续挑战。TTRL方法成功提升了假设新颖性，无需参考答案。

Conclusion: 基于PIM的SGI定义、以工作流程为中心的基准和实证洞察，为真正参与科学发现的AI系统奠定了基础。研究揭示了当前AI在科学任务中的局限性，并提出了改进方向。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [71] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个面向LLM智能体的计划感知自动上下文工程框架，通过多任务相关性建模、计划结构分析、指令协同优化和函数保持压缩来优化智能体状态，显著减少上下文负载并提高任务准确性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在复杂多步骤工作流中的部署，上下文迅速膨胀，需要有效的压缩方法。现有工作大多忽略了智能体推理的多步骤和计划感知特性，导致上下文管理效率低下。

Method: PAACE框架包含两个核心组件：PAACE-Syn（大规模合成智能体工作流生成器，带有逐步压缩监督标注）和PAACE-FT（从成功教师演示中训练的蒸馏计划感知压缩器家族）。采用next-k-task相关性建模、计划结构分析、指令协同优化和函数保持压缩等技术。

Result: 在AppWorld、OfficeBench和8-Objective QA等长视野基准测试中，PAACE在提高智能体正确性的同时显著降低上下文负载。在AppWorld上达到比所有基线更高的准确性，同时降低峰值上下文和累积依赖。蒸馏后的PAACE-FT保留了教师模型97%的性能，同时将推理成本降低了一个数量级以上。

Conclusion: PAACE为LLM智能体提供了一种有效的计划感知上下文压缩框架，能够在保持性能的同时大幅降低计算成本，实现了计划感知压缩在实际部署中的可行性。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [72] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 本文提出关系中心知识图谱问答新范式，通过子图选择、多阶段图剪枝和强化学习微调LLM，解决传统实体中心问答的局限性，生成捕捉实体间语义连接的紧凑信息子图。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱问答主要关注返回单个答案实体的实体中心查询，但现实世界查询往往是关系性的，需要理解实体之间的关联。现有方法无法有效处理这种关系中心查询，因为候选子图数量庞大，且平凡或过于常见的连接会掩盖独特且信息丰富的答案。

Method: 提出UniRel-R1统一框架，包含三个核心组件：1）子图选择模块；2）多阶段图剪枝策略；3）使用强化学习微调的大型语言模型。奖励函数设计鼓励生成紧凑且特定的子图，包含更多信息性关系和较低度的中间实体。

Result: 大量实验表明，UniRel-R1在连接性和奖励分数上相比Vanilla基线取得显著提升，并且能够有效泛化到未见过的实体和关系，证明了方法的有效性和泛化能力。

Conclusion: 关系中心知识图谱问答是传统实体中心问答的重要补充，UniRel-R1框架通过集成子图选择、多阶段剪枝和强化学习微调，能够有效识别独特且信息丰富的语义连接，为理解实体间关联提供了新的解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [73] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用大语言模型驱动的虚拟社会模拟研究现实威胁与象征威胁对人类冲突的影响，发现现实威胁直接增加敌意，而象征威胁影响较弱且完全通过内群体偏见中介，仅在现实威胁缺失时增加敌意。


<details>
  <summary>Details</summary>
Motivation: 人类冲突通常归因于物质条件和象征价值受到的威胁，但两者如何相互作用以及哪个占主导地位尚不清楚。研究进展受到因果控制薄弱、伦理约束和时间数据稀缺的限制。

Method: 使用大语言模型驱动的虚拟社会模拟，独立操纵现实威胁和象征威胁，跟踪行动、语言和态度。通过表征分析检验底层LLM如何编码不同威胁状态，并验证操纵如何映射到这些状态。

Result: 底层LLM将现实威胁、象征威胁和敌意编码为不同的内部状态；现实威胁直接增加敌意，而象征威胁效应较弱，完全通过内群体偏见中介，且仅在现实威胁缺失时增加敌意；非敌对群体接触缓冲冲突升级，结构不对称使敌意集中在多数群体中。

Conclusion: 虚拟社会模拟为威胁驱动的冲突提供了因果解释：现实威胁是冲突的主要驱动因素，象征威胁通过内群体偏见间接作用，且两者存在交互效应。非敌对接触和结构因素在冲突动态中起重要作用。

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [74] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 将AIXI强化学习智能体扩展到更广泛的效用函数类别，处理信念分布中某些假设只预测有限历史前缀的问题，探讨死亡解释与不精确概率两种视角，并研究Choquet积分的可计算性。


<details>
  <summary>Details</summary>
Motivation: AIXI智能体需要处理信念分布中某些假设只能预测有限历史前缀的情况，这通常被解释为"死亡概率"（半测度损失）。作者希望探索更自然的解释——将信念分布视为不精确概率分布，并研究相应的效用计算方法。

Method: 1) 将AIXI智能体推广到更广泛的效用函数类别；2) 分析"死亡解释"视角下的效用分配方法；3) 提出将信念分布视为不精确概率分布的替代视角；4) 使用不精确概率理论中的Choquet积分计算期望效用；5) 研究这些方法的可计算性水平。

Result: 1) 标准递归值函数可以作为Choquet积分的特例恢复；2) 在死亡解释下最一般的期望效用不能表征为Choquet积分；3) 建立了AIXI扩展与不精确概率理论之间的联系。

Conclusion: 通过将AIXI智能体扩展到更广泛的效用函数并引入不精确概率理论，为处理信念分布中的有限预测问题提供了新的理论框架，但死亡解释下的最一般期望效用与Choquet积分方法存在本质差异。

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [75] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 本文提出了一种ASP求解器在循环中的方法，通过求解器引导的指令调优来改进LLMs生成答案集编程代码的能力，仅需自然语言问题描述和解决方案即可训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编程语言上表现良好，但在领域特定语言（如答案集编程ASP）的代码生成方面仍面临挑战。ASP是一种解决组合搜索问题的有效方法，但LLMs在ASP代码生成上的效果受到预训练阶段示例数量有限的限制。

Method: 提出ASP求解器在循环中的方法，通过求解器引导的指令调优来训练LLMs。方法仅需自然语言问题描述和解决方案：1) 从LLMs采样ASP语句作为程序延续；2) 利用ASP声明式编程的特性（部分编码逐步缩小解空间），基于求解器反馈将样本分为选择和拒绝实例；3) 对整理的数据进行监督微调；4) 使用包含最佳N采样的求解器引导搜索进一步提高鲁棒性。

Result: 实验表明，该方法在两种不同的提示设置和两个数据集上都取得了持续改进。

Conclusion: 提出的ASP求解器在循环中的方法有效解决了LLMs在ASP代码生成中的语义解析挑战，通过求解器引导的指令调优显著提升了模型性能。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [76] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出基于Solomonoff启发的LLM假设加权方法，在不确定性下平衡准确性和简洁性，应用于Mini-ARC基准任务


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏数据的现实世界任务时，难以在评估多个候选解决方案时平衡准确性和简洁性，需要系统性的泛化能力

Method: 提出Solomonoff启发的方法，根据简洁性和预测拟合度对LLM生成的假设进行加权，应用于Mini-ARC基准任务，为每个单元预测生成Solomonoff加权混合

Result: 相比贝叶斯模型平均（BMA），Solomonoff评分将概率更均匀地分布在竞争假设之间，而BMA将权重集中在最可能但可能有缺陷的候选上；该方法即使在假设有噪声或部分错误时也能产生保守、不确定性感知的输出

Conclusion: 算法信息论先验对于可解释、可靠的多假设不确定性推理具有重要价值

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [77] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的可解释多模态检索增强生成方法，通过两阶段强化微调框架提升多模态大语言模型的推理能力，在WebQA和MultimodalQA数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有MMRAG方法缺乏对检索和响应生成背后推理逻辑的清晰解释，限制了结果的可解释性。为了解决这一缺陷，作者提出引入强化学习来增强多模态检索增强生成的推理能力。

Method: 采用两阶段强化微调框架：第一阶段使用基于规则的强化微调对多模态文档进行粗粒度点式排序，过滤显著不相关文档；第二阶段使用基于推理的强化微调联合优化细粒度列表式排序和答案生成，引导模型输出可解释的推理逻辑。

Result: 在WebQA和MultimodalQA两个多模态检索增强生成基准数据集上取得了最先进的结果，并通过全面的消融实验验证了方法的有效性。

Conclusion: 通过引入强化学习到多模态检索增强生成中，提出的两阶段强化微调框架成功提升了多模态大语言模型的推理能力，实现了可解释的多模态检索增强生成，为复杂多模态场景提供了更可信的生成结果。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [78] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: UmniBench是一个针对统一多模态模型（UMMs）的全维度评估基准，能够在单一评估过程中同时评估理解、生成和编辑能力，覆盖13个主要领域和200多个概念。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型的评估是分离的，分别用不同的数据集评估理解和生成能力。缺乏一个能够全面评估UMMs所有能力的综合基准。

Method: UmniBench利用UMM自身通过其理解能力来评估生成和编辑能力。基于人工检查的提示和问答对，采用简单但有效的评估范式。基准覆盖13个主要领域和200多个概念，并能解耦单独评估各项能力。

Result: 基于UmniBench对24个流行模型进行了基准测试，包括UMMs和单能力大模型。该基准为统一模型提供了更全面客观的评估视角。

Conclusion: UmniBench为统一多模态模型提供了全面的全维度评估基准，能够支持社区模型性能改进，为UMMs评估提供了更全面的视角。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [79] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: EventGPT：基于GPT架构的球员条件化价值感知下一事件预测模型，用于足球转会分析，通过反事实模拟评估球员在不同战术环境中的表现


<details>
  <summary>Details</summary>
Motivation: 现有转会评估方法依赖静态统计数据或事后价值模型，无法捕捉球员在新战术环境或不同队友配合下的适应性表现，需要一种能预测球员在不同情境下表现的方法

Method: 基于GPT风格自回归变换器构建球员条件化、价值感知的下一事件预测模型，将比赛处理为离散标记序列，联合预测下一持球动作的类型、位置、时间和残差持球价值(rOBV)，通过替换球员嵌入进行反事实模拟

Result: 在英超五个赛季的事件数据评估中，EventGPT在下一事件预测准确性和空间精度上优于现有序列基线，通过案例研究展示了在转会分析中的实际效用

Conclusion: EventGPT提供了一个原则性方法来评估转会适配性，能够模拟球员在不同球队或战术结构中的行为分布和价值变化，为转会决策提供更科学的依据

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [80] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 论文提出从算法信息论角度定义"概念"，将其视为与智能体整体经验结构相关的信息对象，通过可逆一致性关系和冗余信息度量来形式化概念发现与演化，并建立多智能体概念对齐的通信框架。


<details>
  <summary>Details</summary>
Motivation: 人类概念本身具有流动性（如冥王星不再被视为行星），需要超越字典标签的概念定义，建立可修订、可比较、可跨智能体对齐的概念结构形式化框架。

Method: 采用算法信息论视角，将概念定义为通过可逆一致性关系与智能体经验结构相关的信息对象；定义冗余信息度量分解的自然性；建立辩证法优化动态，让竞争概念通过更短条件描述解释新信息；形式化低成本概念传输和多智能体对齐机制。

Result: 提出了基于信息结构的概念形式化框架，使概念存在成为可验证的结构性主张；建立了概念演化（扩展、收缩、分裂、合并）的优化动态；实现了通过小规模基础/种子在多智能体间重建相同概念的通信机制。

Conclusion: 从算法信息论角度为概念发现和演化提供了严格的形式化基础，使AI能够从原始经验中自主发现和修订概念，并支持多智能体间的概念对齐和高效通信。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [81] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 该研究将Rashomon效应从分类任务扩展到序列决策领域，证明了在强化学习中存在多个内部结构不同但行为完全相同的策略，并展示了这种多样性集合在分布偏移下的鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但尚未在序列决策（强化学习）中得到探索。研究者希望验证在序列决策中是否存在类似现象，即多个策略在相同环境下表现出完全相同的行为，但内部特征依赖不同。

Method: 使用形式化验证方法构建和比较每个策略在环境中的完整概率行为。通过对比策略的状态访问和动作选择来验证行为一致性，同时分析内部特征归因的差异。

Result: 实验证明Rashomon效应确实存在于序列决策中。从Rashomon集合构建的集成策略比单个策略对分布偏移具有更强的鲁棒性。此外，从该集合推导的宽松策略在保持最优性能的同时减少了验证的计算需求。

Conclusion: Rashomon效应在序列决策中普遍存在，这种多样性可以被利用来构建更鲁棒的集成策略，并为策略验证提供计算效率更高的方法，对强化学习的理论和实践都有重要意义。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [82] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（GPT-4o）的医疗诊断聊天机器人，结合检索增强生成和可解释AI技术，通过动态对话提取症状并提供透明诊断推理，在准确性上优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临诊断效率低下、成本上升和专家资源有限等问题，导致治疗延误和不良健康结果。现有AI诊断系统缺乏交互性和透明度，难以在实际临床环境中有效应用。

Method: 开发基于大语言模型（GPT-4o）的诊断聊天机器人，采用检索增强生成技术和可解释AI方法。系统通过动态对话提取和规范化症状，使用相似性匹配和自适应提问优先诊断，并通过思维链提示提供透明推理。

Result: 与朴素贝叶斯、逻辑回归、SVM、随机森林和KNN等传统机器学习模型相比，LLM系统表现出色，达到90%的准确率和100%的Top-3准确率。

Conclusion: 该研究展示了基于大语言模型的诊断系统在医疗领域的潜力，为实现更透明、交互性强且临床相关的AI医疗应用提供了有前景的方向。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [83] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 本文提出了定时奖励机（TRMs），这是对传统奖励机的扩展，通过加入时间约束来增强强化学习中奖励规范的表达能力，并开发了相应的模型无关RL算法来学习满足时间约束的最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了其在时间敏感应用中的使用。为了在强化学习中表达包含时间约束的非马尔可夫奖励，需要扩展奖励规范的形式化方法。

Method: 提出定时奖励机（TRMs），将时间约束整合到奖励结构中；开发了基于表格Q学习的模型无关RL框架，通过定时自动机的抽象将TRM整合到学习中，并采用利用TRM结构的反事实想象启发式方法来改进搜索。

Result: 实验表明，该算法能够在流行的RL基准测试中学习到实现高奖励且满足TRM指定时间约束的策略；比较研究显示了不同TRM语义下的性能表现，并验证了反事实想象启发式方法的有效性。

Conclusion: 定时奖励机为强化学习提供了更丰富的奖励规范表达能力，能够处理时间敏感任务，相关算法能够有效学习满足时间约束的最优策略。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [84] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 研究通过跨国实验发现，AI拟人化设计对用户信任和参与度的影响并非普遍一致，而是受到文化因素的调节，挑战了现有AI治理的一刀切模式。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统日益模仿人类特征，引发了关于拟人化可能导致错误信任或情感依赖的担忧。然而，现有安全框架主要基于西方人群的理论假设，缺乏对全球用户多样性的考虑，且拟人化设计与用户行为之间的因果关系尚未在真实人机交互中得到验证。

Method: 采用两个大规模跨国实验，涉及10个不同国家的3500名参与者，进行实时开放式的人机交互。通过实验方法测试拟人化设计杠杆对用户拟人化感知的因果影响，并考察文化因素对行为结果的调节作用。

Result: 用户评估AI拟人化程度时更关注交互性线索（如对话流畅度、理解用户视角）而非理论特征（如感知或意识）。拟人化设计能因果性地增加用户的拟人化感知，但并未普遍提高用户参与度和信任的行为测量。文化因素调节了拟人化与行为结果之间的关系：某些设计选择在特定文化（如巴西）中增强信任，在其他文化（如日本）中可能产生相反效果。

Conclusion: 研究挑战了拟人化AI设计必然带来风险的普遍观点，揭示了人机交互的复杂文化调节机制。结果表明AI治理需要超越一刀切的模式，考虑文化多样性对设计效果的影响。

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [85] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 论文提出"推理定律(LoRe)"框架，通过计算定律和准确率定律形式化大型推理模型的理想推理行为，并开发LoRe-Bench基准测试单调性和组合性，发现现有模型缺乏组合性，通过微调方法改进后能提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型(LRMs)性能优越，但其推理行为常常违反直觉，导致推理能力不足。需要理论框架来形式化理想的推理行为，以理解和改进LRMs的推理模式。

Method: 1) 提出推理定律(LoRe)框架，包括计算定律(推理计算量应与问题复杂度线性相关)和准确率定律；2) 通过单调性和组合性两个可衡量属性检验定律；3) 开发LoRe-Bench基准系统评估这些属性；4) 设计微调方法强制计算定律的组合性。

Result: 评估显示大多数推理模型具有合理的单调性但缺乏组合性。通过微调强制计算定律组合性后，模型在多个基准测试上的推理性能得到一致提升，并揭示了属性和定律之间的协同效应。

Conclusion: 推理定律(LoRe)为理解和改进大型推理模型的推理行为提供了统一框架。更好的定律遵从性带来推理性能的提升，表明形式化推理定律是改进LRMs推理能力的有效途径。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>
