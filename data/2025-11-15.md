<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文介绍了编码器增强的因果解码器模型架构，该架构在训练效率上优于因果变换器，并能实现更高的压缩率。研究表明，通过估计每个token的熵来训练模型，使其接近但不超出估计的熵值，可以显著提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语言预测受到语言内在信息熵的限制，当前最有效的语言压缩算法是因果大语言模型，但使用这些模型准确估计语言熵在计算上不可行。本文旨在开发更高效的模型架构来估计语言熵并提高泛化能力。

Method: 引入编码器增强的因果解码器模型架构，该架构在训练效率上优于因果变换器。通过估计每个token的熵，训练模型使其接近但不超出估计的熵值。

Result: 实验证明，编码器增强的因果解码器模型在压缩率上优于因果变换器。当模型训练到接近但不超出估计的每个token熵值时，表现出比不考虑熵的模型更好的泛化能力。

Conclusion: 通过考虑语言的信息熵限制来训练模型，可以显著提高模型的泛化能力。编码器增强的因果解码器架构为实现这一目标提供了有效的解决方案。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [2] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: SSR是一个新颖的LLM推理框架，通过将模型响应分解为可验证的子问题-子答案对，实现细粒度评估和精确优化，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗略的自验证和自校正，限制了在复杂任务上的有效性，需要更精细的推理评估和优化方法。

Method: 将模型响应分解为可验证的子问题-子答案对，通过受控重解和自一致性检查进行步骤级置信度估计，精确定位不可靠步骤并迭代优化。

Result: 在五个推理基准测试和三个LLM上的实证结果显示，SSR始终优于最先进的迭代自优化基线方法。

Conclusion: SSR不仅提升了性能，还提供了一种原则性的黑盒方法来评估和理解LLM的内部推理过程。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [3] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella是一个完全开源的30亿参数语言模型家族，使用公开可用数据和代码库训练，在AMD MI300X GPU上开发，包含预训练、指令调优和对齐。尽管预训练token较少，但在完全开源模型中达到SOTA，并发布了支持128K上下文长度的Instella-Long和专注于数学推理的Instella-Math两个变体。


<details>
  <summary>Details</summary>
Motivation: 当前高性能语言模型多为闭源或部分开源，限制了透明度和可复现性，需要开发完全开源且性能优越的替代方案。

Method: 使用AMD Instinct MI300X GPU进行大规模预训练、通用指令调优和人类偏好对齐，并开发了支持长上下文和数学推理的专门变体。

Result: Instella在完全开源模型中达到最先进水平，与同规模领先开源权重模型竞争，并提供了128K上下文长度和数学推理增强功能。

Conclusion: Instella为社区提供了透明、高性能且多功能的替代方案，推动了开放和可复现的语言建模研究。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [4] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant是一种仅对权重进行后训练量化的方法，通过成对旋转量化和通道级缩放来抑制异常值，减小量化误差，在推理任务上比AWQ平均提升2.4%准确率，且开销低于10%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型权重和激活中的异常值会导致量化误差和精度下降，特别是在推理任务中误差会在长思维链中累积。现有方法要么无法充分抑制异常值，要么在推理时引入显著开销。

Method: 结合硬件高效的独立Givens旋转和通道级缩放，均衡通道间幅度并缩小每个量化组内的动态范围，同时协同设计推理内核以充分利用GPU并行性，保持旋转和缩放操作在运行时轻量。

Result: 在推理任务上比AWQ平均提升2.4%准确率，且开销低于10%。

Conclusion: 为更高效和准确部署推理大语言模型铺平了道路。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 本文提出了两个参数化的多臂老虎机算法家族，利用离线数据学习近最优算法，在满足特定凹性条件时获得更强的性能保证，同时保证在良好实例上的最优臂识别和恶劣实例上的最坏情况保障。


<details>
  <summary>Details</summary>
Motivation: 改进型多臂老虎机问题在不确定性下分配努力的正式模型，应用于投资研究、临床试验和超参数选择等场景。现有算法存在悲观的最坏情况保证，需要设计能获得更强数据依赖保证的算法。

Method: 提出了两个参数化算法家族：第一个包含先前工作的最优随机算法，在臂奖励曲线满足额外凹性性质时可获得更强的性能保证；第二个家族包含既能保证在良好实例上识别最优臂，又能在恶劣实例上回归最坏情况保证的算法。

Result: 通过离线数据学习每个家族的近最优算法，获得了更强的数据依赖保证，无需实际验证假设是否满足，在特定条件下实现了关于臂数k的最优依赖。

Conclusion: 采用统计学习视角处理老虎机奖励优化问题，实现了更强的数据依赖性能保证，为改进型多臂老虎机问题提供了更实用的解决方案。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 本文提出了一种验证模拟中发现的自动驾驶车辆故障场景在真实世界中是否可重现的方法，通过定义场景匹配的正式规范并开发高效的查询算法，在真实数据集中定位匹配的模拟场景。


<details>
  <summary>Details</summary>
Motivation: 解决仿真测试中的sim-to-real差距问题，确保在仿真环境中发现的故障场景能够在真实系统中重现，避免将仿真数据的人为产物误认为真实问题。

Method: 引入基于Scenic概率编程语言的场景程序来表示抽象场景，开发查询算法在标记的时间序列传感器数据中匹配特定场景，识别符合场景定义的数据子集。

Result: 实验表明，该算法在查询场景时比最先进的商业视觉大语言模型更准确且快几个数量级，能够随着查询时间序列数据长度的增加而扩展。

Conclusion: 该方法为验证仿真发现的故障场景在真实世界中的可重现性提供了有效工具，有助于弥合仿真与真实环境之间的差距。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
