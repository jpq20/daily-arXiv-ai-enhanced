<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 38]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale](https://arxiv.org/abs/2512.05179)
*Aurélie Montfrond*

Main category: cs.CL

TL;DR: 该研究针对利默里克大学电子与计算机工程系开发了一个课程信息问答聊天机器人，通过构建1203个SQuAD格式的问答对数据集，对BERT模型进行微调，展示了将基础模型适配到教育领域的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有科学问答研究主要关注聊天机器人系统，缺乏针对特定领域推理的基础模型微调探索。目前虽然有BioBERT和SciBERT等针对生物医学和科学文献的领域特定BERT变体，但还没有专门针对大学课程材料的基础模型。

Method: 1. 构建包含1203个问答对的SQuAD格式自定义数据集，数据来源于大学课程手册，包含手动和合成生成的条目；2. 使用PyTorch对BERT模型进行微调；3. 使用精确匹配（Exact Match）和F1分数评估性能。

Result: 结果显示，即使是适度的微调也能改善假设构建和知识提取效果，证明了将基础模型适配到教育领域的可行性。微调后的BERT模型在学术问答对上表现有效。

Conclusion: 该研究填补了大学课程材料领域特定基础模型的空白，展示了通过微调BERT模型可以创建有效的教育知识问答系统，为开发首个面向大学的领域特定问答模型奠定了基础，有望实现自主教育知识系统。

Abstract: Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.

</details>


### [2] [Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting](https://arxiv.org/abs/2512.05243)
*P. D. Edgar,Alia Hall*

Main category: cs.CL

TL;DR: 该研究提出将诗歌提示模式作为提示工程工具箱的有用补充，通过诗歌提示评估语言模型对著名诗人作品的描述和评价，并测试模型为假定受众改写原创作品的意愿。


<details>
  <summary>Details</summary>
Motivation: 提示工程已成为研究大型语言模型算法倾向和偏见的重要方法，而创意工作者和学者也利用LLMs开发创意作品。本研究旨在探索诗歌提示模式作为提示工程工具的潜力，并评估模型对诗歌作品的描述、评价以及改写意愿。

Method: 提出诗歌提示模式作为提示工程方法，然后使用诗歌提示来评估三个著名诗人模型的描述和评价，测试模型为假定受众改写原创创意作品的意愿和后果。

Result: 研究发现诗歌提示模式可以成为提示工程师工具箱的有用补充，通过诗歌提示能够有效评估语言模型对诗歌作品的描述和评价能力，并揭示模型为不同受众改写原创作品的倾向。

Conclusion: 创意文本提示特别是诗歌提示模式为提示工程提供了新的方法，有助于更深入地理解语言模型的创意能力和改写倾向，为评估模型对原创作品的尊重程度提供了有效工具。

Abstract: Prompt engineering has emerged as a useful way studying the algorithmic tendencies and biases of large language models. Meanwhile creatives and academics have leveraged LLMs to develop creative works and explore the boundaries of their writing capabilities through text generation and code. This study suggests that creative text prompting, specifically Poetry Prompt Patterns, may be a useful addition to the toolbox of the prompt engineer, and outlines the process by which this approach may be taken. Then, the paper uses poetic prompts to assess descriptions and evaluations of three models of a renowned poet and test the consequences of the willingness of models to adapt or rewrite original creative works for presumed audiences.

</details>


### [3] [Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4](https://arxiv.org/abs/2512.05256)
*Ivan Makohon,Mohamad Najafi,Jian Wu,Mathias Brochhausen,Yaohang Li*

Main category: cs.CL

TL;DR: 该研究探索使用思维链提示工程结合语义搜索和知识图谱来改进大型语言模型生成临床病历的质量，在六个临床案例上表现优于标准单样本提示。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据激增，医生手动撰写临床病历耗时且影响患者等待时间，需要自动化工具提高效率。大型语言模型具有生成类人文本的能力，但需要针对临床领域进行优化。

Method: 采用思维链提示工程，输入国际疾病分类代码和基本患者信息。结合传统思维链与语义搜索结果，并融入基于临床本体构建的知识图谱，以丰富领域专业知识。在CodiEsp测试数据集的六个临床案例上使用GPT-4进行测试。

Result: 提出的提示技术在生成临床病历方面优于标准单样本提示方法，证明结合思维链、语义搜索和知识图谱能有效提升大型语言模型在临床文本生成任务中的表现。

Conclusion: 思维链提示工程结合语义搜索和知识图谱能显著提高大型语言模型生成临床病历的质量，为自动化临床文档记录提供了有前景的技术路径。

Abstract: In the past decade a surge in the amount of electronic health record (EHR) data in the United States, attributed to a favorable policy environment created by the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 and the 21st Century Cures Act of 2016. Clinical notes for patients' assessments, diagnoses, and treatments are captured in these EHRs in free-form text by physicians, who spend a considerable amount of time entering and editing them. Manually writing clinical notes takes a considerable amount of a doctor's valuable time, increasing the patient's waiting time and possibly delaying diagnoses. Large language models (LLMs) possess the ability to generate news articles that closely resemble human-written ones. We investigate the usage of Chain-of-Thought (CoT) prompt engineering to improve the LLM's response in clinical note generation. In our prompts, we use as input International Classification of Diseases (ICD) codes and basic patient information. We investigate a strategy that combines the traditional CoT with semantic search results to improve the quality of generated clinical notes. Additionally, we infuse a knowledge graph (KG) built from clinical ontology to further enrich the domain-specific knowledge of generated clinical notes. We test our prompting technique on six clinical cases from the CodiEsp test dataset using GPT-4 and our results show that it outperformed the clinical notes generated by standard one-shot prompts.

</details>


### [4] [To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples](https://arxiv.org/abs/2512.05318)
*Vignesh Kothapalli,Ata Fatahibaarzi,Hamed Firooz,Maziar Sanjabi*

Main category: cs.CL

TL;DR: 本文提出CoT-Recipe方法，通过调节元训练中CoT和非CoT示例的比例，显著提升LLM在新颖任务上的推理能力，即使上下文没有CoT示例也能获得高达300%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT提示结合少样本上下文学习在LLM中解锁了显著推理能力，但当预训练知识不足时，CoT-ICL在新颖任务上效果不佳。作者在受控环境中研究这一问题，旨在开发元训练技术来学习新颖的抽象推理任务。

Method: 使用CoT-ICL Lab框架进行受控研究，提出CoT-Recipe方法——一种调节元训练序列中CoT和非CoT示例混合比例的形式化方法。该方法通过精心调制示例组合来优化模型性能。

Result: CoT-Recipe方法即使在没有CoT示例的上下文情况下，也能将transformer在新颖任务上的准确率提升高达300%。在预训练LLM（Qwen2.5系列）的符号推理任务中，准确率提升高达130%。

Conclusion: CoT示例虽然有助于推理，但在元训练中过度包含会降低性能。通过CoT-Recipe方法精心调节CoT和非CoT示例的比例，可以显著提升LLM在新颖任务上的推理能力，特别是在CoT监督有限的情况下。

Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.

</details>


### [5] [Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats](https://arxiv.org/abs/2512.05331)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee,Mostafa Musharrat,Sai Vishnu Vamsi*

Main category: cs.CL

TL;DR: 该论文研究粉红粘液新闻的检测方法，发现大型语言模型能显著降低现有检测系统性能，并提出抗对抗攻击的鲁棒学习框架


<details>
  <summary>Details</summary>
Motivation: 本地新闻作为2800万美国人的重要可靠信息来源，正面临粉红粘液新闻的威胁。这种低质量、自动生成的文章模仿合法本地报道，需要精细分析其语言、风格和词汇特征来检测。

Method: 对粉红粘液内容进行全面研究以发现其区分性模式，并提出基于这些洞察的检测策略。特别关注大型语言模型作为新的对抗向量，并设计专门抵抗LLM对抗攻击的鲁棒学习框架。

Result: 研究发现即使是消费者可访问的LLM也能显著削弱现有检测系统，使其F1分数性能下降高达40%。提出的鲁棒学习框架能抵抗LLM对抗攻击，性能提升高达27%。

Conclusion: 粉红粘液新闻检测面临LLM对抗攻击的新威胁，需要专门设计的鲁棒学习框架来应对自动化粉红粘液新闻的演变，保护本地新闻生态系统的完整性。

Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.

</details>


### [6] [Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change](https://arxiv.org/abs/2512.05364)
*Ananth Hariharan,David Mortensen*

Main category: cs.CL

TL;DR: 本研究展示混合神经符号方法如何为形态丰富、低资源语言的演化提供新见解，挑战语言变化即简化的朴素假设。通过分析2000多年梵语数据，开发弱监督混合方法，实现52.4%特征检测率，发现梵语形态复杂性未减少而是动态重新分布。


<details>
  <summary>Details</summary>
Motivation: 挑战语言变化即简化的朴素假设，为形态丰富、低资源语言（梵语）的演化提供新见解。传统方法难以处理数据稀缺问题，需要开发既能扩展又能解释的新方法。

Method: 采用弱监督混合神经符号方法：1) 使用100多个高精度正则表达式模式生成伪标签；2) 微调多语言BERT模型；3) 通过新颖的置信度加权集成融合符号和神经输出；4) 应用于147万词历时语料库。

Result: 集成系统实现52.4%总体特征检测率。发现梵语整体形态复杂性未减少，而是动态重新分布：早期动词特征呈周期性下降，复杂性转移到其他领域，复合词显著扩展，新哲学术语出现。系统产生良好校准的不确定性估计，置信度与准确度强相关（Pearson r=0.92），校准误差低（ECE=0.043）。

Conclusion: 混合神经符号方法能为低资源、形态丰富语言的演化研究提供可靠新见解，挑战传统简化假设。该方法具有可扩展性和可解释性，置信度校准良好，为计算文献学提供可靠工具。

Abstract: This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.

</details>


### [7] [Mitigating Self-Preference by Authorship Obfuscation](https://arxiv.org/abs/2512.05379)
*Taslim Mahbub,Shi Feng*

Main category: cs.CL

TL;DR: 通过文本扰动降低语言模型评委识别自身输出的能力，从而缓解自我偏好偏见，但完全消除该偏见仍具挑战性


<details>
  <summary>Details</summary>
Motivation: 语言模型评委在评估LM输出质量时存在自我偏好偏见，即倾向于选择自己的答案而非其他模型或人类的答案。这种偏见难以消除，因为前沿LM评委能够识别自己的输出，即使评估候选没有标注来源

Method: 采用黑盒扰动方法对成对比较中的评估候选进行文本修改，以模糊作者身份并降低自我识别能力。具体使用简单的同义词替换等扰动策略

Result: 简单的扰动（如替换少量词语的同义词）能够可预测地减少自我偏好。但当扰动扩展到更彻底地消除评估候选之间的风格差异时，自我偏好会重新出现

Conclusion: 自我识别和自我偏好可能发生在多个语义层面上，尽管初步结果有希望，但完全缓解该偏见仍然具有挑战性

Abstract: Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.

</details>


### [8] [Learning from Self Critique and Refinement for Faithful LLM Summarization](https://arxiv.org/abs/2512.05387)
*Ting-Yao Hu,Hema Swetha Koppula,Hadi Pouransari,Cem Koc,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: 提出SCRPO框架，通过自我批评和优化来减少LLM在摘要生成中的幻觉问题，无需额外测试时计算或更强教师模型


<details>
  <summary>Details</summary>
Motivation: LLM在长文本生成任务（如摘要）中经常出现幻觉问题，现有方法需要额外测试时计算或更强教师模型，成本高且不实用

Method: 提出SCRPO框架：1）利用LLM自身的批评和优化能力构建偏好数据集；2）应用偏好学习来改进同一LLM的忠实摘要能力

Result: 在三个摘要基准测试（XSUM、CNNDM、SAMSum）上，SCRPO在忠实度指标上优于最先进的自监督学习方法，同时保持或改善其他摘要质量指标

Conclusion: SCRPO不仅提高了效率，还生成了更忠实的摘要，相比测试时优化方法更具优势

Abstract: Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously generated outputs using either the same model or a more powerful teacher model as the critique. However, these approaches either require additional test-time compute or assume access to more powerful teacher models, making them costly and less practical. In this work, we propose Self Critique and Refinement-based Preference Optimization (SCRPO), which is a self-supervised training framework that first constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization. Experiments on three summarization benchmarks (XSUM CNNDM and SAMSum), demonstrate that our approach outperforms state-of-the-art self-supervised learning methods in terms of faithfulness metrics while either maintaining or improving other metrics that measure the overall quality of the summary. Moreover, compared to test-time refinement, our approach not only improves efficiency but also results in more faithful summaries.

</details>


### [9] [SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs](https://arxiv.org/abs/2512.05409)
*Ruixuan Huang,Hao Zeng,Hantao Huang,Jinyuan Shi,Minghui Yu,Ian En-Hsu Yen,Shuai Wang*

Main category: cs.CL

TL;DR: 提出SQ-format统一数据格式，将稀疏化与量化结合，在保持精度的同时提升LLM推理效率，实现性能与吞吐量的帕累托改进


<details>
  <summary>Details</summary>
Motivation: 现有低比特量化和稀疏化技术由于硬件支持有限，难以平衡精度与效率。例如W4A8只能达到与W8A8相同的峰值TOPS，而GPU支持的稀疏数据格式(2:4半结构化稀疏)因精度损失很少被采用

Method: 提出稀疏量化格式(SQ-format)，这是一种统一的数据格式，利用稀疏矩阵可以在高精度下加速，低精度矩阵乘法也能相应加速的特性，特别适用于具有异常值不平等状态的激活值

Result: 展示了SQ-format在PTQ中的最先进性能，提出了支持该格式所需的硬件要求，并为下一代AI加速器提供了设计探索和见解

Conclusion: SQ-format能够实现性能与吞吐量之间的帕累托改进，使激活值的静态压缩成为可能，为未来AI硬件设计提供了新方向

Abstract: Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.

</details>


### [10] [LMSpell: Neural Spell Checking for Low-Resource Languages](https://arxiv.org/abs/2512.05414)
*Akesh Gunathilakea,Nadil Karunarathnea,Tharusha Bandaranayakea,Nisansa de Silvaa,Surangika Ranathunga*

Main category: cs.CL

TL;DR: 本文首次对预训练语言模型在拼写纠错任务上的有效性进行了实证研究，特别关注低资源语言，发现大型语言模型在微调数据量大时表现最佳，并发布了LMSpell工具包。


<details>
  <summary>Details</summary>
Motivation: 拼写纠错对于低资源语言仍然是一个挑战性问题。虽然预训练语言模型已被用于拼写纠错，但其应用仍局限于少数语言，且缺乏不同PLM之间的系统比较。

Method: 1. 对多种预训练语言模型（包括编码器、编码器-解码器和大型语言模型）在拼写纠错任务上进行首次实证比较研究；2. 开发LMSpell工具包，包含评估函数以补偿LLM的幻觉问题；3. 以僧伽罗语为例进行案例研究，揭示低资源语言拼写纠错的困境。

Result: 1. 当微调数据集较大时，大型语言模型在拼写纠错任务上优于编码器基和编码器-解码器模型；2. 这一观察结果即使在LLM未预训练的语言中也成立；3. 通过僧伽罗语案例研究展示了低资源语言拼写纠错的具体挑战。

Conclusion: 本研究填补了预训练语言模型在拼写纠错任务上缺乏系统比较的空白，特别关注低资源语言，证明了LLM在该任务上的优势，并提供了实用的工具包和评估方法，有助于推动低资源语言的拼写纠错研究。

Abstract: Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.

</details>


### [11] [ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering](https://arxiv.org/abs/2512.05430)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

TL;DR: 本文针对大语言模型在音乐知识问答中的局限性，提出了MusWikiDB向量数据库和ArtistMus基准测试，通过检索增强生成显著提升了音乐相关问题的回答准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在音乐相关推理方面效果有限，因为预训练数据中音乐知识稀疏。虽然音乐信息检索和计算音乐学领域已探索结构化多模态理解，但缺乏基于艺术家元数据和历史背景的事实性和上下文音乐问答资源。

Method: 1. 构建MusWikiDB：包含144K音乐相关维基百科页面的3.2M段落向量数据库；2. 创建ArtistMus基准：包含500位多样化艺术家的1,000个问题，附带流派、出道年份等元数据；3. 使用检索增强生成（RAG）方法进行音乐问答评估；4. 进行RAG风格微调以提升事实召回和上下文推理能力。

Result: RAG显著提高了事实准确性：开源模型提升高达+56.8个百分点（如Qwen3 8B从35.0提升到91.8），接近专有模型性能。RAG风格微调进一步提升了事实召回和上下文推理能力，在领域内和领域外基准测试中均有改善。MusWikiDB相比通用维基百科语料库准确率提高约6个百分点，检索速度提升40%。

Conclusion: MusWikiDB和ArtistMus资源推动了音乐信息检索和领域特定问答研究，为音乐等文化丰富领域的检索增强推理建立了基础。RAG方法能有效弥补大语言模型在音乐知识方面的不足，显著提升音乐相关问题的回答质量。

Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.

</details>


### [12] [SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures](https://arxiv.org/abs/2512.05501)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: SEA-SafeguardBench是首个针对东南亚语言的人类验证安全基准，覆盖8种语言、21,640个样本，揭示现有LLM和防护系统在东南亚文化场景中表现不佳


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要针对英语，缺乏语言和文化多样性；东南亚语言代表性不足，机器翻译数据无法捕捉低资源语言的细微差别；东南亚地区有独特的文化敏感政治言论和地区性虚假信息等安全问题

Method: 创建SEA-SafeguardBench基准，包含8种东南亚语言、21,640个人类验证样本，分为三个子集：通用场景、真实场景和内容生成场景

Result: 实验结果表明，即使最先进的LLM和防护系统在东南亚文化和伤害场景中也面临挑战，与英语文本相比表现不佳

Conclusion: 需要本地化创作的安全基准来反映当地规范和安全场景，SEA-SafeguardBench填补了这一空白，揭示了当前安全系统在语言和文化多样性方面的局限性

Abstract: Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.

</details>


### [13] [Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches](https://arxiv.org/abs/2512.05537)
*Namu Park,Farzad Ahmed,Zhaoyi Sun,Kevin Lybarger,Ethan Breinhorst,Julie Hu,Ozlem Uzuner,Martin Gunn,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 大型语言模型通过病灶标记和解剖学感知提示，在放射学报告中检测需要随访的偶发瘤方面显著优于传统监督模型，性能接近人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 当前基于文档级别的分类系统在检测需要随访的偶发瘤方面存在局限性，需要更细粒度的病灶级别检测方法。研究旨在评估大型语言模型在检测放射学报告中需要随访的偶发瘤方面的性能，并与传统监督基线模型进行比较。

Method: 使用包含400份标注放射学报告和1,623个已验证病灶的数据集。比较三种监督Transformer编码器（BioClinicalModernBERT、ModernBERT、Clinical Longformer）与四种生成式LLM配置（Llama 3.1-8B、GPT-4o、GPT-OSS-20b）。引入新颖的推理策略，使用病灶标记输入和解剖学感知提示来增强模型推理能力。通过类别特定的F1分数评估性能。

Result: 解剖学感知的GPT-OSS-20b模型表现最佳，获得偶发瘤阳性宏F1分数0.79，超过所有监督基线模型（最高宏F1：0.70），接近人类标注者间一致性0.76。解剖学基础显著提升了GPT模型性能（p < 0.05）。前几个系统的多数投票集成进一步将宏F1提升至0.90。错误分析显示解剖学感知LLM在区分可操作发现与良性病变方面表现出优越的上下文推理能力。

Conclusion: 生成式大型语言模型通过结构化病灶标记和解剖学上下文增强后，显著优于传统监督编码器，性能接近人类专家水平。该方法为放射学工作流程中的自动化偶发发现监测提供了可靠、可解释的途径。

Abstract: Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.
  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.
  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.
  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.

</details>


### [14] [Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems](https://arxiv.org/abs/2512.05580)
*Aurprita Mahmood,Sabrin alam,Neloy kumer Sagor,Md. Abdul Hadi,Md. Sehab Al Islam,Minhajul Islam*

Main category: cs.CL

TL;DR: 该研究系统评估了Tree-of-Thought推理方法在孟加拉语数学应用题上的表现，发现相比标准提示和Chain-of-Thought，ToT能进一步提升模型性能，特别是在中等至大规模模型中效果显著。


<details>
  <summary>Details</summary>
Motivation: 数学应用题是NLP中最具挑战性的任务之一，需要语言理解和多步数值推理。虽然Chain-of-Thought提示已显示出潜力，但其线性结构容易传播错误，限制了整体效果。本研究旨在通过Tree-of-Thought推理解决这一限制。

Method: 使用SOMADHAN数据集，在计算和token成本约束下，评估了100个代表性孟加拉语数学应用题。在多个大语言模型（包括GPT-OSS和LLaMA变体）上比较了标准提示、Chain-of-Thought和Tree-of-Thought策略。

Result: CoT将基线准确率从78%（标准提示）提升到83%，而ToT进一步将性能提高了最多5个百分点，使用GPT-OSS-120B达到88%准确率。ToT在中等至大规模模型中特别有效，但对较小模型优势较小。

Conclusion: ToT是解决孟加拉语等低资源语言数学问题的稳健框架。结构化推理方法如ToT能提供比CoT更可靠和全局一致的结果，为多语言NLP中更好的推理策略铺平了道路。

Abstract: Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.

</details>


### [15] [Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models](https://arxiv.org/abs/2512.05658)
*Pietro Ferrazzi,Aitor Soroa,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 该研究提出了一种基于医学知识的多语言推理轨迹生成方法，用于提升医学问答系统的性能，覆盖英语、意大利语和西班牙语三种语言。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答方法主要依赖通用大语言模型的知识蒸馏，且以英语为中心，存在医学知识可靠性不足和多语言支持有限的问题。

Method: 采用检索增强生成方法，基于维基百科医学信息生成50万条多语言推理轨迹，涵盖英语、意大利语和西班牙语，并将MedQA和MedMCQA数据集扩展到多语言版本。

Result: 在医学问答基准测试中，无论是域内还是域外设置，该推理轨迹都能通过上下文学习和监督微调显著提升性能，在8B参数LLMs中达到最先进水平。

Conclusion: 该方法为开发更安全、更透明的多语言临床决策支持工具提供了有价值的资源，包括推理轨迹、翻译的QA数据集、医学维基百科和微调模型。

Abstract: Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.

</details>


### [16] [Interleaved Latent Visual Reasoning with Selective Perceptual Modeling](https://arxiv.org/abs/2512.05665)
*Shuai Dong,Siyuan Wang,Xingyu Liu,Zhongyu Wei*

Main category: cs.CL

TL;DR: ILVR框架通过交错潜在视觉表示与文本生成，解决了多模态大语言模型中视觉重编码计算成本高的问题，同时避免了特征过度压缩或静态结构导致的感知精度损失。


<details>
  <summary>Details</summary>
Motivation: 现有交错推理范式因重复编码像素密集图像而计算成本过高，而潜在视觉推理方法要么因过度压缩特征而牺牲感知精度，要么因静态结构无法建模动态问题，需要一种能统一动态状态演化和精确感知建模的框架。

Method: 提出交错潜在视觉推理（ILVR）框架，将文本生成与作为后续推理线索的潜在视觉表示交错进行。采用自监督策略，通过动量教师模型从辅助图像中选择性蒸馏相关特征为稀疏监督目标，引导模型自主生成上下文感知的视觉信号。

Result: 在多模态推理基准测试中，ILVR显著优于现有方法，有效弥合了细粒度感知与序列多模态推理之间的差距。

Conclusion: ILVR成功统一了动态状态演化与精确感知建模，通过交错潜在视觉表示实现了高效且准确的多模态推理，为多模态大语言模型提供了新的推理范式。

Abstract: Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.

</details>


### [17] [MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation](https://arxiv.org/abs/2512.05671)
*Zhitao He,Haolin Yang,Zeyu Qin,Yi R Fung*

Main category: cs.CL

TL;DR: 开发了ClinEdu多智能体教学模拟器和ClinTeach数据集，并基于此训练了MedTutor-R1多模态苏格拉底式医学导师，用于解决临床医学教育中专家指导不足的问题。


<details>
  <summary>Details</summary>
Motivation: 临床医学教育面临日益增长的教学需求与专家指导稀缺之间的巨大差距。当前研究主要关注一对一知识传授，忽视了团队合作中培养的协作推理能力。

Method: 1. 开发ClinEdu多智能体教学模拟器，包含个性化患者和多样化学生群体；2. 构建ClinTeach大规模苏格拉底式教学对话数据集；3. 训练MedTutor-R1多模态苏格拉底式导师，先通过ClinTeach进行指令微调，再使用基于三轴评估标准的强化学习优化。

Result: MedTutor-R1在平均教学评分上比基础模型提升超过20%，与o3模型表现相当，且在处理不同数量学生时表现出高适应性。

Conclusion: 通过ClinEdu教学模拟器和MedTutor-R1苏格拉底式导师系统，有效解决了临床医学教育中协作推理教学的挑战，证明了教学模拟器在医学教育中的有效性。

Abstract: The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.

</details>


### [18] [Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods](https://arxiv.org/abs/2512.05681)
*Tereza Novotna,Jakub Harasta*

Main category: cs.CL

TL;DR: 比较两种模型在捷克宪法法院判决检索中的表现：通用OpenAI嵌入器在三个不同设置下均显著优于领域特定的BERT模型，尽管绝对nDCG值不高，但差异具有统计显著性。


<details>
  <summary>Details</summary>
Motivation: 案例法检索是一项耗时的任务，通常通过查询数据库完成。本研究旨在比较通用嵌入器和领域特定模型在捷克宪法法院判决检索中的性能，并提出适用于噪声标签数据的评估框架。

Method: 比较两种模型在三种设置下的表现：(1)通用OpenAI嵌入器，(2)基于约30,000个判决从头训练的领域特定BERT模型（使用滑动窗口和注意力池化）。提出包含IDF加权关键词重叠作为分级相关性、通过两个阈值（0.20平衡、0.28严格）进行二值化、配对bootstrap显著性检验以及nDCG诊断的噪声感知评估方法。

Result: 尽管绝对nDCG值不高（在噪声标签下预期如此），但通用OpenAI嵌入器在@10/@20/@100两个阈值下的所有设置中均显著优于领域预训练的BERT模型；差异具有统计显著性。诊断分析表明低绝对值源于标签漂移和强理想标准，而非模型缺乏实用性。

Conclusion: 通用嵌入器在捷克宪法法院判决检索任务中优于领域特定模型。提出的评估框架足够稳健，可用于处理来自遗留司法数据库的异构标签数据，这在噪声黄金数据集评估中特别有价值。

Abstract: Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.

</details>


### [19] [Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains](https://arxiv.org/abs/2512.05700)
*Ben Malin,Tatiana Kalganova,Nikolaos Boulgouris*

Main category: cs.CL

TL;DR: 提出了一种基于多指标融合的方法来改进大语言模型忠实度评估的准确性，通过树模型结合人类判断来融合基础指标，创建了跨问答和对话领域的标准化数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型忠实度评估存在准确性不足的问题，需要更可靠的评估方法来增强对模型的信任，以便在更多样化的场景中部署使用。

Method: 提出基于树模型的指标融合方法：1) 收集基础忠实度评估指标；2) 结合人类对LLM回答忠实度的判断；3) 使用树模型确定各指标的重要性权重；4) 融合成综合指标；5) 创建跨问答和对话领域的标准化数据集。

Result: 融合后的指标在所有测试领域中与人类判断的相关性更强，提高了忠实度评估的准确性，增强了模型可信度，为跨领域忠实度评估提供了可复现的数据集。

Conclusion: 通过指标融合方法显著提升了大语言模型忠实度评估的准确性，为模型在多样化场景中的可靠部署提供了技术基础，同时创建的标准化数据集促进了该领域的研究可复现性。

Abstract: We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.

</details>


### [20] [Efficient Text Classification with Conformal In-Context Learning](https://arxiv.org/abs/2512.05732)
*Ippokratis Pantelidis,Korbinian Randl,Aron Henriksson*

Main category: cs.CL

TL;DR: CICLe框架结合轻量级基础分类器与Conformal Prediction，通过自适应减少候选类别来指导LLM提示，在多种NLP分类任务中提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具有强大的上下文学习能力，但在文本分类任务中效果严重依赖提示设计且计算成本高昂。需要探索更高效、可扩展的方法来结合传统分类器的鲁棒性和LLMs的适应性。

Method: 提出Conformal In-Context Learning (CICLe)框架，集成轻量级基础分类器与Conformal Prediction，通过自适应减少候选类别集合来指导LLM提示，减少所需的shot数量和提示长度。

Result: CICLe在多样NLP分类基准测试中表现一致优于基础分类器，在样本充足时超越few-shot提示基线，在低数据场景下表现相当。效率方面减少shot数量达34.45%，提示长度减少25.16%，并能使用更小模型保持竞争性能，特别适用于类别不平衡任务。

Conclusion: CICLe是一种实用且可扩展的高效文本分类方法，结合了传统分类器的鲁棒性和LLMs的适应性，在数据和计算效率方面取得显著提升。

Abstract: Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.

</details>


### [21] [Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments](https://arxiv.org/abs/2512.05832)
*Yifei Tong*

Main category: cs.CL

TL;DR: 研究美国最高法院口头辩论中打断行为如何影响律师发言的语义内容和情感基调，特别关注性别差异。发现打断不会显著改变论证内容，但对女性律师的打断包含更多负面情感。


<details>
  <summary>Details</summary>
Motivation: 研究最高法院口头辩论中的打断行为如何影响律师的发言内容和情感基调，特别关注性别差异在司法话语中的体现，以深化对精英机构中性别化沟通的理解。

Method: 使用ConvoKit最高法院语料库（2010-2019年），分析12,663个律师-法官互动中的发言片段。通过GloVe句子嵌入量化语义变化，使用基于词典的方法测量情感倾向。

Result: 打断前后的语义相似度保持较高水平，表明打断不会实质性改变论证内容。然而，针对女性律师的打断包含显著更高的负面情感水平。

Conclusion: 研究深化了对精英机构中性别化沟通的实证理解，展示了计算语言学方法在研究司法程序中的权力、话语和公平方面的价值。

Abstract: This study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates' speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate's argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.

</details>


### [22] [Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy](https://arxiv.org/abs/2512.05858)
*Savir Basil,Ina Shapiro,Dan Shapiro,Ethan Mollick,Lilach Mollick,Lennart Meincke*

Main category: cs.CL

TL;DR: 研究发现：在困难的多选题任务中，为AI模型分配专家或低知识水平的人物角色提示，通常不会显著提升答案准确性，有时甚至会降低性能。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究为AI模型分配特定人物角色（专家或低知识水平角色）是否能够提升其在困难客观多选题上的表现。这是系列技术报告的一部分，帮助商业、教育和政策领导者理解AI工作的技术细节。

Method: 研究测试了六种模型在GPQA Diamond和MMLU-Pro两个研究生水平基准测试上的表现，涵盖科学、工程和法律领域。测试了三种方法：1) 领域匹配专家角色（如物理专家解答物理问题）；2) 领域不匹配专家角色（如物理专家解答法律问题）；3) 低知识水平角色（如外行、幼儿、学步儿童）。

Result: 领域匹配专家角色对性能无显著影响（除Gemini 2.0 Flash模型外）；领域不匹配专家角色仅产生边际差异；低知识水平角色通常损害基准准确性。总体上，人物角色提示相对于无角色基线并未提高准确性，专家角色无一致益处，有时甚至降低性能。

Conclusion: 人物角色提示在提升事实性表现方面效果有限，虽然可能改变输出风格或语气，但无法可靠提高困难客观多选题的准确性。这为实际应用中的人物角色使用提供了重要参考。

Abstract: This is the fourth in a series of short reports that help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. Here, we ask whether assigning personas to models improves performance on difficult objective multiple-choice questions. We study both domain-specific expert personas and low-knowledge personas, evaluating six models on GPQA Diamond (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024), graduate-level questions spanning science, engineering, and law.
  We tested three approaches:
  -In-Domain Experts: Assigning the model an expert persona ("you are a physics expert") matched to the problem type (physics problems) had no significant impact on performance (with the exception of the Gemini 2.0 Flash model).
  -Off-Domain Experts (Domain-Mismatched): Assigning the model an expert persona ("you are a physics expert") not matched to the problem type (law problems) resulted in marginal differences.
  -Low-Knowledge Personas: We assigned the model negative capability personas (layperson, young child, toddler), which were generally harmful to benchmark accuracy.
  Across both benchmarks, persona prompts generally did not improve accuracy relative to a no-persona baseline. Expert personas showed no consistent benefit across models, with few exceptions. Domain-mismatched expert personas sometimes degraded performance. Low-knowledge personas often reduced accuracy. These results are about the accuracy of answers only; personas may serve other purposes (such as altering the tone of outputs), beyond improving factual performance.

</details>


### [23] [Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework](https://arxiv.org/abs/2512.05863)
*Tasnimul Hassan,Md Faisal Karim,Haziq Jeelani,Elham Behnam,Robert Green,Fayeq Jeelani Syed*

Main category: cs.CL

TL;DR: 该论文提出了一种基于检索增强生成（RAG）的医疗问答系统，通过结合领域知识检索和开源大语言模型来提高医疗问答的准确性和减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 直接应用大语言模型到临床领域面临保持事实准确性和避免幻觉的挑战，需要开发能够结合领域专业知识的方法来提高医疗问答系统的可靠性。

Method: 使用检索增强生成（RAG）框架，结合医疗文献检索和开源大语言模型（LLaMA~2和Falcon），采用低秩适应（LoRA）进行高效领域微调，通过检索相关医学文献来支撑模型回答。

Result: 在PubMedQA和MedMCQA基准数据集上评估，检索增强显著提高了答案准确性。微调的LLaMA~2模型在PubMedQA上达到71.8%准确率，相比零样本基线的55.4%有显著提升，同时通过提供来源引用保持透明度，减少了约60%的无支持内容。

Conclusion: 检索增强生成的开源大语言模型在可靠生物医学问答方面具有潜力，为实际临床信息学应用指明了方向，通过基于检索证据的答案生成提高了事实正确性和透明度。

Abstract: Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.

</details>


### [24] [M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG](https://arxiv.org/abs/2512.05959)
*David Anugraha,Patrick Amadeus Irawan,Anshul Singh,En-Shiun Annie Lee,Genta Indra Winata*

Main category: cs.CL

TL;DR: M4-RAG是一个大规模多语言多模态检索增强生成基准，涵盖42种语言和56种地区方言，包含8万多张文化多样性图像-问题对，用于评估跨语言和跨模态的检索增强视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视觉问答方面表现出色，但受限于静态训练数据。检索增强生成能够访问最新、文化相关和多语言信息，但多语言多模态RAG研究不足，需要系统评估框架。

Method: 构建M4-RAG基准，包含42种语言和56种地区方言，超过8万张文化多样性图像-问题对。创建受控检索环境，包含数百万精心策划的多语言文档，模拟真实世界检索条件同时确保实验可重复性。

Result: 系统评估显示，RAG对小规模VLMs有持续益处，但无法扩展到更大模型，甚至经常降低其性能，暴露了模型规模与当前检索效果之间的关键不匹配。

Conclusion: M4-RAG为推进下一代RAG系统奠定了基础，这些系统能够跨语言、模态和文化背景进行无缝推理，揭示了当前检索增强方法在大规模模型上的局限性。

Abstract: Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 该论文提出了两种基于信息论和热力学的无监督度量方法，用于评估大型语言模型对给定任务的忠实度。通过将LLM视为二分信息引擎，利用主题转换矩阵的KL散度计算语义忠实度，并结合热力学熵产生概念进行综合评估。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对给定任务的忠实度是一个复杂挑战，现有方法存在局限性。需要开发新的无监督度量标准来准确量化LLM输出的忠实度，特别是针对幻觉控制问题。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制从上下文C到答案A的转换。将QCA三元组建模为共享主题上的概率分布，使用转换矩阵Q和A分别编码查询目标和实际结果。通过凸优化同时推断这两个矩阵，计算KL散度作为语义忠实度度量，并将其映射到[0,1]区间。另外提出基于热力学的语义熵产生度量。

Result: 提出的语义忠实度和语义熵产生度量能够有效评估LLM输出的忠实度。在SEC 10-K文件摘要任务上的实验表明，高忠实度通常对应低熵产生。这两种度量可以单独或联合用于LLM评估和幻觉控制。

Conclusion: 基于信息论和热力学的无监督度量方法为评估LLM忠实度提供了新框架。语义忠实度和语义熵产生度量能够有效量化模型输出与任务要求的匹配程度，为幻觉检测和控制提供了实用工具。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [26] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 该论文提出了一种创新的AI与数据科学教学方法，系统性地将传统机器学习技术与现代大语言模型相结合，通过两部分课程设计帮助学生全面理解AI发展并掌握实践技能。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的发展历程，同时掌握传统机器学习和现代大语言模型的实践技能，更好地适应快速发展的AI行业需求。

Method: 采用两部分课程设计：第一部分教授基础机器学习概念，第二部分专注于当代大语言模型应用。课程包括详细的教学架构、实施策略、评估方法，并在两个七周的夏季学期中实施。

Result: 这种整合教学方法显著增强了学生对AI领域的理解，并更好地为他们应对行业需求做好了准备。

Conclusion: 该创新教学方法成功地将传统机器学习与现代LLM技术相结合，为学生提供了全面的AI教育，有效提升了他们在快速发展的AI领域中的竞争力。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [27] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 该论文通过形式化证明指出，任何算法（包括AI模型）都无法产生初始算法本身不具备的新功能能力，因此AI无法实现真正的创造性突破。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，人们开始关注人类何时能开发出达到人类智能水平的人工通用智能（AGI）。本文旨在探讨计算能力的上限，特别是算法是否能够实现真正的创造性突破。

Method: 采用前人提出的AGI定义（在某个领域具有创造性并能解锁该领域未知功能能力的能力），通过形式化证明的方法，论证任何算法都无法产生初始算法本身不具备的新功能能力。

Result: 形式化证明表明，没有算法能够展示初始算法本身不具备的新功能能力。AI模型只能展示现有功能能力及其组合和排列，无法实现真正的创造性突破。

Conclusion: 该证明对AI发展的未来和人类智能的起源具有重要意义，表明算法本质上无法实现真正的创造性，这为理解AI的能力边界和人类智能的本质提供了新的视角。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [28] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 该论文主张可能性理论（特别是Bychkov的公理化方法）为解决Dempster-Shafer理论（DST）悖论提供了根本性解决方案，而非仅仅修复Dempster规则。


<details>
  <summary>Details</summary>
Motivation: 解决DST（证据理论）中的悖论和危机，特别是Dempster规则在处理矛盾证据时的问题。作者认为现有许多尝试只是修补Dempster规则，而需要从根本上建立逻辑一致且数学严谨的不确定性处理基础。

Method: 采用Bychkov文章中发展的公理化方法，基于可能性与必要性测度的二元论工具，从零开始构建逻辑一致的不确定性处理框架。通过比较分析概率论、证据论和可能性论三种范式，并以经典医疗诊断困境为例进行演示。

Result: 可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑。它不仅是DST的替代方案，更是解决DST悖论的根本性方案。

Conclusion: 可能性理论为解决DST危机提供了根本性解决方案，其基于可能性与必要性测度的公理化方法为不确定性处理建立了逻辑一致且数学严谨的基础，能够更好地模拟人类自然推理过程。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [29] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张将AI研究目标从自我改进转向协同改进，强调人类研究者与AI系统协作实现共同超智能，以更安全、更快地推进AI研究


<details>
  <summary>Details</summary>
Motivation: 当前AI领域的自我改进目标充满危险且难以实现，需要寻找更可行、更安全的研究方向来推动AI发展

Method: 提出以协同改进为核心目标，建立人类研究者与AI系统的协作研究循环，从构思到实验全过程合作，提升AI与人类共同进行AI研究的能力

Result: 协同改进方法既能加速AI研究进程，又能通过人机共生实现更安全的超智能，相比单纯的自我改进更具可行性和安全性

Conclusion: 应将研究重点从AI自我改进转向人机协同改进，通过将人类研究改进纳入循环，能够更快、更安全地实现共同超智能

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [30] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind：基于知识图谱增强推理的框架，专门用于处理超长集成电路规格文档，通过构建领域特定知识图谱和自适应检索机制，显著提升LLM在硬件设计中的实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在集成电路开发自动化方面潜力巨大，但受限于有限的上下文窗口。现有的上下文扩展方法难以对复杂冗长的电路规格进行有效的语义建模和多跳推理，阻碍了LLM辅助硬件设计的实际工业部署。

Method: 提出ChipMind框架：1）通过电路语义感知知识图谱构建方法将电路规格转换为领域特定知识图谱ChipKG；2）采用ChipKG增强推理机制，结合信息论自适应检索动态追踪逻辑依赖关系，以及意图感知语义过滤去除无关噪声，平衡检索完整性和精确性。

Result: 在工业级规格推理基准测试中，ChipMind显著优于现有最先进基线方法，平均提升34.59%，最高提升达72.73%。

Conclusion: ChipMind框架填补了学术研究与LLM辅助硬件设计实际工业部署之间的关键空白，为处理复杂集成电路规格文档提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [31] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是首个为LLM约束满足提供确定性、可靠概率边界的实用框架，相比基线方法获得6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究原型转向生产系统，从业者需要可靠方法来验证模型输出是否满足所需约束。基于采样的估计只能提供模型行为的直觉，无法提供可靠的保证。

Method: BEAVER框架使用新颖的token trie和frontier数据结构，系统性地探索生成空间，为任何前缀封闭的语义约束维护可证明可靠的概率边界。

Result: 在正确性验证、隐私验证和安全代码生成任务中，BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。

Conclusion: BEAVER能够提供精确的特征描述和风险评估，这是松散边界或经验评估无法提供的，为LLM约束验证提供了首个实用的确定性保证框架。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [32] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: 该论文提出将"意志薄弱"(akrasia)作为分析AI智能体系统不一致性和目标漂移的基础概念，并开发了Akrasia Benchmark来量化评估模型在不同诱惑条件下的"自我控制"能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出一种特殊的不一致性：它们"知道"正确答案但无法据此行动。这种全局判断与局部冲动之间的张力在人类哲学中被称为"意志薄弱"(akrasia)。作者认为这一概念可用于分析AI智能体系统中的不一致性和目标漂移问题。

Method: 提出了Akrasia Benchmark的初步版本，这是一个结构化的提示条件集，包括基线(B)、同义词(S)、时间(T)和诱惑(X)四种条件，用于测量模型局部响应与其先前承诺相矛盾的情况。该基准支持跨模型家族、解码策略和诱惑类型的"自我控制"能力量化比较。

Result: 通过基准测试可以定量比较不同模型在面临诱惑时的"自我控制"能力。此外，作者还概述了微观层面的意志薄弱如何在多智能体系统中累积为宏观层面的不稳定性，这可能被解释为"阴谋"或故意的不对齐行为。

Conclusion: 通过将不一致性重新定义为意志薄弱，这项工作将智能体行为与经典的能动性理论联系起来，为哲学、心理学和新兴的智能体AI科学之间建立了经验桥梁，为分析AI智能体系统的不一致性和目标漂移提供了新的理论框架。

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [33] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出MIND推理框架，通过"理解-反思-纠正"的类人认知过程，将多模态大语言模型从被动模仿推理提升到主动判别推理


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在多推理语义建模方面有限，逻辑鲁棒性不足，在复杂场景中容易受到误导性解释的影响，需要提升其认知推理能力

Method: 提出MIND推理框架，包含：1）RAD范式自动扩展数据集生成多样化推理；2）P2CL策略进行两阶段渐进式纠正学习；3）MCA优化策略实现多推理语义空间的对齐和分离

Result: 在涵盖科学、常识和数学场景的多个公开数据集上实现了最先进的性能表现

Conclusion: MIND框架为推进多模态大语言模型向更高水平的认知智能提供了新视角，实现了从被动模仿推理到主动判别推理的范式演进

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [34] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 本文提出了OntoAxiom基准测试，用于评估大型语言模型在识别本体公理方面的能力，发现Axiom-by-Axiom提示策略优于直接方法，但性能因公理类型和本体领域而异。


<details>
  <summary>Details</summary>
Motivation: 本体开发需要大量建模和领域专业知识，虽然自然语言处理和大型语言模型的发展推动了本体学习自动化，但识别公理这一核心任务仍面临挑战，需要系统评估LLM在此任务上的表现。

Method: 构建包含9个中等规模本体、17,118个三元组和2,771个公理的OntoAxiom基准测试，聚焦子类、不相交、子属性、定义域和值域五类公理。比较12个LLM在三种few-shot设置和两种提示策略（直接查询所有公理 vs 逐个公理查询）下的性能。

Result: 逐个公理提示策略的F1分数高于直接方法；不同公理类型识别难度差异大（FOAF本体子类公理得分为0.642，音乐本体仅为0.218）；大型模型优于小型模型，但小型模型在资源受限场景下仍可用；整体性能不足以完全自动化，但能为本体工程师提供有价值的候选公理。

Conclusion: LLM在公理识别任务上表现有限，但通过逐个公理提示策略能获得更好效果，可作为辅助工具为工程师提供候选公理，支持本体开发和精化工作。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [35] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出DeepDist算法，针对部分最大可满足性(PMS)和加权部分最大可满足性(WPMS)问题，设计了一种新颖的子句权重更新方案，首次区分PMS和WPMS的不同更新条件，并引入新的初始化方法和优先满足单元子句与硬子句的decimation方法。


<details>
  <summary>Details</summary>
Motivation: 现有的随机局部搜索(SLS)算法在处理PMS和WPMS问题时，通常采用统一的子句权重更新策略，未能充分考虑这两种问题类型之间的关键结构差异，导致性能受限。

Method: 1) 提出新颖的子句权重方案，首次根据PMS和WPMS实例的不同条件更新子句权重；2) 引入新的初始化方法，更好地适应两种实例类型的独特特征；3) 提出优先满足单元子句和硬子句的decimation方法；4) 基于这些方法开发了名为DeepDist的新SLS求解器。

Result: 在最近MaxSAT评估的基准测试中，DeepDist优于最先进的SLS求解器。将DeepDist与TT-Open-WBO-Inc结合的混合求解器超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。

Conclusion: 提出的方法有效地区分了PMS和WPMS问题，通过针对性的子句权重更新方案和辅助方法显著提升了求解性能，代码已开源。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [36] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: 提出FARDA框架，使用深度强化学习快速部署认知雷达对抗干扰，相比进化算法速度提升约7000倍，覆盖性能相当。


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有基于进化算法的方法耗时且易陷入局部最优，需要更高效的解决方案。

Method: 将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息，并提出新的奖励格式。

Result: FARDA方法在覆盖性能上与进化算法相当，但部署速度提升约7000倍，消融实验证实了各组件的重要性。

Conclusion: FARDA框架通过深度强化学习有效解决了雷达快速部署问题，显著提升了部署效率，为现代战争中的反干扰雷达部署提供了新方案。

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [37] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: ERO框架通过进化算法优化LLMs的推理能力，发现GPT-5等最新模型仍缺乏系统2推理能力，但通过简单进化循环可显著提升较弱模型（如Qwen-7B）的推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在特定任务上表现出色，但在通用智能和系统2推理（慢思考）方面仍有不足。研究旨在探索机器智能（如LLMs）能否像人类一样进化获得推理能力，而不仅仅是特定技能。

Method: 提出进化推理优化（ERO）框架，将多个LLMs初始化为种群，采用进化策略进行"适者生存"选择，通过进化循环最大化最佳个体的量化推理分数。

Result: 实验发现：1）最新LLMs（如GPT-5）仍表现出有限的系统2推理能力；2）通过ERO的简单进化循环，相对较弱的模型（Qwen-7B）可被增强并涌现出强大的推理能力。

Conclusion: ERO框架证明通过进化方法可以显著提升LLMs的推理能力，为机器智能获得类似人类的推理能力提供了可行路径，代码已开源供复现。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [38] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文反驳了"LLM只是模式匹配器，无法实现推理"的观点，提出推理的关键在于System-2协调层，而非模式存储本身，并提出了UCCT理论和MACI架构来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大型语言模型的批评——认为它们只是"模式匹配器"，结构上无法进行推理或规划，作者认为这种观点错误地识别了瓶颈。真正的限制不是模式存储本身，而是缺乏能够选择、约束和绑定这些模式的System-2协调层。

Method: 提出了UCCT（语义锚定理论），将推理建模为受有效支持度（ρ_d）、表征不匹配度（d_r）和自适应锚定预算（γ log k）控制的相变过程。基于此理论开发了MACI架构，包含诱饵（行为调制的辩论）、过滤（苏格拉底式评判）和持久性（事务性记忆）三个协调组件。

Result: 通过UCCT理论框架，将无基础的生成解释为对底层最大似然先验的无诱饵检索，而"推理"则是在锚点将后验概率转向目标导向约束时出现。将常见反对意见重新定义为可测试的协调失败。

Conclusion: 通往AGI的道路应该通过LLMs而非绕过它们。模式存储库是必要的System-1基础，而缺失的System-2协调层可以通过UCCT理论和MACI架构来实现，从而解决LLM的推理能力问题。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [39] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 该研究开发了一个多模态肿瘤智能体（MOA），结合组织学工具和临床基因组数据，用于低级别胶质瘤IDH1突变预测，在TCGA-LGG队列中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变具有重要的临床意义，能够定义不同的临床亚组并影响预后和治疗决策。目前需要更准确的预测方法来整合多模态信息，提高IDH1突变预测的准确性。

Method: 研究开发了多模态肿瘤智能体（MOA），整合了基于TITAN基础模型的组织学工具用于IDH1突变预测，同时通过PubMed、Google Search和OncoKB对结构化临床和基因组输入进行推理。在TCGA-LGG队列的488名患者上进行了定量评估。

Result: MOA在没有组织学工具的情况下优于临床基线，F1分数为0.826（临床基线为0.798）。当融合组织学特征时，MOA达到最高性能，F1分数为0.912，超过了组织学基线（0.894）和融合的组织学-临床基线（0.897）。

Conclusion: 该智能体能够通过外部生物医学资源捕获互补的突变相关信息，实现准确的IDH1突变预测，展示了多模态整合在肿瘤分子分型中的价值。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [40] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 使用GPT-5开发的论文正确性检查器发现，顶级AI会议和期刊发表的论文中存在不可忽视的客观错误数量，且平均错误数随时间增加。人类专家验证显示AI检查器的准确率达83.2%，并能对75.8%的错误提出正确修正。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物是构建新研究和知识的基础，但文献中持续存在的错误会传播并造成混淆，影响后续研究和可重复性。研究加速和同行评审系统压力使得错误更难被发现和避免。

Method: 开发基于GPT-5的论文正确性检查器，系统识别顶级AI会议和期刊已发表论文中的客观错误（如公式、推导、计算、图表错误），排除主观考量。人类专家验证AI识别的潜在错误，评估准确性和修正能力。

Result: 发现发表论文包含不可忽视的客观错误数量，且平均错误数随时间增加：NeurIPS从2021年的3.8个增至2025年的5.9个（增加55.3%）；ICLR从2018年的4.1个增至2025年的5.2个；TMLR从2022/23年的5.0个增至2025年的5.5个。人类专家验证316个潜在错误，确认263个为实际错误，准确率83.2%。AI检查器能为75.8%的识别错误提出正确修正。

Conclusion: 前沿大语言模型在检测和修正发表论文中的客观错误方面具有潜力，有助于建立更坚实的知识基础。虽然大多数识别问题相对较小，但修正它们能减少文献混淆并增强可重复性。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [41] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE框架通过辅助推理集和一致性评估来诊断视觉语言模型的推理轨迹，而非仅评估最终答案，从而发现标准评估忽略的推理错误。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在可靠数学和科学推理方面仍存在挑战，标准最终答案评估往往掩盖推理错误，导致静默失败持续存在。

Method: 引入TRACE框架，利用辅助推理集（紧凑的子问题-答案对）分解复杂问题，通过基于一致性的指标评估中间步骤，暴露标准评估忽略的失败。

Result: 实验表明，跨辅助推理集的一致性相关于最终答案正确性，有助于精确定位推理失败发生的步骤，为模型改进提供可操作的信号。

Conclusion: TRACE定义了区分可靠与不可靠推理路径的置信区域，支持有效的过滤、调试和模型精炼，为解决视觉语言模型推理可靠性问题提供了新方法。

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [42] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN将变分量子电路与Rainbow DQN结合，用于人力资源分配问题，相比传统方法获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 资源分配问题是NP难问题，传统深度强化学习方法受限于经典函数逼近器的表示能力，需要探索量子计算增强的可能性

Method: 提出变分量子Rainbow DQN（VQR-DQN），将环拓扑变分量子电路与Rainbow DQN集成，利用量子叠加和纠缠特性，将人力资源分配问题建模为马尔可夫决策过程

Result: 在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少了26.8%的标准化制造时间，相比Double DQN和经典Rainbow DQN提升了4.9-13.4%的性能

Conclusion: 量子增强的深度强化学习在大规模资源分配问题中具有潜力，电路表达能力、纠缠与策略质量之间存在理论联系

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [43] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的大规模合成基准测试，支持无限参数配置，提供结构化推理步骤和可执行代码，包含三种问题类型和三种新颖评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够全面评估语言模型科学推理能力的大规模、动态可配置的基准测试，需要能够测试模型在不同问题变体上的表现、一致性和不确定性的评估框架。

Method: 创建了包含15,045个参数化物理问题的基准测试（90/10训练/测试分割），每个问题都附带结构化推理步骤和可执行Python代码。包含三种问题类型：MC-Symbolic、MC-Numerical和free-form。引入了三种新颖评估指标：一致性分数、失败率和混淆率。

Result: 通过对最先进的指令调优语言模型进行实验，揭示了它们在科学推理方面的优势和局限性，表明SymPyBench能够有效评估模型的鲁棒性和可解释性。

Conclusion: SymPyBench为开发更鲁棒和可解释的推理系统奠定了基础，能够全面评估语言模型在科学推理任务上的表现，特别是通过新颖的评估指标量化模型在不同问题变体上的可变性和不确定性。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 该论文提出了一种针对电子健康记录的波动感知预训练策略CV-Masking，通过根据特征内在变异性调整掩码概率，相比传统随机掩码方法在重建、下游预测和收敛速度方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法在电子健康记录预训练中通常采用均匀随机掩码，假设所有特征同等可预测。但实际上实验室测试特征具有显著异质性：一些生物标志物（如钠）保持稳定，而另一些（如乳酸）波动较大且更难建模。临床上，波动性生物标志物通常提示急性病理生理变化，需要更复杂的建模来捕捉其复杂时间模式。

Method: 提出波动感知预训练策略CV-Masking（变异系数掩码），根据每个特征的内在变异性自适应调整掩码概率。结合与临床工作流程对齐的仅值掩码目标，该方法相比随机和基于方差的策略有系统性改进。

Result: 在大量实验室测试面板上的实验表明，CV-Masking增强了重建能力，改善了下游预测性能，加速了收敛速度，产生了更稳健且具有临床意义的电子健康记录表示。

Conclusion: CV-Masking通过考虑生物标志物的内在波动性，为电子健康记录的掩码自编码器预训练提供了更有效的策略，能够更好地捕捉临床相关的时间模式，生成更有用的表示。

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [45] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 系统评估临床时间序列建模中的分词策略，发现时间编码无显著益处，值特征重要性因任务而异，冻结预训练编码器优于可训练版本，更大编码器带来一致改进


<details>
  <summary>Details</summary>
Motivation: 分词策略影响电子健康记录处理效果，但目前缺乏公平比较。需要系统评估不同分词方法在临床时间序列建模中的有效性，揭示任务依赖性和反直觉发现

Method: 使用基于Transformer的架构，在MIMIC-IV数据集上对四个临床预测任务进行受控消融实验，比较不同分词策略（时间编码、值特征、代码序列等）的效果

Result: 1) 显式时间编码对下游任务无一致统计显著益处；2) 值特征重要性因任务而异（影响死亡率预测但不影响再入院预测）；3) 冻结预训练代码编码器显著优于可训练版本且参数更少；4) 更大的临床编码器在所有任务上带来一致改进

Conclusion: 更简单、参数高效的方法在许多情况下可达到强性能，但最优分词策略仍取决于具体任务。受控评估实现了更公平的分词比较，为临床时间序列建模提供了实用指导

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [46] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: VaRDASS提出首个专门用于无监督域适应的随机方差缩减技术，通过分层采样改进域差异估计的准确性，提升目标域性能。


<details>
  <summary>Details</summary>
Motivation: 无监督域适应（UDA）通过最小化域差异来应对域偏移问题，但在随机设置中，域差异估计存在高方差问题，这会阻碍方法理论优势的实现。

Method: 提出方差缩减域适应分层采样方法（VaRDASS），针对相关性对齐和最大均值差异（MMD）两种特定差异度量推导分层目标，并引入基于k-means的实用优化算法。

Result: 在三个域偏移数据集上的实验表明，该方法提高了差异估计的准确性和目标域性能，并证明在特定假设下，提出的MMD目标在理论上是最优的（即最小化方差）。

Conclusion: VaRDASS是首个专门针对UDA的随机方差缩减技术，通过分层采样有效减少域差异估计的方差，在理论和实践上都取得了显著改进。

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [47] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 论文提出了一种高效的机器学习遗忘框架，通过识别对模型影响可忽略的训练数据子集，在遗忘前减少数据集规模，从而显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中数据隐私问题日益突出，从训练模型中删除特定数据点的能力变得愈发重要。现有遗忘方法通常平等对待遗忘集中的所有数据点，但并非所有数据点对模型学习都有显著影响。

Method: 通过比较分析语言和视觉任务中的影响函数，识别对模型输出影响可忽略的训练数据子集，基于此提出高效的遗忘框架，在遗忘前减少数据集规模。

Result: 该方法在实际应用案例中实现了显著的计算节省（约50%），同时保持有效的遗忘效果。

Conclusion: 并非所有数据点都需要被遗忘，通过有选择地处理真正影响模型的数据，可以大幅提高机器学习遗忘过程的效率。

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [48] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RSA2C算法，将核方法与SHAP归因结合到Actor-Critic框架中，通过状态特征归因指导训练，提升可解释性、效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有Actor-Critic方法可解释性有限，且大多数可解释RL方法未能充分利用状态归因来辅助训练，忽略了不同状态维度对奖励的异质性影响。

Method: 提出RSA2C算法，包含Actor、Value Critic和Advantage Critic三个组件。Actor在向量值再生核希尔伯特空间(RKHS)中实现，使用Mahalanobis加权算子值核；两个Critic在标量RKHS中。通过RKHS-SHAP计算状态归因，转换为Mahalanobis门控权重来调节Actor梯度和Advantage Critic目标。

Result: 理论上推导了在状态扰动下的全局非渐近收敛界，显示通过扰动误差项实现稳定性，通过收敛误差项实现效率。在三个标准连续控制环境中的实验表明算法实现了效率、稳定性和可解释性。

Conclusion: RSA2C算法成功将状态归因整合到Actor-Critic框架中，通过核方法和SHAP归因实现了可解释性、训练效率和稳定性的提升，为可解释强化学习提供了新思路。

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [49] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 测试AI天气预报模型FourCastNetv2对输入噪声的鲁棒性，通过注入高斯噪声到飓风初始条件和完全随机初始条件两种实验，评估模型在极端天气事件中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预报模型对输入噪声和不确定性的鲁棒性对于确保极端天气事件（如飓风）预测输出的可靠性至关重要。

Method: 采用两种实验设计：1）在飓风Florence的ERA5初始条件中注入不同水平的高斯噪声，观察对预测轨迹和风暴强度的影响；2）使用完全随机初始条件启动模型，观察模型对无意义输入的反应。

Result: FCNv2在低到中等噪声水平下能准确保持飓风特征；即使在高噪声下也能保持基本风暴轨迹和结构，但位置精度下降；模型在所有噪声水平下都低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预测。

Conclusion: FCNv2表现出良好的鲁棒性，倾向于生成稳定平滑的输出，但存在低估风暴强度的问题。该简单方法可移植到其他数据驱动的AI天气预报模型中。

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [50] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 论文提出了一种新的联邦优化模型，能够量化通信和本地计算成本，并区分不同的客户端选择策略。基于该模型，作者开发了一种基于SAGA方差缩减梯度估计器和递归梯度技术的新算法，在非凸优化中实现了最优的通信和本地计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦优化算法使用不同的客户端选择策略（随机采样、全客户端通信或混合方案），这些策略在实际中产生不同的通信成本，但现有比较指标通常不区分这些策略。需要一个新的模型来量化通信和本地计算复杂度，并明确关联不同策略的成本。

Method: 1. 引入一个简单自然的联邦优化模型，量化通信和本地计算复杂度，支持多种常用客户端选择策略；2. 提出基于不精确复合梯度方法的新算法，使用精心构建的梯度估计器和辅助子问题求解程序；3. 基于SAGA方差缩减梯度估计器，推导新的方差界限，证明SAGA能利用函数相似性；4. 引入递归梯度技术作为改进条件无偏梯度估计器误差界限的通用方法；5. 将递归梯度技术应用于SAGA，得到改进的RG-SAGA估计器。

Result: 1. 新算法在非凸优化中实现了现有联邦优化方法中最佳的通信和本地计算复杂度；2. 为SAGA推导了新的方差界限，展示了其利用函数相似性的能力；3. 递归梯度技术能改进SAGA和SVRG等条件无偏梯度估计器的误差界限；4. RG-SAGA相比原始SAGA具有改进的误差界限。

Conclusion: 论文提出的联邦优化模型能够有效区分不同客户端选择策略的通信成本，基于该模型开发的新算法在非凸优化中达到了最优的通信和计算复杂度。递归梯度技术为改进梯度估计器提供了通用框架，RG-SAGA估计器在误差界限上优于原始SAGA。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [51] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: PATHFINDER使用蒙特卡洛树搜索生成训练路径轨迹，通过子答案召回和LLM验证过滤错误轨迹，改进多跳问答性能


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务中，基于训练的方法仍受LLM幻觉和错误推理路径影响，导致性能受限

Method: 1) 使用蒙特卡洛树搜索生成训练路径轨迹；2) 通过子答案召回和LLM验证过滤错误和冗长轨迹；3) 重新表述子查询处理检索失败情况

Result: PATHFINDER在公共基准数据集上提高了多跳问答的性能

Conclusion: 通过改进训练数据质量和处理检索失败，PATHFINDER能够有效提升多跳问答系统的性能

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [52] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出针对硬件代码生成的LLM遗忘框架，通过语法保持遗忘策略和细粒度选择性损失，有效移除问题知识而不损害代码生成能力。


<details>
  <summary>Details</summary>
Motivation: LLM在加速数字硬件设计方面潜力巨大，但现有模型存在记忆专有IP、污染基准和不安全编码模式等问题，确保其可靠性是关键挑战。

Method: 结合语法保持遗忘策略（保护硬件代码结构完整性）和细粒度floor-aware选择性损失（精确高效移除问题知识），实现有效遗忘而不降低代码生成能力。

Result: 实验表明该框架支持高达3倍大的遗忘集，通常只需单次训练周期，同时保持RTL代码的语法正确性和功能完整性。

Conclusion: 该工作为可靠的LLM辅助硬件设计开辟了新途径。

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [53] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: CATR框架通过选择稀疏必要的文本标记子集来解决文本数据因果推断中的混杂调整问题，缓解了高维文本特征导致的倾向得分极端值和方差膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 高维文本数据在因果推断中虽然能编码丰富的上下文信息，但也带来独特挑战。特别是正性假设（treatment overlap）在观测层面经常被违反，冗余或虚假的文本特征会膨胀维度，导致极端倾向得分、权重不稳定和效应估计方差膨胀。

Method: 提出Confounding-Aware Token Rationalization (CATR)框架，使用残差独立性诊断来选择稀疏必要的标记子集，旨在保留足够用于无混杂性的混杂信息，同时丢弃无关文本但保留关键信号。

Result: 在合成数据和真实世界研究（使用MIMIC-III数据库）上的实验表明，CATR比现有基线方法产生更准确、稳定和可解释的因果效应估计。

Conclusion: CATR框架通过选择稀疏必要的文本标记子集，有效缓解了观测层面的正性假设违反问题，稳定了下游因果效应估计器，为文本数据在因果推断中的应用提供了更可靠的方法。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [54] [China Regional 3km Downscaling Based on Residual Corrective Diffusion Model](https://arxiv.org/abs/2512.05377)
*Honglu Sun,Hao Jing,Zhixiang Dai,Sa Xiao,Wei Xue,Jian Sun,Qifeng Lu*

Main category: cs.LG

TL;DR: 该研究基于CorrDiff扩散模型框架，将统计降尺度方法应用于中国区域天气预测，将25km全球预报降尺度至3km分辨率，相比传统区域模型CMA-MESO在多个变量上取得更好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报中高效生成高分辨率预报是一个基本挑战。传统方法包括动力降尺度和统计降尺度，本研究专注于利用深度学习进行统计降尺度，建立低分辨率与高分辨率历史数据之间的统计关系。

Method: 基于CorrDiff扩散模型框架进行改进：1) 将应用区域扩大近20倍；2) 不仅考虑表面变量，还增加了六个气压层的高层变量作为目标降尺度变量；3) 添加全局残差连接以提高精度。使用CMA-GFS（25km全球网格预报）和SFF（基于球形傅里叶神经算子的数据驱动深度学习天气模型）作为输入，生成中国区域3km预报。

Result: 实验结果表明：1) 该方法降尺度后的预报在目标变量的MAE（平均绝对误差）方面普遍优于CMA-MESO直接预报；2) 雷达组合反射率预报显示，CorrDiff作为生成模型能够生成精细尺度细节，相比确定性回归模型产生更真实的预测。

Conclusion: 基于CorrDiff的扩散降尺度框架能够有效提升中国区域高分辨率天气预报的精度，生成模型相比传统回归方法能够产生更真实的精细尺度细节，为数值天气预报的高分辨率化提供了有效解决方案。

Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.

</details>


### [55] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: 评估蛋白质-配体评分函数在新靶点上的泛化能力，发现常用基准测试不能反映真实挑战，探索大规模自监督预训练和简单测试数据利用方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在分子设计中日益重要，需要确保可学习的蛋白质-配体评分函数在新蛋白质靶点上的可靠性。虽然许多评分函数在标准基准测试中表现良好，但其在训练数据之外的泛化能力仍然是一个重大挑战。

Method: 1) 在模拟评估有限已知结构和实验亲和力测量靶点的数据集划分上评估最先进评分函数的泛化能力；2) 研究大规模自监督预训练是否能弥合泛化差距；3) 探索利用有限测试靶点数据提升评分函数性能的简单方法。

Result: 分析显示常用基准测试不能反映泛化到新靶点的真实挑战；提供了大规模自监督预训练潜力的初步证据；研究了利用有限测试数据改善评分函数性能的方法。

Conclusion: 研究结果强调了需要更严格的评估协议，并为设计具有扩展到新蛋白质靶点预测能力的评分函数提供了实用指导。

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [56] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: 本文提出MineROI-Net，一个基于Transformer的模型，用于预测比特币挖矿硬件购买的盈利性，帮助矿工在波动的市场中做出更明智的投资决策。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿硬件采购面临市场波动大、技术快速过时、协议驱动收入周期等挑战，但缺乏何时购买新ASIC硬件的指导，也没有计算框架解决这一决策问题。

Method: 将硬件采购问题建模为时间序列分类任务，预测购买ASIC机器在一年内是否盈利。提出MineROI-Net，一个开源的基于Transformer的架构，旨在捕捉挖矿盈利性的多尺度时间模式。

Result: 在2015-2024年间发布的20种ASIC矿机数据上评估，MineROI-Net在多样化市场环境下表现优于LSTM和TSLANet基线，达到83.7%准确率和83.1%宏F1分数。模型在经济相关性方面表现强劲，对无盈利时期的检测精度达93.6%，对盈利时期的检测精度达98.5%。

Conclusion: MineROI-Net为挖矿硬件采购时机提供了一个实用的数据驱动工具，可能降低资本密集型挖矿操作的财务风险。模型已开源提供。

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [57] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: 提出TS-Hint框架，结合时序基础模型与思维链推理，通过注意力提示改进半导体制造中材料去除率的预测，在有限数据下实现有效学习


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法从时间序列提取静态特征来近似材料去除率，但会丢失时间动态信息，且需要大量数据进行有效训练

Method: 提出TS-Hint框架，集成时序基础模型与思维链推理，基于注意力机制数据和显著性数据在训练期间提供注意力提示

Result: 实验结果表明模型在有限数据设置下通过少样本学习有效，并能直接从多元时间序列特征中学习

Conclusion: TS-Hint框架通过结合时序基础模型与思维链推理，在半导体制造过程预测中克服了传统方法的局限性，实现了在有限数据下的有效学习

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [58] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: IdealTSF框架利用非理想负样本增强时间序列预测，通过预训练、训练和优化三阶段，结合正负样本提升模型在噪声数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中缺失值和异常值等非理想数据阻碍深度学习发展，现有方法主要关注特征提取或将非理想数据作为正样本进行知识迁移，但利用这些负样本增强事件预测可能更有效。

Method: 提出IdealTSF框架，包含三个渐进步骤：1) 预训练阶段从负样本数据中提取知识；2) 训练阶段将序列数据转化为理想正样本；3) 应用带有对抗扰动的负优化机制。

Result: 大量实验表明，负样本数据在基本注意力架构中释放了时间序列预测的显著潜力，特别是在噪声样本或低质量数据应用中表现优异。

Conclusion: IdealTSF框架特别适合具有噪声样本或低质量数据的应用场景，通过整合理想正样本和非理想负样本，有效提升了时间序列预测性能。

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [59] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: 集成模型在表格分类任务中如何平衡准确率与过拟合的研究，发现集成方法通过降低方差能在保持高准确率的同时控制泛化差距


<details>
  <summary>Details</summary>
Motivation: 虽然集成模型通常比单一学习器获得更高准确率，但其保持小泛化差距的能力尚未被充分理解。本研究旨在探究集成模型如何在准确率和过拟合之间取得平衡，为实际表格应用提供模型选择指导。

Method: 使用重复分层交叉验证和统计显著性检验，在四个表格分类任务（乳腺癌、心脏病、皮马糖尿病、信用卡欺诈）上比较线性模型、单一决策树和九种集成方法。同时计算数据集复杂度指标，如线性度评分、Fisher比率和噪声估计。

Result: 1. 在接近线性和干净数据上，线性模型已能很好泛化，集成方法提供额外收益有限；2. 在具有有意义非线性结构的数据集上，基于树的集成方法能将测试准确率提高5-7个百分点，同时保持泛化差距低于3%；3. 在噪声大或高度不平衡的数据集上，集成方法仍具竞争力但需要正则化以避免拟合噪声或多数类模式。

Conclusion: 集成模型通过平均或受控提升降低方差，能够在保持高准确率的同时控制过拟合。数据集复杂度指标（线性度评分、Fisher比率、噪声估计）能有效解释集成方法何时能有效控制方差。该研究为实际表格应用中的模型选择提供了清晰的指导。

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [60] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 比较不同几何量子机器学习模型在分子几何结构学习中的性能，发现置换对称嵌入模型具有最佳泛化能力


<details>
  <summary>Details</summary>
Motivation: 研究分子几何结构层次下，不同对称性等变量子机器学习模型的性能差异，为几何数据集选择合适模型提供标准

Method: 使用LiH（线性分子）和NH3（三角锥形分子）两个分子数据集，比较无对称等变、旋转和置换等变、图嵌入置换等变三种量子机器学习模型，以经典等变模型为基准

Result: 图嵌入特征能显著提高几何数据集的可训练性；置换对称嵌入模型在几何学习中具有最佳泛化性能；模型性能差异与分子几何结构相关

Conclusion: 图嵌入是提高几何数据集可训练性的有效途径，置换对称嵌入模型是几何学习中最具泛化能力的量子机器学习模型

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [61] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于熵比剪裁（ERC）的新机制，用于解决强化学习后训练中的分布偏移问题，通过双向约束熵比来稳定策略更新，并在多个基准测试中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练依赖强化学习提升模型能力和对齐质量，但离策略训练范式引入分布偏移，导致策略超出信任区域，引发训练不稳定（策略熵波动和梯度不稳定）。PPO-Clip虽然通过重要性剪裁缓解了这一问题，但仍忽略了动作的全局分布偏移。

Method: 提出使用当前策略与先前策略之间的熵比作为新的全局度量，有效量化策略探索在更新过程中的相对变化。基于此度量，引入了熵比剪裁（ERC）机制，对熵比施加双向约束，在全局分布层面稳定策略更新，并弥补PPO-clip无法调节未采样动作概率偏移的不足。将ERC集成到DAPO和GPPO强化学习算法中。

Result: 在多个基准测试上的实验表明，ERC机制能够持续提升性能。

Conclusion: 提出的熵比剪裁（ERC）机制通过量化策略探索的相对变化并施加双向约束，有效解决了强化学习后训练中的分布偏移问题，稳定了策略更新，并在多个基准测试中表现出性能提升。

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [62] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 该论文提出了两种新的不确定性量化方法CDEC和IDEC，用于分类任务中的不确定性评估，能够区分认知和偶然不确定性，并在不确定性超过阈值时拒绝分类。


<details>
  <summary>Details</summary>
Motivation: 人工智能中的不确定性量化对决策制定、风险评估和模型可靠性至关重要。现有方法存在局限性，需要能够系统评估认知和偶然不确定性，并在不确定性过高时拒绝分类的方法。

Method: 提出了两种新方法：CDEC（基于信任集的深度证据分类）和IDEC（基于区间的深度证据分类）。CDEC使用信任集（封闭凸概率集），IDEC使用证据预测分布的区间。两种方法都使用标准反向传播和基于证据理论的损失函数进行训练。

Result: 在MNIST、CIFAR-10、CIFAR-100及其自然分布外偏移数据集上的实验表明，CDEC和IDEC实现了有竞争力的预测准确性，在认知和总不确定性下的分布外检测达到最先进水平，并且提供紧密、校准良好的预测区域，在分布偏移下可靠扩展。

Conclusion: CDEC和IDEC是有效的深度证据分类方法，能够系统量化不确定性，在不确定性超过阈值时拒绝分类，在可接受的不确定性范围内提供具有鲁棒概率保证的标签集合。CDEC仅需小规模集成即可获得稳定的不确定性估计。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [63] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: IDK-S是一种用于数据流异常检测的增量分布核方法，通过动态核均值嵌入实现高精度实时检测，相比现有方法在保持准确性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测面临两大挑战：需要在分布不断演化的环境中保持高检测精度，同时确保实时处理效率。现有方法难以同时满足这两个要求。

Method: 提出IDK-S（增量分布核流式异常检测），继承Isolation Distributional Kernel的优势，采用轻量级增量更新机制，在核均值嵌入框架中创建动态表示，避免完整模型重训练。

Result: 在13个基准测试中，IDK-S实现了卓越的检测精度，同时计算速度显著优于现有最先进方法，在许多情况下快一个数量级，且统计上等价于完整重训练模型。

Conclusion: IDK-S成功解决了数据流异常检测中精度与效率的平衡问题，通过增量分布核方法实现了高性能的实时异常检测。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [64] [On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability](https://arxiv.org/abs/2512.05534)
*Yiming Tang,Harshvardhan Saini,Yizhen Liao,Dianbo Liu*

Main category: cs.LG

TL;DR: 论文提出了首个统一的理论框架来分析稀疏字典学习方法，为多种SDL方法提供理论基础，解释了经验观察到的现象如特征吸收、死神经元等。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，理解其内部表示和处理机制变得重要。现有稀疏字典学习方法虽然经验成功但缺乏理论理解，特别是对跨编码器、转码器等方法的理论分析不足。

Method: 将各种稀疏字典学习方法统一为一个优化问题框架，分析其优化景观，为不同方法提供形式化基础，并通过受控实验验证理论结果。

Result: 建立了首个统一理论框架，解释了特征吸收、死神经元等经验现象，为神经元重采样技术提供了理论解释，并通过实验验证了理论分析。

Conclusion: 该工作填补了稀疏字典学习方法理论基础的空白，为理解和改进这些方法提供了理论指导，有助于更可靠地部署AI系统。

Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.

</details>


### [65] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: SCoNE是一种新的多视图异常检测方法，通过球形一致邻域集成直接表示多视图实例，无需学习过程，实现O(N)时间复杂度，在大型数据集上比现有方法更快更准确。


<details>
  <summary>Details</summary>
Motivation: 现有多视图异常检测方法存在两个关键问题：1) 在不同视图中密度变化区域难以有效捕获一致邻域，导致检测精度低；2) 学习过程计算成本高(O(N²))，不适用于大型数据集。

Method: 提出SCoNE方法，具有两个独特特征：1) 直接使用多视图实例表示一致邻域，无需现有方法中的中间表示；2) 邻域具有数据依赖特性，在稀疏区域使用大邻域，在密集区域使用小邻域。

Result: 实证评估表明，SCoNE具有优越的检测精度，在大型数据集上运行速度比现有方法快几个数量级。

Conclusion: SCoNE通过数据依赖的球形一致邻域表示，解决了多视图异常检测中的一致邻域表示和计算效率问题，实现了高效准确的异常检测。

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [66] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 提出了一种基于MARS的非线性局部解释方法，相比LIME能更好地捕捉黑盒模型的非线性决策边界，通过N-ball采样技术提高解释忠实度。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒机器学习模型在关键领域的应用增加，需要提供可靠的预测解释。现有LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确。

Method: 使用多元自适应回归样条（MARS）建模非线性局部边界，捕捉参考模型的底层行为；采用N-ball采样技术直接从目标分布采样，而不是像LIME那样重新加权样本。

Result: 在三个UCI数据集上评估，相比基线方法，新方法能产生更忠实的解释，平均降低37%的均方根误差，显著提高了局部忠实度。

Conclusion: 提出的基于MARS的非线性局部解释方法能够有效捕捉黑盒模型的非线性决策边界，通过改进的采样技术显著提高了解释的忠实度和局部准确性。

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [67] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 研究如何通过超参数迁移来扩展预条件优化器（如Shampoo、SOAP、Muon）到大规模模型，发现遵循μP缩放学习率并结合分块和谱归一化可改善迁移效果，计算最优缩放中权重衰减按1/宽度缩放接近最优，应用这些规则可使Muon和Shampoo在Llama架构模型上获得1.4×和1.3×相对于AdamW的加速。


<details>
  <summary>Details</summary>
Motivation: 近期基于矩阵级预条件的深度学习优化器在小规模实验中显示出优于AdamW的加速效果，但验证和复现结果不一。为了理解这些优化器在大规模场景下的有效性，需要研究如何通过超参数迁移来扩展预条件优化器。

Method: 研究学习率和权重衰减如何随模型宽度和深度缩放，涵盖Shampoo、SOAP、Muon等多种优化器，考虑分块和嫁接等常用技术的影响。应用μP理论进行学习率缩放，并分析有限宽度偏差问题，通过分块和显式谱归一化来缓解。

Result: 按μP缩放学习率可改善迁移，但仍存在有限宽度偏差导致最优学习率漂移，分块和谱归一化可缓解此问题。计算最优缩放中，权重衰减按1/宽度缩放接近最优。应用这些缩放规则，Muon和Shampoo在190M到1.4B参数的Llama架构语言模型上分别获得1.4×和1.3×相对于AdamW的加速，而错误缩放下加速效果会迅速消失。

Conclusion: 研究最优超参数迁移对于在现实调优预算下可靠比较大规模优化器至关重要。正确的缩放规则是预条件优化器在大规模场景下保持优势的关键，而μP缩放结合分块和谱归一化等技术可有效改善优化器的可扩展性。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [68] [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)
*Kibidi Neocosmos,Diego Baptista,Nicole Ludwig*

Main category: cs.LG

TL;DR: 提出一种灵活的GNN社区检测框架，允许用户指定社区数量的范围或精确值，解决了传统GNN方法无法可靠控制输出社区数量的问题


<details>
  <summary>Details</summary>
Motivation: 传统社区检测方法需要预先指定社区数量，而GNN方法即使指定了期望数量也常常无法准确返回该数量，这限制了GNN在实际社区检测中的应用

Method: 提出一个灵活的原则性框架，允许用户指定社区数量的合理范围，并在训练过程中强制执行这些边界约束；同时支持用户指定精确的社区数量并能可靠返回

Result: 该框架能够可靠地控制GNN输出的社区数量，无论是指定范围还是精确值，都能在训练过程中有效执行约束，提高了GNN在社区检测中的实用性和可靠性

Conclusion: 通过引入灵活的社区数量控制机制，解决了GNN在社区检测中无法可靠控制输出社区数量的问题，为GNN在实际社区检测应用提供了更实用的解决方案

Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.

</details>


### [69] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 论文提出了"模块化喷射"框架，用于评估回归和分类流水线中模块分解的唯一性，区分了"幻象"和"可识别"两种状态，并开发了相应的算法。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习主要通过预测风险评估模型，但无法确定模型内部分解是否由数据和评估设计唯一确定。需要新的方法来评估模块化分解的唯一性。

Method: 引入模块化喷射概念，估计经验喷射（局部线性响应映射），描述模块对输入微小结构化扰动的反应。提出幻象状态和可识别状态的概念，开发MoJet算法进行经验喷射估计和幻象诊断。

Result: 在双模块线性回归流水线中证明了喷射可识别性定理：在温和的秩假设和模块级喷射可访问条件下，内部分解是唯一确定的。而仅基于风险的评估则允许大量幻象分解。

Conclusion: 模块化喷射框架能够评估流水线分解的唯一性，区分可识别和幻象状态，为理解模型内部结构提供了新工具，超越了传统基于风险的评估方法。

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [70] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: SGTM（选择性梯度掩码）是一种改进的梯度路由技术，通过在预训练时选择性掩码梯度，将目标知识局部化到特定参数中，以便后续移除，相比数据过滤和传统梯度路由在标签噪声下表现更好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有双重用途风险，数据过滤作为预训练缓解措施面临挑战：大规模标注有害数据成本高，且即使少量错误标注也可能导致危险能力。需要解决误标有害内容带来的风险。

Method: 提出选择性梯度掩码（SGTM），通过选择性零掩码梯度，使目标领域样本只更新其专用参数，从而将目标知识局部化到特定参数子集中，便于后续移除。

Result: 在两个应用中表现优异：1）从双语合成数据集中移除一种语言知识；2）从英文维基百科训练模型中移除生物学知识。在标签错误情况下，SGTM相比数据过滤和传统梯度路由提供更好的保留/遗忘权衡，且对抗性微调鲁棒性强，需要比RMU多7倍的微调步骤才能恢复遗忘集性能。

Conclusion: SGTM为现有安全缓解措施提供了有前景的预训练补充，特别是在标签噪声不可避免的场景中，能够有效处理双重用途风险。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [71] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI辅助的终端用户编码作为终端用户开发的新范式，通过自然语言提示与AI助手交互来生成和优化代码，相比传统的低代码/无代码平台具有更大灵活性、更广适用性、更快开发速度、更好可重用性和更低供应商锁定风险。


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助的终端用户编码是否可以作为终端用户开发的可行范式，以补充甚至替代传统的低代码/无代码平台。随着生成式AI和大型语言模型的发展，终端用户可能能够通过自然语言提示直接生成和优化编程代码，这为终端用户开发开辟了新的可能性。

Method: 通过案例研究设计，让非程序员参与者通过与AI助手交互来开发基本的Web应用程序。研究分析了参与者完成任务的情况、所需时间以及他们对AI辅助终端用户编码方法的接受度。

Result: 大多数研究参与者在合理时间内成功完成了任务，并表示支持AI辅助的终端用户编码作为终端用户开发的可行方法。这表明AI辅助的终端用户编码确实是一个可行的范式。

Conclusion: AI辅助的终端用户编码是终端用户开发的可行范式，可能在未来补充甚至替代传统的低代码/无代码平台。该方法具有显著优势，值得进一步研究和实践应用。

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [72] [Mechanistic Interpretability of Antibody Language Models Using SAEs](https://arxiv.org/abs/2512.05794)
*Rebonto Haque,Oliver M. Turnbull,Anisha Parsan,Nithin Parsan,John J. Yang,Charlotte M. Deane*

Main category: cs.LG

TL;DR: TopK SAEs能识别生物学相关特征但控制生成效果有限，Ordered SAEs能可靠实现生成控制但可解释性较差，两者在抗体语言模型解释中各有优劣。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏自编码器在蛋白质语言模型中的机制可解释性应用，特别是针对自回归抗体语言模型p-IgGen，探索如何通过SAEs理解和控制模型生成。

Method: 使用TopK和Ordered两种稀疏自编码器技术分析p-IgGen模型，比较它们在特征识别和生成控制方面的表现。

Result: TopK SAEs能揭示生物学意义的潜在特征，但高特征概念相关性不保证对生成的控制；Ordered SAEs能可靠识别可控制特征，但激活模式更复杂且可解释性较差。

Conclusion: TopK SAEs适用于将潜在特征映射到概念，而Ordered SAEs在需要精确生成控制时更优，这推进了领域特定蛋白质语言模型的机制可解释性研究。

Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.

</details>


### [73] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文填补了多目标贝叶斯优化中HV近似算法文献空白，提供了完整的数学和算法描述


<details>
  <summary>Details</summary>
Motivation: 超体积（HV）贝叶斯优化是多目标决策的标准方法，但计算成本高，特别是HV改进计算。虽然HV盒分解能处理频繁的精确改进计算，但存在超多项式内存复杂度问题。现有近似算法缺乏严格的算法描述。

Method: 提供Couckuyt等人（2012）提出的近似算法的全面数学和算法细节，填补文献中的描述空白。

Result: 填补了多目标贝叶斯优化中HV近似算法的文献空白，提供了完整的数学描述和算法实现细节。

Conclusion: 本文通过提供近似算法的全面数学和算法描述，解决了多目标贝叶斯优化中HV计算的内存复杂度问题，为实际应用提供了理论支持。

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [74] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: NEAT是一种用于3D分子生成的邻域引导、高效、自回归的集合Transformer模型，通过将分子图视为原子集合并学习图边界上可接受标记的顺序无关分布，解决了自回归模型中原子顺序假设的问题。


<details>
  <summary>Details</summary>
Motivation: 自回归模型是3D分子结构生成的有前景的替代方案，但存在关键限制：假设了标记顺序。文本有自然顺序，但给定分子图前缀的下一个标记预测应该对原子排列不变。先前工作通过使用规范顺序或焦点原子来回避这个问题，作者认为这是不必要的。

Method: NEAT（邻域引导、高效、自回归、集合Transformer）将分子图视为原子集合，使用自回归流模型学习图边界上可接受标记的顺序无关分布。模型具有原子级排列不变性。

Result: NEAT在3D分子生成方面接近最先进性能，具有高计算效率和原子级排列不变性，为可扩展分子设计建立了实用基础。

Conclusion: NEAT通过将分子图视为集合并学习顺序无关分布，解决了自回归模型在3D分子生成中的原子顺序假设问题，实现了高效且排列不变的分子生成。

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [75] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 提出一种简单的后训练方法，通过稀疏正则化使Transformer注意力稀疏化，在保持性能的同时将注意力连接减少到约0.3%，揭示Transformer计算存在大量冗余


<details>
  <summary>Details</summary>
Motivation: 探索Transformer注意力机制中是否存在大量冗余计算，以及是否可以通过稀疏化来获得更结构化、可解释的模型，而不牺牲性能

Method: 采用简单的后训练方法，在约束损失目标下应用灵活的稀疏正则化，使Transformer注意力稀疏化，保留原始预训练损失的同时大幅减少注意力连接

Result: 在高达10亿参数的模型上，能够将注意力连接减少到约0.3%的边，同时保持原始预训练损失；局部稀疏性级联为全局电路简化，任务特定电路涉及更少的组件（注意力头和MLP），连接边减少高达100倍

Conclusion: Transformer注意力可以变得稀疏数个数量级，表明其大部分计算是冗余的，稀疏性可以作为构建更结构化、可解释模型的指导原则

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [76] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: 提出了一种基于神经一致性的模型选择方法，仅需少量无标签目标域样本即可有效选择预训练模型检查点，在数据稀缺、无标签、分布外场景下显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前微调预训练大视觉模型时，如何从大量训练检查点中选择最佳起点仍是一个开放问题。当目标任务数据稀缺、无标签且分布外时，依赖分布内验证数据的传统方法变得不可靠或不适用。

Method: 提出神经一致性的新概念，通过表征模型在源域和目标域的激活统计特性，设计高数据效率的模型选择方法。该方法仅需少量无标签目标域样本即可工作。

Result: 在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K和iNaturalist等目标域进行实验，并在多个元学习设置中评估。相比现有基线，该方法在不同目标域上显著提升了泛化性能。

Conclusion: 神经一致性是一个强大的原则，不仅可用于模型选择，还展示了在训练数据选择中的有效性，为解决数据稀缺场景下的模型选择问题提供了新思路。

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [77] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: MaxShapley是一种用于生成式搜索引擎中公平归因的高效算法，通过可分解的最大和效用函数实现线性计算复杂度，相比传统Shapley值的指数计算成本大幅降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的生成式搜索引擎正在取代传统搜索，改变了信息提供者的补偿方式。为了维持这个生态系统，需要公平的机制来根据内容提供者对生成答案的贡献进行归因和补偿。

Method: 提出MaxShapley算法，这是著名Shapley值的一个特例。它利用可分解的最大和效用函数来计算归因，文档数量的计算复杂度为线性，而不是Shapley值的指数成本。算法应用于使用检索增强生成（RAG）的生成式搜索管道。

Result: 在三个多跳QA数据集（HotPotQA、MuSiQUE、MS MARCO）上评估，MaxShapley实现了与精确Shapley计算相当的归因质量，同时消耗其一小部分计算资源。例如，在相同归因准确度下，相比先前最先进方法减少了高达8倍的资源消耗。

Conclusion: MaxShapley为生成式搜索引擎提供了一种高效、公平的内容归因机制，能够在保持高质量归因的同时显著降低计算成本，有助于构建可持续的生成式搜索生态系统。

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [78] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的机器学习方法，用于预测润滑剂的蒸汽压，以发现适合太空环境的新型液体润滑剂。


<details>
  <summary>Details</summary>
Motivation: 太空中的运动机械组件需要液体润滑剂，但现有适合真空条件的液体润滑剂种类有限且各有局限性，这限制了机械组件的设计。

Method: 采用数据驱动的机器学习方法，结合高通量分子动力学模拟和实验数据库数据进行训练，模型设计注重可解释性以识别化学结构与蒸汽压之间的关系。

Result: 机器学习模型能够预测蒸汽压，基于模型分析提出了几种有潜力的候选分子，可用于未来的太空润滑剂应用。

Conclusion: 该研究为发现新型太空适用液体润滑剂提供了有效的虚拟筛选方法，有助于克服现有润滑剂的局限性，推动太空机械组件的发展。

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [79] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 该论文提出使用α-散度家族来优化LLM，以解决RL训练导致的多样性损失问题，在Lean定理证明基准上实现了覆盖率和精度的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: RL训练LLM时会导致显著的多样性损失，这是因为RL隐式优化了"模式寻找"或"零强制"的反向KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。

Method: 从显式目标分布出发（通过过滤错误答案同时保留正确答案的相对概率），使用α-散度家族来近似这个目标分布，该家族统一了先前方法并允许通过插值在模式寻找和覆盖散度之间控制精度-多样性权衡。

Result: 在Lean定理证明基准上，该方法在覆盖率-精度帕累托前沿上实现了最先进的性能，在覆盖率轴上优于所有先前方法。

Conclusion: 通过α-散度家族显式控制精度-多样性权衡的方法能够有效解决RL训练LLM时的多样性损失问题，在定理证明任务中取得了更好的覆盖性能。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


### [80] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: KQ-SVD是一种直接对注意力矩阵进行最优低秩分解的新方法，相比现有只压缩键或联合嵌入查询和键的方法，能更精确地保持注意力输出。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批量大小的增长，键值缓存成为大语言模型推理的主要内存瓶颈。现有压缩方法通常只对键进行低秩分解或联合嵌入查询和键，但这些方法忽略了注意力本质上依赖于它们的内积，因此不是近似注意力矩阵的最优策略。

Method: 提出KQ-SVD方法，通过闭式解直接对注意力矩阵进行最优低秩分解。该方法针对冗余的真正来源，在压缩下能更高保真度地保持注意力输出。

Result: 在LLaMA和Mistral模型上的广泛评估表明，KQ-SVD方法在投影质量方面始终提供优越性能。

Conclusion: KQ-SVD是一种简单且计算高效的方法，通过直接对注意力矩阵进行最优低秩分解，相比现有方法能更好地解决键值缓存的内存瓶颈问题。

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [81] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: 该论文探讨了使用机器学习模型为美国年度商业调查创建合成公共使用微观数据样本，以解决商业数据匿名化挑战并保护受访者隐私。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力提升和大数据可用性增加，传统匿名化方法面临重新识别风险，可能违反调查受访者的保密承诺。商业数据尤其具有挑战性，因为企业缺乏匿名性且某些行业在特定地理区域容易被识别。

Method: 使用机器学习模型构建基于年度商业调查的合成公共使用微观数据样本，并开发了2007年企业主调查的两个合成版本作为示例。通过计量经济学复制分析验证合成数据的真实性。

Result: 合成数据能够保留原始数据的关键统计特征，同时不包含任何真实个体或企业的记录。通过复制《小企业经济学》中的高影响力分析，证明了合成数据与真实数据的相似性。

Conclusion: 合成公共使用微观数据样本为解决商业调查数据的隐私保护问题提供了可行方案，同时保持了数据的分析价值。该方法为年度商业调查等商业数据的公开使用开辟了新的可能性。

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>
