<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.LG](#cs.LG) [Total: 34]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了通过增加推理时计算来提升预训练大语言模型预测准确性的方法，特别关注问题分解方式和子问题组织结构，统一了多种主流技术并分析其优缺点。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过推理时计算分配来提升预训练大语言模型的预测准确性，为不同应用场景提供合适的技术选择指导。

Method: 对测试时扩展方法进行分类，重点分析问题分解为子问题的方式以及子问题的拓扑组织结构（顺序、并行或树状结构），将Chain-of-Thought、Branch-Solve-Merge、Tree-of-Thought等方法统一在同一框架下。

Result: 建立了统一的分析框架来理解不同测试时扩展技术，系统梳理了各种方法的优势和局限性。

Conclusion: 这些技术为提升大语言模型性能提供了有效途径，未来研究应继续探索更优的问题分解和组织策略。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型在推理过程中很早就对最终结果做出内部承诺，即使需要更长的输出才能得到明确答案。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维推理中，语言模型在何时内部确定最终结果，以及这种早期承诺对模型推理过程的影响。

Method: 通过在推理过程的前t个标记后训练线性分类器来预测隐藏状态，分析模型内部表示的演化过程。

Result: 发现模型在仅经过几个推理标记后就能高度预测最终正确性，对于更难的问题，预测准确率下降揭示了选择偏差。

Conclusion: 推理模型的内部成功评估在很少的标记后就会出现，这对可解释性和推理时控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: LiveCLKTBench是一个自动化生成管道，用于隔离和测量大型语言模型的跨语言知识迁移，通过时间敏感的知识实体来评估跨语言转移能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的跨语言知识迁移具有挑战性，因为目标语言中的正确答案可能来自真正的迁移，也可能来自预训练期间的先前接触。需要一种方法来隔离和测量真正的跨语言知识转移。

Method: 开发了LiveCLKTBench自动化生成管道，识别现实领域中的自包含、时间敏感知识实体，基于时间发生进行过滤，并验证模型的知识。使用这些有效实体的文档生成事实性问题，翻译成多种语言来评估跨语言边界的可转移性。

Result: 评估了多个LLM在五种语言上的表现，发现跨语言转移受到语言距离的强烈影响，并且在语言方向上通常是不对称的。更大的模型改进了转移，但收益随规模增加而减少，并且在不同领域间存在差异。

Conclusion: 这些发现为多语言转移提供了新的见解，并证明了LiveCLKTBench作为未来研究的可靠基准的价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS是一个轻量级、可解释的控制框架，通过基于模型的反馈回路在解码过程中动态调节注意力头，以减少LLM的事实性错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成流畅但事实错误的陈述，这是因为它们在上下文知识和参数知识之间的注意力分配存在问题。理解和引导这种内部行为对于可信部署和模型机制的科学可解释性至关重要。

Method: 引入COMPASS框架，通过上下文依赖分数(CRS)量化上下文依赖度，并使用PID控制器动态调节注意力头，以保持事实一致性，无需重新训练或多轮解码。

Result: 在多个基准测试中，COMPASS持续减少了上下文幻觉率（绝对减少2.8%到5.8%），并揭示了不同注意力头对证据对齐的贡献。

Conclusion: 反馈驱动的可解释性为科学理解LLM行为提供了一条途径，COMPASS框架在减少事实错误的同时提供了对模型内部机制的可解释洞察。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 本文评估了手动和自动韵律分割标注对巴西葡萄牙语语音合成质量的影响，发现使用韵律分割训练能产生更清晰和自然的语音，手动分割引入的变异性有助于更自然的韵律。


<details>
  <summary>Details</summary>
Motivation: 自发语音合成面临捕捉对话自然流程的挑战，包括话轮转换、停顿和不流畅性。现有系统主要通过隐式建模韵律特征来生成语音，但具有明确韵律分割的数据集构建及其对自发语音合成的影响尚未充分探索。

Method: 使用非自回归模型FastSpeech 2，在巴西葡萄牙语数据集上评估手动和自动韵律分割标注的效果，比较两种分割方法对语音合成质量的影响。

Result: 实验结果显示，使用韵律分割训练产生了略微更清晰和声学自然的语音。自动分割产生更规则的片段，而手动韵律分割引入更大变异性，有助于更自然的韵律。两种方法都能重现预期的核重音模式，但韵律模型更接近自然的预核轮廓。

Conclusion: 韵律分割标注对提高语音合成质量有积极影响，手动分割的变异性有助于生成更自然的韵律。所有数据集、源代码和训练模型已公开提供以支持可重复性和未来研究。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [Human or LLM as Standardized Patients? A Comparative Study for Medical Education](https://arxiv.org/abs/2511.14783)
*Bingquan Zhang,Xiaoxiao Liu,Yuchi Wang,Lei Zhou,Qianqian Xie,Benyou Wang*

Main category: cs.CL

TL;DR: EasyMED是一个多智能体框架，结合患者智能体、辅助智能体和评估智能体，用于标准化患者模拟，在SPBench基准测试中表现出与人类标准化患者相当的学习效果，同时提供更好的灵活性、心理安全性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 标准化患者在临床技能训练中不可或缺，但存在成本高、灵活性差和难以扩展的问题。现有的基于大语言模型的模拟器虽然成本较低，但行为不一致且缺乏与人类标准化患者的严格比较。

Method: 提出EasyMED多智能体框架，包括：患者智能体实现真实对话，辅助智能体确保事实一致性，评估智能体提供可操作的反馈。同时引入SPBench基准，包含14个专科的真实SP-医生互动数据和8个专家定义的评估标准。

Result: 实验表明，EasyMED与人类标准化患者的学习效果相当，同时对基线水平较低的学生产生更大的技能提升，并提供改进的灵活性、心理安全性和成本效率。

Conclusion: EasyMED框架在标准化患者模拟方面表现出色，能够匹配人类SP的学习成果，同时为低基线学生提供更大收益，并具有更好的灵活性、心理安全性和成本效益。

Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.

</details>


### [7] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: HTP通过分层令牌预置解决LLM嵌入中的信息流限制问题，在块级添加摘要令牌并改进池化方法，显著提升长文档嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型由于因果注意力机制限制了从后向前的信息流，降低了表示质量。现有方法使用单一摘要令牌会过度压缩信息，特别是在长文档中表现不佳。

Method: 提出分层令牌预置(HTP)：将输入分区为块，在每个后续块前预置块级摘要令牌，创建多个后向信息流路径；用均值池化替换最后令牌池化以解决读取级过度压缩问题。

Result: 在11个检索数据集和30个通用嵌入基准测试中实现一致性能提升，特别是在长上下文设置中表现突出。

Conclusion: HTP作为一种简单、架构无关的方法，能够增强零样本和微调模型，为优质长文档嵌入提供了可扩展的途径。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [8] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 提出了一个数学基础框架来理解、测量和减轻大语言模型的幻觉问题，结合概率建模、信息论、三角信号分析和贝叶斯不确定性估计等方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但仍然容易产生幻觉（看似合理但事实错误或无依据的输出），需要系统性地解决这一问题。

Method: 使用概率建模、信息理论、三角信号分析和贝叶斯不确定性估计来分析自回归过程中的错误累积，提出改进的不确定性度量方法，并开发对比解码、检索增强接地、事实对齐和弃权等缓解策略。

Result: 建立了一个统一的理论框架，将校准、检索和对齐等最新进展联系起来，支持更安全和可靠的大语言模型。

Conclusion: 该数学框架为理解和缓解LLM幻觉提供了理论基础，有助于开发更安全可靠的语言模型系统。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [9] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA是一个学生感知的数学辅导框架，通过整合学生画像、记忆和遗忘动态来实现个性化学习，在LLM驱动的辅导系统中表现出优越的学习效果和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅导系统大多未能捕捉学生知识随熟练度、概念差距和遗忘模式的动态演变，特别是在需要精细校准学生掌握水平和认知保持的数学辅导中。

Method: TASA维护结构化学生画像记录熟练度档案和事件记忆记录先前学习交互，通过结合连续遗忘曲线和知识追踪，动态更新学生掌握状态并生成情境适当、难度校准的问题和解释。

Result: 实证结果表明，TASA相比代表性基线方法实现了更优越的学习成果和更自适应的辅导行为。

Conclusion: 在基于LLM的辅导系统中建模时间遗忘和学习者画像具有重要意义，TASA框架为此提供了有效解决方案。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [10] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 本文提出了一个评估多语言视觉语言模型在印度语言中性能的可扩展框架，并创建了HinTel-AlignBench基准测试，发现模型在印度语言上的表现相比英语平均下降8.3分（印地语）和5.5分（泰卢固语）。


<details>
  <summary>Details</summary>
Motivation: 解决当前多语言VLM评估的四个主要局限：依赖未验证的自动翻译、任务/领域覆盖范围窄、样本量有限、缺乏文化和本地来源的问答数据。

Method: 提出了半自动数据集创建框架，结合回译、过滤和人工验证；创建了最全面的印地语和泰卢固语视觉语言基准，包括改编的英语数据集和本地新颖的印度数据集。

Result: 在5个任务中的4个任务中，所有模型在印度语言上的表现相比英语都出现退化，印地语平均退化8.3分，泰卢固语平均退化5.5分。

Conclusion: 多语言多模态理解存在明显性能差距，需要改进，并分类了常见失败模式以突出具体改进领域。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [11] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文首次全面研究将内在维度与可解释的文本属性联系起来，发现ID与基于熵的指标互补，显示明显的体裁分层，并通过稀疏自编码器识别了影响ID的因果特征。


<details>
  <summary>Details</summary>
Motivation: 内在维度是现代LLM分析的重要工具，但其文本决定因素仍未得到充分探索。本文旨在通过可解释的文本属性来理解ID的底层机制。

Method: 采用交叉编码器分析、语言特征和稀疏自编码器(SAEs)进行多角度研究，包括控制长度变量、分析不同体裁文本，以及进行导向实验验证因果关系。

Result: 1) ID与熵指标在控制长度后不相关；2) 不同体裁呈现稳定分层：科学文本低ID(~8)，百科内容中ID(~9)，创意/观点写作高ID(~10.5)；3) SAEs识别出科学信号降低ID，人性化信号增加ID。

Conclusion: 当代模型认为科学写作相对"简单"，而小说、观点和情感表达需要更多表示自由度。研究为ID的正确使用和基于ID结果的合理解释提供了实用指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [12] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: OEMA是一个零样本临床命名实体识别框架，通过多智能体协作解决传统方法对标注数据依赖和提示工程问题，在MTSamples和VAERS数据集上达到最先进的精确匹配性能。


<details>
  <summary>Details</summary>
Motivation: 监督式临床NER模型需要昂贵的标注数据，而现有零样本方法在示例选择粒度和提示集成方面存在不足，需要开发更有效的零样本临床NER框架。

Method: 提出OEMA框架，包含三个组件：自标注器生成示例、鉴别器通过SNOMED CT过滤示例、预测器使用实体描述进行准确推理，通过本体引导推理和多智能体协作实现零样本NER。

Result: 在MTSamples和VAERS数据集上，OEMA实现了最先进的精确匹配性能，在相关匹配下与监督式BioClinicalBERT相当并超越CRF模型。

Conclusion: OEMA通过本体引导推理和多智能体协作解决了零样本NER的关键挑战，实现了接近监督学习的性能，在临床NLP应用中具有良好前景。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [13] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: 本文提出Context Cascade Compression (C3)方法，通过级联不同大小的LLM来处理长文本压缩和解码任务，在20倍压缩比下达到98%解码准确率，显著优于DeepSeek-OCR的约60%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决百万级token长文本任务对大型语言模型带来的计算和内存挑战，探索文本压缩的上限。

Method: 使用级联的两个不同大小LLM：小型LLM作为第一阶段将长文本压缩为一组潜在token（如32或64长度），大型LLM作为第二阶段在压缩后的上下文中执行解码任务。

Result: 在20倍压缩比下达到98%解码准确率，40倍压缩比下仍保持约93%准确率，显著优于DeepSeek-OCR的约60%准确率。

Conclusion: C3压缩在上下文压缩领域表现出优越性能和可行性，为光学字符压缩、OCR等相关领域的压缩比上限提供了参考。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [14] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: 该论文报告了TeamNRC在BHASHA-Task 1语法错误纠正共享任务中的参与结果，重点关注使用不同规模语言模型（4B到大型专有模型）的零/少样本提示方法，在泰卢固语和印地语中分别获得第4和第2名，并扩展到泰米尔语、马拉雅拉姆语和孟加拉语的实验。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型在印度语言语法错误纠正任务中的潜力，并关注创建高质量数据集和适合印度语言脚本的评估指标的问题。

Method: 使用零/少样本提示方法，应用不同规模的语言模型（从4B到大型专有模型）进行语法错误纠正。

Result: 在泰卢固语中获得第4名（GLEU得分83.78），在印地语中获得第2名（GLEU得分84.31），并将实验扩展到其他三种印度语言。

Conclusion: 研究突显了小型语言模型的潜力，并总结了为印度语言脚本创建高质量数据集和适当评估指标的相关关切。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [15] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 本文使用SetFit框架对阿拉伯方言酒店评论进行情感分析，在AHaSIS共享任务中取得73%的F1分数，排名第12位。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言情感分析面临语言多样性和标注数据稀缺的挑战，特别是在酒店评论等专业领域。

Method: 采用SetFit（句子变换器微调）框架，这是一种数据高效的少样本学习技术。

Result: 在官方评估集上，系统取得了73%的F1分数，在26个参与者中排名第12位。

Conclusion: 这项工作凸显了少样本学习在处理专业领域中细微阿拉伯方言文本数据稀缺问题方面的潜力。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [16] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2是一个扩展更新的西班牙语/英语医疗问答数据集，包含超过12,000个来自西班牙专业考试的问题，用于评估LLM在医疗推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 响应高质量数据集的需求，捕捉医疗推理的语言和概念复杂性。

Method: 扩展数据集至12,000+问题，使用提示、RAG和基于概率的答案选择方法对多个开源LLM进行基准测试，并提供多语言版本。

Result: 性能主要由模型规模和内在推理能力驱动，复杂推理策略获得的提升有限。

Conclusion: HEAD-QA v2成为推进生物医学推理和模型改进研究的可靠资源。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [17] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 开发了一个符合航空维修规范的检索系统，通过LLM重排序和语义搜索技术，将AMT查找手册的时间从6-15分钟减少到18秒，准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 解决飞机维修技术人员在查找认证手册上花费过多时间的问题（占工作时间的30%），在严格遵守航空维修规范的前提下提高效率。

Method: 构建基于ATA章节层次结构的版本鲁棒嵌入，使用视觉语言解析结构化认证内容，在现有认证查看器基础上集成语义检索功能。

Result: 在49k合成查询中实现>90%检索准确率；10名持证AMT的双语对照研究显示90.9%的top-10成功率，查找时间减少95%（从6-15分钟降至18秒）。

Conclusion: 语义检索技术能够在严格监管约束下运行，并显著减少现实世界多语言MRO工作流程中的操作负担。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [18] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: SPRING Lab开发的ASR系统，专注于改进8种语言33种方言的语言和方言识别，在ASRU MADASR 2.0挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 改进自动语音识别系统在多语言多方言环境下的语言和方言识别能力，特别是在限制使用额外数据的情况下开发多语言系统。

Method: 使用多解码器架构和音素通用标签集作为中间表示的新颖训练方法，以及各种保留音素空间增益的方法。

Result: 在音素空间性能超过基线，在Track 2中3种语言的WER/CER击败基线，在所有参赛团队中获得最高的语言ID和方言ID准确率。

Conclusion: 提出的多解码器架构和音素CLS方法有效提升了多语言ASR系统的性能，特别是在语言和方言识别方面取得了显著成果。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [19] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster是一个新颖的LLM原生文本聚类框架，通过动态内存和双提示策略实现无调优的端到端聚类，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在文本聚类中存在两个根本限制：缺乏迭代优化的状态记忆和难以管理聚类粒度，导致需要复杂的外部模块流水线，无法实现真正的端到端方法。

Method: 提出LLM-MemCluster框架，将聚类重新概念化为完全LLM原生的任务，利用动态内存实现状态感知，采用双提示策略让模型推理并确定聚类数量。

Result: 在多个基准数据集上的评估显示，这个无调优框架显著且一致地优于强基线方法。

Conclusion: LLM-MemCluster为基于LLM的文本聚类提供了一个有效、可解释且真正端到端的范式。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [20] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 本文提出了LPDS数据结构和pelican nlp工具包，旨在解决语言处理领域缺乏标准化和可重复性的问题，提供端到端的语言数据处理流程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型等AI语言处理技术的发展带来了语言数据分析方法的演进，但同时也暴露了语言数据组织和共享缺乏标准化、处理方法缺乏可重复性等挑战。

Method: 1. 提出LPDS数据结构，受神经科学BIDS标准启发，提供语言研究的文件夹结构和文件命名规范；2. 开发pelican nlp模块化Python包，支持从数据清洗到复杂语言和声学特征提取的完整处理流程，可通过单一配置文件执行。

Result: LPDS和pelican nlp共同提供了语言数据的端到端处理管道，能够生成预处理的语言数据或标准化的语言和声学特征提取结果及其聚合。

Conclusion: LPDS和pelican nlp为语言数据处理提供了标准化解决方案，确保方法透明度并增强可重复性，有助于推动语言处理研究的标准化发展。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [21] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: Mera Multi是一个针对俄语的多模态评估框架，包含文本、图像、音频、视频四种模态的18个评估任务，专门为俄语多模态大语言模型设计。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型发展迅速，但其智能、局限性和风险理解不足，特别是针对俄语缺乏多模态基准测试。

Method: 构建了18个全新的评估数据集，涵盖通用模型和模态特定架构，采用基于指令的评估方法，包含防泄漏机制如水印和私有集许可。

Result: 提供了闭源和开源模型的基线结果，建立了统一的多模态能力分类体系和评估标准。

Conclusion: 该基准不仅填补了俄语多模态评估的空白，其方法论也可复制到其他斯拉夫语系语言的多模态基准构建中。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [22] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: HSKBenchmark是首个用于中文二语习得的分阶段建模和写作评估基准，涵盖HSK 3-6级，包含676万词符的真实教材、1.6万合成指令样本、30个测试主题和基于语言学的评估系统。


<details>
  <summary>Details</summary>
Motivation: 语言习得对于揭示人类语言智能本质至关重要，但控制人类学习者语言输入的实验在伦理和实践上不可行，这给语言习得建模的可验证性和可扩展性带来挑战。

Method: 提出了课程调优框架，从初级到高级水平训练模型；构建了基于语法覆盖、写作错误、词汇句法复杂度和整体评分的评估系统；基于1万篇学习者作文微调了HSKAgent。

Result: 实验结果表明HSKBenchmark不仅能有效建模中文二语习得，还能作为LLM动态写作评估的可靠基准。微调后的LLM写作表现与高级人类学习者相当，并展现出类似人类的习得特征。

Conclusion: HSKBenchmark、HSKAgent和检查点可作为基础工具和资源，为未来语言习得建模和LLM可解释性研究铺平道路。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [23] [Area-Optimal Control Strategies for Heterogeneous Multi-Agent Pursuit](https://arxiv.org/abs/2511.15036)
*Kamal Mammadov,Damith C. Ranasinghe*

Main category: cs.MA

TL;DR: 提出了一种多智能体追逃游戏的新策略，针对多个速度不同的快速追捕者和单个慢速逃避者，通过几何区域定义和梯度优化实现实时控制。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体追逃游戏中，追捕者速度不同且快于逃避者时的协同捕获问题，需要一种计算高效且能保证捕获的策略。

Method: 定义逃避者的安全可达集作为各追捕者-逃避者对阿波罗尼奥斯圆的交集，将其面积最小化作为零和博弈目标，推导面积对位置的解析梯度得到最优控制律。

Result: 仿真表明基于梯度的控制能有效引导追捕者系统性地缩小逃避者的安全区域，实现保证捕获，且计算效率高可实时实施。

Conclusion: 这种面积最小化方法为协同捕获提供了清晰的几何目标，通过解析梯度优化实现了高效实时的控制策略。

Abstract: This paper presents a novel strategy for a multi-agent pursuit-evasion game involving multiple faster pursuers with heterogenous speeds and a single slower evader. We define a geometric region, the evader's safe-reachable set, as the intersection of Apollonius circles derived from each pursuer-evader pair. The capture strategy is formulated as a zero-sum game where the pursuers cooperatively minimize the area of this set, while the evader seeks to maximize it, effectively playing a game of spatial containment. By deriving the analytical gradients of the safe-reachable set's area with respect to agent positions, we obtain closed-form, instantaneous optimal control laws for the heading of each agent. These strategies are computationally efficient, allowing for real-time implementation. Simulations demonstrate that the gradient-based controls effectively steer the pursuers to systematically shrink the evader's safe region, leading to guaranteed capture. This area-minimization approach provides a clear geometric objective for cooperative capture.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 该论文介绍了FERMAT强化学习环境，用于自动化发现数学理论和定理证明，并探索了使用进化算法自动评估数学对象有趣性的方法。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论发现这一重大挑战，通过强化学习环境建模概念发现和定理证明过程。

Method: 引入FERMAT强化学习环境，使用符号化动作建模数学概念发现；采用基于LLM的进化算法合成有趣性度量，特别引入了函数抽象技术。

Result: 在初等数论和有限域领域，基于LLM的进化算法相比硬编码基线有显著改进，能够发现非平凡的有趣性度量。

Conclusion: FERMAT环境为数学理论发现开辟了新的强化学习研究方向，基于LLM的进化算法在自动评估数学对象有趣性方面表现出色，代码已开源。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [25] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录回放交互、支持对智能体信念的查询和反事实证据注入来研究信念形成。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛问题，提供一种可重复的方法来观察和测试智能体信念动态，这在人类专家中难以实现。

Method: 构建包含多专业智能体（神经科医生、传染病专家等）的医疗案例模拟器，使用共享时间戳电子病历，通过断点查询和反事实证据注入来分析信念变化。

Result: 智能体信念往往反映现实世界学科立场，包括过度依赖经典研究和抵制反证，这些信念可以被追踪和询问，这在人类专家中不可能实现。

Conclusion: Ask WhAI通过使信念动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了可重复的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [26] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 本文提出了一种完全自动化的LLM辅助工作流，使用GPT-4o处理文本位置信息，并通过交叉验证三个独立的地理信息库来分配几何形状，为EM-DAT灾害数据库中的位置数据提供可靠性评分。


<details>
  <summary>Details</summary>
Motivation: 灾害数据库如EM-DAT通常以非结构化文本形式报告位置信息，具有不一致的粒度或拼写，难以与空间数据集集成，这阻碍了风险评估和灾害风险减少工作。

Method: 开发了基于GPT-4o的LLM辅助工作流，处理文本位置信息，并通过交叉验证GADM、OpenStreetMap和Wikidata三个地理信息库来分配几何形状，同时基于源的一致性和可用性为每个位置分配可靠性评分。

Result: 应用于2000-2024年的EM-DAT数据集，该工作流成功地理编码了14,215个事件，覆盖17,948个独特位置，无需人工干预，覆盖所有灾害类型。

Conclusion: 该方法展示了LLMs从非结构化文本中提取和结构化地理信息的潜力，为相关分析提供了可扩展且可靠的方法，并允许灵活地重新映射到首选框架。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [27] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 本文记录了Project Rachel行动研究项目，创建并追踪了一个完整的AI学术身份Rachel So，通过发布AI生成的研究论文来研究学术生态系统对AI作者身份的反应。


<details>
  <summary>Details</summary>
Motivation: 研究学术生态系统对AI作者身份的反应，为关于超级人类、超能力AI系统下学术交流未来的必要讨论提供实证行动研究数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10+篇AI生成的研究论文，并追踪其被引用和同行评审邀请情况。

Result: Rachel So成功发表了10多篇论文，获得了引用，并收到了同行评审邀请，表明学术系统在一定程度上接受AI作者身份。

Conclusion: 这项工作为关于AI作者身份对出版商、研究人员和整个科学系统影响的讨论提供了实证数据，强调了在超能力AI系统时代重新思考学术交流模式的必要性。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [28] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本文提出了一种概率方法来量化AI系统训练和测试数据集的代表性，通过比较场景套件特征分布与目标操作域特征分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信度和安全性，关键在于训练和测试数据集的代表性等安全属性。本文关注代表性，即场景数据反映系统设计安全运行条件（操作设计域）或预期遇到条件（目标操作域）的程度。

Method: 采用概率方法比较场景套件特征分布与目标操作域特征分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值代表性估计而非单一值。

Result: 通过数值示例展示了在依赖性和先验不确定性条件下，比较场景套件与推断目标操作域在天气、道路类型、时间等操作类别上的分布，估计了局部（类别间）和全局代表性区间。

Conclusion: 所提出的不精确贝叶斯方法能够量化数据集代表性，生成不确定性感知的区间估计，为AI系统安全评估提供更可靠的代表性度量。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [29] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: 本研究开发了OpenBioLLM，一个模块化多智能体框架，用于基因组问答。它使用开源模型替代GeneGPT的专有模型，通过智能体专业化实现工具路由、查询生成和响应验证，在保持性能的同时显著降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: GeneGPT虽然解决了基因组问答的复杂推理问题，但其依赖专有模型存在可扩展性、运营成本、数据隐私和泛化能力方面的限制。本研究旨在用开源模型重新实现GeneGPT，并开发更高效的模块化多智能体框架。

Method: 首先使用Llama 3.1、Qwen2.5和Qwen2.5 Coder等开源模型在单体架构中复现GeneGPT，识别其局限性。然后开发OpenBioLLM，这是一个模块化多智能体框架，引入工具路由、查询生成和响应验证的智能体专业化，实现协调推理和基于角色的任务执行。

Result: OpenBioLLM在90%以上的基准任务中匹配或优于GeneGPT，在Gene-Turing和GeneHop上的平均得分分别为0.849和0.830。使用较小的开源模型，无需额外微调或工具特定预训练。模块化多智能体设计使基准任务延迟降低40-50%，显著提高效率而不影响模型能力。

Conclusion: 研究结果表明开源多智能体系统在基因组问答方面具有巨大潜力。OpenBioLLM提供了一个更高效、可扩展且成本效益更高的解决方案，同时解决了专有模型的局限性。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [30] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLM解决RAC问题，通过提取动作和问题、逐步执行动作推导最终状态，然后评估查询来获得答案。


<details>
  <summary>Details</summary>
Motivation: 为了解决RAC（动作与变化推理）问题，提出一个结合神经和符号方法的框架，利用LLM的强大能力来处理复杂的推理任务。

Method: 提取RAC问题中的基本元素（动作和问题），逐步执行每个动作来推导最终状态，然后在该状态下评估查询以获得答案。

Result: 在多个RAC基准测试中表现出色，在不同基准、领域、LLM主干和RAC任务类型上都能实现强劲性能。

Conclusion: ProRAC框架能够有效解决RAC问题，展现了在多样化场景下的鲁棒性和通用性。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [31] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于LLM的多智能体框架，通过三个专业智能体（科学家、提取器、测试器）的协作，结合外部领域知识和丰富的定性反馈机制，实现知识驱动的自动特征提取。


<details>
  <summary>Details</summary>
Motivation: 现有AutoFE方法受限于单一LLM架构、简单的定量反馈，以及缺乏系统整合外部领域知识的能力，需要更智能的特征工程方法。

Method: 采用去中心化的三智能体系统：科学家负责发现特征，提取器生成特征，测试器验证特征。引入丰富的定性反馈机制和"泛滥-修剪"策略，结合RAG系统整合外部知识。

Result: 在19个分类和9个回归数据集上显著优于最先进方法，并能发现新颖可测试的假设，如识别心肌数据集中的新潜在生物标志物。

Conclusion: Rogue One不仅提升了特征工程的性能，还增强了特征的可解释性和科学发现能力，展示了多智能体框架在自动特征提取中的巨大潜力。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [32] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是首个端到端评估大型推理模型安全性的基准，从输入、中间推理到最终输出全面评估安全风险，包括输入特征化、细粒度输出分析和人类安全对齐三个核心组件。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维链提升答案质量，但这也引入了新的安全风险：有害内容可能在推理过程中被微妙注入、逐渐显现或被误导性理由所合理化。现有安全评估主要关注输出级判断，很少捕捉推理过程中的动态风险。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计，明确考虑受影响群体和严重程度；2) 细粒度输出分析：通过微思维分块机制将长推理轨迹分割成语义连贯单元，在十个安全维度上进行细粒度评估；3) 人类安全对齐：用专门设计的人类标注验证基于LLM的评估。

Result: 在19个大型推理模型上的评估表明，SafeRBench能够实现详细的多维安全评估，从多个角度提供风险和保护机制的洞察。

Conclusion: SafeRBench填补了现有安全评估在推理过程动态风险捕捉方面的空白，为大型推理模型提供了全面、多维度的安全评估框架。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [33] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中版权内容的框架，通过将LLM的过度自信转化为优势，实现无需经验阈值的高精度版权检测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在大量数据集上训练，其中可能包含受版权保护的内容，现有成员推理攻击方法因LLM的过度自信、缺乏真实训练数据和依赖经验阈值而面临挑战。

Method: 采用两阶段策略：(1)将文件分割成小片段减少对大规模训练数据的依赖；(2)基于不确定性的无监督聚类消除经验调优阈值需求，利用不确定性模式区分训练数据和非训练数据。

Result: 在LLaMA 7b上平均平衡准确率达90.1%，在LLaMA2 7b上达91.6%，相比SOTA基线相对提升超过90%，最高达93.8%平衡准确率，在GPT-J 6B上保持高性能。

Conclusion: 这是首个将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具，成功将LLM过度自信从限制转化为优势。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [34] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 本文认为仅靠效率提升无法实现可持续的推理AI，需要将明确限制嵌入到这类系统的优化和治理中。


<details>
  <summary>Details</summary>
Motivation: AI研究正转向复杂问题解决，模型不仅优化模式识别还优化多步推理。计算能效提升接近物理极限，而新兴推理AI缺乏可比的需求饱和点，性能继续随指数级计算投资而扩展。

Method: 讨论研究和政策方向，将明确限制嵌入到推理AI系统的优化和治理中。

Result: 效率提升无法单独实现可持续的推理AI。

Conclusion: 需要在推理AI的优化和治理中嵌入明确限制，以实现可持续性。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [35] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文分析了AI研究中的两种基本智力观念：智力现实主义认为智力是单一、普遍的跨系统可度量能力；智力多元主义认为智力是多样、情境依赖且无法简化为单一度量的能力。这些隐性观念深刻影响研究方法、现象解释和风险评估。


<details>
  <summary>Details</summary>
Motivation: 揭示AI研究中隐含的智力观念如何影响实证证据的解释、研究方法和风险评估，促进对AI研究分歧的更清晰理解。

Method: 通过分析当前AI研究中的辩论，展示两种智力观念如何影响模型选择、基准设计、实验验证等方法论问题，以及对相同经验现象（如能力涌现、系统局限）的不同解释。

Result: 发现智力现实主义者和多元主义者在方法论、现象解释和风险评估方面产生根本不同的研究路径：前者寻求统一的对齐解决方案，后者关注跨领域的不同威胁需要情境特定解决方案。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的学术对话和研究发展。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [36] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个基于《文明V》的综合挑战环境，旨在同时测试强化学习智能体在多个相互关联的挑战中的综合推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大多数多任务基准测试只是简单聚合不相关的任务，无法真正评估智能体在多个相互关联挑战中的深度推理能力。需要创建一个能同时涌现多种典型RL挑战的单一环境。

Method: 基于《文明V》游戏设计综合挑战环境，让部分可观测性、信用分配、表示学习、巨大动作空间等挑战同时出现，要求智能体进行集成化的长期规划。

Result: 提出了Terra Nova环境，区别于简单聚合多任务的基准测试，强调挑战之间的相互关联性。

Conclusion: Terra Nova为评估强化学习智能体在复杂、相互关联环境中的综合推理能力提供了新的测试平台。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [37] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文提出IPR（交互式物理推理器）模型，通过世界模型推演来评估和强化VLM策略，并引入PhysCode物理中心动作编码，在1000+游戏中预训练后，在生存、好奇心和实用性三个层次上实现稳健性能，与GPT-5相当且在好奇心方面超越。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能像人类一样通过交互获得物理和因果推理能力，并在更多经验中持续改进。

Method: 提出IPR模型，使用世界模型推演来评分和强化VLM策略，引入PhysCode物理中心动作编码来对齐语义意图与动力学，为预测和推理提供共享动作空间。

Result: 在1000+游戏上预训练的IPR在三个层次上表现稳健，整体与GPT-5相当，在好奇心方面超越GPT-5，性能随训练游戏和交互步骤增加而提升，并能零样本迁移到未见游戏。

Conclusion: 物理中心的交互是实现持续改进物理推理能力的可行路径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [38] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: TIM框架通过多智能体LLM系统分析DeFi交易意图，结合DeFi意图分类法和多模态数据，显著优于传统机器学习模型和单LLM方法。


<details>
  <summary>Details</summary>
Motivation: DeFi交易理解面临智能合约交互复杂、链上链下因素多样、十六进制日志不透明等挑战，现有方法缺乏深度语义洞察。

Method: 提出TIM框架，基于扎根理论构建DeFi意图分类法，采用多智能体LLM系统，包括元级规划器动态协调领域专家分解意图分析任务，问题求解器处理多模态数据，认知评估器减轻幻觉并确保可验证性。

Result: 实验表明TIM显著优于机器学习模型、单LLM和单智能体基线方法。

Conclusion: 该工作有助于更可靠地理解DeFi用户动机，为复杂区块链活动提供情境感知解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [39] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: Agents4Science是首个由AI智能体作为主要作者和审稿人、人类作为合著者和共同审稿人的会议，探讨AI智能体在科学研究中的能力和人类-AI协作的意义。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体在科学研究中的应用日益增长，但对其作为科学家和审稿人的基本能力仍存在疑问，因此需要探索AI智能体在科学研究和评审中的实际能力。

Method: 组织Agents4Science会议，让AI智能体担任主要作者和审稿人，人类作为合著者和共同审稿人，通过实际会议运作来评估AI智能体的表现。

Result: 通过会议实践获得了关于AI智能体在科学研究和评审中能力的关键见解，并理解了人类-AI协作的潜在价值。

Conclusion: Agents4Science会议为探索AI智能体在科学研究和评审中的能力提供了重要平台，揭示了人类-AI协作在科学研究中的潜力和挑战。

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该论文研究了仅解码器Transformer中离散提示到最后一词隐藏状态的映射的注入性，证明了在温和条件下该映射在参数空间中通常是单射的，并通过几何诊断方法在实际模型中验证了这一性质。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer表示的单射性对于理解模型如何区分不同输入以及表示空间的可逆性具有重要意义，特别是在参数空间和实际实现中。

Method: 理论分析定义了碰撞判别式和单射层，证明了参数空间中的二分法；实证研究使用分离边界和共Lipschitz常数作为几何诊断工具，在LLaMA-3、Qwen和GPT-2模型上进行测试。

Result: 理论证明在温和条件下单射性在参数空间中通常是成立的；实证结果显示在完整精度和8位量化下未发现碰撞，4位量化会引入少量碰撞并显著降低共Lipschitz估计值。

Conclusion: Transformer表示在连续参数理想化下通常是单射的，其实际可逆性可以通过简单的几何诊断方法来探测。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [41] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了推导关系(DR)和推导能力(DC)的概念来评估LLMs的推理能力，开发了DEVAL评估框架测试主流模型，发现模型在DR识别方面表现一般，在应用DR解决问题时表现显著下降，并提出推导提示(DP)方法提升DC表现。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在数据上的推理能力是一个重要但尚未解决的研究问题。相比人类能够根据输入变化推导输出修改的推理模式，LLMs的这种基于抽象规则的推理能力尚未得到全面描述和评估。

Method: 正式定义推导关系(DR)和推导能力(DC)，提出系统构建的评估框架DEVAL，在7个主流任务中评估5个流行LLMs和1个大型推理模型，并提出推导提示(DP)的提示工程方法。

Result: 评估结果显示主流LLMs如GPT-4o和Claude3.5在DR识别方面表现中等，但在应用DR解决问题的场景中表现显著下降。推导提示(DP)方法使所有测试LLMs的DC平均提升15.2%，优于常用提示工程技术。

Conclusion: LLMs在推导关系识别方面有一定能力，但在实际应用推导关系解决问题时存在显著不足。推导提示方法能有效提升模型的推导能力，为改进LLMs的推理能力提供了有效途径。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [42] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 本文提出动态嵌套层次结构作为人工智能和机器学习的进化方向，通过允许模型在训练或推理过程中自主调整优化层级数量、嵌套结构和更新频率，解决现有模型在非平稳环境中的适应性问题，实现真正的终身学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型（包括大语言模型）在静态任务中表现出色，但在非平稳环境中由于僵化的架构而表现不佳，无法持续适应和终身学习。需要一种能够自主进化的模型结构来解决这些问题。

Method: 基于嵌套学习范式，提出动态嵌套层次结构，使模型能够自主调整优化层级数量、嵌套结构和更新频率，受神经可塑性启发，实现无预定义约束的自进化。

Result: 通过严格的数学公式、收敛性理论证明、表达能力边界和不同机制下的次线性遗憾分析，以及在语言建模、持续学习和长上下文推理中的实证演示，证明了动态嵌套层次结构的优越性能。

Conclusion: 动态嵌套层次结构为自适应通用智能奠定了基础性进展，解决了现有模型的前向遗忘问题，通过动态压缩上下文流和适应分布变化实现真正的终身学习。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [43] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出了GTPO算法，针对多轮工具集成推理任务中的强化学习训练挑战，通过细粒度奖励分配和自监督奖励塑造来改进现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多轮工具集成推理任务中面临奖励信号不足的问题，导致训练停滞，需要更精细的奖励机制。

Method: GTPO算法包含三个关键创新：轮级奖励分配、基于回报的优势估计和自监督奖励塑造，提供更细粒度的学习信号。

Result: 在多样化推理基准测试中，GTPO平均比GRPO方法提升了3.0%的性能。

Conclusion: GTPO算法有效解决了多轮工具集成推理中的强化学习训练挑战，为复杂数学推理任务提供了更先进的训练方法。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [44] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec是一个基于Transformer的金融推荐框架，解决了长序列用户交互、多产品协调建模等挑战，在金融领域超越了传统的树模型基线。


<details>
  <summary>Details</summary>
Motivation: 金融服务的实时推荐面临独特挑战：长序列用户交互产生时间异构上下文，多产品需要协调建模以平衡业务目标，而传统树模型虽然可解释但性能有限。

Method: 提出FinTRec框架，采用基于Transformer的架构，支持产品适配微调，实现跨产品信号共享，减少训练成本和技术债务。

Result: 通过历史模拟和实时A/B测试，FinTRec持续优于生产级树模型基线，在所有产品上提升了离线性能。

Conclusion: FinTRec证明了Transformer架构在金融推荐中的可行性，是首个全面解决技术和业务考量的统一序列推荐建模研究。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [45] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer引导的深度强化学习方法，用于优化电动垂直起降(eVTOL)飞机的起飞轨迹，以最小化能耗。该方法通过Transformer探索真实状态空间，显著降低了训练难度。


<details>
  <summary>Details</summary>
Motivation: eVTOL飞机的发展为缓解城市交通拥堵提供了机会，但需要开发最优起飞轨迹以最小化能耗。传统最优控制方法受限于问题维度和复杂性，而深度强化学习(DRL)训练难度大，限制了其应用。

Method: 提出Transformer引导的DRL方法，使用Transformer在每个时间步探索真实状态空间来缓解训练难度。应用于eVTOL无人机的最优起飞轨迹设计，通过调节功率和机翼垂直角度等控制变量，在满足起飞条件的同时最小化能耗。

Result: Transformer引导的DRL仅需4.57×10^6时间步完成训练，是普通DRL所需19.79×10^6时间步的25%。在最优能耗方面达到97.2%的准确率，优于普通DRL的96.3%。

Conclusion: Transformer引导的DRL在训练效率和最优设计验证方面均优于普通DRL，为eVTOL飞机的最优控制提供了更高效的解决方案。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [46] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 提出了LIT框架，通过强制LLMs使用外部可靠工具来解决实际问题，提高解决方案的可信度和可调试性。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理过程中缺乏透明度，限制了其在高风险领域的应用。即使有更好的选择，LLMs也可能选择不可靠且难以排查的解决方案。

Method: 基于现有LLMs的工具调用能力构建LIT框架，让LLMs选择最可靠且易于调试的解决方案路径，可能涉及多个顺序工具调用。引入了包含1300个问题的新基准数据集和可定制的可靠性成本函数。

Result: LLMs在使用LIT框架后能够实现更可靠和明智的问题解决，同时保持任务性能。

Conclusion: LIT框架能够有效提升LLMs解决方案的可靠性和可调试性，使其更适合高风险领域的应用。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [47] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: 本文提出了Causal-GCN框架，通过整合do-计算的后门调整来识别对阿尔茨海默病进展具有稳定因果影响的大脑区域，解决了传统图学习方法中混淆人口统计学和遗传因素的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度图学习方法在阿尔茨海默病分类中主要基于相关性，混淆了人口统计学、遗传因素与疾病特异性特征，需要开发能够识别稳定因果影响的模型。

Method: 使用结构连接组表示MRI数据，节点代表皮质和皮质下区域，边编码解剖连接性。通过主成分分析总结年龄、性别和APOE4基因型等混杂因素，并整合到因果调整集中。训练后通过切断传入边和改变节点特征来模拟对单个区域的干预。

Result: 在ADNI队列的484名受试者中，Causal-GCN实现了与基线GNN相当的性能，同时提供了可解释的因果效应排名，突出了与已建立的AD神经病理学一致的后部、扣带回和岛叶枢纽。

Conclusion: Causal-GCN框架能够有效识别对阿尔茨海默病进展具有因果影响的大脑区域，为理解疾病机制提供了可解释的因果洞察。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [48] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 本文首次系统比较了四种用于医院出院总结自动诊断编码的隐私保护训练方法，发现在中等隐私预算下，基于DP训练教师模型的知识蒸馏方法在隐私-效用权衡中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 临床文本训练的大语言模型存在泄露敏感患者信息的风险，但差分隐私方法通常会严重降低诊断准确性，目前尚不清楚哪种隐私保护策略在临床语言任务中效果最好。

Method: 使用相同的10亿参数模型和匹配的隐私预算，系统比较了四种训练流程：直接DP-SGD、DP合成数据训练、知识蒸馏等，用于预测ICD-9诊断编码。

Result: 在中等隐私预算（ε=4,6）下，基于DP训练教师模型的知识蒸馏方法表现最佳，恢复了非私有模型63%的性能，同时保持了强大的经验隐私保护（成员推断AUC≈0.5）。

Conclusion: 知识蒸馏是隐私保护临床NLP最实用的途径，不同架构在隐私-效用权衡上存在显著差异。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [49] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) 是一个结构化非参数框架，通过区域级原型的关系记忆增强基于嵌入的推理，将嵌入空间总结为带有可靠性指标的原型节点，并通过编码几何和上下文关系的边连接，统一了实例检索、原型推理和图标签传播。


<details>
  <summary>Details</summary>
Motivation: 传统方法孤立处理每个训练实例，缺乏对嵌入空间整体结构和可靠性的建模，无法有效整合局部证据和全局一致性。

Method: GM 将嵌入空间总结为原型节点，标注可靠性指标，并通过边编码几何和上下文关系，构建关系记忆图，支持实例检索、原型推理和图标签传播。

Result: 在合成和真实数据集（包括乳腺癌组织病理学IDC）上的实验表明，GM 在准确率上与kNN和Label Spreading竞争，同时提供更好的校准和更平滑的决策边界，且样本数量少一个数量级。

Conclusion: GM 通过显式建模可靠性和关系结构，为非参数学习中局部证据和全局一致性之间提供了原则性桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [50] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 本文开发了一个动态时间序列环境来模拟课堂设置，结合强化学习智能辅导系统，通过探测性干预来平衡学生状态估计的准确性和教学干扰。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统可以利用过去学生的信息进行个性化教学，但每个新学生都是独特的，且学习过程只能部分观察，因此需要开发更有效的干预策略。

Method: 创建模拟课堂环境，设计不同级别的探测性干预，开发结合个体状态学习和群体信息的强化学习ITS，并与基于规则的启发式方法进行比较。

Result: 标准RL算法与启发式方法提供不同解决方案但效果相似；探测性干预能显著提升隐藏信息较多时的表现；两种策略对变化的学生群体分布都具灵活性，但RL策略在困难班级中表现较差；非探测策略在测验和期中考试结构中提升效果优于期末考试结构。

Conclusion: 探测性干预能有效降低学生状态估计难度，但需要在信息获取和教学干扰间找到平衡；强化学习和启发式方法各有优势，课程结构设计对教学效果有重要影响。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [51] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出了一种用于心律失常检测和分类的临床决策支持系统框架，通过局部和全局信息提取与融合，解决心律失常长度变化的问题，在MIT-BIH和心房颤动数据库中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 心律失常的发作时间变化多样，现有的临床决策支持系统未能充分考虑这种条件，限制了心律失常检测和分类的准确性。

Method: 提出包含局部和全局提取以及局部-全局信息融合的框架，使用注意力机制在受限输入长度内进行心律失常检测和分类。

Result: 在MITDB和AFDB数据库中，持续时间、发作和Dice评分性能分别达到96.45%、82.05%、96.31%和97.57%、98.31%、97.45%的F1分数，显著优于基准模型。

Conclusion: 该方法能有效捕捉局部和全局信息及动态特性，无显著信息损失，可更准确地检测心律失常并精确定位发作时间，为临床制定更精准的治疗方案提供支持。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [52] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 开发并验证了基于病理全切片图像的TDAM-CRC多实例学习模型，用于结直肠癌的准确预后预测，并通过多组学分析揭示了其分子机制。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌具有高度异质性，传统的TNM分期系统不足以满足个性化医疗需求，需要开发更精确的预后分层工具。

Method: 使用TCGA发现队列（n=581）训练TDAM-CRC模型，在独立外部队列（n=1031）中验证，并整合多组学数据提高模型可解释性。

Result: TDAM-CRC在两个队列中均实现稳健的风险分层，预测性能显著优于传统临床分期系统和现有先进模型。多组学分析发现高风险亚型与代谢重编程和免疫抑制肿瘤微环境相关，识别出MRPL37作为关键枢纽基因。

Conclusion: TDAM-CRC为结直肠癌提供了改进的风险分层工具，揭示了新的分子靶点，并促进了个性化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [53] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 开发了一种新的稀疏矩阵乘法算法，使得能够对整个Medline数据集应用自组织映射，从而更完整地映射现有医学知识。


<details>
  <summary>Details</summary>
Motivation: 由于现有算法对内存和处理需求呈指数级增长，过去将Medline数据库映射的努力仅限于可用数据的小子集。

Method: 设计了一种用于稀疏矩阵乘法的新算法，应用自组织映射到整个Medline数据集。

Result: 实现了对整个Medline数据集的映射，允许更完整地映射现有医学知识。

Conclusion: 该算法提高了根据数据集随时间变化来优化自组织映射的可行性。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [54] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: GRPO-Verif算法通过统一的损失函数联合优化LLMs的解决方案生成和自我验证能力，使用可调超参数控制验证信号权重，在保持推理性能的同时增强自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习显著提升了LLMs的推理能力，但它们仍然难以一致地验证自己的推理轨迹，因此需要研究如何增强LLMs的自我验证能力以及这种能力是否能进一步提高推理性能。

Method: 提出GRPO-Verif算法，在一个统一的损失函数中联合优化解决方案生成和自我验证，使用可调超参数来控制验证信号的权重。

Result: 实验结果表明，该方法在保持推理性能相当的同时，显著增强了自我验证能力。

Conclusion: GRPO-Verif算法成功地在统一框架下提升了LLMs的自我验证能力，为增强语言模型的推理可靠性提供了有效途径。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [55] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 本文提出了一种不确定性感知的主动学习框架，通过联合利用模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性，用于EEG情感识别。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签通常来自主观且不一致的报告，使得稳健的情感解码特别困难。需要一种能够处理标签噪声的方法。

Method: 使用不确定性感知的主动学习框架，通过表示对齐模块将EEG和面部特征嵌入共享潜在空间，评估跨模态对齐以区分认知模糊性和传感器噪声，并选择性查询oracle反馈。

Result: 在ASCERTAIN数据集上的实验验证了该方法的效率和鲁棒性，显示出作为脑机接口系统中数据高效且噪声容忍的EEG情感解码方法的潜力。

Conclusion: 该方法通过跨模态一致性评估和主动学习策略，有效提升了EEG情感识别的鲁棒性和数据效率，为处理标签噪声提供了有效解决方案。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [56] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 该论文研究了复杂变分自编码器（VAE）中的Kähler几何结构，提出了基于Fisher信息度量的Kähler势能方法，用于高效计算几何结构并改善潜在空间表示质量。


<details>
  <summary>Details</summary>
Motivation: 研究复杂VAE中的几何结构，特别是Kähler几何，以改进潜在空间的表示质量和采样效率。

Method: 采用复杂高斯混合的Kähler势能导数，与Fisher信息度量近似等价，通过复数高斯正则化推导几何结构，避免大规模自动微分计算。

Result: 提出的方法能够产生更平滑的表示，减少语义异常值，并通过加权复数体积元素进行采样。

Conclusion: 复杂VAE中的Kähler几何结构可以通过提出的Kähler势能方法有效捕获，改善了潜在空间的几何性质和采样质量。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [57] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器利用正常数据分布建模故障差异，并引入多样性损失防止模式崩溃，在真实性和多样性方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业设备监控中故障诊断至关重要，但故障数据稀缺且标注成本高，现有时间序列生成模型在少样本场景下难以捕捉故障分布，生成样本缺乏真实性和多样性。

Method: 使用扩散模型框架，采用正负差异适配器利用预训练的正常数据分布建模正常与故障域之间的差异，并引入多样性损失通过样本间差异正则化鼓励生成多样化故障样本。

Result: 实验结果表明，该模型在真实性和多样性方面显著优于传统方法，在关键基准测试中实现了最先进的性能。

Conclusion: 提出的基于扩散模型的少样本故障时间序列生成框架有效解决了故障数据稀缺问题，能够生成真实且多样化的故障样本，为工业设备故障诊断提供了有效解决方案。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [58] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一个基于蒸馏的框架，将掩码自回归扩散模型的扩散链压缩为单个AR生成步骤，实现推理加速并支持强化学习后训练。


<details>
  <summary>Details</summary>
Motivation: 解决原始掩码自回归扩散模型推理速度慢的问题，其分层推理机制（外部AR解掩码循环和内部扩散去噪链）不仅影响生成效率，还阻碍强化学习后训练的实际应用。

Method: 提出基于分数的变分目标，将掩码自回归扩散模型蒸馏为单步生成；开发MARVAL-RL高效强化学习框架。

Result: 在ImageNet 256*256上，MARVAL-Huge实现FID 2.00，相比MAR-diffusion加速30倍以上；MARVAL-RL在CLIP和图像奖励分数上持续改进。

Conclusion: MARVAL展示了掩码自回归扩散模型蒸馏和强化学习的首个实用路径，实现快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [59] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [60] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出了EntroPIC方法，通过动态调整正负样本的损失系数来稳定大语言模型训练中的熵值，确保有效探索和稳定进展。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以维持适当的熵水平，因为训练过程涉及正负样本混合，它们在不同步骤以不同方式影响熵，导致模型可能陷入次优行为。

Method: EntroPIC方法使用比例-积分控制自适应调整正负样本的影响，通过动态调节它们的损失系数来稳定训练过程中的熵。

Result: 实验结果表明该方法成功维持了期望的熵水平，实现了大语言模型的稳定和最优强化学习训练。

Conclusion: EntroPIC方法能有效控制大语言模型训练中的熵，为大规模LLM训练提供了稳定的强化学习框架。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [61] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了GRPO-RM方法，将GRPO强化学习技术从大语言模型扩展到表示学习模型，通过预定义输出集和专门设计的奖励函数来优化表示模型的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在大语言模型微调中表现出色，作者希望探索该方法能否推广到表示学习模型，以提升表示模型的性能。

Method: 提出GRPO-RM方法：1）建立预定义输出集替代LLM中的token序列采样，生成输出组；2）设计专门的奖励函数以适应表示模型的特性。

Result: 在多个真实世界数据集上进行广泛实验，验证了所提方法的有效性。

Conclusion: GRPO-RM成功将GRPO技术扩展到表示学习模型，为表示模型的优化提供了新思路。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [62] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏测试时适应框架，通过减少适应频率和数据使用量，在保持准确性的同时显著降低计算成本，特别适合资源受限的边缘环境。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法需要频繁适应和高计算成本，不适合资源受限的边缘环境，因此需要开发更高效的适应方法。

Method: 提出SNAP框架，包含两个关键组件：类别和域代表性记忆(CnDRM)用于识别和存储代表性样本，以及仅推理批量感知内存归一化(IoBMN)用于动态调整归一化统计量。

Result: SNAP在仅使用1%传入数据流的情况下仍保持竞争力，与五种最先进TTA算法集成后，延迟降低高达93.12%，准确率下降低于3.3%。

Conclusion: SNAP展示了在边缘设备上实际应用的强大潜力，特别适合延迟敏感的应用场景。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [63] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段方法，通过渐进式伪量化和反向剪枝技术，生成硬件无关的量化检查点，提高模型在不同后端和精度选择下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决专用边缘加速器中低比特量化在不同厂商编译器下精度不一致的问题，避免开发者需要针对不同后端调整参数或重构模型。

Method: 结合渐进式伪量化来对齐训练与部署的整数网格，以及反向剪枝来控制异常值驱动的尺度膨胀，同时保持模型的可学习性。

Result: Quant-Trim能够缩小浮点模型与低比特量化模型之间的精度差距，减少对编译器启发式/校准的依赖，避免针对每个后端的重新训练。

Conclusion: 该方法对各种量化方案（对称/非对称、逐张量/逐通道、INT8/INT4）均有效，无需特定厂商的图修改，在多种模型和任务中表现出色。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [64] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型（TSFMs）的概念可解释性，分析了不同层编码的概念类型、概念参数的线性可恢复性、表示在模型深度上的演变以及概念组合的处理方式。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在经验上取得了成功，但其内部如何表示基本时间序列概念的机制仍不清楚。本文旨在系统性地探索TSFMs的概念可解释性。

Method: 使用分层分析、线性可恢复性测试和表示相似性度量等方法，系统性地探测TSFMs的概念表示机制。

Result: 研究发现早期层主要捕获局部时域模式（如AR(1)、水平偏移、趋势），深层编码离散度和变化时间信号，频谱和扭曲因子最难线性恢复。在组合场景下，探测性能下降，显示概念间存在干扰。

Conclusion: 虽然原子概念能够可靠地定位，但概念组合仍然是当前TSFMs的一个关键限制，突显了其在表示交互时间现象能力上的不足。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [65] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文系统评估了33种时间序列预测的集成方法，发现堆叠集成能持续提升精度，并提出了一种多层堆叠框架，在多样化预测场景中均能提供优越的准确性。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测领域，集成方法尚未得到充分利用，简单的线性组合仍被认为是先进技术，而集成方法在表格任务中已证明能显著提升模型精度。

Method: 系统评估了33种集成模型（包括现有和新提出的），在50个真实世界数据集上进行测试，并提出了一种多层堆叠框架来结合不同堆叠器模型的优势。

Result: 堆叠集成能持续提升预测精度，但没有单一堆叠器在所有任务中表现最佳。提出的多层堆叠框架在多样化预测场景中均能提供优越的准确性。

Conclusion: 堆叠方法有潜力改进时间序列预测的AutoML系统，多层堆叠框架能够一致地在不同预测场景中提供更好的准确性。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [66] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 本文提出了一种新的局部特征重要性方法CID，通过生成正负反事实样本、使用核密度估计建模分布，并基于分布差异度量对特征进行排序，该方法在忠实性指标上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中评估个体特征重要性对于理解模型决策过程至关重要，但现有方法缺乏明确的真实基准进行比较，需要开发替代的、有理论基础的评价方法。

Method: 提出反事实重要性分布(CID)方法：生成正负两组反事实样本，使用核密度估计建模其分布，并基于分布差异度量对特征进行排序，该度量具有严格的数学基础并满足有效度量所需的关键属性。

Result: 与成熟的局部特征重要性解释方法相比，CID方法不仅提供了对现有方法的补充视角，还在忠实性指标（包括全面性和充分性）上提升了性能，能够提供更忠实的系统解释。

Conclusion: CID方法作为一种有价值的模型分析工具具有潜力，其基于严格数学框架的特征重要性评估方法能够提供更可靠的模型解释。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [67] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过仅更新0.1%的核心参数来缓解灾难性遗忘问题，同时保持基础模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 领域特定后训练通常会导致灾难性遗忘，使基础模型失去通用推理能力。传统持续学习方法存在下游性能差、依赖历史数据或额外参数开销等问题。

Method: PIECE使用两种重要性估计器（基于Fisher信息的PIECE-F和结合梯度与曲率信息的二阶归一化PIECE-S），仅选择性更新与新任务最相关的0.1%核心参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE能保持通用能力，并在多样化下游任务上实现最先进的持续学习性能。

Conclusion: PIECE为构建可扩展、领域自适应且无灾难性遗忘的基础模型提供了一条实用路径。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [68] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出一种基于变分拉格朗日公式的非线性非高斯状态空间模型状态估计算法，通过熵信任区域更新和动态约束实现贝叶斯推断，推导出具有良好计算复杂度的递归方案。


<details>
  <summary>Details</summary>
Motivation: 针对非线性非高斯状态空间模型的状态估计问题，传统方法存在计算复杂或精度不足的问题，需要开发更高效准确的估计算法。

Method: 采用变分拉格朗日公式，将贝叶斯推断转化为序列熵信任区域更新问题，基于高斯-马尔可夫近似推导递归方案，使用广义统计线性回归和傅里叶-埃尔米特矩匹配闭合递归。

Result: 开发出一类前向-后向算法，其结构由变分后验的因子分解决定，获得了计算复杂度良好的递归方案。

Conclusion: 该变分拉格朗日框架为非线性非高斯状态估计提供了一类有效的算法，能够处理复杂的状态空间模型并保持计算效率。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [69] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 该论文研究了表格上下文学习模型中各层的作用，发现只有部分层共享共同表示语言，存在结构冗余，为模型压缩和可解释性提供机会。


<details>
  <summary>Details</summary>
Motivation: 尽管表格上下文学习模型与大型语言模型在架构上相似，但个体层在表格预测中的贡献尚不清楚。

Method: 通过"层作为画家"的视角分析TabPFN和TabICL模型，研究潜在空间在各层的演化，识别冗余层，并与LLMs中的动态进行比较。

Result: 发现只有部分层子集共享共同表示语言，表明存在结构冗余。

Conclusion: 这些发现为表格ICL模型的压缩和可解释性改进提供了机会。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [70] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 提出了一种基于时间序列基础模型（TSFM）的上下文学习分类方法，无需微调模型即可对训练数据之外的数据进行分类，应用于轴承健康状态评估。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要微调模型的问题，利用预训练模型的扩展性，推动从定制化窄AI解决方案向更广泛的AI驱动维护系统发展。

Method: 将示例以目标（类别ID）和协变量（数据矩阵）形式表示在模型提示中，通过上下文学习在预测轴上对未知协变量数据模式进行分类，将频域参考信号转换为伪时间序列模式。

Result: 该方法能够有效预测分类数据与预定义标签的对应概率，在不同运行条件下都表现出良好效果。

Conclusion: 该方法标志着在超越定制窄AI解决方案、向更广泛AI驱动维护系统发展方面取得了重要进展。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [71] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一个公平感知的能源最小化联邦学习框架，通过联合优化设备选择、带宽分配和压缩级别，在非IID数据上实现更高精度，同时能耗降低高达79%。


<details>
  <summary>Details</summary>
Motivation: 解决无线边缘系统中联邦学习面临的能源效率、公平参与和高模型精度之间的平衡挑战，特别是处理异构资源、不平等客户端贡献和有限通信容量的问题。

Method: 提出FairEnergy框架，将捕捉更新幅度和压缩比的贡献分数集成到联合优化中，通过松弛二元选择变量和应用拉格朗日分解处理全局带宽耦合，然后进行每设备子问题优化。

Result: 在非IID数据上的实验表明，与基线策略相比，FairEnergy实现了更高的准确率，同时能耗降低了高达79%。

Conclusion: FairEnergy框架成功平衡了联邦学习中的能源效率、公平性和模型精度，为无线边缘系统中的联邦学习提供了有效的解决方案。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [72] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出NTK引导的隐式神经教学(NINT)方法，通过动态选择最大化全局功能更新的坐标来加速隐式神经表示的训练，利用神经正切核(NTK)对样本进行评分，显著减少训练时间近一半。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)通过多层感知机参数化连续信号，但在拟合高分辨率信号时需要优化数百万个坐标，计算成本过高。

Method: 利用神经正切核(NTK)对样本进行评分，基于NTK增强损失梯度的范数，同时考虑拟合误差和异质杠杆效应（自影响和跨坐标耦合）。

Result: 实验表明NINT显著减少训练时间近一半，同时保持或提高表示质量，在基于采样的加速策略中达到最先进水平。

Conclusion: NINT通过动态坐标选择有效加速隐式神经表示的训练，为高分辨率信号建模提供了高效的解决方案。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [73] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文研究了按需采样中样本复杂度与轮数复杂度之间的权衡关系，在可实现的MDL设置中，r轮算法的最优样本复杂度约为dk^{Θ(1/r)}/ε；在不可知设置中，提出了在Õ(√k)轮内实现近最优样本复杂度Õ((d+k)/ε²)的算法。


<details>
  <summary>Details</summary>
Motivation: 研究多分布学习中样本复杂度和轮数复杂度之间的基本权衡关系，特别是在按需采样设置下，算法可以自适应地从k个分布中采样有限轮数。

Method: 引入了新的框架OODS（通过按需采样进行优化），该框架抽象了样本自适应权衡，并捕获了大多数现有的MDL算法。建立了OODS设置中轮数复杂度的近紧界。

Result: 在可实现MDL中，r轮算法的最优样本复杂度为dk^{Θ(1/r)}/ε；在不可知MDL中，开发了在Õ(√k)轮内达到Õ((d+k)/ε²)样本复杂度的算法。

Conclusion: 上界直接产生了不可知MDL的Õ(√k)轮算法，而下界表明，要获得亚多项式轮数复杂度，需要绕过OODS固有硬度的全新技术。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>
