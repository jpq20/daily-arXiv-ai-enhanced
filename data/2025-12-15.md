<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 27]
- [cs.LG](#cs.LG) [Total: 32]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ASR Under the Stethoscope: Evaluating Biases in Clinical Speech Recognition across Indian Languages](https://arxiv.org/abs/2512.10967)
*Subham Kumar,Prakrithi Shivaprakash,Abhishek Manoharan,Astut Kurariya,Diptadhi Mukherjee,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 该研究首次系统评估了ASR在印度多语言临床环境中的性能，发现不同模型和语言间存在显著差异，并揭示了与说话者角色和性别相关的系统性性能差距，强调了开发包容性ASR系统的重要性。


<details>
  <summary>Details</summary>
Motivation: ASR在临床记录中应用日益广泛，但其在印度多语言和人口多样化医疗环境中的可靠性尚不明确，需要进行系统性评估以确保公平部署。

Method: 使用真实世界临床访谈数据，涵盖卡纳达语、印地语和印度英语，比较了包括Indic Whisper、Whisper、Sarvam、Google speech to text、Gemma3n、Omnilingual、Vaani和Gemini在内的领先ASR模型，评估了跨语言、说话者和人口亚组的转录准确性。

Result: 结果显示模型和语言间存在显著差异，某些系统在印度英语上表现良好但在代码混合或方言语音上失败，发现了与说话者角色（患者vs临床医生）和性别相关的系统性性能差距。

Conclusion: 研究强调了为印度医疗生态系统开发文化和人口包容性ASR的必要性，提供了全面的多语言基准和公平性分析，对临床环境中ASR的公平部署提出了重要关切。

Abstract: Automatic Speech Recognition (ASR) is increasingly used to document clinical encounters, yet its reliability in multilingual and demographically diverse Indian healthcare contexts remains largely unknown. In this study, we conduct the first systematic audit of ASR performance on real world clinical interview data spanning Kannada, Hindi, and Indian English, comparing leading models including Indic Whisper, Whisper, Sarvam, Google speech to text, Gemma3n, Omnilingual, Vaani, and Gemini. We evaluate transcription accuracy across languages, speakers, and demographic subgroups, with a particular focus on error patterns affecting patients vs. clinicians and gender based or intersectional disparities. Our results reveal substantial variability across models and languages, with some systems performing competitively on Indian English but failing on code mixed or vernacular speech. We also uncover systematic performance gaps tied to speaker role and gender, raising concerns about equitable deployment in clinical settings. By providing a comprehensive multilingual benchmark and fairness analysis, our work highlights the need for culturally and demographically inclusive ASR development for healthcare ecosystem in India.

</details>


### [2] [Benchmarking Automatic Speech Recognition Models for African Languages](https://arxiv.org/abs/2512.10968)
*Alvin Nahabwe,Sulaiman Kagumire,Denis Musinguzi,Bruno Beijuka,Jonah Mubuuke Kyagaba,Peter Nabende,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CL

TL;DR: 本文系统评估了四种先进ASR模型在13种非洲语言上的表现，通过1-400小时不同数据量的微调，揭示了各模型在不同资源条件下的性能特点和数据效率差异。


<details>
  <summary>Details</summary>
Motivation: 非洲语言的自动语音识别面临标记数据有限和缺乏系统指导的问题。虽然已有大型预训练系统，但它们在非洲低资源环境中的比较行为尚未得到统一系统研究。

Method: 在13种非洲语言上对Whisper、XLS-R、MMS和W2v-BERT四种先进ASR模型进行基准测试，使用1-400小时不同规模的转录数据进行微调，分析错误率并探究模型在不同条件下的行为差异。

Result: MMS和W2v-BERT在极低资源条件下数据效率更高；XLS-R在数据增加时扩展性更好；Whisper在中等资源条件下表现优势。外部语言模型解码在某些情况下能改进性能，但在声学和文本资源对齐不佳时可能引入额外错误。

Conclusion: 研究揭示了预训练覆盖度、模型架构、数据集领域和资源可用性之间的相互作用，为设计面向代表性不足语言的ASR系统提供了实用见解。

Abstract: Automatic speech recognition (ASR) for African languages remains constrained by limited labeled data and the lack of systematic guidance on model selection, data scaling, and decoding strategies. Large pre-trained systems such as Whisper, XLS-R, MMS, and W2v-BERT have expanded access to ASR technology, but their comparative behavior in African low-resource contexts has not been studied in a unified and systematic way. In this work, we benchmark four state-of-the-art ASR models across 13 African languages, fine-tuning them on progressively larger subsets of transcribed data ranging from 1 to 400 hours. Beyond reporting error rates, we provide new insights into why models behave differently under varying conditions. We show that MMS and W2v-BERT are more data efficient in very low-resource regimes, XLS-R scales more effectively as additional data becomes available, and Whisper demonstrates advantages in mid-resource conditions. We also analyze where external language model decoding yields improvements and identify cases where it plateaus or introduces additional errors, depending on the alignment between acoustic and text resources. By highlighting the interaction between pre-training coverage, model architecture, dataset domain, and resource availability, this study offers practical and insights into the design of ASR systems for underrepresented languages.

</details>


### [3] [MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA](https://arxiv.org/abs/2512.10996)
*Seonok Kim*

Main category: cs.CL

TL;DR: MedBioRAG是一个检索增强生成模型，通过结合语义和词汇搜索、文档检索和监督微调，显著提升了生物医学问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成技术显著提升了大型语言模型处理复杂问答任务的能力，但在生物医学领域仍需要专门优化的模型来改善问答性能。

Method: MedBioRAG结合了语义搜索和词汇搜索进行文档检索，通过监督微调优化模型，能够高效检索和排序相关生物医学文档，生成精确且上下文感知的回答。

Result: 在NFCorpus、TREC-COVID、MedQA、PubMedQA和BioASQ等基准数据集上，MedBioRAG在文本检索、封闭式问答和长格式问答任务中均优于之前的SOTA模型和GPT-4o基础模型，显著提升了NDCG、MRR分数、准确率和ROUGE分数。

Conclusion: 研究证明了基于语义搜索的检索和LLM微调在生物医学应用中的有效性，MedBioRAG为生物医学问答任务提供了强大的解决方案。

Abstract: Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.

</details>


### [4] [KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering](https://arxiv.org/abs/2512.10999)
*Xin Sun,Zhongqi Chen,Xing Zheng,Qiang Liu,Shu Wu,Bowen Song,Zilei Wang,Weiqiang Wang,Liang Wang*

Main category: cs.CL

TL;DR: KBQA-R1：通过强化学习将知识库问答从文本模仿转向交互优化，引入参考拒绝采样解决冷启动问题，在多个数据集上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的KBQA方法存在两种失败模式：要么生成未验证模式存在的幻觉查询，要么采用僵化的模板化推理而缺乏对环境的真正理解。需要解决从文本模仿到交互优化的范式转变。

Method: 提出KBQA-R1框架，将KBQA视为多轮决策过程，模型学习使用动作列表导航知识库，利用组相对策略优化（GRPO）基于具体执行反馈而非静态监督来优化策略。引入参考拒绝采样（RRS）数据合成方法，通过严格对齐推理轨迹与真实动作序列来解决冷启动问题。

Result: 在WebQSP、GrailQA和GraphQuestions数据集上的广泛实验表明，KBQA-R1实现了最先进的性能，有效地将LLM推理建立在可验证的执行基础上。

Conclusion: KBQA-R1通过强化学习实现了从文本模仿到交互优化的范式转变，解决了当前KBQA方法的局限性，在多个基准测试中取得了SOTA结果，证明了基于执行反馈的优化策略的有效性。

Abstract: Knowledge Base Question Answering (KBQA) challenges models to bridge the gap between natural language and strict knowledge graph schemas by generating executable logical forms. While Large Language Models (LLMs) have advanced this field, current approaches often struggle with a dichotomy of failure: they either generate hallucinated queries without verifying schema existence or exhibit rigid, template-based reasoning that mimics synthesized traces without true comprehension of the environment. To address these limitations, we present \textbf{KBQA-R1}, a framework that shifts the paradigm from text imitation to interaction optimization via Reinforcement Learning. Treating KBQA as a multi-turn decision process, our model learns to navigate the knowledge base using a list of actions, leveraging Group Relative Policy Optimization (GRPO) to refine its strategies based on concrete execution feedback rather than static supervision. Furthermore, we introduce \textbf{Referenced Rejection Sampling (RRS)}, a data synthesis method that resolves cold-start challenges by strictly aligning reasoning traces with ground-truth action sequences. Extensive experiments on WebQSP, GrailQA, and GraphQuestions demonstrate that KBQA-R1 achieves state-of-the-art performance, effectively grounding LLM reasoning in verifiable execution.

</details>


### [5] [MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data](https://arxiv.org/abs/2512.11074)
*Christopher Driggers-Ellis,Detravious Brinkley,Ray Chen,Aashish Dhawan,Daisy Zhe Wang,Christan Grant*

Main category: cs.CL

TL;DR: MultiScript30k扩展了Multi30k数据集，支持阿拉伯语、西班牙语、乌克兰语和简体繁体中文，解决了原数据集仅限欧洲语言的限制。


<details>
  <summary>Details</summary>
Motivation: Multi30k数据集仅支持捷克语、英语、法语和德语四种欧洲语言，限制了多模态机器翻译研究在更多语言上的发展。原数据集仅代表拉丁字母的欧洲语言，导致对多样化语言的研究停滞不前。

Method: 使用NLLB200-3.3B模型将Multi30k的英文版本翻译成阿拉伯语、西班牙语、乌克兰语、简体中文和繁体中文，创建包含超过30000个句子的MultiScript30k数据集。

Result: 相似性分析显示，除繁体中文外，所有语言都达到大于0.8的余弦相似度和小于0.000251的对称KL散度，与之前的扩展数据集相当。COMETKiwi评估显示混合结果：阿拉伯语版本与ArEnMulti30k相当，但乌克兰语版本比Multi30k-Uk低6.4%。

Conclusion: MultiScript30k成功扩展了Multi30k数据集，支持更多语言和文字系统，为多模态机器翻译研究提供了更广泛的语言资源，尽管在某些语言上的翻译质量仍有提升空间。

Abstract: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.

</details>


### [6] [Applying NLP to iMessages: Understanding Topic Avoidance, Responsiveness, and Sentiment](https://arxiv.org/abs/2512.11079)
*Alan Gerber,Sam Cooperman*

Main category: cs.CL

TL;DR: 本文介绍了针对iMessage本地存储文件开发的文本消息分析器，通过该工具可以回答五个主要研究问题，包括主题建模、响应时间、不情愿评分和情感分析等。


<details>
  <summary>Details</summary>
Motivation: 随着社会对短格式电子通信的依赖日益增加，用户很少思考公司能从他们的消息平台收集哪些信息。虽然大多数公司严密保护数据，但苹果为Mac上的iMessage用户提供了一个本地存储文件，这为分析个人消息数据创造了机会。

Method: 开发了一个iMessage文本消息分析器，利用苹果在Mac上提供的本地存储文件，该文件包含所有消息和附加元数据。通过该分析器进行数据探索性分析。

Result: 该分析器能够回答五个主要研究问题：主题建模、响应时间分析、不情愿评分和情感分析。展示了如何利用这些数据回答相关问题。

Conclusion: 该iMessage文本消息分析器展示了个人消息数据的分析潜力，为未来iMessage数据研究提供了工具和可能性。

Abstract: What is your messaging data used for? While many users do not often think about the information companies can gather based off of their messaging platform of choice, it is nonetheless important to consider as society increasingly relies on short-form electronic communication. While most companies keep their data closely guarded, inaccessible to users or potential hackers, Apple has opened a door to their walled-garden ecosystem, providing iMessage users on Mac with one file storing all their messages and attached metadata. With knowledge of this locally stored file, the question now becomes: What can our data do for us? In the creation of our iMessage text message analyzer, we set out to answer five main research questions focusing on topic modeling, response times, reluctance scoring, and sentiment analysis. This paper uses our exploratory data to show how these questions can be answered using our analyzer and its potential in future studies on iMessage data.

</details>


### [7] [Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution](https://arxiv.org/abs/2512.11108)
*Jonathan Kamp,Roos Bakker,Dominique Blok*

Main category: cs.CL

TL;DR: 该论文研究了特征归因方法在语言模型解释中的偏见问题，提出了一个模型和方法无关的评估框架，通过三个指标系统评估不同归因方法的词汇和位置偏见。


<details>
  <summary>Details</summary>
Motivation: 不同特征归因方法（如Integrated Gradient）在相同输入上可能产生差异很大的解释，这种不一致性导致用户可能不信任这些解释，或者过度信任有偏见的解释。需要超越表面不一致性，结构化地理解这些方法的偏见。

Method: 提出了一个模型和方法无关的评估框架，包含三个评估指标。系统评估了两种transformer模型的词汇偏见和位置偏见：首先在人工数据的伪随机分类任务中进行控制实验，然后在自然数据的因果关系检测任务中进行半控制实验。

Result: 发现词汇偏见和位置偏见在模型比较中结构性地不平衡，在一个类型上得分高的模型在另一个类型上得分低。还发现产生异常解释的方法更可能自身存在偏见。

Conclusion: 需要结构化地理解和评估特征归因方法的偏见，而不是仅仅关注表面不一致性。词汇偏见和位置偏见之间存在权衡关系，异常解释方法往往存在更多偏见。

Abstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.

</details>


### [8] [FIBER: A Multilingual Evaluation Resource for Factual Inference Bias](https://arxiv.org/abs/2512.11110)
*Evren Ayberk Munis,Deniz Yılmaz,Arianna Muti,Çağrı Toraman*

Main category: cs.CL

TL;DR: FIBER是一个多语言基准测试，用于评估大语言模型在单实体和多实体设置中的事实知识，包含英语、意大利语和土耳其语的句子补全、问答和对象计数预测任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多关注单实体事实和单语言数据，缺乏对多语言和多实体事实知识的系统性评估，因此需要开发一个更全面的基准来评估大语言模型的事实可靠性和偏见。

Method: 创建FIBER多语言基准数据集，包含英语、意大利语和土耳其语的三种任务类型：句子补全、问答和对象计数预测。使用该基准评估提示语言是否在实体选择中引入推理偏见，并比较模型在单实体与多实体问题上的表现。

Result: 提示语言会影响模型输出，特别是与语言对应国家相关的实体，31%的主题表现出大于0.5的事实推理偏见分数。土耳其提示比意大利语在83%的主题中表现出更高偏见。模型在处理多实体问题时比单实体问题更困难，英语表现最佳，土耳其语和意大利语得分明显较低。较大模型（Llama-3.1-8B和Qwen-2.5-7B）表现优于较小的3B-4B模型。

Conclusion: 大语言模型的事实知识评估需要考虑多语言和多实体维度，提示语言会影响模型输出并引入语言依赖性偏见，模型在多实体问题上表现更差，较大模型通常表现更好，但性能仍存在语言差异。

Abstract: Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.

</details>


### [9] [SciLaD: A Large-Scale, Transparent, Reproducible Dataset for Natural Scientific Language Processing](https://arxiv.org/abs/2512.11192)
*Luca Foppiano,Sotaro Takeshita,Pedro Ortiz Suarez,Ekaterina Borisova,Raia Abu Ahmad,Malte Ostendorff,Fabio Barth,Julian Moreno-Schneider,Georg Rehm*

Main category: cs.CL

TL;DR: SciLaD是一个大规模科学语言数据集，包含超过1000万篇英文科学文献和3500万篇多语言文献，使用开源工具构建，并基于该数据集训练了RoBERTa模型，性能与同类模型相当。


<details>
  <summary>Details</summary>
Motivation: 构建一个完全基于开源框架和公开数据源的大规模科学语言数据集，以促进科学自然语言处理和理解的研究，同时展示开源工具在大规模科学数据整理中的可行性。

Method: 使用开源工具构建可扩展的数据集生成流程，创建了两个版本的数据集：一个经过整理的英文数据集（1000万+文献）和一个多语言的TEI XML数据集（3500万+文献）。基于该数据集预训练了RoBERTa模型，并在多个基准测试上进行了评估。

Result: 成功构建了SciLaD数据集，包含超过4500万篇科学文献。基于该数据集训练的RoBERTa模型在综合基准测试中表现与相似规模的其他科学语言模型相当，验证了数据集的质量和实用性。

Conclusion: SciLaD数据集证明了开源工具能够支持大规模科学数据整理并保持高质量。该数据集和评估流程的发布有助于促进科学自然语言处理领域的可重复性、透明度和进一步研究。

Abstract: SciLaD is a novel, large-scale dataset of scientific language constructed entirely using open-source frameworks and publicly available data sources. It comprises a curated English split containing over 10 million scientific publications and a multilingual, unfiltered TEI XML split including more than 35 million publications. We also publish the extensible pipeline for generating SciLaD. The dataset construction and processing workflow demonstrates how open-source tools can enable large-scale, scientific data curation while maintaining high data quality. Finally, we pre-train a RoBERTa model on our dataset and evaluate it across a comprehensive set of benchmarks, achieving performance comparable to other scientific language models of similar size, validating the quality and utility of SciLaD. We publish the dataset and evaluation pipeline to promote reproducibility, transparency, and further research in natural scientific language processing and understanding including scholarly document processing.

</details>


### [10] [Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges](https://arxiv.org/abs/2512.11258)
*Di Wu,Ruiyu Fang,Liting Jiang,Shuangyong Song,Xiaomeng Huang,Shiquan Wang,Zhongqiu Li,Lingling Shi,Mengjiao Bao,Yongxiang Li,Hao Huang*

Main category: cs.CL

TL;DR: 本文对多意图口语理解（SLU）领域进行了系统性综述，涵盖解码范式、建模方法、性能比较以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多意图SLU涉及多个意图检测和槽位填充两个任务，能更好地反映现实应用场景，因此受到越来越多的研究关注。然而，目前缺乏对该领域现有研究的全面系统性综述。

Method: 本文从两个视角对多意图SLU研究进行深入概述：解码范式和建模方法。在此基础上，比较代表性模型的性能并分析其优缺点。

Result: 提供了多意图SLU领域的系统性综述，总结了现有研究的进展、性能比较结果以及不同方法的优缺点分析。

Conclusion: 本文讨论了多意图SLU当前面临的挑战，并展望了未来研究方向，希望为推进该领域研究提供有价值的见解和参考。

Abstract: Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.

</details>


### [11] [Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach](https://arxiv.org/abs/2512.11261)
*Yun-Chung Liu,Rui Yang,Jonathan Chong Kai Liew,Ziran Yin,Henry Foote,Christopher J. Lindsell,Chuan Hong*

Main category: cs.CL

TL;DR: 提出了一种两阶段动态少样本学习方法，利用低成本和高性能LLM结合，提高系统评价中标题和摘要筛选的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 随着研究出版物快速增长，系统评价的标题和摘要筛选步骤变得极其耗时耗力，需要更高效的方法来减轻人工负担。

Method: 采用两阶段动态少样本学习方法：第一阶段使用低成本LLM进行初步筛选，第二阶段对低置信度实例使用高性能LLM重新评估，在控制计算成本的同时提升筛选性能。

Result: 在10个系统评价中评估该方法，结果显示其具有良好的泛化能力和成本效益，能够减少人工筛选负担并加速系统评价过程。

Conclusion: 两阶段动态少样本学习方法能够有效提高系统评价中标题和摘要筛选的效率和性能，具有实际应用潜力。

Abstract: Systematic reviews are a key component of evidence-based medicine, playing a critical role in synthesizing existing research evidence and guiding clinical decisions. However, with the rapid growth of research publications, conducting systematic reviews has become increasingly burdensome, with title and abstract screening being one of the most time-consuming and resource-intensive steps. To mitigate this issue, we designed a two-stage dynamic few-shot learning (DFSL) approach aimed at improving the efficiency and performance of large language models (LLMs) in the title and abstract screening task. Specifically, this approach first uses a low-cost LLM for initial screening, then re-evaluates low-confidence instances using a high-performance LLM, thereby enhancing screening performance while controlling computational costs. We evaluated this approach across 10 systematic reviews, and the results demonstrate its strong generalizability and cost-effectiveness, with potential to reduce manual screening burden and accelerate the systematic review process in practical applications.

</details>


### [12] [When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents](https://arxiv.org/abs/2512.11277)
*Mrinal Rawat,Arkajyoti Chakraborty,Neha Gupta,Roberto Pieraccini*

Main category: cs.CL

TL;DR: 该论文提出了一种使用强化学习（RL）来提升大语言模型推理能力的框架，通过Group Relative Policy Optimization（GRPO）优化推理步骤和工具调用，相比传统监督微调（SFT）方法在推理质量和工具调用精度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在数据分布变化时泛化能力有限，而高质量推理标注数据收集困难、成本高且难以扩展。最近推理导向模型（如o1和R1）显示出比非推理模型更好的泛化能力和可靠性，但缺乏有效的训练方法。

Method: 提出一个强化学习框架，让LLM生成推理步骤来指导工具调用（如函数调用）和最终答案生成。使用Group Relative Policy Optimization（GRPO）进行优化，奖励机制围绕工具准确性和答案正确性设计，使模型能够迭代优化推理和行动策略。

Result: 实验结果显示，该方法在推理质量和工具调用精度上都有提升：相比无显式推理的SFT模型获得1.5%的相对改进，相比基础Qwen3-1.7B模型获得40%的性能增益。

Conclusion: 通过强化学习统一推理和行动学习，可以构建更强大、更具泛化能力的对话智能体，为LLM的推理能力训练提供了一种有效且可扩展的方法。

Abstract: Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.

</details>


### [13] [AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference](https://arxiv.org/abs/2512.11280)
*Kuan-Wei Lu,Ding-Yong Hong,Pangfeng Liu*

Main category: cs.CL

TL;DR: AdaSD是一种自适应推测解码方法，通过动态调整生成长度和接受标准来加速大语言模型推理，无需额外训练或超参数调优


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数规模增大导致推理速度显著下降，现有推测解码方法需要额外训练、超参数调优或模型任务预分析，限制了实际应用

Method: 提出自适应推测解码(AdaSD)，引入两个自适应阈值：一个基于令牌熵决定何时停止候选令牌生成，另一个基于Jensen-Shannon距离决定令牌接受，两者在推理过程中实时更新

Result: 在基准数据集上的实验表明，AdaSD相比标准推测解码实现了高达49%的加速，同时将准确率下降控制在2%以内

Conclusion: AdaSD提供了一种无需预分析或微调的实用解决方案，能够实现高效且自适应的LLM推理，与现成模型兼容

Abstract: Large language models (LLMs) have achieved remarkable performance across a wide range of tasks, but their increasing parameter sizes significantly slow down inference. Speculative decoding mitigates this issue by leveraging a smaller draft model to predict candidate tokens, which are then verified by a larger target model. However, existing approaches often require additional training, extensive hyperparameter tuning, or prior analysis of models and tasks before deployment. In this paper, we propose Adaptive Speculative Decoding (AdaSD), a hyperparameter-free decoding scheme that dynamically adjusts generation length and acceptance criteria during inference. AdaSD introduces two adaptive thresholds: one to determine when to stop candidate token generation and another to decide token acceptance, both updated in real time based on token entropy and Jensen-Shannon distance. This approach eliminates the need for pre-analysis or fine-tuning and is compatible with off-the-shelf models. Experiments on benchmark datasets demonstrate that AdaSD achieves up to 49\% speedup over standard speculative decoding while limiting accuracy degradation to under 2\%, making it a practical solution for efficient and adaptive LLM inference.

</details>


### [14] [CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise](https://arxiv.org/abs/2512.11282)
*Qingsen Ma,Dianyun Wang,Ran Jing,Yujun Sun,Zhenbo Xu*

Main category: cs.CL

TL;DR: CIP是一个轻量级即插即用的因果提示框架，通过构建实体、动作和事件之间的因果序列来减少大语言模型在处理长噪声检索上下文时的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长且嘈杂的检索上下文时经常产生幻觉，因为它们依赖虚假相关性而非真正的因果关系。需要一种方法来改善模型的事实基础和可解释性。

Method: 提出CIP框架，构建实体、动作和事件之间的因果序列，并将其注入提示中引导推理。通过因果干预和反事实推理来抑制非因果推理路径。

Result: 在7个主流语言模型（包括GPT-4o、Gemini 2.0 Flash和Llama 3.1）上的实验显示，CIP将可归因率提高了2.6个点，因果一致性得分提高了0.38，有效信息密度提高了四倍，端到端响应延迟降低了55.1%。

Conclusion: 因果推理可能成为改善大语言模型可解释性、稳定性和效率的有前景范式。

Abstract: Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.

</details>


### [15] [LegalRikai: Open Benchmark -- A Benchmark for Complex Japanese Corporate Legal Tasks](https://arxiv.org/abs/2512.11297)
*Shogo Fujita,Yuji Naraki,Yiqing Zhu,Shinsuke Mori*

Main category: cs.CL

TL;DR: LegalRikai是一个面向日本企业法律实践的基准测试，包含4个复杂任务，由法律专业人士创建，包含100个需要长格式结构化输出的样本，通过人类和自动化评估验证了LLM在法律文档编辑中的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了在法律领域推动更面向实践的研究，需要创建一个能够模拟真实日本企业法律实践的基准测试，以评估大型语言模型在复杂法律任务中的表现。

Method: 创建了LegalRikai基准测试，包含4个复杂任务，由法律专业人士在律师监督下制作100个样本。使用GPT-5、Gemini 2.5 Pro和Claude Opus 4.1等领先LLM进行评估，同时进行人类评估和自动化评估。

Result: 人类评估发现抽象指令会导致不必要的修改，揭示了模型在文档级编辑方面的弱点。自动化评估在具有明确语言基础的标准上与人类判断一致，但评估结构一致性仍然具有挑战性。自动化评估可作为专家有限时的筛选工具。

Conclusion: 提出了数据集评估框架，以促进法律领域更面向实践的研究，展示了自动化评估作为筛选工具的实用性，特别是在专家资源有限的情况下。

Abstract: This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.

</details>


### [16] [qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs](https://arxiv.org/abs/2512.11366)
*Shreya Shukla,Aditya Sriram,Milinda Kuppur Narayanaswamy,Hiteshi Jain*

Main category: cs.CL

TL;DR: qa-FLoRA：一种无需数据和训练的查询自适应LoRA融合方法，通过测量基础模型与适配器之间的分布差异动态计算层级融合权重，在多领域复合任务上优于静态融合和训练无关基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业任务部署中需要领域特定的参数高效微调（LoRA），但现有LoRA融合方法存在局限性：静态权重方法对所有参与LoRA分配相同相关性，而监督训练方法需要大量数据和为每个可能的LoRA组合进行训练。如何有效融合适配器处理复杂的多领域复合查询成为关键挑战。

Method: 提出qa-FLoRA方法，这是一种无需数据和训练的查询自适应LoRA融合方法。通过测量基础模型与相应适配器之间的分布差异，动态计算层级融合权重。该方法不需要复合训练数据或领域代表性样本，可直接应用于现有适配器集合。

Result: 在涵盖数学、编程和医疗领域的九个多语言复合任务上的广泛实验表明：qa-FLoRA在LLaMA-2上比静态融合提升约5%，在LLaMA-3上提升约6%；相比训练无关基线，在LLaMA-2上提升约7%，在LLaMA-3上提升约10%。同时显著缩小了与监督基线的差距。层级融合权重分析揭示了可解释的融合模式。

Conclusion: qa-FLoRA提供了一种有效的数据和训练无关的LoRA融合方法，能够动态适应查询需求，在多领域复合任务上表现优异，同时融合权重具有可解释性，为鲁棒的多领域适应提供了有效解决方案。

Abstract: The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.

</details>


### [17] [Mining Legal Arguments to Study Judicial Formalism](https://arxiv.org/abs/2512.11374)
*Tomáš Koref,Lena Held,Mahammad Namazov,Harun Kumru,Yassine Thlija,Christoph Burchard,Ivan Habernal*

Main category: cs.CL

TL;DR: 该研究开发了自动化方法来检测和分类捷克最高法院判决中的司法推理，挑战了关于中欧和东欧形式主义司法的普遍观点，并展示了法律论证挖掘在计算法学研究中的潜力。


<details>
  <summary>Details</summary>
Motivation: 法院需要为其判决提供理由，但大规模系统分析司法推理仍然困难。该研究旨在反驳关于中欧和东欧形式主义司法的说法，通过开发自动化方法来检测和分类司法推理。

Method: 创建了MADON数据集，包含捷克两个最高法院的272个判决，专家标注了9,183个段落和8种论证类型。使用30万捷克法院判决语料库，通过持续预训练调整transformer LLMs以适应捷克法律领域，并采用非对称损失和类别加权等方法处理数据集不平衡问题。

Result: 最佳模型成功检测论证性段落（82.6% macro-F1），分类传统法律论证类型（77.5% macro-F1），并将判决分类为形式主义/非形式主义（83.2% macro-F1）。结合ModernBERT、Llama 3.1和传统特征机器学习的三阶段流水线在决策分类方面取得了有希望的结果。

Conclusion: 法律论证挖掘能够实现可靠的司法哲学分类，并展示了在计算法学研究中其他重要任务中的潜力。该方法论易于在不同司法管辖区复制，所有流水线、数据集、指南、模型和源代码均已公开。

Abstract: Courts must justify their decisions, but systematically analyzing judicial reasoning at scale remains difficult. This study refutes claims about formalistic judging in Central and Eastern Europe (CEE) by developing automated methods to detect and classify judicial reasoning in Czech Supreme Courts' decisions using state-of-the-art natural language processing methods. We create the MADON dataset of 272 decisions from two Czech Supreme Courts with expert annotations of 9,183 paragraphs with eight argument types and holistic formalism labels for supervised training and evaluation. Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining and experiment with methods to address dataset imbalance including asymmetric loss and class weighting. The best models successfully detect argumentative paragraphs (82.6\% macro-F1), classify traditional types of legal argument (77.5\% macro-F1), and classify decisions as formalistic/non-formalistic (83.2\% macro-F1). Our three-stage pipeline combining ModernBERT, Llama 3.1, and traditional feature-based machine learning achieves promising results for decision classification while reducing computational costs and increasing explainability. Empirically, we challenge prevailing narratives about CEE formalism. This work shows that legal argument mining enables reliable judicial philosophy classification and shows the potential of legal argument mining for other important tasks in computational legal studies. Our methodology is easily replicable across jurisdictions, and our entire pipeline, datasets, guidelines, models, and source codes are available at https://github.com/trusthlt/madon.

</details>


### [18] [Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis](https://arxiv.org/abs/2512.11388)
*Felipe Ribeiro Fujita de Mello,Hideyuki Takada*

Main category: cs.CL

TL;DR: 研究数据选择对开放大语言模型机器翻译微调的影响，发现语义选择器优于词汇和几何启发式方法，即使数据差异小于3%也会显著影响性能


<details>
  <summary>Details</summary>
Motivation: 研究数据选择策略对机器翻译微调效果的影响，特别是在开放大语言模型场景下，探索不同数据选择方法的有效性差异

Method: 使用日英双语语料库，在受控训练条件下比较五种数据选择方法：TF-IDF（词汇统计）、COMET Kiwi（语义质量评估）、QuRate（质量评估）、FD-Score（几何距离）和随机选择

Result: 语义选择器（如COMET Kiwi）在性能上持续优于词汇统计（TF-IDF）和几何距离（FD-Score）等启发式方法；即使所选数据差异小于3%，对模型性能的影响仍然显著

Conclusion: 微调过程对数据质量极为敏感，语义质量评估是选择训练数据的关键因素，即使是微小的数据差异也会对最终翻译性能产生实质性影响

Abstract: We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.

</details>


### [19] [Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction](https://arxiv.org/abs/2512.11399)
*Galann Pennec,Zhengyuan Liu,Nicholas Asher,Philippe Muller,Nancy F. Chen*

Main category: cs.CL

TL;DR: 提出一种基于轻量级视频描述模型和LLM的关键片段选择方法，用于构建多模态视频摘要，在保持低计算成本的同时提高摘要质量。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型处理长视频时容易丢失重要视觉信息，且需要设计成本效益高的长视频分析工具。

Method: 将视频分割为短片段，使用轻量级视频描述模型生成每个片段的紧凑视觉描述，然后通过大语言模型选择K个包含最相关视觉信息的关键片段用于多模态摘要。

Result: 在MovieSum数据集上，该方法达到接近参考片段的摘要性能，比随机片段选择捕获更多相关视频信息，同时保持低计算成本。

Conclusion: 提出的片段选择方法能够有效识别关键视频时刻用于多模态摘要，在计算效率和摘要质量之间取得良好平衡。

Abstract: Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.

</details>


### [20] [CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare](https://arxiv.org/abs/2512.11437)
*Akash Ghosh,Srivarshinee Sridhar,Raghav Kaushik Ravi,Muhsin Muhsin,Sriparna Saha,Chirag Agarwal*

Main category: cs.CL

TL;DR: CLINIC是一个全面的多语言基准测试，用于评估医疗领域语言模型的可信度，涵盖5个关键维度、18个任务和15种语言，揭示了现有模型在事实准确性、偏见、隐私等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 语言模型在医疗系统中的应用前景广阔，但缺乏对其可信度的可靠评估，特别是在多语言医疗环境中。现有模型主要针对高资源语言训练，难以处理中低资源语言的医疗查询复杂性，这在全球医疗部署中构成重大挑战。

Method: 提出了CLINIC基准测试，系统性地评估语言模型在五个可信度维度上的表现：真实性、公平性、安全性、鲁棒性和隐私性。通过18个多样化任务，涵盖15种语言（覆盖各大洲），涉及疾病状况、预防措施、诊断测试、治疗、手术和药物等关键医疗主题。

Result: 广泛评估显示，语言模型在事实正确性方面存在困难，在不同人口统计和语言群体中表现出偏见，容易受到隐私泄露和对抗性攻击的影响。

Conclusion: 通过揭示这些缺陷，CLINIC为增强语言模型在全球医疗领域中的覆盖范围和安全性的基础工作奠定了基础，特别是在多样化语言环境中。

Abstract: Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.

</details>


### [21] [Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction](https://arxiv.org/abs/2512.11502)
*Kai Golan Hashiloni,Brenda Kasabe Nokai,Michal Shevach,Esthy Shemesh,Ronit Bartin,Anna Bergrin,Liran Harel,Nachum Dershowitz,Liat Nadai Arad,Kfir Bar*

Main category: cs.CL

TL;DR: 提出一个新的希伯来语医疗语言模型，用于从电子健康记录中提取结构化临床时间线，构建患者旅程。模型基于DictaBERT 2.0，在500多万条去标识化医院记录上持续预训练。


<details>
  <summary>Details</summary>
Motivation: 需要从希伯来语电子健康记录中提取结构化临床时间线来构建患者旅程，但缺乏专门针对希伯来语医疗文本的模型。

Method: 基于DictaBERT 2.0架构，在500多万条去标识化的医院记录上进行持续预训练。引入两个新的数据集（内科/急诊科和肿瘤科）用于评估事件时间关系。采用词汇适应技术提高标记效率。

Result: 模型在两个新数据集上都表现出色。词汇适应提高了标记效率，去标识化不影响下游任务性能，支持隐私保护的模型开发。

Conclusion: 成功开发了一个有效的希伯来语医疗语言模型，能够从电子健康记录中提取临床时间线，在保护隐私的前提下实现患者旅程构建。模型在伦理限制下可供研究使用。

Abstract: We present a new Hebrew medical language model designed to extract structured clinical timelines from electronic health records, enabling the construction of patient journeys. Our model is based on DictaBERT 2.0 and continually pre-trained on over five million de-identified hospital records. To evaluate its effectiveness, we introduce two new datasets -- one from internal medicine and emergency departments, and another from oncology -- annotated for event temporal relations. Our results show that our model achieves strong performance on both datasets. We also find that vocabulary adaptation improves token efficiency and that de-identification does not compromise downstream performance, supporting privacy-conscious model development. The model is made available for research use under ethical restrictions.

</details>


### [22] [Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs](https://arxiv.org/abs/2512.11509)
*Mohor Banerjee,Nadya Yuki Wangsajaya,Syed Ali Redha Alsagoff,Min Sen Tan,Zachary Choy Kit Chun,Alvin Chan Guo Wei*

Main category: cs.CL

TL;DR: 研究探索了三种减少大语言模型幻觉的技术（CoVe、DoLa、RAG）对模型创造力的影响，发现在发散性创造力方面，CoVe增强、DoLa抑制、RAG影响最小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具有强大的自然语言理解和推理能力，但存在幻觉问题（生成事实错误内容）。现有减少幻觉的方法对创造力的影响尚未被探索，而AI辅助科学发现既需要事实准确性又需要创造性假设生成。

Method: 研究调查了三种幻觉减少技术：验证链（CoVe）、层对比解码（DoLa）和检索增强生成（RAG）。在多个模型家族（LLaMA、Qwen、Mistral）和不同规模（1B-70B参数）上，使用两个创造力基准（NeoCoder和CS4）进行评估。

Result: 这些方法对发散性创造力有相反的影响：CoVe增强发散性思维，DoLa抑制发散性思维，而RAG影响最小。研究结果为科学应用中平衡事实准确性和创造性探索提供了指导。

Conclusion: 在需要平衡事实准确性和创造性探索的科学应用中，应根据具体需求选择合适的幻觉减少方法：CoVe适合需要增强创造性的场景，DoLa适合需要严格控制事实准确性的场景，RAG则影响较小。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.

</details>


### [23] [Visualizing token importance for black-box language models](https://arxiv.org/abs/2512.11573)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出DBSA方法，用于审计黑盒大语言模型，分析每个输入token对输出的敏感度，无需模型内部信息。


<details>
  <summary>Details</summary>
Motivation: 现有LLM审计方法通常关注特定方面（如偏见检测），缺乏对黑盒模型中输入token如何影响输出的系统性理解。实际应用中，许多LLM通过不可访问的API端点部署，需要轻量级、模型无关的分析工具。

Method: 提出Distribution-Based Sensitivity Analysis (DBSA)，一种轻量级、模型无关的方法，无需对LLM做分布假设。通过分析输出对每个输入token的敏感度，提供即插即用的可视化探索工具。

Result: DBSA能够帮助用户检查LLM输入，发现现有可解释性方法可能忽略的敏感度问题。通过示例展示了该方法在实际应用中的有效性。

Conclusion: DBSA为从业者提供了一个实用的黑盒LLM审计工具，能够快速分析输入token对输出的影响，有助于在高风险领域确保LLM部署的可靠性。

Abstract: We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.

</details>


### [24] [Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols](https://arxiv.org/abs/2512.11614)
*Björn Deiseroth,Max Henning Höth,Kristian Kersting,Letitia Parcalabescu*

Main category: cs.CL

TL;DR: 该论文提出了一个基于Merlin-Arthur协议的RAG训练框架，将检索增强生成系统视为交互式证明系统，通过对抗性训练提高LLM的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统将检索视为弱启发式而非可验证证据，导致LLM在没有支持的情况下回答、在上下文不完整或误导时产生幻觉、依赖虚假证据。

Method: 采用Merlin-Arthur协议框架：Arthur（生成器LLM）在未知来源的问题上训练，Merlin提供有用证据，Morgana注入对抗性误导上下文。使用线性时间XAI方法识别和修改对Arthur最有影响的证据。

Result: 在三个RAG数据集和两种不同规模的模型家族上，M/A训练的LLM显示出改进的groundedness、完整性、正确性和拒绝行为，减少了幻觉，无需手动标注不可回答问题。检索器也通过自动生成的M/A硬正负样本提高了召回率和MRR。

Conclusion: 自主交互式证明风格监督为可靠的RAG系统提供了原则性和实用性的路径，将检索文档视为可验证证据而非建议。

Abstract: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.

</details>


### [25] [Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling](https://arxiv.org/abs/2512.11635)
*Keerthana Murugaraj,Salima Lamsiyah,Marten During,Martin Theobald*

Main category: cs.CL

TL;DR: 使用BERTopic模型分析1955-2018年报纸档案中核能安全相关主题的演变，克服传统LDA方法在处理历史文本时的局限性


<details>
  <summary>Details</summary>
Motivation: 传统主题建模方法（如LDA）在处理大规模历史报纸档案时存在局限，难以捕捉话题演化和动态性，需要更先进的方法来分析核能安全相关公共话语的长期趋势

Method: 采用BERTopic神经主题建模方法，利用基于transformer的嵌入技术从1955-2018年的报纸文章中提取和分类主题，分析主题分布及其时间演化

Result: BERTopic能够有效识别核能和核武器相关主题的共现模式及其重要性随时间的变化，揭示了公共话语的长期趋势和转变，证明了该方法在历史研究中的可扩展性和上下文敏感性

Conclusion: BERTopic作为传统方法的替代方案，为从报纸档案中提取历史话语提供了更丰富的见解，有助于历史、核能和社会科学研究，同时指出了当前局限性和未来研究方向

Abstract: Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.

</details>


### [26] [Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks](https://arxiv.org/abs/2512.11718)
*Sergey Pankratov,Dan Alistarh*

Main category: cs.CL

TL;DR: 该论文建立了确定性推测生成算法的首个"紧"下界，通过将token生成过程与分支随机游走类比，证明了每个推测迭代成功预测的token数期望值存在理论上限。


<details>
  <summary>Details</summary>
Motivation: 推测生成技术通过并行验证多个草稿token来加速大语言模型推理，但对其可实现的加速上限缺乏理论理解。本文旨在建立推测生成算法的基本性能极限。

Method: 将token生成过程与分支随机游走进行类比，分析最优草稿树选择问题。在基本假设下，通过理论推导建立数学界限。

Result: 证明了每个推测迭代成功预测的token数期望值存在上界：𝔼[X] ≤ (μ+ μ₂)log(P)/μ² + O(1)，其中P是验证器容量，μ是验证器输出分布的期望熵，μ₂是期望第二对数矩。

Conclusion: 该研究为并行token生成的极限提供了新的理论见解，可指导未来推测解码系统的设计。在Llama模型上的实证评估验证了理论预测的紧性。

Abstract: Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\mathbb{E}[X] \leq (μ+ μ_{(2)})\log(P )/μ^2 + O(1)$, where $P$ is the verifier's capacity, $μ$ is the expected entropy of the verifier's output distribution, and $μ_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.

</details>


### [27] [SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support](https://arxiv.org/abs/2512.11755)
*Yuming Feng,Xinrui Jiang*

Main category: cs.CL

TL;DR: SUMFORU是一个可引导的评论摘要框架，通过用户角色对齐实现个性化购买决策支持，在一致性、事实基础和偏好对齐方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 在线产品评论包含丰富但嘈杂的信息，现有LLM摘要器过于通用且无法考虑个人偏好，限制了实际应用价值。

Method: 构建基于Amazon 2023评论数据集的高质量数据管道，采用两阶段对齐方法：1) 通过非对称知识蒸馏进行角色感知的监督微调；2) 使用偏好估计器进行强化学习与AI反馈，捕捉细粒度的角色相关信号。

Result: 在基于规则、LLM和人工评估指标上均表现一致提升，在所有评估设置中取得最高性能，并能有效泛化到未见过的产品类别。

Conclusion: 可引导的多元化对齐方法为构建下一代个性化决策支持系统提供了前景。

Abstract: Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems](https://arxiv.org/abs/2512.10975)
*Matvey Nepomnyaschiy,Oleg Pereziabov,Anvar Tliamov,Stanislav Mikhailov,Ilya Afanasyev*

Main category: cs.LG

TL;DR: 提出了一种用于训练多模态情感识别系统的多智能体框架，通过模块化架构实现新模态的灵活集成和过时组件的无缝替换，同时降低训练计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态深度学习的情感识别模型虽然精度高，但训练和维护计算密集，且对模态变化的适应性不足，需要更灵活、可扩展的解决方案。

Method: 设计了一个多智能体框架，其中每个模态编码器和融合分类器作为自主智能体，由中央监督器协调。支持视觉、音频和文本模态，分类器作为共享决策智能体。

Result: 通过概念验证实现证明了该方法的可行性，框架不仅提高了训练效率，还支持新模态（如通过emotion2vec的音频特征）的模块化集成。

Conclusion: 该多智能体框架为HAI场景中的具身和虚拟智能体提供了更灵活、可扩展和可维护的感知模块设计方案，改善了训练效率和系统适应性。

Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.

</details>


### [29] [TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis](https://arxiv.org/abs/2512.10973)
*Jiang Liu,Yujie Li,Chan Zhou,Yihao Xie,Qilong Sun,Xin Shu,Peiwei Li,Chunyong Yang,Yiziting Zhu,Jiaqi Zhu,Yuwen Chen,Bo An,Hao Wu,Bin Yi*

Main category: cs.LG

TL;DR: 本研究提出了一种基于强化学习的框架，用于优化外科脓毒症患者的个性化肝素治疗，通过连续cxSOFA评分和TECM评估矩阵，显著降低了死亡率和住院时间。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是由严重感染引起的危及生命的疾病，会导致急性器官功能障碍。目前需要数据驱动的指标和连续奖励函数来优化外科脓毒症患者的个性化肝素治疗。

Method: 使用MIMIC-IV v1.0和eICU v2.0数据库数据，开发了新的强化学习框架：1) 将离散SOFA评分转换为连续cxSOFA用于更细致的状态和奖励函数；2) 基于cxSOFA以逐步方式定义"好"或"坏"的治疗策略；3) 提出治疗效应比较矩阵(TECM)来评估治疗策略。应用了Q-Learning、DQN、DDQN、BCQ和CQL等强化学习算法。

Result: 在AI衍生的策略中，cxSOFA-CQL模型表现最佳，将死亡率从1.83%降低到0.74%，平均住院时间从11.11天减少到9.42天。TECM在不同模型中显示出一致的结果，证明了框架的稳健性。

Conclusion: 提出的强化学习框架能够实现外科脓毒症患者肝素治疗的可解释和稳健优化。连续cxSOFA评分和基于TECM的评估提供了细致的治疗评估，显示出改善临床结果和决策支持可靠性的潜力。

Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.

</details>


### [30] [MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax](https://arxiv.org/abs/2512.10991)
*Zhanpeng Chen,Weihao Gao,Shunyu Wang,Yanan Zhu,Hong Meng,Yuexian Zou*

Main category: cs.LG

TL;DR: MolSculpt是一个新颖的3D分子生成框架，通过"雕刻"化学语法来生成精确的3D分子几何结构，将1D分子基础模型的化学知识注入3D扩散模型，实现跨模态信息整合。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用1D表示（如SELFIES）确保分子有效性，但未能充分利用1D模型中丰富的化学知识，导致1D语法生成与3D几何实现之间存在脱节。需要弥合这一差距以生成更精确的3D分子几何结构。

Method: MolSculpt基于冻结的1D分子基础模型和3D分子扩散模型构建。引入可学习查询从基础模型中提取固有化学知识，然后通过可训练投影器将这些跨模态信息注入扩散模型的条件空间，以指导3D几何生成。通过端到端优化将1D潜在化学知识深度整合到3D生成过程中。

Result: 实验表明，MolSculpt在从头3D分子生成和条件3D分子生成方面实现了最先进的性能，在GEOM-DRUGS和QM9数据集上显示出优越的3D保真度和稳定性。

Conclusion: MolSculpt成功地将1D化学知识与3D几何生成相结合，通过跨模态信息整合提高了3D分子生成的准确性和稳定性，为药物发现和材料科学提供了有效的工具。

Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.

</details>


### [31] [Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning](https://arxiv.org/abs/2512.11095)
*Sana Rahmani,Javad Hashemi,Ali Etemad*

Main category: cs.LG

TL;DR: 该研究首次系统性地将部分标签学习（PLL）方法应用于心电图（ECG）诊断，评估了9种PLL算法在不同类型标签模糊性下的表现，发现现有方法对不同类型的模糊性鲁棒性差异显著。


<details>
  <summary>Details</summary>
Motivation: 现实世界的心电图诊断存在固有的标签模糊性问题，源于重叠病症和诊断分歧。然而当前ECG模型都假设标签是干净无歧义的，这限制了模型在真实条件下的发展和有意义的评估。虽然部分标签学习（PLL）框架旨在从模糊标签中学习，但其在医疗时间序列领域（特别是ECG）的有效性尚未得到充分探索。

Method: 将9种PLL算法适配到多标签ECG诊断任务中，使用多样化的临床动机模糊性生成策略进行评估，包括非结构化（如随机）和结构化模糊性（如心脏病专家推导的相似性、治疗关系和诊断分类）。在PTB-XL和Chapman数据集上进行实验。

Result: 实验表明，PLL方法对不同类型和程度的模糊性表现出显著不同的鲁棒性。通过深入分析，识别了当前PLL方法在临床环境中的关键局限性。

Conclusion: 该研究为ECG诊断开发鲁棒且临床对齐的模糊性感知学习框架指明了未来方向，强调了在真实世界医疗应用中考虑标签模糊性的重要性。

Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.

</details>


### [32] [Limits and Gains of Test-Time Scaling in Vision-Language Reasoning](https://arxiv.org/abs/2512.11109)
*Mohammadjavad Ahmadpour,Amirmahdi Meighani,Payam Taebi,Omid Ghahroodi,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文系统研究了测试时扩展（TTS）在视觉语言模型中的应用效果，发现闭源模型能从结构化推理和迭代自优化中受益，而开源模型表现不一致，外部验证效果最好，迭代优化反而可能降低性能。TTS效果高度依赖于数据集和任务类型。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展（TTS）在提升大型语言模型推理能力方面表现出色，但在多模态系统如视觉语言模型中的应用尚未充分探索。本文旨在系统研究推理时间方法在不同视觉语言模型上的应用效果，填补这一研究空白。

Method: 采用系统性实证研究方法，在开源和闭源视觉语言模型上应用推理时间方法，包括结构化推理、迭代自优化和外部验证等策略，在不同基准测试上进行评估。

Result: 闭源模型能持续从结构化推理和迭代自优化中受益；开源模型表现不一致，外部验证提供最可靠的性能提升，而迭代优化往往降低性能；TTS效果高度数据集依赖，在多步推理任务上改善明显，但在感知密集型基准上提升有限。

Conclusion: TTS不是通用解决方案，必须根据模型能力和任务特性进行定制。这推动了未来自适应TTS策略和多模态奖励模型的研究方向。

Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.

</details>


### [33] [Fairness-Regularized Online Optimization with Switching Costs](https://arxiv.org/abs/2512.11131)
*Pengfei Li,Yuelin Han,Adam Wierman,Shaolei Ren*

Main category: cs.LG

TL;DR: 本文研究公平性与动作平滑性同时优化的在线凸优化问题，提出FairOBD算法，在动态计算资源配置中实现公平性正则化成本最小化。


<details>
  <summary>Details</summary>
Motivation: 公平性和动作平滑性是在线优化中的两个关键考虑因素，但现有研究尚未同时解决这两个问题。本文旨在研究带有切换成本的公平性正则化平滑在线凸优化这一新挑战性场景。

Method: 提出FairOBD算法：1) 通过引入辅助变量将长期公平性成本分解为一系列在线成本；2) 利用辅助变量正则化在线动作以实现公平结果；3) 采用新方法处理切换成本。

Result: 理论证明：即使没有切换成本，任何在线算法也无法实现相对于离线最优算法的次线性遗憾或有限竞争比。FairOBD算法针对参数化约束的最优离线算法基准，提供了最坏情况下的渐进竞争比。

Conclusion: FairOBD算法能有效调和命中成本、切换成本和公平性成本之间的张力，在动态计算资源配置实验中相比现有基线方案能更好地减少总公平性正则化成本并促进公平结果。

Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.

</details>


### [34] [The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions](https://arxiv.org/abs/2512.11138)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: Vekua Layer (VL) 是一种基于广义解析函数理论的微分谱方法，通过将假设空间限制在控制微分算子的核中，将学习任务从非凸优化转化为凸最小二乘问题，在椭圆PDE上实现了机器精度重建和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在参数化物理场方面表现出色，但存在谱偏差和非凸优化的计算开销问题。需要一种更高效、更稳定的方法来学习物理场表示。

Method: 提出Vekua Layer (VL)，基于广义解析函数理论，将假设空间限制在控制微分算子的核中（使用调和基和傅里叶-贝塞尔基），将学习任务转化为严格凸的最小二乘问题，通过线性投影求解。

Result: 在齐次椭圆PDE上，VL相比SIRENs实现了机器精度（MSE ≈ 10^-33）的精确重建，在非相干传感器噪声下表现出更好的稳定性（MSE ≈ 0.03），并能从部分边界数据通过解析延拓实现全局场的"全息"外推。

Conclusion: VL作为一种基于物理的谱滤波器，通过将学习任务转化为凸优化问题，克服了传统INRs的谱偏差和计算效率问题，在物理场表示方面具有优越的精度、稳定性和外推能力。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.

</details>


### [35] [Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles](https://arxiv.org/abs/2512.11145)
*Lennard Manuel,Hamid Gadirov,Steffen Frey*

Main category: cs.LG

TL;DR: 提出一种增强型自编码器框架，结合聚类损失和对比损失，用于高维科学集合数据的可视化与分析。


<details>
  <summary>Details</summary>
Motivation: 高维复杂的科学集合数据在分析和可视化方面面临挑战，传统降维技术和自编码器难以有效处理此类数据。

Method: 使用EfficientNetV2为无标签数据生成伪标签，构建增强型自编码器框架，联合优化重构损失、基于软轮廓分数的聚类损失和对比损失，最后用UMAP生成2D投影。

Result: 在土壤通道结构和液滴冲击薄膜两个科学集合数据集上，结合聚类或对比损失的模型在特征提取和可视化方面略优于基线方法。

Conclusion: 提出的增强型自编码器框架能有效改善高维科学集合数据的可视化效果和可解释性，为复杂科学数据的分析提供了新方法。

Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.

</details>


### [36] [Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities](https://arxiv.org/abs/2512.11178)
*Takuya Kurihana,Xiaojian Zhang,Wing Yee Au,Hon Yung Wong*

Main category: cs.LG

TL;DR: 提出一个异构数据管道，用于融合时变、空间变化和多模态的城市数据，通过图学习整合空间同质性，实现跨领域和跨地区的城市问题预测。


<details>
  <summary>Details</summary>
Motivation: 现代城市依赖数据驱动决策，但城市数据存在异构格式、分散收集、标准不一的问题。国家级数据集虽然广泛可用，但具有显著异质性和多模态特性，需要统一处理框架。

Method: 提出异构数据管道，执行跨领域数据融合，处理时变、空间变化的时间序列数据。数据学习模块将空间变化数据集的同质性整合到图学习中，将不同地区的信息嵌入模型。

Result: 使用来自多个城市的50多个数据源（如拼车、交通事故、犯罪报告等）进行五个真实世界观察。结果显示框架具有强预测性能，迁移到新地区或领域时只需最小重新配置。

Conclusion: 该研究推进了以可扩展方式构建数据驱动的城市系统的目标，解决了智慧城市分析中最紧迫的挑战之一，展示了框架的通用性和灵活性。

Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.

</details>


### [37] [Progress over Points: Reframing LM Benchmarks Around Scientific Objectives](https://arxiv.org/abs/2512.11183)
*Alwin Jin,Sean M. Hendryx,Vaskar Nath*

Main category: cs.LG

TL;DR: 该论文提出从静态基准测试转向面向进展的基准测试，以NanoGPT速度挑战为实例，将基准测试本身作为科学进步的目标，而非仅仅是模型比较的工具。


<details>
  <summary>Details</summary>
Motivation: 当前基于静态、已解决问题（如数学应用题）的基准测试虽然能展示基本能力获取，但限制了可衡量和激励的进步类型。需要一种能直接推动科学进展的基准测试范式。

Method: 提出面向进展的基准测试框架，以NanoGPT速度挑战为实例：标准化数据集切片、参考模型和训练工具、丰富遥测数据，包含运行时验证和防作弊检查。评估重点放在实现的科学增量上。

Result: 在该环境中实现了新的最先进训练时间，比先前记录提高了3秒，并定性观察到新颖算法思想的出现。基准测试促进了语言建模堆栈的可复用改进。

Conclusion: 该研究旨在推动社区从静态问题排行榜转向对开放式但可测量的科学问题进行测试时研究。在这种新范式中，基准测试的进展就是科学的进展，从而将"基准测试"重新定义为科学进步的工具。

Abstract: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing "benchmarking" as a vehicle for scientific advancement.

</details>


### [38] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 提出梯度投影框架，通过投影梯度到敏感特征嵌入空间的正交补空间，实现概念级别的选择性遗忘，解决扩散模型中的记忆化问题


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型中的记忆化带来安全和知识产权风险，传统去记忆化技术无法系统防止概念级别特征的内化，丢弃所有包含敏感特征的图像会浪费宝贵训练数据

Method: 梯度投影框架：在反向传播期间识别并切除与禁止属性嵌入对齐的训练信号，将每个梯度更新投影到敏感特征嵌入空间的正交补空间，从而消除其对模型权重的影响

Result: 框架大幅减少记忆化，同时严格保持生成质量和语义保真度，与现有防御方法兼容，为对抗特征提取攻击提供有效保护

Conclusion: 通过将记忆化控制重构为选择性学习，该方法为IP安全和隐私保护的生成式AI建立了新范式，实现了概念级别的选择性遗忘

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [39] [Fast EXP3 Algorithms](https://arxiv.org/abs/2512.11201)
*Ryoma Sato,Shinji Ito*

Main category: cs.LG

TL;DR: EXP3算法可实现每轮常数时间运行，作者提出了更实用的算法，并分析了这些算法的遗憾界与时间复杂度的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 虽然EXP3算法在对抗性多臂赌博机问题中表现良好，但其实现复杂度可能较高。作者旨在开发更高效的算法版本，在保持良好理论保证的同时降低计算开销。

Method: 作者指出EXP3算法可以在每轮中以常数时间实现，并提出了更实用的算法变体。这些方法通过优化算法结构和实现细节来平衡计算效率与理论性能。

Result: 研究展示了如何在保持EXP3算法理论保证的同时实现常数时间每轮运行，并提供了不同算法变体在遗憾界和时间复杂度之间的具体权衡分析。

Conclusion: EXP3算法可以实现高效的时间复杂度，通过适当的算法设计可以在计算效率和理论性能之间找到平衡，为实际应用提供了更实用的解决方案。

Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.

</details>


### [40] [Latent Variable Causal Discovery under Selection Bias](https://arxiv.org/abs/2512.11219)
*Haoyue Dai,Yiwen Qiu,Ignavier Ng,Xinshuai Dong,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文研究了在存在选择偏差的情况下进行潜变量因果发现的问题，提出利用协方差子矩阵的秩约束作为处理选择偏差的新工具。


<details>
  <summary>Details</summary>
Motivation: 处理潜变量因果发现中的选择偏差是一个重要但尚未充分探索的问题，主要是因为缺乏合适的统计工具。现有的处理潜变量的工具都没有针对选择偏差进行适配。

Method: 研究秩约束作为条件独立性约束的推广，利用线性高斯模型中协方差子矩阵的秩。提供了这种秩约束的图论特征化，并展示了如何利用该工具在存在选择偏差的情况下识别经典潜变量模型。

Result: 研究发现，尽管选择偏差会显著复杂化联合分布，但偏差协方差矩阵中的秩仍然保留了关于因果结构和选择机制的有意义信息。证明了经典的单因子模型在选择偏差下可以被识别。

Conclusion: 秩约束是处理潜变量因果发现中选择偏差的有效工具，模拟和真实世界实验证实了使用秩约束的有效性。

Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.

</details>


### [41] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出ASR-KF-EGR框架，通过可逆软冻结机制在推理时动态暂停低重要性token的KV更新，减少55-67%的KV缓存占用，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文LLM部署中的内存限制问题，传统方法要么永久丢弃上下文（影响质量），要么需要大量内存。需要一种无需训练、能动态管理KV缓存的方法。

Method: 1. 可逆软冻结机制：在滑动注意力窗口中识别低重要性token，暂时暂停其KV更新；2. 所有token保存在GPU外存储，按需恢复；3. 亚线性冻结调度：冻结时长随重复检测次数亚线性增长，避免过度压缩；4. 无需微调，架构无关。

Result: 在LLaMA-3 8B上的初步实验显示：1. 主动KV缓存大小减少55-67%；2. 保持生成质量；3. 通过needle-in-haystack检索测试；4. 无需训练，适用于内存受限环境。

Conclusion: ASR-KF-EGR为长上下文LLM的内存受限部署提供了实用解决方案，通过动态KV缓存管理在保持质量的同时显著减少内存占用，且无需模型微调。

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [42] [Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language](https://arxiv.org/abs/2512.11251)
*Yunkai Zhang,Yawen Zhang,Ming Zheng,Kezhen Chen,Chongyang Gao,Ruian Ge,Siyuan Teng,Amine Jelloul,Jinmeng Rao,Xiaoyuan Guo,Chiang-Wei Fang,Zeyu Zheng,Jie Yang*

Main category: cs.LG

TL;DR: Insight Miner是一个用于生成高质量时间序列描述的多模态大模型，通过TS-Insights数据集进行指令微调，在时间序列分析任务上超越了现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多个领域都很重要，但从中挖掘洞察需要深厚的领域专业知识，这个过程耗时耗力。因此需要开发能够自动生成高质量时间序列描述和洞察的AI系统。

Method: 提出了Insight Miner多模态大模型，并创建了TS-Insights数据集（包含100k时间序列窗口，来自20个预测数据集）。使用创新的智能体工作流：先用统计工具从原始时间序列提取特征，再用GPT-4合成连贯的趋势描述。在TS-Insights上进行指令微调。

Result: Insight Miner在生成时间序列描述和洞察方面超越了最先进的多模态模型（如LLaVA和GPT-4）。

Conclusion: 这项工作展示了利用多模态大模型进行时间序列分析的有前景方向，是让大语言模型将时间序列作为原生输入模态的基础性步骤。

Abstract: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \textbf{TS-Insights}\footnote{Available at \href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.

</details>


### [43] [QGEC : Quantum Golay Code Error Correction](https://arxiv.org/abs/2512.11307)
*Hideo Mukai,Hoshitaro Ohnishi*

Main category: cs.LG

TL;DR: 提出基于Golay码的量子错误校正方法QGEC，使用Transformer解码器，在多种噪声模型下评估性能，结果显示Golay码比toric码在更少数据量子位下获得更高解码精度。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在特定问题上比经典计算机有更低的计算负载，但量子比特易受外部噪声影响。量子错误校正（QEC）对处理量子比特至关重要，需要从稳定子生成器的综合征测量结果预测实际错误。

Method: 提出量子Golay码错误校正（QGEC）方法，使用经典信息论中的高效编码方法Golay码。采用Transformer进行解码计算，在由生成多项式定义的码空间中评估解码器精度，使用三种不同权重集和三种不同比特翻转错误与相位翻转错误相关性的噪声模型。

Result: 较小相关性的噪声模型给出更好的精度，而生成多项式的权重对解码器精度影响很小。Golay码（需要23个数据量子位，码距为7）比toric码（需要50个数据量子位，码距为5）实现了更高的解码精度。

Conclusion: 使用Transformer实现量子错误校正可能使Golay码更高效地实现容错量子计算，表明在量子错误校正中，码距比数据量子位数对解码性能有更重要的影响。

Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.

</details>


### [44] [Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315)
*Pranav Guruprasad,Sudipta Chowdhury,Harsh Sikka,Mridul Sharma,Helen Lu,Sean Rivera,Aryan Khurana,Hangliang Ren,Yangyue Wang*

Main category: cs.LG

TL;DR: MultiNet v1.0是一个统一基准测试，用于评估视觉语言模型和视觉语言动作模型在六个核心能力领域的跨领域泛化能力，发现当前基础模型在未见领域存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体评估方法分散在不同基准测试中，难以准确评估基础模型是否真正超越了训练分布而实现泛化。需要统一的评估框架来测量模型在跨领域任务中的泛化能力。

Method: 提出了MultiNet v1.0基准测试，涵盖六个核心能力领域：视觉基础、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制。在GPT-5、Pi0和Magma等模型上进行评估。

Result: 所有评估模型都未表现出一致的泛化能力，在未见领域、不熟悉模态或跨领域任务转移时都出现显著性能下降。具体表现为模态错位、输出格式不稳定和领域转移下的灾难性知识退化。

Conclusion: 当前基础模型在泛化智能方面仍存在明显差距，MultiNet v1.0为诊断这些差距和指导未来通用智能体开发提供了标准化评估基础。

Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.

</details>


### [45] [TV2TV: A Unified Framework for Interleaved Language and Video Generation](https://arxiv.org/abs/2512.05103)
*Xiaochuang Han,Youssef Emad,Melissa Hall,John Nguyen,Karthik Padthe,Liam Robbins,Amir Bar,Delong Chen,Michal Drozdzal,Maha Elbayad,Yushi Hu,Shang-Wen Li,Sreya Dutta Roy,Jakob Verbeek,XuDong Wang,Marjan Ghazvininejad,Luke Zettlemoyer,Emily Dinan*

Main category: cs.LG

TL;DR: TV2TV是一个创新的视频生成框架，通过交替生成文本和视频帧，让模型先"用文字思考"再"用像素行动"，显著提升了视频质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在处理需要复杂语义分支或高层次推理的视频内容时存在困难，需要一种能够进行开放文本推理和控制的视频生成方法。

Method: 提出TV2TV统一生成框架，将视频生成分解为交替的文本和视频生成过程。采用混合变换器架构联合学习语言建模和视频流匹配，在推理时动态决定何时在文本生成和视频帧生成之间切换。

Result: 在视频游戏数据实验中，TV2TV在视觉质量和可控性方面都有显著提升。在自然视频（体育视频）上训练也表现出强大的视觉质量和提示对齐能力。

Conclusion: TV2TV代表了向具有开放文本推理和控制能力的视频生成迈出的有希望的一步，通过让语言模型塔承担更多决策责任，实现了更好的视频生成效果。

Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.

</details>


### [46] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 提出塑性-天花板框架，将后训练性能分解为基础SFT性能和后续RL塑性，确定SFT-then-RL顺序管道为最优标准，并提供具体扩展指南。


<details>
  <summary>Details</summary>
Motivation: 当前后训练中结合监督微调(SFT)和强化学习(RL)的方法虽然有效，但如何最优利用专家轨迹的问题仍未解决，需要理论框架来指导实践。

Method: 提出塑性-天花板框架，将性能分解为基础SFT性能和RL塑性；通过大量基准测试，建立SFT-then-RL顺序管道；推导出三个具体扩展指南：1)在SFT稳定或轻度过拟合阶段转向RL；2)数据规模决定后训练潜力，轨迹难度作为性能乘数；3)使用最小SFT验证损失选择专家轨迹。

Result: 确定SFT-then-RL顺序管道优于同步方法，解决了稳定性问题；提供了具体的扩展指导原则，包括转向RL的最佳时机、数据规模和轨迹难度的作用，以及选择专家轨迹的指标。

Conclusion: 塑性-天花板框架为后训练提供了理论基础，SFT-then-RL顺序管道是最优标准，提出的扩展指南能够最大化专家轨迹的价值，为实际应用提供了可操作的指导。

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [47] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 本文提出了一种基于对称性的扩散策略引导框架，将等变扩散策略与强化学习结合，通过利用对称性提高样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然等变扩散策略结合了扩散模型的生成能力和几何对称性的泛化优势，但使用标准（非等变）强化学习进行微调会忽略对称性，导致样本效率低下和不稳定。需要开发能够利用对称性的引导框架。

Method: 首先理论证明等变扩散过程的扩散过程是等变的，从而诱导出适合等变扩散引导的群不变潜在噪声MDP。基于此理论，提出了原则性的对称感知引导框架，并通过实验比较标准、等变和近似等变强化学习策略。

Result: 实验表明，在对称性引导过程中利用对称性能带来显著好处：提高样本效率、防止价值发散，即使在演示数据极其有限的情况下也能实现强大的策略改进。同时确定了严格等变性在对称性破坏下的实际边界。

Conclusion: 对称性感知的强化学习引导框架能够有效利用等变扩散策略的对称性，显著提升样本效率和策略性能，为对称性环境下的策略优化提供了理论依据和实用方法。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [48] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: NSPO是一种新颖的强化学习框架，通过将安全策略梯度几何投影到通用任务的零空间，在保证大语言模型安全对齐的同时避免核心能力遗忘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实应用中需要确保其行为符合人类价值观、社会规范和伦理原则，但传统的强化学习安全对齐方法往往会导致模型遗忘已学习的通用能力，即"对齐税"问题。

Method: 提出零空间约束策略优化(NSPO)框架，将安全策略梯度几何投影到通用任务的零空间中，从而在安全对齐的同时保留模型原有的核心能力。

Result: NSPO在实验中大幅优于现有方法，在数学、代码和指令跟随等通用任务上保持准确性的同时，实现了最先进的安全性能。该方法数据效率高，仅需PKU-SafeRLHF中40%的公共人类标注安全数据。

Conclusion: NSPO通过零空间投影技术有效解决了大语言模型安全对齐中的能力遗忘问题，在保证安全性的同时保持了模型的通用能力，且具有较高的数据效率。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>


### [49] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: Co-TSFA是一个对比学习框架，通过区分短期异常和持久性分布漂移来改进时间序列预测，在异常条件下提升性能的同时保持正常数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列预测中，有些异常事件具有持久影响需要响应，而有些短期异常应该被忽略。标准预测模型无法区分这两种情况，要么对噪声过度反应，要么错过真正的分布漂移。

Method: 提出Co-TSFA对比学习框架，通过输入增强和输入-输出增强分别建模预测无关和预测相关的异常，引入潜在输出对齐损失将表示变化与预测变化关联，鼓励对无关扰动的不变性同时保持对有意义分布漂移的敏感性。

Result: 在Traffic、Electricity基准数据集和真实世界现金需求数据集上的实验表明，Co-TSFA在异常条件下提升了性能，同时在正常数据上保持了准确性。

Conclusion: Co-TSFA通过对比学习有效区分了短期异常和持久性分布漂移，为时间序列预测中的异常处理提供了新思路，在保持正常预测能力的同时增强了模型的鲁棒性。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


### [50] [Sliced ReLU attention: Quasi-linear contextual expressivity via sorting](https://arxiv.org/abs/2512.11411)
*Siwan Boufadène,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 提出切片ReLU注意力机制，通过一维投影和排序实现O(n log n)复杂度，适合长上下文，保持理论表达能力


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制（如softmax和ReLU-based）在长上下文场景下计算复杂度高，需要一种既能保持表达能力又能实现高效计算的新注意力机制

Method: 提出切片ReLU注意力机制，不直接对成对点积应用非线性，而是对键-查询差异的一维投影进行操作，利用排序实现准线性复杂度，构建可微分非对称核

Result: 该机制可实现O(n log n)复杂度，适合长上下文；理论证明保持表达能力，能执行非平凡序列到序列解缠任务，满足上下文通用逼近性质；小规模实验显示实际应用潜力

Conclusion: 切片ReLU注意力机制在计算效率和理论表达能力之间取得了良好平衡，为长上下文处理提供了有前景的解决方案

Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

</details>


### [51] [Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584)
*Stefan Tabakov,Asen Popov,Dimitar Dimitrov,S. Ensiye Kiyamousavi,Vladimir Hristov,Boris Kraychev*

Main category: cs.LG

TL;DR: 本文提出原子动作切片（AAS）方法，将长时程演示分解为短时、类型化的原子动作，以提升VLA模型的泛化能力，特别是在需要新技能或物体组合的任务中。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型泛化能力较差，特别是在需要新技能或物体组合的任务中表现不佳。需要一种方法能够更好地分解复杂任务，使规划器更易使用且策略更易学习。

Method: 提出原子动作切片（AAS）方法，将长时程演示分解为短时、类型化的原子动作。使用LIBERO演示生成包含2124个原子片段的验证数据集，标注动作类型、时间跨度和置信度。使用更强的分割模型（Gemini 2.5 Pro）匹配规划器定义的规划，并在关键帧抖动下保持鲁棒性。

Result: 在LIBERO-Goal数据集上，使用原子数据集微调CLIP-RT+将任务成功率从94.2%提升至95.3%；在LIBERO-Long数据集上从83.8%提升至88.8%。更强的分割模型能紧密匹配规划器定义的规划，而较小模型在多物体任务上表现较差。

Conclusion: 原子动作切片（AAS）方法能有效提升VLA模型的泛化能力，特别是在需要新技能或物体组合的任务中。公开发布了GATE-VLAP数据集，为相关研究提供了有价值的资源。

Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)

</details>


### [52] [Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces](https://arxiv.org/abs/2512.11448)
*Arghya Pratihar,Arnab Seal,Swagatam Das,Inesh Chattopadhyay*

Main category: cs.LG

TL;DR: HypeGBMS：将高斯模糊均值漂移扩展到双曲空间，用于处理具有层次结构的数据集，在非欧几里得设置中显著优于传统均值漂移方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯模糊均值漂移（GBMS）在欧几里得空间中能有效识别任意形状的聚类，但对于具有层次或树状结构的数据集表现不佳，需要扩展到能更好捕捉层次结构的几何空间。

Method: 将GBMS扩展到双曲空间，用双曲距离替换欧几里得计算，使用Möbius加权均值确保所有更新与空间几何保持一致，保留密度寻求行为的同时有效捕捉潜在层次结构。

Result: 在11个真实世界数据集上的实验评估表明，HypeGBMS在非欧几里得设置中显著优于传统均值漂移聚类方法，证明了其鲁棒性和有效性。

Conclusion: HypeGBMS桥接了经典均值漂移聚类和双曲表示学习，为弯曲空间中的基于密度的聚类提供了原则性方法，能有效处理具有层次结构的数据集。

Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.

</details>


### [53] [xGR: Efficient Generative Recommendation Serving at Scale](https://arxiv.org/abs/2512.11529)
*Qingxiao Sun,Tongxuan Liu,Shen Zhang,Siyu Wu,Peijun Yang,Haotian Liang,Menxin Li,Xiaolong Ma,Zhiwei Liang,Ziyi Ren,Minchao Zhang,Xinyu Liu,Ke Zhang,Depei Qian,Hailong Yang*

Main category: cs.LG

TL;DR: xGR是一个面向生成式推荐系统的服务系统，通过统一处理prefill和decode阶段、早期排序终止和基于掩码的项目过滤、以及重构整体流水线，在严格延迟约束下实现了至少3.49倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统（GR）虽然使用基于注意力的架构，但其工作负载与LLM服务有显著差异。GR通常处理长提示但产生短固定长度输出，且由于大beam宽度导致每个解码阶段计算成本高，同时在巨大项目空间中进行beam搜索时排序开销特别耗时。现有系统难以满足高并发场景下的严格低延迟要求。

Method: 1. 通过分阶段计算和分离的KV缓存统一处理prefill和decode阶段；2. 实现早期排序终止和基于掩码的项目过滤，重用数据结构；3. 重构整体流水线以利用多级重叠和多流并行。

Result: 在真实世界推荐服务数据集上的实验表明，xGR在严格延迟约束下相比最先进的基线实现了至少3.49倍的吞吐量提升。

Conclusion: xGR是一个专门为生成式推荐系统设计的服务系统，通过创新的架构优化有效解决了GR特有的性能瓶颈，在高并发场景下满足严格低延迟要求，显著提升了系统吞吐量。

Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

</details>


### [54] [Parametric Numerical Integration with (Differential) Machine Learning](https://arxiv.org/abs/2512.11530)
*Álvaro Leitao,Jonatan Ráfales*

Main category: cs.LG

TL;DR: 本文提出了一种基于微分学习的机器学习方法来解决参数积分问题，该方法在训练中融入导数信息，相比传统机器学习方法在多个积分问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: 参数积分在科学计算和工程应用中广泛存在，传统数值方法在处理高维或复杂参数空间时面临挑战。机器学习方法为此提供了新途径，但标准架构在精度和效率方面仍有改进空间。

Method: 提出微分学习框架，在训练过程中融入导数信息。该方法应用于三类代表性积分问题：统计泛函（包括矩和累积分布函数）、通过切比雪夫展开的函数逼近、以及微分方程直接产生的积分。

Result: 在所有测试案例中，基于微分机器学习的方法均优于标准架构，实现了更低的均方误差、更好的可扩展性和更高的样本效率。这些案例涵盖了从光滑闭式基准到具有挑战性的数值积分。

Conclusion: 微分学习框架为参数积分问题提供了有效的机器学习解决方案，通过利用导数信息显著提升了模型的性能和效率，在多种应用场景中展现出优越性。

Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.

</details>


### [55] [Fully Inductive Node Representation Learning via Graph View Transformation](https://arxiv.org/abs/2512.11561)
*Dooho Lee,Myeong Kong,Minho Jeong,Jaemin Yoo*

Main category: cs.LG

TL;DR: 提出图视图空间和Graph View Transformation (GVT)方法，实现无需重新训练的跨数据集全归纳推理，在27个节点分类基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在未见数据集上的泛化能力是构建基础模型的关键，但图结构数据中特征空间的维度和语义差异巨大，任何特征空间变换都可能破坏对未见数据集的归纳适用性，限制了图模型的设计空间。

Method: 引入视图空间作为新的表示轴，提出图视图变换(GVT)——一种在视图空间中节点和特征置换等变的映射，并构建循环GVT作为全归纳节点表示学习模型。

Result: 在OGBN-Arxiv上预训练并在27个节点分类基准上评估，循环GVT比现有全归纳图模型GraphAny提升8.93%，比12个单独调优的GNN至少提升3.30%。

Conclusion: 视图空间为全归纳节点表示学习提供了原则性和有效的理论基础，证明了该方法在跨数据集泛化方面的优越性。

Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

</details>


### [56] [Bridging Streaming Continual Learning via In-Context Large Tabular Models](https://arxiv.org/abs/2512.11668)
*Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros*

Main category: cs.LG

TL;DR: 论文提出使用大型上下文表格模型作为流式持续学习的桥梁，通过将无界数据流实时压缩为紧凑摘要，同时满足流学习的数据压缩需求和持续学习的经验回放需求。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习和流学习研究各自独立，缺乏统一的解决方案。持续学习关注长期记忆但缺乏实时约束，流学习强调快速适应但忽视遗忘问题。需要一种能同时处理概念漂移和灾难性遗忘的流式持续学习方法。

Method: 提出使用大型上下文表格模型作为核心架构，通过两种数据选择原则构建流式持续学习框架：1) 分布匹配（平衡可塑性与稳定性），2) 分布压缩（通过多样化和检索机制控制内存大小）。

Result: 论文提出了一个理论框架，将流学习和持续学习统一在流式持续学习范式下，通过大型上下文表格模型实现数据流的实时压缩和知识保留。

Conclusion: 大型上下文表格模型为流式持续学习提供了自然桥梁，通过分布匹配和分布压缩原则，能够同时满足流学习的数据压缩需求和持续学习的经验回放需求，为解决概念漂移和灾难性遗忘提供了统一的理论框架。

Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

</details>


### [57] [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760)
*Aditya Tripathi,Karan Sharma,Rahul Mishra,Tapas Kumar Maiti*

Main category: cs.LG

TL;DR: SpectralKrum是一种新的联邦学习防御方法，通过谱子空间估计结合几何邻居选择来抵御拜占庭攻击，在非IID数据分布下表现优于传统方法，但对某些特定攻击类型效果有限。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在客户端本地保留数据的架构存在根本性漏洞：拜占庭客户端可以注入任意损坏的更新来破坏全局模型。现有的鲁棒聚合方法在理想化假设下提供理论保证，但当客户端数据分布异质（非IID）且攻击者能够观察或近似防御机制时，其有效性会大幅降低。

Method: SpectralKrum融合了谱子空间估计和基于几何邻居的选择方法。核心洞察是：尽管存在每个客户端的异质性，良性优化轨迹会集中在低维流形附近，可以从历史聚合中估计该流形。方法将传入更新投影到学习到的子空间中，在压缩坐标中应用Krum选择，并过滤那些正交残差能量超过数据驱动阈值的候选更新。

Result: 在CIFAR-10数据集上使用狄利克雷分布的非IID分区（alpha=0.1）评估，SpectralKrum在8个鲁棒基线和7种攻击场景下进行了超过56,000轮训练的实验。结果显示，SpectralKrum在方向性和子空间感知攻击（自适应转向、缓冲区漂移）方面具有竞争力，但在标签翻转和最小最大攻击下优势有限，因为恶意更新在谱上与良性更新难以区分。

Conclusion: SpectralKrum是一种无需辅助数据、完全在模型更新上操作并保持联邦学习隐私特性的防御方法。它在非IID数据分布下对某些攻击类型有效，但对于恶意更新在谱上与良性更新难以区分的攻击场景，其优势有限。

Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.

</details>


### [58] [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784)
*Etienne Boursier,Claire Boyer*

Main category: cs.LG

TL;DR: 该论文提出了一个基于测度的统一框架，用于分析单层softmax注意力机制在有限和无限提示下的行为，证明了在长提示情况下softmax注意力收敛到线性算子，并建立了输出和梯度的非渐近集中界。


<details>
  <summary>Details</summary>
Motivation: softmax注意力是transformer架构的核心组件，但其非线性结构给理论分析带来了重大挑战。研究者希望建立一个统一的理论框架，理解softmax注意力在有限和无限提示下的行为，特别是其收敛性和训练动态。

Method: 开发了一个基于测度的统一框架，利用softmax算子在无限提示极限下收敛到线性算子的性质。对于i.i.d.高斯输入，建立了输出和梯度的非渐近集中界，量化了有限提示模型接近无限提示对应模型的速度。在上下文线性回归的特定情况下，利用可处理的无限提示动态来分析有限提示长度下的训练。

Result: 证明了softmax注意力在无限提示极限下收敛到线性算子，建立了输出和梯度的非渐近集中界，并证明这种集中在具有次高斯标记的一般上下文学习设置中在整个训练轨迹上保持稳定。在上下文线性回归中，能够使用无限提示动态来分析有限提示训练。

Conclusion: 当提示足够长时，为线性注意力开发的优化分析可以直接转移到softmax注意力，表明大提示softmax注意力继承了其线性对应物的分析结构。这为研究大提示机制下softmax注意力层的训练动态和统计行为提供了一个原则性且广泛适用的工具包。

Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

</details>


### [59] [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793)
*Ahmad Shamail,Claire McWhite*

Main category: cs.LG

TL;DR: 提出一种基于几何模式的L形分析方法，通过随机顺序添加元素并观察贡献变化来发现特征间的交互、冗余和独立关系。


<details>
  <summary>Details</summary>
Motivation: 许多系统存在复杂的组件交互关系，有些特征相互增强，有些提供冗余信息，有些独立贡献。需要一种统一的方法来发现和量化这些交互结构。

Method: 通过随机顺序添加元素，多次试验中绘制元素贡献变化图，观察L形模式。提出L-score连续度量（-1到+1），量化协同、独立和冗余关系。可视化二维点云，分析特征主导性。

Result: 方法能够区分交互、独立和冗余模式：冗余对形成L形（仅先添加元素贡献），协同对形成L形（仅元素共同贡献），独立元素显示顺序不变分布。L-score提供连续量化。

Conclusion: 该方法提供了一种统一的几何方法来揭示交互结构，适用于任何可以增量评估性能的领域，仅需成对测量即可自然涌现高阶交互关系。

Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [60] [Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs](https://arxiv.org/abs/2512.11689)
*Manuela Chacon-Chamorro,Juan Sebastián Pinzón,Rubén Manrique,Luis Felipe Giraldo,Nicanor Quijano*

Main category: cs.MA

TL;DR: 该研究比较了多智能体系统中人类与LLM智能体的合作韧性，发现在有沟通条件下人类群体表现最佳，而沟通也能提升LLM智能体的韧性，但仍低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 建立合作韧性的基准测试框架，系统比较人类与基于大型语言模型的智能体在多智能体系统中的表现，为设计具有亲社会性和韧性的智能体提供参考。

Method: 使用Melting Pot套件中的"公地悲剧"环境，在持续破坏性条件和间歇性环境冲击下，比较人类群体和LLM智能体在有/无显式沟通条件下的合作韧性表现。

Result: 有沟通的人类群体在所有组中表现出最高的合作韧性；沟通也能提升LLM智能体的韧性，但性能仍低于人类；在更严苛的长期环境中，人类能够维持共享资源并保持高韧性。

Conclusion: 人类在不利社会条件下的决策可以为设计促进亲社会和韧性行为的人工智能体提供参考，通信机制对提升合作韧性至关重要。

Abstract: This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 本文提出了CORL框架，使用强化学习端到端微调MILP方案，以最大化其在实际数据上的操作性能。


<details>
  <summary>Details</summary>
Motivation: 传统的混合整数线性规划（MILP）方法在建模随机现实世界问题时存在困难，导致实际性能不佳。现有的机器学习方法通常依赖监督学习，需要真实最优决策，并使用MILP梯度的替代方法。

Method: 提出CORL框架，将分支定界算法求解的MILP转化为可微分的随机策略，使其与强化学习兼容，从而能够端到端地使用真实世界数据进行微调。

Result: 在简单的组合顺序决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架通过强化学习端到端优化MILP方案，能够直接最大化操作性能，为组合顺序决策问题提供了新的解决方案。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [62] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver框架通过模块化协作和双级规划，在固定预算下优化多智能体系统中的测试时计算分配，显著提升协作性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术（如重复采样、自我验证）主要针对单智能体，缺乏在多智能体系统中分配计算以促进协作的原则性机制，特别是在明确预算约束下。

Method: 提出FutureWeaver框架：1）通过自博弈反思抽象历史轨迹中的重复交互模式，形成可调用函数封装的模块化协作单元；2）采用双级规划架构，在推理当前任务状态的同时推测未来步骤，优化计算分配。

Result: 在复杂智能体基准测试中，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在推理时优化多智能体协作的有效性。

Conclusion: FutureWeaver为多智能体系统中的测试时计算分配提供了系统化解决方案，通过模块化协作和前瞻性规划实现了预算约束下的高效协作优化。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [63] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 本文首次为大型视觉语言模型(LVLMs)引入了专门的CAPTCHA基准测试CAPTURE，涵盖4种主要类型和25种子类型，来自31个供应商，用于全面评估LVLMs解决CAPTCHA的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉CAPTCHA的基准测试存在局限性，无法全面覆盖所有CAPTCHA类型，且缺乏专门针对LVLMs的基准测试。先前研究根据各自目标定制基准，导致评估不全面。

Method: 创建了CAPTURE CAPTCHA基准测试，包含4种主要CAPTCHA类型和25种子类型，数据来自31个供应商。该基准具有广泛的类别多样性、大规模数据以及专门为LVLMs定制的标签。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决CAPTCHA方面表现不佳。

Conclusion: CAPTURE基准填补了先前研究在数据全面性和标签针对性方面的空白，为LVLMs提供了多维度的全面评估工具，揭示了当前模型在解决CAPTCHA任务上的不足。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [64] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个任务完成框架，使基于LLM的智能体能够在强化学习形式化的环境中，按照明确的行为指导行动，提高可靠性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和生成方面表现出强大能力，但在多轮任务中的行为往往缺乏可靠性和可验证性，需要一种能够提供明确行为指导的框架。

Method: 框架包含三个组件：轻量级任务分析器（选择推理和生成策略）、推理模块（学习可验证的观察-动作映射）、生成模块（通过验证或确定性合成确保约束合规输出）。这些组件在与环境交互过程中协同演化。

Result: 智能体在与环境交互过程中，框架的三个组件协同演化，产生可信赖的行为。

Conclusion: 该框架能够使基于LLM的智能体在强化学习形式化的环境中按照明确的行为指导行动，提高任务完成的可靠性和可验证性。

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [65] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在显式token成本和延迟预算下构建成本效益多智能体系统的框架，采用"先骨干后拓扑"设计，相比现有方法在相同预算下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统在web规模应用中成本效益日益重要，但现有方法很少在显式的token成本和延迟预算下建模和优化，导致预算约束时设计次优。

Method: 采用"先骨干后拓扑"设计：1) 骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干智能体；2) 自适应MAS拓扑生成：通过智能体表示学习、门控和延迟感知拓扑合成指导智能体间通信。

Result: 在14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%的性能提升，在延迟预算下实现高达22%的性能提升，并在性能-预算曲线上表现出强AUC。

Conclusion: AgentBalance是一个有效的预算感知多智能体系统构建框架，可作为现有MAS的插件提升性能，并能很好地泛化到未见过的LLM，适用于实际的预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [66] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 研究发现当前XAI中常用的保真度评估指标（插入和删除）存在严重问题：基线选择会偏向特定归因方法，甚至线性模型在不同基线下也会得出矛盾的最优方法结论。作者提出基线应满足两个理想属性：移除信息和不产生过度分布外图像，发现现有基线都无法同时满足，存在权衡。最后提出一种基于特征可视化的模型依赖基线，能在移除信息的同时避免过度分布外问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能（XAI）中广泛使用的归因方法评估指标（插入和删除）存在根本性问题：基线选择会系统性偏向某些归因方法，导致评估结果不可靠。更令人担忧的是，即使是简单线性模型在不同基线下也会得出相互矛盾的最优方法结论，这引发了对现有评估体系有效性的质疑。

Method: 首先分析现有基线的缺陷，提出基线应满足的两个理想属性：1）移除信息能力；2）不产生过度分布外图像。通过实验验证现有基线都无法同时满足这两个标准。然后提出一种新颖的模型依赖基线，利用特征可视化技术人工生成既能移除信息又不会过度分布外的基线图像，从而改善现有基线的权衡问题。

Result: 研究发现当前所有测试基线都存在信息移除与分布外图像生成之间的权衡：要么能有效移除信息但产生过度分布外图像，要么不产生分布外图像但信息移除效果差。提出的模型依赖基线在权衡方面优于现有基线，能在移除信息的同时避免过度分布外问题，为归因方法评估提供了更可靠的基准。

Conclusion: 归因方法评估中基线选择对结果有决定性影响，现有基线存在系统性缺陷。提出的模型依赖基线通过特征可视化技术改善了信息移除与分布外图像生成之间的权衡，为XAI评估提供了更可靠的基准，有助于更公平地比较不同归因方法的性能。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [67] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 本文首次系统比较了经典优化方法与AI方法在"No-Three-In-Line"问题上的表现，ILP在19×19网格内获得最优解，PatternBoost在14×14网格内匹配最优性能，PPO在10×10网格内完美但11×11失败。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line问题是组合几何中的著名问题，传统ILP方法虽然能保证最优解但面临指数级规模扩展问题，而机器学习方法为模式近似提供了有前景的替代方案，需要系统比较经典优化与AI方法的性能。

Method: 应用PatternBoost变压器学习和强化学习（PPO）首次解决该问题，并与传统ILP方法进行比较。ILP保证最优解，PatternBoost通过模式学习近似，PPO通过强化学习探索解空间。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格内获得完美解但在11×11网格失败，约束违规阻止有效配置。

Conclusion: 经典优化方法对于精确解仍然必不可少，而AI方法在较小实例上提供有竞争力的性能，混合方法为扩展到更大问题规模提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [68] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID框架系统评估AI文本检测器在7大类社会语言学因素上的偏见，发现检测器对少数群体文本存在系统性偏见，召回率低。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中广泛应用，但缺乏对更广泛社会语言学因素偏见的系统性评估。先前研究仅发现对英语学习者的孤立偏见案例。

Method: 提出BAID评估框架，包含7大类（人口统计、年龄、教育水平、方言、正式程度、政治倾向、主题）超过20万样本，并为每个样本生成保留内容但反映子群体写作风格的合成版本。

Result: 评估4个开源最先进的AI文本检测器，发现检测性能存在一致差异，特别是对少数群体文本的召回率低。

Conclusion: BAID提供了可扩展、透明的AI检测器审计方法，强调在工具部署前需要进行偏见感知评估。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [69] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 研究通过模拟真实临床场景，系统评估主流大语言模型从含噪声的患者主诉中提取核心医疗信息的能力，发现所有模型均存在不同程度功能缺陷，并首次提出"AI-MASLD"概念，警示AI在医疗应用中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在含噪声和冗余信息的真实临床场景中提取核心医疗信息的能力，验证其是否存在类似代谢功能障碍相关脂肪性肝病的功能衰退现象，为AI在医疗领域的应用提供安全警示。

Method: 采用基于标准化医疗探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四种主流LLMs作为研究对象。使用包含20个医疗探针、覆盖5个核心维度的评估系统模拟真实临床沟通环境，所有探针均有临床专家定义的金标准答案，由两名独立临床医生采用双盲、逆评分量表进行评估。

Result: 所有测试模型均表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差。在极端噪声条件下，大多数模型出现功能崩溃。值得注意的是，GPT-4o在深静脉血栓继发肺栓塞风险评估中做出严重误判。

Conclusion: 首次实证确认LLMs在处理临床信息时表现出类似代谢功能障碍的特征，提出创新概念"AI-MASLD"。研究为AI在医疗领域的应用提供了重要安全警示，强调当前LLMs必须在人类专家监督下作为辅助工具使用，其理论知识与实际临床应用之间仍存在显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [70] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文提出AI基准测试需要从静态评估转向动态自适应框架，以解决模型记忆、资源需求高、与现实应用脱节等问题，并倡导建立AI基准测试教育体系。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试面临多重挑战：静态基准容易被大语言模型记忆，导致评估结果与现实性能脱节；基准设计复杂且资源需求高；缺乏专业知识和硬件访问；现有基准过于关注顶级硬件峰值性能，无法指导多样化的实际应用场景。

Method: 提出构建动态自适应基准测试框架，包含：1）支持模型、数据和平台的持续演化；2）保持透明度、可复现性和可解释性；3）通过技术革新和系统化教育实现民主化；4）建立AI基准测试教育体系（AI Benchmark Carpentry）；5）促进社区协作（如MLCommons、DOE万亿参数联盟等经验）。

Result: 识别了基准测试的关键障碍：高资源需求、专用硬件访问有限、基准设计专业知识缺乏、结果与应用领域关联不确定性。提出动态基准测试应支持应用相关比较，实现基于上下文的明智决策。

Conclusion: 动态包容的基准测试对于跟上AI发展步伐、支持负责任、可复现和可访问的AI部署至关重要。社区努力可为AI基准测试教育提供基础，确保评估与AI演进同步，并促进科学评估与部署风险的对齐。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [71] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结构因果模型的能源需求预测方法，利用天气和日历信息的因果关系构建贝叶斯模型，在测试集上实现了3.84%的MAPE预测精度。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受天气条件（温度、湿度、风速、太阳辐射）和日历信息（小时、月份）等多种因素影响，这些因素之间存在因果依赖关系，使得简单的基于相关性的学习方法难以充分处理这一复杂问题。

Method: 提出结构因果模型来解释变量间的因果关系，通过全面分析验证因果信念。基于获得的因果洞察作为先验知识，构建贝叶斯模型进行预测。

Result: 因果模型揭示了能源需求对温度波动的响应具有季节依赖性敏感性，冬季能源需求方差较低，因为温度变化与日常活动模式之间存在解耦效应。贝叶斯模型在未见数据上实现了3.84%的MAPE测试性能，跨两年数据的交叉验证平均MAPE为3.88%，表现出强大的鲁棒性。

Conclusion: 通过结合因果洞察的贝叶斯模型能够有效预测能源需求，实现了最先进的预测性能，为能源需求预测提供了新的方法框架。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>
