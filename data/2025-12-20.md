<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: GraFT框架通过限制输出词汇空间，显著提升自然语言到时序逻辑的翻译准确率


<details>
  <summary>Details</summary>
Motivation: 现有方法在原子命题提取、共指消解和有限数据学习方面存在困难，需要更高效的自然语言到时序逻辑翻译框架

Method: 提出Grammar Forced Translation (GraFT)框架，通过限制每一步的有效输出词汇空间来降低任务复杂度，利用每个问题的独特属性减少解空间

Result: 在CW、GLTL和Navi基准测试中，GraFT将端到端翻译准确率平均提升5.49%，域外翻译准确率平均提升14.06%

Conclusion: 通过解空间缩减策略，GraFT框架显著提升了自然语言到时序逻辑翻译的准确性和学习效率

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [2] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种信息论方法来量化语音中仅由韵律（而非文本）传达的信息量及其内容，发现韵律在传达讽刺和情感信息方面比文本多一个数量级的信息。


<details>
  <summary>Details</summary>
Motivation: 韵律（语音的旋律）传达了文本或文字通常无法捕捉的关键信息，但现有方法难以量化韵律单独传达的信息量及其具体内容。

Method: 使用大型语音和语言模型估计话语意义维度（如情感）与其通信通道（如音频或文本）之间的互信息，量化音频和文本在讽刺、情感和疑问性方面传达的信息。

Result: 对于讽刺和情感，音频通道（即韵律通道）传输的信息量比纯文本通道多一个数量级以上；对于疑问性，韵律提供的额外信息相对较少。

Conclusion: 提出将这种方法扩展到更多意义维度、通信通道和语言的系统性研究计划。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [3] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache是一种新颖的层级缓存框架，通过基于输入序列语义相似性重用中间激活来加速Transformer推理，实现高达3.1倍的推理加速，精度损失小于0.5%。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型虽然性能优异，但推理延迟高，限制了实时和大规模部署。现有的令牌级键值缓存机制在适用范围和效果上存在局限。

Method: 提出LLMCache框架：1）模型无关，适用于编码器和解码器架构；2）支持任意Transformer层的缓存；3）引入轻量级指纹机制匹配语义相似输入；4）提出自适应淘汰策略管理缓存陈旧性。

Result: 在BERT和GPT-2模型上，在SQuAD、WikiText-103和OpenBookQA数据集上测试，实现最高3.1倍的推理加速，精度损失小于0.5%。

Conclusion: LLMCache是一种实用且通用的解决方案，能够有效优化Transformer在现实应用中的推理性能，为实时和大规模部署提供了可行途径。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [4] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: Transformer模型在变量符号含义不固定的算术任务中，仍能学习到符号推理机制，包括交换复制、单位元识别和基于封闭性的消元三种机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现transformer在固定符号含义的算术任务中会发展出几何嵌入，但本研究探索在符号含义随序列变化（变量）的更具挑战性设置下，模型如何发展推理机制。

Method: 设计新任务：符号到代数群元素的分配随序列变化；创建有针对性的数据分布来因果测试假设机制；训练transformer模型并分析其学习到的机制。

Result: 模型在任务上达到接近完美的准确率，甚至能泛化到未见过的代数群；识别出三种一致学习的机制：交换复制（专用头复制答案）、单位元识别（区分包含单位元的事实）、基于封闭性的消元（跟踪群成员资格以约束有效答案）。

Conclusion: 与固定符号设置中的几何表示互补，当训练模型在变量符号含义不固定的上下文中进行推理时，模型会发展出符号推理机制。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 提出Generative Adversarial Reasoner框架，通过对抗强化学习联合训练推理器和判别器，提升LLM数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有具备显式推理能力的大语言模型在数学推理中仍存在计算错误、逻辑脆弱和表面合理但无效步骤等问题，需要改进推理质量

Method: 采用生成对抗推理器框架，包含推理器和判别器两个LLM组件，通过计算高效的审查调度将推理链划分为逻辑完整的切片，判别器评估每个切片的合理性并提供结构化理由，通过对抗强化学习联合训练

Result: 在多个数学基准测试中取得一致提升，在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3，DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7

Conclusion: 提出的对抗训练框架通过密集、校准良好的步骤级奖励补充稀疏的精确匹配信号，改善了信用分配，提高了样本效率，增强了LLM的整体推理质量

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [6] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE框架首次将形式化方法引入LLM压缩，利用信号时序逻辑（STL）在压缩过程中形式化指定和执行语言特性，通过STL鲁棒性引导的贝叶斯优化探索分层量化和剪枝配置，无需重训练或微调即可生成满足语言约束的压缩模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言任务中表现出色，但需要大量计算资源，限制了其在资源受限的边缘设备上的部署。现有的压缩技术（如量化和剪枝）往往会降低关键的语言特性，并且缺乏保持模型行为的正式保证。

Method: 提出TOGGLE框架，利用信号时序逻辑（STL）形式化指定和执行语言特性。采用STL鲁棒性引导的贝叶斯优化，系统探索分层量化和剪枝配置，生成满足指定语言约束的压缩模型，无需重训练或微调。

Result: 在四种LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B）上评估TOGGLE，实现了高达3.3倍的计算成本（FLOPs）降低和高达68.8%的模型大小减少，同时满足所有语言特性。

Conclusion: TOGGLE代表了形式化方法首次集成到LLM压缩中，实现了在边缘硬件上高效、可验证的LLM部署，为资源受限环境下的模型压缩提供了具有形式化保证的新方法。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [7] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协调协作实现，而非单一系统。为此需要超越个体对齐，建立分布式的AGI安全框架，包括虚拟智能体沙盒经济、市场机制、审计和声誉管理等集体风险缓解措施。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AGI系统的防护，忽视了"拼凑式AGI"假说——即通用能力可能首先通过多个子AGI智能体的协调协作实现。随着具备工具使用、通信协调能力的先进AI智能体快速部署，这种集体智能带来的安全风险变得紧迫。

Method: 提出分布式AGI安全框架，核心是设计和实施虚拟智能体沙盒经济（不可渗透或半渗透），通过稳健的市场机制管理智能体间交易，配合适当的可审计性、声誉管理和监督机制来缓解集体风险。

Result: 建立了从个体对齐到集体对齐的理论框架，为应对拼凑式AGI风险提供了系统性解决方案，强调需要超越传统个体智能体评估和对齐方法。

Conclusion: 拼凑式AGI假说需要认真对待，并应指导相应安全保障措施的发展。分布式AGI安全框架为管理智能体群体协调带来的集体风险提供了必要的方法论基础。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [8] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 论文提出了社会责任感堆栈（SRS）框架，这是一个六层架构，将社会价值观嵌入AI系统作为显式约束、保障措施、行为接口、审计机制和治理流程。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统越来越多地部署在影响人类行为、机构决策和社会结果的领域，但现有的负责任AI和治理工作提供了重要的规范原则，却缺乏在整个系统生命周期中可执行的工程机制。

Method: 引入社会责任感堆栈（SRS）框架，将责任建模为社会技术系统的闭环监督控制问题，整合设计时保障措施与运行时监控和机构监督。开发统一的基于约束的表述，引入安全包络和反馈解释。

Result: 展示了公平性、自主性、认知负担和解释质量如何被持续监控和执行。通过临床决策支持、协作自动驾驶车辆和公共部门系统的案例研究，说明SRS如何将规范目标转化为可操作的工程和运营控制。

Conclusion: SRS框架连接了伦理学、控制理论和AI治理，为可问责、自适应和可审计的社会技术AI系统提供了实用基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 利用联邦学习平台在多机构间协作训练机器学习模型，用于胶原VI相关肌营养不良症的诊断，相比单机构模型显著提升诊断性能


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断面临数据稀缺和分散的挑战，跨机构数据共享存在隐私、监管和物流障碍，需要一种能在保护数据隐私的前提下实现多机构协作的解决方案

Method: 采用Sherpa.ai联邦学习平台，在两个国际组织的分布式数据集上协作训练机器学习模型，使用胶原VI免疫荧光显微镜图像进行训练，模型能分类三种主要致病机制

Result: 联邦学习模型获得0.82的F1分数，显著优于单机构模型（0.57-0.75），证明联邦学习能大幅提升诊断效用和泛化能力

Conclusion: 联邦学习为罕见病诊断提供了有效的多机构协作解决方案，不仅能提高诊断准确性，还能支持不确定意义变异的解释和指导测序策略优先排序

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>
