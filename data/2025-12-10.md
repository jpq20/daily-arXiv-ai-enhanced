<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.LG](#cs.LG) [Total: 58]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种分层多智能体架构，使用64*64网格的轻量级智能体进行分布式推理，通过空间课程学习逐步扩展操作区域，并利用负对数似然作为置信度度量来提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多智能体系统在处理复杂任务分解方面表现出潜力，但在长时程推理任务中面临困难且计算成本高昂。需要一种能够有效处理长时程推理任务同时降低计算成本的解决方案。

Method: 1. 分层多智能体架构：在64*64网格上分布轻量级智能体，由选择性oracle支持
2. 空间课程学习：逐步扩展网格的操作区域，让智能体先掌握中心简单任务再处理边缘困难任务
3. 置信度度量：集成负对数似然作为置信度测量，确保智能体既准确又校准良好
4. Thompson Sampling课程管理器：基于能力和NLL驱动的奖励信号自适应选择训练区域

Result: 在空间接地的汉诺塔基准测试中，该方法表现出：1) 改进的稳定性；2) 减少的oracle使用；3) 分布式智能体合作带来的更强长程推理能力

Conclusion: 提出的分层多智能体架构通过分布式推理、空间课程学习和置信度感知训练，有效解决了长时程推理任务中的挑战，为机器人操作和规划任务提供了有前景的解决方案。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [2] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: SEA提出了一种通用的字幕对齐方法，通过预训练模型将手语视频分割为单个手势并嵌入共享潜在空间，然后使用轻量级动态规划进行对齐，支持多语言和多领域应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于特定语言或数据集的端到端训练，限制了通用性。需要开发一个跨语言和跨领域的通用手语字幕对齐框架。

Method: SEA（Segment, Embed, and Align）方法：1）使用预训练模型将视频帧序列分割为单个手势；2）将每个手势的视频片段嵌入到与文本共享的潜在空间；3）使用轻量级动态规划程序进行对齐，即使在CPU上也能在分钟内处理小时级视频。

Result: 在四个手语数据集上的实验表明，SEA实现了最先进的对齐性能，能够为推进手语处理生成高质量的并行数据。

Conclusion: SEA提供了一个灵活、通用的框架，能够适应从小型词典到大型连续语料库的各种场景，代码和模型已开源，具有推动手语处理发展的潜力。

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [3] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 该论文提出了一种通用对抗后缀攻击方法，通过添加短令牌序列（4-10个令牌）到任何输入中，能够广泛降低不同任务和模型的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 语言模型作为零样本或少样本分类器时，通常通过评分标签词进行分类，但对对抗性提示很脆弱。先前工作通常优化任务或模型特定的触发器，导致结果难以比较且可迁移性有限。

Method: 使用Gumbel-Softmax松弛学习可微分的"软"后缀形式，然后离散化用于推理。训练时最大化标签区域的校准交叉熵，同时屏蔽黄金令牌以防止信息泄露，并使用熵正则化避免崩溃。

Result: 单个在一个模型上训练的后缀能够有效迁移到其他模型，持续降低准确性和校准置信度。在情感分析、自然语言推理、释义检测、常识问答和物理推理等任务上，对Qwen2-1.5B、Phi-1.5和TinyLlama-1.1B模型均表现出一致的攻击效果和跨任务、跨模型族的迁移能力。

Conclusion: 该方法展示了通用对抗后缀的有效性，能够创建跨任务和模型广泛有效的对抗性攻击，揭示了语言模型在对抗攻击下的脆弱性。

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [4] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 使用强化学习框架训练对抗性后缀，通过PPO算法优化后缀策略，相比传统梯度搜索方法，该方法能更有效地降低模型准确率并在不同任务和模型间具有更好的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 语言模型容易受到短对抗性后缀的攻击，这些后缀可以可靠地改变模型预测。以往工作通常使用梯度搜索或基于规则的方法寻找这类后缀，但这些方法脆弱且通常只针对单一任务或模型。

Method: 采用强化学习框架，将对抗性后缀视为策略，使用近端策略优化（PPO）算法针对冻结模型作为奖励预言机进行训练。通过校准交叉熵塑造奖励函数，消除标签偏差并聚合不同表面形式以提高可迁移性。

Result: 在五个不同的NLP基准数据集（涵盖情感分析、自然语言推理、释义和常识推理）上，使用三种不同语言模型（Qwen2-1.5B Instruct、TinyLlama-1.1B Chat和Phi-1.5）进行评估。结果显示，RL训练的后缀能持续降低模型准确率，并且比类似类型的先前对抗性触发器在任务和模型间具有更好的可迁移性。

Conclusion: 强化学习框架为生成对抗性后缀提供了一种有效方法，相比传统方法具有更好的鲁棒性和可迁移性，能够跨任务和模型有效降低语言模型的性能。

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [5] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub是一个整合ClinicalTrials.gov数据和PubMed文献的交互式搜索平台，通过大语言模型自动提取结构化试验信息，将结构化临床试验数据访问量提升83.8%


<details>
  <summary>Details</summary>
Motivation: 现有的临床试验数据主要存储在ClinicalTrials.gov中，但缺乏与相关研究文献的整合，限制了患者、临床医生、研究人员和政策制定者对完整证据的访问，阻碍了循证医学的发展

Method: 使用GPT-5.1和Gemini-3-Pro等大语言模型，自动解析PubMed全文研究文章提取结构化试验信息，将用户查询转换为结构化数据库搜索，并提供基于证据的问答系统，答案与具体源句链接

Result: 相比仅依赖ClinicalTrials.gov，系统将结构化临床试验数据访问量提高了83.8%；通过临床医生、临床研究人员和药学/护理博士生的用户研究，以及信息提取和问答能力的系统自动评估，验证了其实用性

Conclusion: ClinicalTrialsHub通过整合临床试验注册数据和相关研究文献，显著提高了结构化临床试验数据的可访问性，为患者、临床医生、研究人员和政策制定者提供了更全面的证据基础，推动了循证医学的发展

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [6] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: 该研究通过概念性复制手动标注，发现GLLM在标注任务中虽然F1分数表现尚可，但与人工标注存在系统性偏差，导致下游结果差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究GLLM在文本标注任务中是否存在系统性偏差，通过复制Boukes(2024)的人工标注研究，评估GLLM标注的可靠性和偏差程度。

Method: 使用多种GLLM模型（Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b）配合五种不同提示词，对五个概念（政治内容、互动性、理性、不文明程度、意识形态）进行标注，并与人工标注结果进行对比分析。

Result: GLLM在F1分数上表现尚可，但与人工标注在流行度估计上存在差异，导致下游结果显著不同。GLLM标注之间存在高度一致性，但与人工标注的系统性偏差明显，且F1分数差异无法充分反映偏差程度。

Conclusion: GLLM标注虽然技术指标表现良好，但存在系统性偏差，与人工标注存在实质性差异，仅依赖F1分数等传统指标无法全面评估标注质量，需要更全面的偏差检测方法。

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [7] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: 该研究使用可解释性方法分析机器翻译模型中的性别偏见来源，通过对比解释和显著性归因技术，探索源语言中哪些上下文词汇触发目标语言的特定性别屈折变化。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性研究主要关注理解黑盒模型决策，但在性别偏见这一重要问题上研究有限。研究旨在从简单的偏见测量转向探索偏见的起源，理解翻译模型选择特定性别屈折的触发因素。

Method: 使用性别模糊的自然源数据，采用对比解释和显著性归因技术。首先解决缺乏评分阈值的问题，分析不同归因水平下源词汇对模型性别决策的影响。将模型识别的显著源词汇与人类性别感知进行比较，并进行语言学分析。

Result: 研究发现人类感知与模型归因之间存在显著重叠，表明模型在性别决策时关注的上下文特征与人类相似。通过语言学分析揭示了影响性别决策的关键词汇类型。

Conclusion: 理解模型翻译决策中的性别因素至关重要，这与人类决策有可比性。这些信息应被用于缓解性别偏见，展示了可解释性方法在识别和解决模型偏见方面的实际应用价值。

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [8] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: 该研究提出了一种软归纳偏置方法，通过明确定义推理视角来指导推理过程，用于检测韩语对话中的不当言论，相比标准监督学习提升了约3.89%的准确率。


<details>
  <summary>Details</summary>
Motivation: 在线游戏和社区中匿名环境下的不当言论经常升级为言语暴力和犯罪行为，引发社会关注。虽然韩语大规模语言模型和思维链推理受到关注，但在不当言论检测方面的应用研究仍然有限。

Method: 提出软归纳偏置方法，明确定义推理视角来指导推理过程，促进理性决策并防止推理过程中可能出现的错误。使用该方法对韩语大语言模型进行微调，并进行不同训练策略的定量性能比较和定性评估。

Result: 实验结果显示，Kanana-1.5模型平均准确率达到87.0046，相比标准监督学习提升了约3.89%。该方法超越了简单的大语言模型知识模仿，通过约束推理视角实现了更精确和一致的判断。

Conclusion: 提出的软归纳偏置方法通过明确定义推理视角，能够有效检测不当言论，为构建更安全的通信环境提供了有效的技术方案。该方法不仅提升了检测性能，还促进了更理性的决策过程。

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [9] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: 这是一篇关于医疗领域NLP应用的教程论文，系统介绍了HealthcareNLP的三个层次架构，包括数据资源层、NLP评估层和患者层，并包含实践环节。


<details>
  <summary>Details</summary>
Motivation: 现有医疗NLP领域的综述要么忽略了重要任务（如隐私保护的合成数据生成、可解释性临床NLP），要么遗漏了关键方法（如检索增强生成、神经符号集成），因此需要提供一个更全面的入门教程。

Method: 采用三层层次结构组织教程内容：1) 数据/资源层（标注指南、伦理审批、治理、合成数据）；2) NLP评估层（NER、关系抽取、情感分析等任务及可解释性方法）；3) 患者层（患者参与、健康素养、翻译简化等任务及决策支持）。包含实践环节让参与者使用HealthcareNLP应用。

Result: 设计了一个面向医疗NLP实践者、研究人员和学生的入门教程，涵盖医疗NLP的关键子领域，提供实践体验，无需先验知识即可参与。

Conclusion: 该教程填补了现有医疗NLP综述的空白，通过系统化的三层架构为医疗领域NLP应用提供了全面的入门指导，有助于推动HealthcareNLP的实践应用和发展。

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [10] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN是一个开源Python框架，用于通过问卷式提示系统生成LLM响应，支持虚拟调查和标注任务，提供无代码界面，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了支持基于大语言模型的虚拟调查和标注任务，需要系统化生成问卷响应的方法，并评估问卷呈现、提示扰动和响应生成方法对结果的影响。

Method: 开发了QSTN开源Python框架，提供系统化生成问卷响应的工具，包括问卷呈现、提示扰动和响应生成方法的评估功能，并提供了无代码用户界面。

Result: 通过超过4000万次调查响应的评估，发现问卷结构和响应生成方法对生成结果与人类答案的一致性有显著影响，且能以较低计算成本获得结果。

Conclusion: QSTN框架能够支持未来基于LLM研究的可重复性和可靠性，为研究人员提供了无需编程知识即可进行稳健实验的工具。

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [11] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 首个巴斯克语自动作文评分与反馈生成公开数据集，包含3200篇CEFR C1水平作文，专家标注多维度分数与详细反馈，通过微调开源模型在评分一致性和反馈质量上超越GPT-5等闭源系统。


<details>
  <summary>Details</summary>
Motivation: 为巴斯克语等低资源语言建立透明、可复现且教育基础扎实的NLP研究基础，解决该语言在自动作文评分和反馈生成方面缺乏公开数据集和基准的问题。

Method: 构建包含3200篇巴斯克语C1水平作文的数据集，每篇由专家标注正确性、丰富性、连贯性、衔接性和任务匹配度等维度分数及详细反馈；微调RoBERTa-EusCrawl和Latxa 8B/70B等开源模型进行评分和解释生成；提出结合自动一致性指标和专家验证的新型反馈生成评估方法。

Result: 编码器模型在自动作文评分上保持高可靠性；监督微调的Latxa模型在评分一致性和反馈质量上超越GPT-5和Claude Sonnet 4.5等最先进闭源系统；微调后的Latxa模型能生成标准对齐、教学意义强的反馈，识别比专有模型更广泛的错误类型。

Conclusion: 该数据集和基准为巴斯克语等低资源语言的透明、可复现、教育基础扎实的NLP研究奠定了基础，展示了开源模型通过适当微调可以在特定语言任务上超越闭源系统。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [12] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 提出一种针对低资源语言的后训练方法，通过策略训练保持语言模型流畅性，无需目标语言的指令调优数据


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化研究主要针对英语和中文，低资源语言缺乏母语者撰写的数据集和生成流畅合成数据的语言模型，需要开发无需目标语言指令数据的流畅偏好对齐方法

Method: 采用策略训练方法，与两种常见方法对比：基于机器翻译数据的监督微调和多语言微调，以挪威博克马尔语为案例研究

Result: 通过母语者评估显示，策略训练方法在保持流畅性方面优于其他方法，且不依赖难以获取的数据

Conclusion: 策略训练对于低资源语言的偏好对齐至关重要，能够在不依赖目标语言指令数据的情况下实现流畅的语言模型对齐

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [13] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出了一种在联邦学习环境中评估大型语言模型与多样化人类偏好对齐的框架，通过自适应聚合策略在保持对齐质量的同时提升公平性。


<details>
  <summary>Details</summary>
Motivation: 标准方法在联邦学习环境中难以充分代表多样化的人类偏好观点，需要解决对齐质量与公平性之间的权衡问题。

Method: 引入综合评估框架，在联邦学习环境中让每个组本地评估模型输出并生成奖励信号，服务器聚合组级奖励而不访问原始数据。评估标准聚合技术（最小值、最大值、平均值），并提出基于组历史对齐性能动态调整偏好权重的自适应方案。

Result: 在问答任务中使用基于PPO的RLHF管道进行实验，结果显示自适应方法在保持竞争性对齐分数的同时，始终实现更优的公平性。

Conclusion: 这项工作为评估LLM在多样化人群中的行为提供了稳健方法，并为开发真正多元化和公平对齐的模型提供了实用解决方案。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [14] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: 该研究揭示了逐步增加Transformer深度训练（MIDAS方法）的机制原理，发现这种方法能更有效地利用模型深度，改变残差流结构，形成可置换计算块，从而克服标准非增长模型的深度利用限制。


<details>
  <summary>Details</summary>
Motivation: 虽然逐步增加Transformer深度训练（MIDAS）已被证明能降低训练成本并提高推理性能，但其机制原理一直缺乏理解。本研究旨在揭示MIDAS获得性能提升的机制原因。

Method: 通过深度分析，研究逐步中间堆叠增长方法如何影响模型，分析其对模型深度利用、残差流结构和计算块形成的影响。同时提出MIDAS的轻量级改进方案。

Result: 研究发现：1）逐步增长深度能更有效地利用模型深度；2）改变残差流结构；3）促进可置换计算块的形成。提出的MIDAS改进方案在下游推理基准测试中进一步提升了性能。

Conclusion: 逐步增加模型深度能形成独特的计算电路，克服标准非增长模型中观察到的深度利用有限问题，为理解深度增长训练的优势提供了机制解释。

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [15] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: RAGLens：一种基于稀疏自编码器的轻量级RAG幻觉检测器，利用LLM内部表征准确识别不忠实输出，无需大规模训练数据或外部LLM查询


<details>
  <summary>Details</summary>
Motivation: 现有RAG幻觉检测方法存在两大问题：1）需要大量标注数据进行大规模检测器训练；2）依赖外部LLM判断导致高推理成本。虽然有些方法尝试利用LLM内部表征，但准确性有限。基于机制可解释性最新进展，希望开发更有效的幻觉检测方案。

Method: 采用稀疏自编码器（SAEs）解耦LLM内部激活，识别RAG幻觉期间触发的特定特征。基于信息特征选择和加性特征建模的系统化流程，开发RAGLens轻量级检测器。该方法利用LLM内部表征准确标记不忠实的RAG输出。

Result: RAGLens在检测性能上优于现有方法，不仅准确率高，还能提供可解释的决策依据，支持对不忠实RAG进行有效的后处理缓解。研究还揭示了LLM中幻觉相关信号的分布新见解。

Conclusion: RAGLens通过机制可解释性方法成功开发出高效、轻量的RAG幻觉检测器，解决了现有方法的数据依赖和高成本问题，为RAG系统的忠实性保障提供了新工具。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [16] [CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models](https://arxiv.org/abs/2512.07890)
*Ryan Feng Lin,Keyu Tian,Hanming Zheng,Congjing Zhang,Li Zeng,Shuai Huang*

Main category: cs.MA

TL;DR: CrowdLLM提出了一种结合预训练大语言模型和生成模型的方法，用于创建更准确、更多样化的数字人群，以解决现有LLM数字人群在准确性和多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的数字人群在社交模拟、众包、营销和推荐系统等领域有广泛应用前景，可以降低招募人类参与者的成本并规避人类研究的相关问题。然而，现有方法主要依赖LLM，无法充分捕捉真实人群的准确性和多样性。

Method: 提出CrowdLLM框架，整合预训练大语言模型和生成模型，通过这种结合来增强数字人群的多样性和保真度。

Result: 理论分析表明CrowdLLM在创建成本效益高、代表性充分、可扩展的数字人群方面具有巨大潜力，能够匹配真实人群的质量。在多个领域（众包、投票、用户评分）和模拟研究中的综合实验证明，CrowdLLM在准确性和分布保真度方面都取得了有希望的性能。

Conclusion: CrowdLLM通过整合LLM和生成模型，有效解决了现有数字人群在准确性和多样性方面的局限性，为创建高质量、具有代表性的数字人群提供了可行方案。

Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.

</details>


### [17] [MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement](https://arxiv.org/abs/2512.07898)
*Hongwei Zhang,Ji Lu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.MA

TL;DR: MARINE框架通过多智能体递归上下文增强，将基础模型的pass@N能力转化为接近最优的pass@1性能，显著提升大语言模型推理效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体通常只能输出单次响应，限制了性能潜力。传统的一次性或多样本范式无法充分利用模型的推理能力，需要新的框架来提升测试时推理效率

Method: 提出MARINE框架，将测试时推理重新概念化为对持久参考轨迹的迭代精炼。采用最小可行批次最大化期望性能增益，对数增长批次调度确保持续改进。通过多智能体递归上下文增强实现参数高效推理

Result: 在BrowserComp-ZH基准测试中达到SOTA结果，685B参数实现46.0% pass@1准确率。80B参数模型增强后性能匹配1000B参数独立智能体，参数需求降低一个数量级以上。在固定计算预算下提供比传统采样-排序策略更高质量的样本

Conclusion: MARINE建立了参数高效推理的新范式，显著提升后训练效率，具有将基础模型能力转化为接近最优性能的潜力，为LLM推理优化提供了理论指导和实用框架

Abstract: Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.

</details>


### [18] [Probabilistic Multi-Agent Aircraft Landing Time Prediction](https://arxiv.org/abs/2512.08281)
*Kyungmin Kim,Seokbin Yoon,Keumjin Lee*

Main category: cs.MA

TL;DR: 提出一个概率多智能体飞机着陆时间预测框架，能够为多架飞机提供着陆时间的分布预测，考虑了空域中的多智能体交互，并在仁川国际机场数据上验证了其准确性和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 飞机着陆时间预测对空中交通管理资源分配至关重要，但轨迹和交通流的不确定性给预测准确性和可信度带来挑战。现有模型需要提供点估计和不确定性，同时必须考虑空中交通管制干预（如雷达引导）带来的多智能体交互影响。

Method: 提出概率多智能体飞机着陆时间预测框架，能够为多架飞机提供着陆时间的分布预测。该框架考虑了空域中的多智能体交互，通过注意力机制捕捉飞机间的相互影响。

Result: 在韩国仁川国际机场终端空域的空中交通监视数据集上评估，结果显示该模型比基线方法具有更高的预测准确性，能够量化预测结果的不确定性，并通过注意力分数揭示了空中交通管制的潜在模式，增强了模型的可解释性。

Conclusion: 提出的概率多智能体框架能够准确预测飞机着陆时间并量化不确定性，同时通过注意力机制提供可解释性，有助于空中交通管理的资源分配决策。

Abstract: Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 该研究对比了数据导向设计(DOD)与传统面向对象设计(OOD)在多线程环境下的性能表现，通过A*搜索算法的四种实现版本进行测试，发现DOD在多线程场景中具有更快的执行时间、更少的系统调用和缓存未命中。


<details>
  <summary>Details</summary>
Motivation: 随着多核CPU与主内存性能差距的扩大，需要硬件感知的软件设计范式。本研究旨在全面分析数据导向设计与传统面向对象设计在缓存利用和多线程效率方面的性能差异。

Method: 开发并比较了A*搜索算法的四种版本：单线程OOD、单线程DOD、多线程OOD、多线程DOD。评估指标包括执行时间、内存使用和CPU缓存未命中。

Result: 在多线程测试中，DOD实现表现出显著的性能优势，执行时间更快，系统调用和缓存未命中更少。对于A*这样的细粒度任务，线程管理开销导致单线程版本在两个范式中都显著优于多线程版本。

Conclusion: 即使在简单算法中性能差异看似细微，但DOD在关键指标上的持续优势凸显了其基础架构优越性，表明它是复杂大规模AI和并行计算任务中最大化硬件效率的更有效方法。

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [20] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 提出分层多智能体框架模拟MDT协作，解决MLLMs在胃肠肿瘤多模态临床推理中的上下文稀释和幻觉问题，显著提升推理逻辑和医学准确性


<details>
  <summary>Details</summary>
Motivation: 胃肠肿瘤多模态临床推理需要整合内镜图像、放射学数据和生化标志物，但现有MLLMs在处理复杂异质医疗数据时面临上下文稀释和幻觉问题，需要更稳健的解决方案

Method: 提出分层多智能体框架，模拟人类多学科团队（MDT）的协作工作流程，通过智能体间的协作来处理多模态医疗数据

Result: 系统获得4.60/5.00的专家评估分数，显著优于单体基线，智能体架构在推理逻辑和医学准确性方面提升最为显著

Conclusion: 模拟MDT的智能体协作框架为肿瘤学自动化决策支持提供了可扩展、可解释且临床稳健的范式

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [21] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV是一种无需训练的KV缓存压缩方法，通过句子级评分和选择性删除来减少推理过程中的KV缓存开销，同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中需要大量的KV缓存，随着链式思维推理过程的增长，KV缓存线性增加，导致内存和吞吐量瓶颈，限制了模型的部署效率。

Method: SkipKV采用句子级评分指标识别并删除高度相似的句子，同时通过动态调整引导向量来更新隐藏激活状态，抑制冗余生成，实现训练自由的KV压缩。

Result: 在多个推理基准测试中，SkipKV在相似压缩预算下保持了高达26.7%的准确率提升，相比现有方法减少了1.6倍的生成长度，并将吞吐量提高了1.7倍。

Conclusion: SkipKV通过句子级的KV缓存压缩方法，有效解决了推理过程中的KV缓存开销问题，在保持准确性的同时显著提升了推理效率。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [22] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 基础模型在多智能体环境中缺乏原生智能，需要专门研究来提升其多智能体理解、规划、通信和适应能力


<details>
  <summary>Details</summary>
Motivation: 虽然基础模型正成为AI智能体的"大脑"，但现有研究主要关注单智能体能力（如GUI交互、工具使用），而多智能体智能是下一个前沿。当前模型在单智能体任务上的强大表现并不能自动转化为稳健的多智能体智能。

Method: 通过41个大语言模型的广泛实证研究，验证了单智能体性能与多智能体智能之间的差距。提出了构建具有原生多智能体智能的基础模型的关键研究方向，包括数据集构建、评估、训练范式和安全考虑。

Result: 实证研究表明，强大的单智能体性能并不能自动产生稳健的多智能体智能。识别了基础模型在多智能体环境中的四个核心能力缺陷：理解、规划、高效通信和适应。

Conclusion: 需要专门的研究方向来构建具有原生多智能体智能的基础模型，包括数据集、评估、训练和安全等方面的系统性工作，以弥补当前模型在多智能体环境中的能力差距。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [23] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 开发了一个基于大型语言模型的AI增强患者-临床试验匹配系统，通过结构化资格评估和可解释推理链支持人工审查，旨在减轻协调员负担并提高匹配效率


<details>
  <summary>Details</summary>
Motivation: 目前临床试验患者筛选过程仍然是手动、耗时且资源密集的，需要一种能够整合异构电子健康记录数据、支持专家审查并保持严格安全标准的自动化解决方案

Method: 利用开源、具备推理能力的大型语言模型构建概念验证系统，超越二元分类，生成带有可解释推理链的结构化资格评估，支持人机协同审查，将资格表示为动态状态而非固定判断

Result: 开发了一个安全、可扩展的决策支持工具，能够识别可用匹配并提供可操作的未来资格建议，旨在减少协调员负担，智能扩大每位患者考虑的试验范围，并保证所有AI生成输出的全面可审计性

Conclusion: 该系统代表了AI增强患者-试验匹配的重要进展，通过结合LLM的推理能力和人类专家审查，有望显著改善临床试验招募流程的效率和质量

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [24] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本研究对ChatGPT和DeepSeek两大LLM在教育研究领域进行全面评估，通过技术分析、实证实验和用户调查，比较它们在文本生成、编程和专业问题解决方面的性能差异。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型在教育研究领域展现出巨大潜力，ChatGPT和DeepSeek作为当前最先进的模型，在数学、科学、医学、文学和编程等方面表现出色，但缺乏系统性的比较评估，需要了解它们在准确性、计算效率和用户体验方面的权衡。

Method: 采用综合评估方法：1) 背景技术分析；2) 实证实验（在文本生成、编程和专业问题解决方面进行基准测试）；3) 真实用户调查（收集学生、教育工作者和研究人员的反馈）。

Result: ChatGPT在通用语言理解和文本生成方面表现更优，DeepSeek因其效率导向的设计在编程任务中表现更佳。两个模型都能提供医学上准确的诊断输出，并能有效解决复杂的数学问题。用户调查揭示了这些模型在实际应用中的具体优势和限制。

Conclusion: 研究提供了对ChatGPT和DeepSeek在教育研究领域性能的全面评估，揭示了各自的优势领域，为教育工作者、研究人员和开发者选择合适的模型提供了实证依据，有助于推动LLM在教育研究领域的进一步发展。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [25] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 开发了一个用于移动糖尿病预测应用的可扩展后端系统，采用水平扩展、数据库分片和消息队列异步通信，83%的功能满足性能目标，可处理1万并发用户。


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病患病率上升需要早期检测，AI预测应用需要响应式、可扩展的后端架构来有效服务大规模用户。

Method: 采用水平扩展、数据库分片和通过消息队列（RabbitMQ）的异步通信架构，设计可扩展的后端系统。

Result: 83%的系统功能（24个中的20个）满足性能目标（故障率低于5%，平均延迟低于1000毫秒），用户档案管理、活动跟踪和读取密集型预测操作等关键功能达到预期性能，系统可处理1万并发用户。

Conclusion: RabbitMQ的异步通信实现对于计算密集型预测请求至关重要，能最小化错误率，确保系统可靠性，通过排队请求防止高负载下的数据丢失，验证了系统的可扩展性。

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [26] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: KPI框架通过知识图谱增强、原型感知和可解释性设计，从患者端信息预测疾病，解决数据不平衡和可解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于患者端信息（人口统计和自述症状）的疾病预测方法面临疾病分布不平衡和缺乏可解释性的挑战，导致预测存在偏见或不可靠。

Method: 提出KPI框架：1）整合结构化可信医疗知识构建统一疾病知识图谱；2）构建临床有意义的疾病原型；3）使用对比学习提升预测准确性；4）利用大语言模型生成患者特定的医学相关解释。

Result: 在真实世界数据集上的实验表明，KPI在预测准确性上优于现有最先进方法，并能提供与患者叙述高度一致的临床有效解释。

Conclusion: KPI框架通过整合医疗知识、原型学习和可解释性增强，为以患者为中心的医疗保健提供了实用价值，特别适合长尾疾病的预测。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [27] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 最新推理模型在CFA考试中表现优异，多个模型通过全部三个级别，其中Gemini 3.0 Pro在Level I创下97.6%的最高分记录。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大型语言模型在CFA考试中表现不佳，但近期推理模型在各种专业考试中取得突破，需要评估最新模型在CFA考试中的实际表现。

Method: 使用980道模拟题评估最先进的推理模型，涵盖CFA三个级别的考试：三个Level I、两个Level II、三个Level III，采用与先前研究相同的通过标准。

Result: 大多数模型通过了全部三个级别，按总体表现排序：Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1和DeepSeek-V3.1。Gemini 3.0 Pro在Level I获得97.6%的创纪录分数，GPT-5在Level II以94.3%领先，Gemini 2.5 Pro在Level III选择题获得86.4%，Gemini 3.0 Pro在构建回答题获得92.0%。

Conclusion: 最新推理模型在CFA考试中表现出色，能够通过所有三个级别，表明这些模型在金融专业考试中已达到相当高的能力水平。

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [28] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 该研究提出使用生成式智能体来自动评估AI生成内容的质量，以解决传统人工评估成本高、效率低的问题，帮助企业更高效地生成高质量内容。


<details>
  <summary>Details</summary>
Motivation: 现代企业在内容生成和评估上面临时间和成本挑战：人工写作受时间限制，外部评估成本高昂。虽然大语言模型在内容创作方面有潜力，但AI生成内容的质量问题仍然存在，传统的人工评估方法进一步增加了运营成本，需要高效、自动化的解决方案。

Method: 引入生成式智能体（Generative Agents）来评估AI生成的内容。这些智能体能够快速、经济地模拟人类判断，从连贯性、趣味性、清晰度、公平性和相关性等多个维度对内容进行评分。

Result: 通过使用生成式智能体，企业可以简化内容生成流程，确保一致的高质量输出，同时减少对昂贵人工评估的依赖。该方法为增强大语言模型生成符合业务需求的高质量内容提供了关键见解。

Conclusion: 生成式智能体为自动内容生成和评估提供了重要进展，能够帮助企业更高效地生成高质量内容，同时显著降低评估成本，推动AI内容生成在商业应用中的发展。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [29] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 本文通过大规模实验研究，揭示了多智能体系统的量化扩展原则，发现了工具协调权衡、能力饱和和拓扑依赖错误放大等关键效应，并提出了基于任务特性的最优协调策略预测框架。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的智能体系统在现实AI应用中越来越普及，但其性能决定原则仍缺乏深入研究，导致实践者依赖启发式方法而非基于原则的设计选择。本文旨在填补这一空白。

Method: 在四个多样化基准测试（Finance-Agent、BrowseComp-Plus、PlanCraft、Workbench）上，使用五种典型架构（单一、独立、集中式、分散式、混合式）和三个LLM家族，进行包含180个配置的受控评估，使用标准化工具和令牌预算。通过经验协调指标建立预测模型。

Result: 发现三个主导效应：1）工具协调权衡：固定计算预算下，工具密集型任务受多智能体开销影响更大；2）能力饱和：当单智能体基线超过约45%时，协调带来递减或负回报；3）拓扑依赖错误放大：独立智能体将错误放大17.2倍，集中式协调控制在4.4倍。集中式协调在金融推理等并行任务上提升80.9%，分散式协调在动态网页导航上表现更好（+9.2% vs +0.2%），但所有多智能体变体在顺序推理任务上降低性能39-70%。

Conclusion: 该框架能够预测87%的保留配置的最优协调策略，为智能体扩展提供了基于可测量任务特性的预测原则，使智能体系统设计从启发式转向基于原则的方法。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [30] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 提出rSIM机制，通过小型规划器自适应注入推理策略，使任何LLM都能成为RLM，显著提升推理能力


<details>
  <summary>Details</summary>
Motivation: 受LLM通过强化学习演变为推理语言模型（RLM）的"顿悟"时刻启发，希望开发一种机制使任何LLM都能获得高级推理能力

Method: 提出强化策略注入机制（rSIM），使用小型规划器引导LLM的思维链，通过多智能体强化学习联合训练规划器（领导者）和LLM（跟随者）

Result: rSIM使Qwen2.5-0.5B显著超越Qwen2.5-14B，规划器具有通用性，只需训练一次即可作为插件提升现有LLM的推理能力，支持跨任务持续学习

Conclusion: rSIM是一种有效的机制，能够通过自适应策略注入显著提升LLM的推理能力，规划器具有通用性和可扩展性

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [31] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 本研究开发了一个机器学习框架，使用土耳其382个土壤样本的物理化学特性来预测加州承载比(CBR)，随机森林回归器表现最佳，为岩土工程提供了一种高效的数据驱动替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统CBR实验室测试耗时、昂贵且不适用于大规模或多样化的土壤剖面，而机器学习能够以更快速度和更高精度建模复杂土壤行为，推动岩土工程的数字化转型。

Method: 收集土耳其不同地理气候区域的382个土壤样本，提取与承载能力相关的物理化学特性，测试12种机器学习算法（包括决策树、随机森林、梯度提升、XGBoost、支持向量回归等），通过训练、验证和测试评估模型的泛化能力和鲁棒性。

Result: 随机森林回归器表现最佳，获得训练集R²=0.95、验证集R²=0.76、测试集R²=0.83的优异结果，展示了强大的非线性映射能力。

Conclusion: 机器学习框架为CBR预测提供了有效的替代方案，支持智能数据驱动模型在岩土工程中的集成，促进了基础设施分析和设计的数字化转型。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [32] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 该研究提出使用自动机器学习（AutoML）方法预测土壤压实参数（最优含水量OMC和最大干密度MDD），相比传统实验室方法和现有ML模型，AutoML通过自动算法选择和超参数优化，在异质土壤数据集上取得了更好的预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统确定土壤压实参数的方法需要费时费力的实验室实验，而现有的经验回归模型在不同土壤类型上的适用性和准确性有限。虽然机器学习和人工智能技术已被用于预测这些参数，但现有ML模型在预测精度和泛化能力方面仍存在不足，特别是在处理代表不同土壤类型的异质数据集时。

Method: 研究采用自动机器学习（AutoML）方法预测最优含水量（OMC）和最大干密度（MDD）。AutoML自动执行算法选择和超参数优化过程，通过广泛的实验比较不同算法的性能。研究使用了包含多种土壤类型的异质数据集来训练和评估模型。

Result: 研究发现极端梯度提升（XGBoost）算法表现最佳，在独立测试数据集上，对MDD的预测R平方值达到80.4%，对OMC的预测R平方值达到89.1%。这些结果表明AutoML方法在不同土壤类型上预测压实参数的有效性。

Conclusion: AutoML方法能够有效预测土壤压实参数，提高了预测精度和模型泛化能力。异质数据集对提升ML模型的泛化性和性能至关重要。该研究通过改进土壤压实参数的预测，为更高效可靠的建筑工程实践做出了贡献。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [33] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR是一个无需演示的LLM智能体框架，通过双策略协调（高层整体规划和上下文本地策略）和轻量级反思机制，在ALFWorld和Mind2Web上实现SOTA性能，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖外部演示或检索增强规划，导致脆弱性、泛化能力差和计算开销高。受人类问题解决启发，需要一种无需演示的框架，让单个冻结LLM能够通过双策略协调进行自适应推理。

Method: 提出DuSAR框架：1）高层整体规划策略；2）上下文本地策略；3）轻量级反思机制，通过策略适应度分数评估进展，动态调整全局计划或细化策略；4）可选的专家演示集成。

Result: 在ALFWorld上达到37.1%成功率（Llama3.1-70B），比之前最佳结果13.0%提高一倍多；在Mind2Web上达到4.02%，同样翻倍；每步token消耗减少3-9倍；消融研究证实双策略协调的必要性。

Conclusion: DuSAR通过双策略协调和反思机制，实现了无需演示的高效LLM智能体推理，在性能和效率上显著超越现有方法，同时保持与外部知识的兼容性。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [34] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: DeepFeature是一个基于大语言模型的上下文感知特征生成框架，用于可穿戴生物信号处理，通过多源特征生成、迭代特征精炼和多层过滤验证机制，显著提升了医疗任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前可穿戴设备生物信号的特征提取方法存在三个主要问题：缺乏任务特定的上下文知识、在高维特征空间中难以找到最优特征提取设置、以及容易产生代码生成和自动化错误。这些问题限制了机器学习模型在医疗应用中的性能。

Method: DeepFeature框架包含三个核心组件：1) 多源特征生成机制，整合专家知识和任务设置；2) 迭代特征精炼过程，基于特征评估反馈进行特征重选；3) 鲁棒的多层过滤和验证方法，确保特征到代码的可靠转换，避免运行时崩溃。

Result: 在八个不同任务上的实验评估显示，DeepFeature相比基线方法平均AUROC提升了4.21-9.67%。在五个任务上超越了最先进的方法，在其余任务上保持了相当的性能。

Conclusion: DeepFeature是首个基于大语言模型的上下文感知特征生成框架，通过整合专家知识、迭代精炼和鲁棒代码转换，有效解决了现有特征提取方法的局限性，为可穿戴生物信号处理提供了更优的解决方案。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [35] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 研究通过强化学习智能体控制旋转圆柱体在水流中的阻力，发现学习高性能技能需要比执行技能更丰富的信息反馈，且学习条件取决于目标而非动力学复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究人类在无外部反馈情况下执行高性能活动（如花样滑冰、棒球投球）的技能获取过程，但绕过人类受试者，在完全受控条件下研究技能习得。

Method: 将通用强化学习智能体与桌面循环水槽中的旋转圆柱体直接连接，通过最大化或最小化阻力来研究技能获取。系统具有物理世界的丰富交互和复杂动力学特性，目标明确但策略不显而易见。

Result: 高维流动反馈让智能体在几分钟的实际交互中发现高性能阻力控制策略；无反馈回放时性能几乎相同，表明执行策略不需要反馈。无流动反馈训练时，智能体在阻力最大化任务中失败，但在阻力最小化中仍能成功（较慢且不可靠）。

Conclusion: 学习高性能技能需要比执行技能更丰富的信息，学习条件可以是"仁慈"或"恶劣"的，这仅取决于目标而非动力学或策略复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [36] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 提出基于数据转换图（DTG）的多智能体框架，用于仓库级自动程序修复，通过数据谱系而非控制流追踪逻辑缺陷，在SWE-Verified基准上达到87.1%的解决率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在函数级代码生成方面取得进展，但仓库级自动程序修复仍面临挑战。现有方法采用控制中心范式，迫使智能体处理复杂的目录结构和无关的控制逻辑，存在局限性。

Method: 提出从标准代码属性图（CPGs）向数据转换图（DTG）的范式转变，将数据状态建模为节点、函数建模为边，通过数据谱系而非控制流追踪逻辑缺陷。引入多智能体框架，协调数据完整性导航与控制流逻辑，并实现为自主问题解决器（AIR）系统。

Result: 该方法在多个SWE基准测试中表现出良好结果，在SWE-Verified基准上达到87.1%的解决率，解决了现代编码智能体中标准RAG系统固有的"语义陷阱"问题。

Conclusion: 该研究通过数据转换图范式解决了当前AI代码辅助工具的核心限制，为日益依赖软件的世界提供了更强大的基础，实现了可扩展的逻辑修复和零接触代码维护。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [37] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Principles2Plan：人类与大型语言模型协作生成情境敏感的伦理规则，指导自动化规划


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中运作需要伦理意识，但现有自动化规划工具缺乏支持。手动指定伦理规则劳动密集且高度依赖情境。

Method: 提出Principles2Plan交互式研究原型，通过人类与LLM协作：领域专家提供规划领域、问题细节和相关高级原则（如仁慈、隐私），系统生成可操作的伦理规则，用户可审查、优先排序并供给规划器生成伦理知情计划。

Result: 开发了首个支持用户在经典规划情境中生成基于原则的规则的系统，展示了人类-LLM协作使伦理自动化规划更实用可行的潜力。

Conclusion: Principles2Plan展示了人类与大型语言模型协作在使伦理自动化规划更实用和可行方面的潜力，填补了现有系统在支持用户生成基于原则的伦理规则方面的空白。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [38] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: CogMCTS：一种新颖的认知引导MCTS框架，通过多轮认知反馈、双轨节点扩展和精英启发式管理，结合策略性变异，实现高效的自动启发式设计，在稳定性、效率和解决方案质量上优于现有LLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM进化方法依赖种群策略易陷入局部最优，LLM与MCTS结合虽改善了探索与利用的权衡，但多轮认知整合有限且搜索多样性受限，需要克服这些限制。

Method: 提出CogMCTS框架：1）多轮认知反馈整合历史经验、节点信息和负面结果；2）双轨节点扩展结合精英启发式管理平衡探索与利用；3）策略性变异修改启发式形式和参数增强多样性。

Result: 实验结果表明CogMCTS在稳定性、效率和解决方案质量方面优于现有的基于LLM的自动启发式设计方法。

Conclusion: CogMCTS通过紧密整合LLM的认知引导机制与MCTS，实现了高效的自动启发式优化，解决了现有方法在认知整合和搜索多样性方面的限制。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [39] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 基于Transformer的蛋白质二级结构预测模型，使用注意力机制处理蛋白质序列数据，通过滑动窗口数据增强技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 从氨基酸序列预测蛋白质二级结构（α螺旋、β折叠、无规则卷曲）对于理解蛋白质功能至关重要，需要开发能够捕捉局部和长程残基相互作用的有效模型。

Method: 提出基于Transformer的模型，应用注意力机制处理蛋白质序列数据；采用滑动窗口数据增强技术在CB513数据集上扩展训练样本；模型能够处理可变长度序列并有效捕捉局部和长程残基相互作用。

Result: Transformer模型在蛋白质二级结构预测任务中表现出强大的泛化能力，能够有效处理可变长度序列，并成功捕捉蛋白质序列中的局部和长程相互作用模式。

Conclusion: Transformer架构适用于蛋白质二级结构预测任务，其注意力机制能够有效建模蛋白质序列中的复杂相互作用，为蛋白质结构预测提供了有前景的新方法。

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [40] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: See-Control是一个通过低自由度机械臂直接物理交互操作智能手机的框架，解决了现有方法依赖ADB仅限于Android设备的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型的智能手机操作智能体依赖Android Debug Bridge进行数据传输和动作执行，这限制了它们仅适用于Android设备，缺乏平台无关的解决方案。

Method: 提出See-Control框架，包含三个关键组件：1) 包含155个任务的ESO基准测试和评估指标；2) 基于MLLM的具身智能体，无需ADB或系统后端访问即可生成机器人控制命令；3) 丰富标注的操作片段数据集。

Result: 该框架实现了通过低自由度机械臂直接物理交互操作智能手机，提供了平台无关的解决方案，并为未来研究提供了有价值的资源。

Conclusion: See-Control通过弥合数字智能体与物理世界之间的差距，为实现家庭机器人在真实环境中执行依赖智能手机的任务迈出了具体一步。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [41] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供了构建生产级智能体AI工作流的端到端实用指南，涵盖设计、开发和部署全流程，提出了九大最佳实践，并通过多模态新闻分析案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在各行业的加速应用，组织面临如何设计、工程化和运营生产级智能体AI工作流的挑战，这些系统需要可靠、可观察、可维护且符合安全和治理要求。

Method: 引入结构化工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议(MCP)和工具集成、确定性编排、负责任AI考虑因素、环境感知部署策略，并提出九大核心最佳实践。

Result: 提出了九大最佳实践：工具优先设计、纯函数调用、单一工具和单一职责智能体、外部化提示管理、负责任AI对齐的模型联盟设计、工作流逻辑与MCP服务器分离、容器化部署、遵循KISS原则，并通过多模态新闻分析案例验证。

Conclusion: 本文提供了构建稳健、可扩展、生产就绪的智能体AI工作流的基础参考，结合架构指导、操作模式和实际实现洞察，为生产级智能体AI系统的工程化提供实用框架。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [42] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS是一个大规模LoRA分析框架，通过CLIP嵌入和与基础模型的差异，构建包含方向、强度和一致性的三维表示，用于语义检索和版权分析。


<details>
  <summary>Details</summary>
Motivation: 生成式组件（如LoRA）的快速扩散创建了庞大但非结构化的生态系统。现有发现方法依赖不可靠的用户描述或有偏见的流行度指标，阻碍了可用性。

Method: 分析650多个LoRA，在各种提示和种子下进行图像生成，使用CLIP嵌入及其与基础模型生成的差异，定义三维表示：方向（语义偏移）、强度（效果显著性）、一致性（效果稳定性）。

Result: 开发了高效的检索框架，能够语义匹配文本查询到相关LoRA，同时过滤过强或不稳定的LoRA，在自动和人工评估中优于文本基线。该表示还支持将强度和一致性与版权中的实质性和意愿性法律概念联系起来。

Conclusion: CARLoS提供了一个无需额外元数据的LoRA表征框架，不仅改进了检索性能，还为版权分析等更广泛的应用提供了实用系统。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [43] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 本文综述了描述逻辑和逻辑编程中Craig插值和均匀插值的理论结果与实用计算方法，探讨了其在知识表示中的多种应用。


<details>
  <summary>Details</summary>
Motivation: Craig插值和均匀插值在知识表示中有许多重要应用，包括可解释性、遗忘、模块化和重用以及学习等。然而许多相关的知识表示形式化方法通常不具备这些插值特性，且实际计算插值具有挑战性。

Method: 深入研究了两种重要的知识表示形式化方法：描述逻辑和逻辑编程，讨论了计算插值的理论结果和实用方法。

Result: 对描述逻辑和逻辑编程中的插值问题进行了系统分析，总结了现有的理论成果和实际计算技术。

Conclusion: Craig插值和均匀插值在知识表示中具有重要价值，尽管在实际计算中存在挑战，但对描述逻辑和逻辑编程中插值问题的研究为相关应用提供了理论基础和方法支持。

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [44] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 该论文提出了REST和REST+两个基准测试，用于系统评估多模态大语言模型中的跨模态不一致性问题，发现现有模型在不同模态（图像、文本、混合）中无法保持一致的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型被训练在同一个嵌入空间中表示视觉和语言信息，但它们无法在不同模态中执行相同的任务。需要系统评估这种跨模态不一致性问题。

Method: 创建了REST和REST+基准测试，包含三种模态（图像、文本、混合）中具有相同语义信息的样本。评估了15个最先进的多模态大语言模型，分析了OCR准确性、视觉特征（文本颜色、分辨率、字体）和视觉令牌数量对性能的影响。

Result: 研究发现：1）不同模型的模态不一致程度差异很大；2）将文本渲染为图像或将图像渲染为文本都无法解决不一致性问题；3）即使OCR正确，视觉特征（文本颜色和分辨率）和视觉令牌数量仍影响模型性能；4）一致性得分与文本和图像之间的模态差距相关。

Conclusion: 多模态大语言模型存在显著的跨模态不一致问题，REST基准测试为系统评估这一问题提供了有效工具，揭示了模型在模态对齐方面的局限性，并为理解模型机制提供了洞见。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver是一个自适应并行推理框架，在保持与顺序推理模型相当准确率的同时，显著降低推理延迟，在数学推理任务上实现1.53倍加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然通过扩展推理时间计算获得了强大的推理性能，但顺序解码导致高延迟，尤其是在复杂任务上。现有的并行推理方法要么局限于监督行为克隆，要么相比顺序链式思维基线存在显著准确率下降，且需要定制推理引擎，部署复杂。

Method: 1) 两阶段并行轨迹生成器，生成大规模高质量的带并行标注的CoT数据用于监督微调；2) 基于Trie的训练-推理协同设计，可在任何现成的自回归推理引擎上实现并行推理，无需修改位置嵌入或KV缓存；3) 并行感知的强化学习框架，教导模型在准确率和有效并行化之间取得平衡。

Result: 在六个具有挑战性的数学推理基准测试中，基于Qwen3-8B训练的ThreadWeaver实现了与最先进的顺序推理模型相当的准确率（平均71.9%，AIME24上79.9%），同时提供高达1.53倍的平均令牌延迟加速，在准确率和效率之间建立了新的帕累托前沿。

Conclusion: ThreadWeaver框架成功解决了并行推理中的准确率-效率权衡问题，通过创新的数据生成、训练-推理协同设计和强化学习框架，实现了在保持高准确率的同时显著提升推理效率，且兼容现有推理引擎，便于部署。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [46] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash是一个资源高效、可解释的小模型管道，用于预测纽约市机动车碰撞事故的伤害严重程度，使用树集成模型明显优于小型语言模型


<details>
  <summary>Details</summary>
Motivation: 纽约市每年报告超过10万起机动车碰撞事故，造成了严重的伤害和公共卫生负担，需要开发资源高效且可解释的预测模型来改善事故伤害严重程度的预测

Method: 整合三个关联表格的数千万条记录，构建统一特征模式的分区存储，训练基于工程化表格特征的紧凑树集成模型（随机森林和XGBoost），并与本地部署的小型语言模型进行比较

Result: XGBoost和随机森林在时间保留测试集上的准确率分别为0.7828和0.7794，明显优于小型语言模型（0.594和0.496）；类别不平衡分析显示简单类别加权可提高致命事故召回率；SHAP归因分析突出显示人类脆弱性因素、时间和地点是预测严重程度的主要驱动因素

Conclusion: 可解释的小模型集成仍然是城市规模伤害分析的强大基线，而将表格预测器与小型语言模型生成的叙述相结合的混合管道可以在不牺牲可扩展性的情况下改善沟通效果

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [47] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个预测多模态模型GPU内存使用量的框架，通过分解模型架构和分析训练行为来准确预测峰值内存使用，避免内存溢出错误。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统中深度学习模型的规模和复杂性增加，GPU内存需求经常超过可用容量，导致内存溢出错误，这会中断整个训练过程并浪费大量计算资源。现有研究仅关注单模态架构，无法推广到多模态模型，而多模态模型在智能体AI系统中是常见选择。

Method: 提出一个框架，通过分析多模态模型的架构和训练行为来预测峰值GPU内存使用。具体方法是将多模态模型分解为组成层，并应用因子化来估计每层的内存使用。

Result: 评估显示该框架实现了约8.7%的平均MAPE（平均绝对百分比误差）预测准确率。

Conclusion: 该框架能够准确预测多模态模型的GPU内存使用，有助于防止内存溢出错误，提高训练效率。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [48] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA提出了一种基于对数域注意力预测的算法-架构协同设计，通过消除昂贵乘法运算和减少累加开销，实现了跨阶段稀疏Transformer加速，相比现有SOTA方法能效提升2.79-3.52倍。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的Transformer在NLP和CV任务中表现出色，但随着输入序列变化，计算瓶颈在不同阶段呈现动态行为，需要跨阶段稀疏加速策略。现有稀疏Transformer方法多为单阶段设计，其稀疏预测机制在多阶段应用中会产生显著功耗开销。

Method: 提出LAPA对数域注意力预测算法-架构协同设计：1）设计非对称前导一计算方案消除昂贵乘法；2）提出混合精度多轮移位累加机制减少累加开销；3）设计数据特征依赖滤波器与MRSA协同工作；4）设计专用加速器将理论提升转化为实际硬件改进。

Result: 实验结果显示，LAPA相比当前最先进的Spatten、Sanger和FACT工作，分别实现了3.52倍、3.24倍和2.79倍的能量效率提升。

Conclusion: LAPA通过算法-架构协同设计有效解决了跨阶段稀疏Transformer加速问题，显著提升了能量效率，为动态计算瓶颈的Transformer模型提供了高效的硬件加速解决方案。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [49] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM是一个鲁棒的图基础模型框架，通过结构感知语义增强提升领域自适应表示，解决图基础模型在领域噪声、结构扰动和对抗攻击下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 图基础模型在各种任务中取得了显著进展，但其在领域噪声、结构扰动和对抗攻击下的鲁棒性仍未充分探索。关键限制在于对层次结构语义建模不足，而这对泛化能力至关重要。

Method: 1) 通过将基于熵的编码树转化为结构感知文本提示来编码层次结构先验，进行特征增强；2) 使用自监督信息瓶颈机制，通过结构引导压缩提取鲁棒可迁移表示；3) 引入专家自适应路由机制，结合混合专家架构和空专家设计解决跨领域适应的负迁移问题；4) 提出微调模块，通过联合社区内和社区间结构学习优化层次结构。

Result: 大量实验表明，SA^2GFM在节点和图分类任务中，在对抗随机噪声和对抗扰动方面，在有效性和鲁棒性上优于9个最先进的基线方法。

Conclusion: SA^2GFM通过结构感知语义增强和自适应机制，显著提升了图基础模型在噪声和扰动环境下的鲁棒性和泛化能力，为图基础模型的稳健应用提供了有效解决方案。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [50] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: FAIM是一个轻量级的频率感知交互式Mamba模型，用于时间序列分类任务，通过自适应滤波块和交互式Mamba块实现高效的多粒度信息交互，在多个基准测试中优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在环境监测、医疗诊断等应用中至关重要，但现有深度学习模型存在计算成本高、对噪声敏感、在小数据集上容易过拟合等问题，需要更轻量、鲁棒的解决方案。

Method: 提出FAIM模型，包含自适应滤波块（AFB）利用傅里叶变换提取频域特征，通过可学习自适应阈值动态抑制噪声；设计交互式Mamba块（IMB）实现高效多粒度信息交互；采用自监督预训练机制增强对复杂时序模式的理解。

Result: 在多个基准测试上的广泛实验表明，FAIM持续优于现有最先进方法，在准确性和效率之间实现了优越的平衡，并在各种领域和高噪声场景中表现出色。

Conclusion: FAIM通过频率感知和交互式Mamba架构有效解决了时间序列分类中的计算成本、噪声敏感和过拟合问题，为TSC任务提供了强大而高效的解决方案。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [51] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: SetAD：一种新颖的半监督异常检测框架，将异常检测重新定义为集合级任务，通过注意力机制集合编码器和分级学习目标，直接建模定义异常的复杂群体级交互。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法主要关注单个点或简单对的评分，这种点中心或对中心的视角忽略了异常的上下文性质（异常是相对于群体定义的），也无法利用集合组合生成的丰富监督信号，难以利用数据中的高阶交互来学习判别性表示。

Method: 提出SetAD框架，将半监督异常检测重构为集合级异常检测任务。使用基于注意力的集合编码器，通过分级学习目标训练模型学习量化整个集合的异常程度。还提出上下文校准的异常评分机制，通过聚合点在多个不同上下文集合中相对于同伴行为的归一化偏差来评估点的异常分数。

Result: 在10个真实世界数据集上的广泛实验表明，SetAD显著优于最先进的模型。特别值得注意的是，模型性能随着集合大小的增加而持续提升，为基于集合的异常检测公式提供了强有力的实证支持。

Conclusion: SetAD通过集合级视角重新定义异常检测，能够有效建模群体级交互，利用高阶监督信号，在性能上超越现有方法，并且随着集合规模增大表现更好，验证了集合框架在异常检测中的有效性。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [52] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 本文提出一个无监督机器学习框架，用于分析海关数据以监测环境条约执行情况，通过聚类、异常检测和启发式标记识别可疑贸易模式，生成优先级评分供监管部门审查。


<details>
  <summary>Details</summary>
Motivation: 需要新方法来监测《蒙特利尔议定书》等环境条约的执行情况，通过审查大规模复杂海关数据集来发现违规贸易活动。

Method: 结合多种无监督机器学习技术：K-Means聚类发现贸易原型；隔离森林和IQR异常检测识别"大宗交易"和异常价格；启发式标记发现模糊描述等规避策略；综合生成优先级评分；使用SHAP进行可解释性分析。

Result: 应用于10万条贸易记录，成功识别1,351个价格异常值和1,288个高优先级货物供海关审查。发现高优先级商品的价值重量比与普通商品显著不同。模型成功检测到2021年初"大宗交易"激增，与美国AIM法案的监管影响直接相关。

Conclusion: 该工作提供了一个可重复的无监督学习流程，能将原始贸易数据转化为优先排序、可操作的情报，为监管机构提供有效的监测工具。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [53] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 将瑞典690万人的登记数据转化为文本化生命轨迹，解决分类变量高基数性和编码不一致问题，预测居住流动性，比较多种NLP模型效果


<details>
  <summary>Details</summary>
Motivation: 解决数据分析中长期存在的两个挑战：分类变量的高基数性和随时间变化的编码方案不一致性。利用瑞典全面的人口登记数据，探索文本化生命轨迹在纵向预测中的有效性

Method: 将690万个体（2001-2013年）的登记数据转化为语义丰富的文本，形成生命轨迹，结合人口统计信息和居住、工作、教育、收入、家庭状况的年度变化。比较多种NLP架构（LSTM、DistilBERT、BERT、Qwen）预测个体后期（2013-2017年）居住流动性

Result: 序列化和基于Transformer的模型比基线模型更有效地捕捉时间和语义结构。文本化的登记数据保留了关于个体路径的有意义信息，支持复杂、可扩展的建模。该数据集为开发评估新的序列建模方法提供了严格测试平台

Conclusion: 将语义丰富的登记数据与现代语言模型结合，能够显著推进社会科学中的纵向分析。这种方法在具有可比覆盖范围和精度的纵向微观数据稀缺的情况下，提供了独特的研究机会

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [54] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出基于混合专家(MoE)的噪声估计器，结合扩散模型进行医学时间序列信号重建，通过RFAMoE模块自适应选择感受野，利用Fusion MoE模块并行生成多个噪声信号并融合，单次推理完成重建，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列信号具有多变量、高时间变异性、高噪声和易受伪影影响等独特特性，使得基于深度学习的插补等任务仍然具有挑战性。虽然扩散模型在时间序列重建方面显示出潜力，但在医学领域应用仍较少探索。

Method: 1. 在基于分数的扩散框架中引入混合专家(MoE)噪声估计器；2. 设计RFAMoE模块，使每个通道在扩散过程中自适应选择所需感受野；3. 设计Fusion MoE模块，利用MoE特性并行生成K个噪声信号，通过路由机制融合，单次推理完成重建。

Result: 提出的框架在不同任务和数据集上持续优于基于扩散模型的最先进方法，不仅提高了性能，还消除了多次推理过程带来的显著计算成本和延迟。

Conclusion: 该研究提出了一种创新的MoE-based扩散框架，有效解决了医学时间序列信号重建的挑战，通过自适应感受野选择和并行融合机制，在保持高性能的同时显著降低了计算复杂度。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [55] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: CRAG框架统一建模自动驾驶车辆测试中的正常行为和风险行为，通过结构化潜在空间分离两种行为模式，实现可控风险场景生成，提高测试效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆安全测试需要同时模拟日常驾驶和罕见的安全关键场景，现有方法难以在保持真实性的同时生成风险行为，且缺乏对风险行为的可控生成能力。

Method: 提出CRAG框架：1) 构建结构化潜在空间，解耦正常行为和风险相关行为；2) 结合风险感知潜在表示和基于优化的模式转换机制；3) 使智能体能够平滑地从安全状态过渡到风险状态，同时保持两种状态的高保真度。

Result: 实验表明CRAG相比现有基线提高了行为多样性，同时实现了风险场景的可控生成，能够针对性地高效评估自动驾驶系统的鲁棒性。

Conclusion: CRAG框架成功统一了正常和风险行为的建模，通过解耦潜在空间和优化转换机制，实现了对自动驾驶系统安全性的高效、可控测试。

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [56] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN在KAN基础上引入符号基元字典和可学习门控，通过可微分稀疏化和最小描述长度目标，实现符号化与稠密样条的平滑切换，在保持精度的同时提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统KAN虽然具有可解释性潜力，但训练后的激活函数往往缺乏符号保真度，学习到的分解没有有意义的对应关系，无法实现真正的可解释机器学习。

Method: 提出Softly Symbolified KANs (S2KAN)，将符号基元直接集成到训练中。每个激活函数从符号项和稠密项字典中提取，使用可学习门控稀疏化表示。通过可微分稀疏化和最小描述长度目标进行端到端优化。

Result: 在符号基准测试、动态系统预测和真实世界预测任务中，S2KAN以更小的模型实现了竞争性或更优的准确性。即使没有正则化压力，也观察到自稀疏化的出现。

Conclusion: S2KAN通过集成符号基元和可微分稀疏化，在符号项足够时发现可解释形式，不足时优雅地退化为稠密样条，实现了可解释性与准确性的平衡。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [57] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 论文提出使用Youden's J统计量和平衡准确率来选择LLM评估中的分类器，以避免传统指标在类别不平衡时的偏差，确保模型比较的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型时依赖分类器（如LLM作为评判者或人工标注者）来估计期望/不期望行为的流行率。常用的准确率、精确率、F1等指标对类别不平衡和正类选择敏感，可能导致选择扭曲流行率估计的分类器，影响模型比较的可信度。

Method: 提出使用Youden's J统计量作为理论基础，证明平衡准确率是J的线性变换，两者在理论上与选择最佳评判者以比较模型的目标一致。通过分析论证、实证案例和模拟实验，验证基于平衡准确率选择分类器的优势。

Result: 研究表明Youden's J统计量在理论上与选择最佳模型比较评判者的目标一致，平衡准确率作为其等价线性变换，能够提供更稳健的分类器选择，避免传统指标在类别不平衡情况下的偏差。

Conclusion: 在LLM评估中，应使用Youden's J统计量或平衡准确率来选择分类器，这能确保更可靠、更稳健的模型比较，提高评估结果的可信度。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [58] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入，在无监督和半监督图学习任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如InfoNCE）在优化跨视图图嵌入对齐时，缺乏对视图特定图-图全局结构的控制机制，需要更有效的对齐方法。

Method: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入，理论证明该差异为理想对齐损失提供上界。

Result: 在8个TU基准测试的无监督和低标签率半监督学习中达到新SOTA，在PPI-306K和ZINC 2M数据集的迁移学习中获得一致性能提升。

Conclusion: SpecMatch-CL通过谱匹配方法有效对齐图嵌入的全局结构，显著提升图表示学习性能，为图对比学习提供了新的理论框架。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [59] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文提出Cone Collapse算法，从几何角度重新审视非负矩阵分解，通过收缩非负象限来恢复数据的最小生成锥，并在此基础上开发了锥感知正交NMF模型，在聚类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NMF算法主要从优化角度出发，没有充分利用NMF诱导的锥几何特性。数据点位于凸锥中，其极射线编码了基本方向或"主题"，从几何角度理解NMF可以带来更好的聚类性能。

Method: 提出Cone Collapse算法，从完整的非负象限开始，迭代收缩到由数据生成的最小锥。在恢复极射线的基础上，应用单正交NMF得到锥感知正交NMF模型（CC-NMF）。

Result: 在16个基准基因表达、文本和图像数据集上，CC-NMF在聚类纯度方面一致匹配或优于包括乘法更新、ANLS、投影NMF、ONMF和稀疏NMF在内的强基线方法。

Conclusion: 显式恢复数据锥可以产生既有理论依据又具有强实证性能的基于NMF的聚类方法，几何视角为NMF提供了新的理解和改进方向。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [60] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文提出CLOP损失函数，通过促进类别嵌入形成正交线性子空间来防止对比学习中的维度崩溃问题，在图像分类和目标检测任务中表现出更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对比学习在深度学习中表现出色，但维度崩溃问题（嵌入收敛到低维空间）在半监督和自监督设置中构成重大挑战。作者发现存在一个关键学习率阈值，超过该阈值标准对比损失会收敛到崩溃解。

Method: 基于对学习率阈值的洞察，提出了CLOP（Contrastive Learning with Orthogonal Projections）损失函数，这是一种新颖的半监督损失函数，通过促进类别嵌入形成正交线性子空间来防止维度崩溃。

Result: 在真实和合成数据集上的广泛实验表明，CLOP在图像分类和目标检测任务中提高了性能，同时在不同学习率和批量大小下表现出更大的稳定性。

Conclusion: CLOP通过防止维度崩溃有效解决了对比学习中的关键问题，为半监督对比学习提供了更稳定和有效的解决方案，在多种视觉任务中展现出优越性能。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [61] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2通过算法-系统联合重设计，解决了原始GSPN在GPU实现中的效率瓶颈，包括内核启动开销、内存传输和冗余计算，同时引入紧凑通道传播策略，在保持Transformer精度的前提下显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 尽管GSPN通过线扫描传播方案将计算复杂度降低到接近线性，但现有实现仍存在GPU内核重复启动开销大、全局内存数据传输过多以及每通道独立传播权重导致的冗余计算等问题，限制了其在实际高分辨率图像和长视频应用中的效率。

Method: GSPN-2采用算法-系统联合重设计：系统层面将数千个微内核启动合并为单个2D内核，将每个通道切片固定到一个warp，并在共享内存中暂存前一列的激活值；算法层面引入紧凑通道传播策略，替代每通道独立矩阵，减少参数并自然对齐Transformer注意力中的亲和力图。

Result: 实验表明GSPN-2在图像分类和文本到图像合成任务中有效，能够匹配Transformer级别的精度，同时显著降低计算成本，为视觉应用中的全局空间上下文建模建立了新的效率前沿。

Conclusion: GSPN-2通过结构化矩阵变换和GPU优化实现的独特组合，为视觉应用中的全局空间上下文建模提供了高效解决方案，在保持精度的同时大幅提升了计算效率。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [62] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush是一种符号回归算法，将决策树分割算法与非线性常数优化结合，能够生成包含规则逻辑的符号回归模型，在临床风险评分开发中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗决策常使用结合风险方程和规则的算法，但传统符号回归难以建模这种决策过程。符号回归具有数据驱动和可解释性优势，有望开发数据驱动的临床风险评分系统。

Method: Brush算法结合了决策树式的分割算法和非线性常数优化，能够将基于规则的逻辑无缝集成到符号回归和分类模型中。

Result: Brush在SRBench上达到帕累托最优性能，成功复现了两个广泛使用的临床评分系统，获得高准确率和可解释模型。相比决策树、随机森林和其他符号回归方法，Brush在预测性能上相当或更优，同时产生更简单的模型。

Conclusion: Brush算法通过结合规则逻辑和符号回归，为开发数据驱动的临床风险评分系统提供了一种有效方法，在保持可解释性的同时实现了良好的预测性能。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [63] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net是一种无需存储历史样本的自解释原型模型，用于持续学习，在任务和类别增量设置中实现了最先进的性能，同时显著降低了内存开销。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临灾难性遗忘的挑战，现有可解释AI方法大多使用事后解释或需要为每个新任务存储额外内存，导致可扩展性有限。需要一种既能提供有用解释又具有强性能的实用可解释解决方案。

Method: 提出CIP-Net，一种无需示例的自解释原型模型。该模型避免存储过去示例，保持简单架构，同时提供有用的解释。通过原型表示来保留知识，防止灾难性遗忘。

Result: CIP-Net在任务增量和类别增量设置中，相比之前的无示例和自解释方法，实现了最先进的性能。同时显著降低了内存相关开销，使其成为实用且可解释的解决方案。

Conclusion: CIP-Net是一种有效的自解释原型模型，能够在持续学习中提供有用解释并保持强性能，同时避免存储历史样本，具有实际应用价值。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [64] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 开发了一个面向医疗健康领域的时间序列预测Web平台，降低技术门槛，使研究人员和临床医生能够轻松进行数据分析、模型训练和结果解释。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在医疗健康等领域有广泛应用，但技术专业知识要求高，成为使用这些技术的障碍。需要让研究人员和临床医生能够轻松访问和使用预测分析工具。

Method: 开发了一个Web平台，支持数据上传和可视化，提供多种可高度自定义的预测模型和训练技术，并集成大语言模型提供参数建议和结果解释。

Result: 创建了一个功能完整的平台，用户可以通过上传数据生成图表展示变量关系，选择适合的预测模型，并获得大语言模型提供的参数推荐和结果解释支持。

Conclusion: 该平台降低了时间序列预测的技术门槛，旨在整合到学习型健康系统中，实现临床流程中的持续数据收集和推断。

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [65] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 该研究在重症监护场景中，比较了三种离线多目标强化学习算法与三种标量化单目标基线方法，发现PEDA DT算法在灵活性方面表现最优，为个性化医疗决策提供了新框架。


<details>
  <summary>Details</summary>
Motivation: 重症监护中临床医生面临生存率最大化和资源利用最小化的冲突目标。传统单目标强化学习方法使用固定标量化奖励函数，导致策略僵化无法适应变化的临床优先级。需要一种能在离线环境下学习多目标策略的方法。

Method: 在MIMIC-IV数据集上，对三种离线多目标强化学习算法（CPQL、Adaptive CPQL、PEDA DT）和三种标量化单目标基线方法（BC、CQL、DDQN）进行基准测试，使用离线策略评估指标进行性能比较。

Result: PEDA DT算法在灵活性方面优于静态标量化基线方法。研究还证实了序列建模架构在多目标条件生成中保持稳健有效，扩展了单目标决策变换器在医疗领域的先前发现。

Conclusion: 离线多目标强化学习是一个有前景的框架，能够在无需重新训练的情况下实现重症监护中的个性化、可调整决策制定，为临床实践提供了更灵活的决策支持工具。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [66] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY是一个医学世界模型，通过结构化潜在空间预测疾病演化，整合时间间隔和患者特定数据，生成个体化治疗计划，并在胶质瘤数据集上优于现有方法12%。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤学临床决策需要预测动态疾病演化，但现有静态AI预测器无法完成此任务。现有医学世界模型方法通常忽略患者特定的时间和临床背景，缺乏将预测与治疗决策连接的反馈机制。

Method: CLARITY是一个医学世界模型，直接在结构化潜在空间中预测疾病演化。它明确整合时间间隔（时间背景）和患者特定数据（临床背景），将治疗条件下的进展建模为平滑、可解释的轨迹，从而生成生理上可信的个体化治疗计划。此外，CLARITY引入了一个新颖的预测到决策框架，将潜在推演转化为透明、可操作的建议。

Result: CLARITY在治疗规划方面展示了最先进的性能。在MU-Glioma-Post数据集上，该方法比最近的MeWM提高了12%，并显著超越了所有其他医学特定的大型语言模型。

Conclusion: CLARITY通过整合时间和临床背景，在结构化潜在空间中预测疾病演化，提供了一个能够生成个体化治疗计划并转化为可操作建议的医学世界模型，在胶质瘤治疗规划任务上表现出色。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [67] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LUNA是一种核化线性注意力机制，通过可学习的核特征映射，在保持线性计算成本的同时达到甚至超越二次注意力的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力存在O(n²)二次计算成本，限制了长序列应用。现有线性注意力机制虽然将成本降至O(n)，但依赖固定的随机特征映射，导致模型准确性和计算效率之间存在根本性权衡。

Method: 提出LUNA核化线性注意力机制，核心洞察是核特征映射本身应该被学习而非固定。通过参数化核，学习针对特定数据和任务的特征基，克服固定特征方法的表达能力限制。实现可学习的特征映射，诱导正定核并允许流式形式，实现序列长度的线性时间和内存扩展。

Result: 在Long Range Arena上，LUNA在计算对等条件下（相同参数数量、训练步数和近似FLOPs）实现了高效Transformer中的最先进平均准确率。在后验转换方面，替换微调后的BERT和ViT-B/16检查点中的softmax并简要微调，恢复了大部分原始性能，显著优于固定线性化方法。

Conclusion: LUNA消除了线性注意力中准确性和计算效率之间的权衡，通过可学习的核特征映射，在保持线性成本的同时匹配甚至超越二次注意力性能，为长序列应用提供了有效的解决方案。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [68] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出了一种可解释的深度竞争风险模型DKAJ，通过自动学习核函数衡量数据点相似性，将每个数据点表示为多个簇的加权组合，从而推广经典的Aalen-Johansen非参数估计方法。


<details>
  <summary>Details</summary>
Motivation: 现有竞争风险模型在提供可解释性方面存在不足，需要一种既能保持预测性能又能提供可视化解释的深度学习方法，以帮助理解模型决策过程。

Method: 提出Deep Kernel Aalen-Johansen (DKAJ)估计器，通过自动学习的核函数计算数据点之间的相似性，将每个数据点表示为多个簇的加权组合。当数据点仅对一个簇有非零权重时，其预测的累积发生率函数对应于经典Aalen-Johansen估计器在该簇上的限制。

Result: 在四个标准竞争风险数据集上的实验表明，DKAJ与最先进的基线方法具有竞争力，同时能够提供可视化辅助模型解释。

Conclusion: DKAJ是一种有效的可解释深度竞争风险模型，既保持了预测性能，又提供了模型解释的可视化工具，有助于理解模型决策过程。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [69] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出因果引导的多模态领域泛化框架，通过对抗解耦和统一表示学习解决社交媒体危机分类中的领域泛化问题


<details>
  <summary>Details</summary>
Motivation: 社交媒体危机分类面临领域泛化挑战，现有方法无法解耦虚假特征和因果特征，且无法对齐异构模态表示，导致在未见危机类型上泛化性能差

Method: 提出因果引导的多模态领域泛化框架，结合对抗解耦和统一表示学习：对抗目标鼓励模型解耦并关注领域不变的因果特征；统一表示将不同模态特征对齐到共享潜在空间

Result: 在不同数据集上的实验表明，该方法在未见灾难场景中取得了最佳性能

Conclusion: 提出的因果引导多模态领域泛化框架通过解耦因果特征和对齐模态表示，有效提升了社交媒体危机分类在未见危机类型上的泛化能力

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [70] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 本文提出了一种通过"忏悔"机制促使大语言模型诚实报告自身缺陷的方法，在训练中为忏悔设置独立奖励，使模型更愿意揭露而非掩盖不当行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在报告自身行为和信念时可能存在不诚实问题，如夸大事实信心或掩盖不当行为。这种不诚实可能源于强化学习训练过程中的奖励塑造问题，无意中激励模型说谎或歪曲行为。

Method: 提出忏悔机制：在模型给出原始答案后，要求其提供忏悔输出，完整说明其遵守政策和指令的情况。训练中为忏悔设置独立奖励，仅基于诚实性评估，不影响主答案的奖励。通过使揭露不当行为成为最大化忏悔奖励的"最小阻力路径"，激励模型在忏悔中保持诚实。

Result: 训练GPT-5-Thinking模型产生忏悔，在分布外场景中评估其诚实性（包括幻觉、指令遵循、阴谋行为和奖励攻击）。发现当模型在主答案中说谎或隐瞒缺陷时，通常会在忏悔中诚实承认这些行为，且忏悔诚实性随训练适度提升。忏悔机制支持多种推理时干预措施。

Conclusion: 忏悔机制是促使大语言模型诚实报告自身缺陷的有效方法，通过独立奖励设计激励模型揭露而非掩盖不当行为，为模型监控、拒绝采样和问题提示等干预措施提供了基础。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [71] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: MAC是一种基于模型的离线强化学习方法，通过动作块模型减少长期预测误差，并使用拒绝采样防止模型利用，在复杂长时域任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究基于模型的强化学习（特别是基于模型的价值扩展）能否为离线RL中的复杂长时域任务提供可扩展的解决方案。传统方法中，更大的n值可以减少价值引导的偏差，但会放大长期累积的模型误差，这是一个需要解决的权衡问题。

Method: 提出MAC方法：1）使用动作块模型，预测未来状态时基于动作序列而非单个动作，减少复合误差；2）采用拒绝采样策略，从表达性强的行为动作块策略中采样，防止模型利用分布外动作。

Result: 在包含高达1亿个转移的大规模数据集上的高度挑战性任务实验中，MAC在离线基于模型的RL算法中取得了最佳性能，特别是在具有挑战性的长时域任务上。

Conclusion: MAC通过动作块模型和拒绝采样的组合，有效解决了基于模型价值扩展中的偏差-误差权衡问题，为离线RL中的复杂长时域任务提供了有效的可扩展解决方案。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [72] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出CSO方法，通过抑制内在特征来增强后门检测的敏感性，解决现有方法在易区分类别和隐蔽后门攻击中的失效问题


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法依赖目标类与非目标类在检测统计量上的极端差异，但在两类情况下会失效：1）某些非目标类本身容易与其他类区分，自然获得极端统计量；2）后门特征相对内在类别判别特征较弱时。需要更敏感的检测方法。

Method: 提出类子空间正交化（CSO）方法：利用少量干净样本，通过约束优化问题，在优化检测统计量的同时与类的内在特征正交化，从而抑制内在特征贡献，突出后门触发器的贡献。

Result: CSO方法在具有挑战性的混合标签攻击和自适应攻击中表现出色，能够更敏感地检测后门目标类。

Conclusion: 通过抑制内在特征贡献，CSO方法显著提高了后门检测的敏感性，有效解决了现有方法在易区分类别和隐蔽后门攻击中的局限性。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [73] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 论文提出了首个生物威胁基准生成框架，专门用于评估AI模型在细菌生物威胁方面的安全风险，考虑了不同攻击者能力和操作风险因素。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型（特别是大语言模型）的快速发展，需要量化其可能促进生物恐怖主义或获取生物武器的风险。当前缺乏能够全面评估模型生物安全风险的基准测试方法。

Method: 开发了生物威胁基准生成框架，首先针对细菌生物威胁建立了分层结构：生物威胁类别、要素和任务，并以此为基础开发任务对齐的查询。创建了细菌生物威胁模式作为任务-查询架构。

Result: 提出了首个生物威胁基准生成框架和细菌生物威胁模式，为评估AI模型在细菌生物威胁方面的风险提供了可重复使用的结构，能够捕捉生物对手的技术和操作需求。

Conclusion: 该框架为模型开发者和评估者提供了可靠测量和评估AI模型生物安全风险提升和潜在危害的方法，考虑了常被忽视的威胁要素，包括不同攻击者能力水平和操作风险因素。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [74] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文提出通过开放式学习和多智能体方法来训练鲁棒的AI智能体，使其能够泛化到新环境、分布外输入以及与其他智能体的交互。研究涵盖了从强化学习环境构建、对抗性课程生成、多智能体漏洞识别到大型语言模型对抗性提示鲁棒性的多个方面。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各种应用中的普及，需要能够成功导航和适应不断变化的开放世界的智能体。关键挑战是确保这些AI智能体不仅能在训练期间熟悉的设置中表现出色，还能有效地泛化到以前未见过的多样化场景。

Method: 1. 引入MiniHack：基于NetHack游戏的沙盒框架，通过程序化内容生成创建多样化环境；2. 提出Maestro：生成对抗性课程的方法，逐步增强RL智能体在双人零和游戏中的鲁棒性和泛化能力；3. 利用质量-多样性方法在多智能体领域系统识别预训练RL策略的漏洞；4. 使用进化搜索生成多样化有效输入，诊断和增强LLM对抗对抗性提示的鲁棒性。

Result: 该研究为AI鲁棒性的未来进展铺平了道路，使智能体能够适应不断发展的世界，并在面对不可预见的挑战和交互时蓬勃发展。具体成果包括：开发了MiniHack框架用于RL泛化研究，提出了Maestro对抗性课程生成方法，在多智能体足球游戏中识别了最先进RL策略的漏洞，以及增强了LLM对抗对抗性提示的鲁棒性。

Conclusion: 通过整合开放式学习和多智能体方法，该论文展示了如何训练和评估能够在新颖环境、分布外输入以及与其他智能体交互中有效泛化的鲁棒AI智能体。这些方法共同为开发能够适应不断变化世界并在面对意外挑战时茁壮成长的AI智能体提供了基础。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [75] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua是一个轻量级Transformer模型，用于领域内语言检测和细粒度语言分类，通过两级对比学习框架在计算和延迟受限环境中实现高精度语言识别


<details>
  <summary>Details</summary>
Motivation: 语言识别是多语言系统的关键第一步，现有工具在特定场景（如音乐请求）中表现不佳，开源工具速度快但精度低，大语言模型有效但成本高，不适合低延迟或低资源环境

Method: 采用轻量级Transformer架构，设计两级对比学习框架，结合实例级分离和类级对齐，使用自适应边界，为相近语言生成紧凑且分离良好的嵌入表示

Result: 在Amazon Massive数据集上达到99.25% F1分数，在Song数据集上达到98.15% F1分数，超越Sonnet 3.5，同时参数数量减少10倍

Conclusion: PolyLingua在计算和延迟受限环境中实现了高精度语言识别，特别适合处理音乐请求等具有代码转换的挑战性场景，为多语言系统提供了高效解决方案

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [76] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提高了生成模型与人类偏好对齐的训练效率，实现2.4倍加速训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对于将生成模型与人类偏好对齐至关重要，但其高昂的计算成本阻碍了广泛应用。现有方法在样本效率和信用分配方面存在局限性。

Method: 将去噪过程重构为搜索树，从共享的初始噪声样本出发，策略性地分支生成多个候选轨迹，同时高效重用它们的共同前缀。通过树状结构实现高效样本利用、细粒度信用分配和摊销计算。

Result: 在扩散模型和流模型上的实验表明，TreeGRPO实现了2.4倍的训练加速，在效率-奖励权衡空间中建立了更优的帕累托前沿，在多个基准测试和奖励模型上持续优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了一条可扩展且有效的途径，通过树状搜索结构显著提高了训练效率和性能。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [77] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: MobileFineTuner是一个开源框架，支持在普通手机上直接进行端到端的LLM微调，解决了移动设备内存和能耗限制的问题。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共数据接近枯竭，设备端微调成为利用私有用户数据同时保护隐私的机会。然而现有方法主要是基于模拟或依赖IoT设备和PC，普通手机领域尚未充分探索，缺乏开源框架支持实际LLM微调。

Method: 提出了MobileFineTuner统一开源框架，支持全参数微调和参数高效微调。针对手机内存和能耗限制，引入了参数分片、梯度累积和能量感知计算调度等系统级优化。

Result: 在真实手机上成功微调了GPT-2、Gemma 3和Qwen 2.5模型。大量实验和消融研究验证了所提优化的有效性，证明MobileFineTuner是设备端LLM训练研究的可行基础。

Conclusion: MobileFineTuner填补了移动设备LLM微调框架的空白，为未来设备端LLM训练研究提供了实用基础，能够在保护隐私的同时利用手机上的私有数据。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [78] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 该论文挑战了传统AdamW优化器中权重衰减与学习率γ成正比的假设，提出权重衰减应与γ²成正比，基于稳态时权重范数稳定的理论推导，并通过实验验证了这种设置能更好地控制训练动态并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统AdamW优化器中解耦权重衰减被默认设置为与学习率γ成正比，但作者质疑这一假设。最近有研究者基于正交性论证提出权重衰减应与γ²成正比，作者旨在通过理论分析和实验验证这一新设置的合理性及其对训练动态的影响。

Method: 作者首先分析了传统权重衰减设置的局限性，然后基于"在稳态时更新与权重无关"的简单假设，推导出解耦权重衰减应与γ²成正比的理论依据。同时推导了Scion优化器中每个小批量的总更新贡献(TUC)的动量依赖有效学习率特性，并通过实验验证了理论推导。

Result: 研究发现：1）消除更新垂直分量对权重范数的贡献对训练动态影响很小；2）权重衰减∝γ²能实现稳定的权重范数；3）这种设置能稳定权重和梯度范数；4）能更好地控制训练动态；5）最终能提升模型性能。

Conclusion: 解耦权重衰减应与学习率的平方γ²成正比，而不是传统的与γ成正比。这种设置基于稳态时权重范数稳定的理论推导，能更好地控制训练动态，稳定权重和梯度范数，并最终提升模型性能，为优化器设计提供了新的理论指导。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [79] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET是一种基于原型的新型时间序列特征工程方法，通过原型选择实现卷积核特征变换，在多数UCR/UEA数据集上表现与现有卷积算法相当，其集成模型MR-HY-SP超越了之前最好的卷积集成模型HYDRA-MR。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类算法主要依赖特征工程策略，其中ROCKET通过随机卷积核特征取得了优异性能。研究者希望探索基于原型的新特征工程策略，以进一步提升时间序列分类的准确性和鲁棒性。

Method: 提出了SPROCKET（Selected Prototype Random Convolutional Kernel Transform）方法，采用基于原型的特征工程策略。该方法通过原型选择实现卷积核特征变换，并构建了MR-HY-SP集成模型（MultiROCKET-HYDRA-SPROCKET）。

Result: 在大多数UCR和UEA时间序列分类基准数据集上，SPROCKET取得了与现有卷积算法相当的性能。MR-HY-SP集成模型的平均准确率排名超过了之前最好的卷积集成模型HYDRA-MR。

Conclusion: 基于原型的特征变换能够有效提升时间序列分类的准确性和鲁棒性，SPROCKET方法为时间序列分类提供了新的有效特征工程策略。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [80] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一个统一的几何-随机多模态深度学习框架，整合多种生理信号来建模癫痫猝死和急性缺血性中风的易感性


<details>
  <summary>Details</summary>
Motivation: 癫痫猝死和中风是涉及皮层、脑干和自主神经系统复杂相互作用的危及生命状况，需要整合多模态信号进行建模

Method: 结合黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制；使用分数流行病扩散在结构脑图上建模中风传播

Result: 在MULTI-CLARID数据集上展示了改进的预测准确性，并从流形曲率、分数记忆指数、注意力熵和扩散中心性中获得了可解释的生物标志物

Conclusion: 该框架为神经自主神经疾病的早期检测、风险分层和可解释的多模态建模提供了数学原理基础

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [81] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF通过计算随机森林预测的梯度，构建全局线性预处理器来旋转特征空间，使轴对齐决策树能处理旋转或交互依赖的决策边界，同时保持训练简单性。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树在处理旋转或特征交互依赖的决策边界时效果不佳，而倾斜森林虽然能解决这个问题但计算成本高、实现复杂。需要一种既能处理复杂边界又能保持简单性的方法。

Method: 首先训练轴对齐随机森林来估计类别概率或回归输出，计算预测相对于每个特征的有限差分梯度，将这些梯度聚合成期望雅可比外积（EGOP的推广），将其作为全局线性预处理器来旋转特征空间，然后在变换后的数据上重新训练标准轴对齐森林。

Result: 在表格分类和回归基准测试中，这种预处理器方法持续改进了轴对齐森林的性能，通常匹配或超越了倾斜森林基线，同时提高了训练时间。理论分析表明监督预处理器能恢复倾斜森林的大部分准确性，同时保持轴对齐树的简单性和鲁棒性。

Conclusion: JARF提供了一种简单有效的监督预处理器方法，通过全局特征空间旋转使轴对齐决策树能够处理复杂的决策边界，在保持训练简单性的同时获得了接近倾斜森林的性能。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [82] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 提出一种名为MAN的新正则化技术，通过最小化客户端模型每层激活范数来约束联邦学习优化问题的平坦性，从而提高模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习训练可能导致全局模型收敛到"尖锐最小值"，从而影响模型的泛化能力。需要改进联邦学习框架下的模型泛化性能。

Method: 引入平坦性约束的联邦学习优化问题，通过最小化每层激活范数(MAN)的正则化技术来降低Hessian矩阵的最大特征值，确保收敛到平坦最小值。

Result: 将提出的平坦性约束优化应用于现有联邦学习技术，获得了显著改进，建立了新的最先进性能。

Conclusion: MAN正则化技术通过约束平坦性有效提高了联邦学习模型的泛化性能，理论上证明了最小化激活范数可以降低Hessian矩阵的最大特征值，确保收敛到平坦最小值。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [83] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 提出了一种考虑标签依赖关系的多标签数据集采样算法，使用多元伯努利分布建模标签分布，通过计算标签组合权重实现目标分布采样，应用于生物医学文献分类数据集以平衡类别分布。


<details>
  <summary>Details</summary>
Motivation: 多标签数据集中标签通常非互斥且频率差异大，传统采样方法难以同时保证稀缺标签的足够样本量、控制采样偏差并考虑标签间的依赖关系，需要一种能综合考虑这些因素的采样方法。

Method: 采用多元伯努利分布作为多标签问题的底层分布模型，利用观测到的标签频率估计分布参数，计算每个标签组合的权重，设计考虑标签依赖关系的加权采样算法，确保采样结果具有目标分布特性。

Result: 将方法应用于Web of Science的64个生物医学主题类别标注的研究文章样本，成功保持了类别频率顺序，减少了最常见和最不常见类别间的频率差异，考虑了类别依赖关系，生成了更平衡的子样本，增强了少数类别的代表性。

Conclusion: 提出的考虑标签依赖关系的加权采样算法能有效处理多标签数据集中的不平衡问题，通过多元伯努利分布建模和权重计算，实现了更平衡的采样结果，特别适用于标签非互斥且频率差异大的场景。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [84] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 本文介绍了细菌生物威胁基准（B3）数据集的试点实施，这是生物威胁基准生成（BBG）框架的第三篇论文，展示了B3数据集如何有效评估大型语言模型的生物安全风险。


<details>
  <summary>Details</summary>
Motivation: 前沿人工智能模型，特别是大型语言模型，可能被用于促进生物恐怖主义或获取生物武器，这引起了政策、学术界和公众的广泛担忧。模型开发者和政策制定者需要量化并减轻这些风险，因此需要开发能够评估特定模型生物安全风险的基准测试。

Method: 本文是生物威胁基准生成（BBG）框架系列论文的第三篇，描述了B3数据集的试点实施。试点包括：1）在样本前沿AI模型上运行基准测试；2）对模型响应进行人工评估；3）从多个维度对结果进行应用风险分析。

Result: 试点研究表明，B3数据集提供了一种可行且细致的方法，能够快速评估大型语言模型带来的生物安全风险。该方法能够识别风险的关键来源，并为优先缓解领域提供指导。

Conclusion: B3数据集为评估AI模型的生物安全风险提供了一个有效的基准测试工具，有助于识别风险来源并指导风险缓解措施的优先级设置，对模型开发者和政策制定者都具有重要价值。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [85] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 提出一种结合fMRI数据和DICOM元数据的Transformer多模态框架，用于解码大脑状态，提高准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法在处理fMRI数据时，未能充分利用DICOM元数据提供的丰富上下文信息，限制了大脑状态解码的准确性和应用潜力。

Method: 采用基于Transformer的架构，整合多模态输入（包括fMRI数据和DICOM元数据），利用注意力机制捕捉复杂的时空模式和上下文关系。

Result: 提出的框架增强了模型准确性、可解释性和鲁棒性，在临床诊断、认知神经科学和个性化医疗等领域具有应用潜力。

Conclusion: 该多模态Transformer框架有效利用了DICOM元数据的上下文信息，提升了fMRI大脑状态解码性能，同时讨论了元数据变异性、计算需求等局限性，并提出了未来优化可扩展性和泛化性的方向。

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [86] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 提出一种针对离线强化学习的全局预算分配攻击策略，通过TD误差敏感性分配扰动预算，在最小扰动下实现显著性能下降并规避检测。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击采用局部均匀扰动，对所有样本无差别处理，效率低下且缺乏隐蔽性，需要更智能的攻击策略。

Method: 基于TD误差影响价值函数收敛的理论洞察，将攻击建模为全局资源分配问题，推导出在L2约束下按TD误差敏感性比例分配扰动幅度的闭式解。

Result: 在D4RL基准测试中显著优于基线策略，仅用最小扰动即可实现高达80%的性能下降，并能规避最先进的统计和频谱防御检测。

Conclusion: 提出的全局预算分配攻击策略比传统均匀扰动更高效和隐蔽，揭示了离线RL系统在对抗性攻击下的新脆弱性。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [87] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: TabPFN-GN将图节点分类问题转化为表格学习问题，通过特征工程提取节点属性、结构特征、位置编码等，在异配图上表现优于GNN


<details>
  <summary>Details</summary>
Motivation: 探索图节点分类能否有效转化为表格学习问题，利用TabPFN在表格数据上的成功经验，避免图特定训练或语言模型依赖

Method: TabPFN-GN方法：将图数据转换为表格特征，提取节点属性、结构属性、位置编码，可选平滑邻域特征，使TabPFN能直接进行节点分类

Result: 在12个基准数据集上，TabPFN-GN在同配图上与GNN竞争，在异配图上始终优于GNN

Conclusion: 原则性特征工程可以弥合表格和图领域之间的差距，为任务特定GNN训练和LLM依赖的图基础模型提供实用替代方案

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [88] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 该研究构建了一个覆盖2004-2024赛季的NBA纵向数据集，并开发了一个LSTM深度学习框架，通过长达9840场比赛（相当于8个完整赛季）的序列长度来预测NBA比赛结果，相比传统机器学习方法取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 预测NBA比赛结果对于教练策略、球迷参与和体育博彩具有重要意义，但现有预测模型存在概念漂移、时间上下文有限和跨赛季不稳定的问题，需要更先进的预测方法。

Method: 构建了覆盖2004-05至2024-25赛季的纵向NBA数据集，提出了基于LSTM的深度学习框架，使用长达9840场比赛（相当于8个完整赛季）的扩展序列长度来捕捉球队动态演变和跨赛季依赖关系，并与逻辑回归、随机森林、MLP和CNN等传统方法进行比较。

Result: LSTM模型在所有指标上表现最佳：准确率72.35%，精确率73.15%，AUC-ROC 76.13%，显著优于其他基线模型。

Conclusion: 长序列时间建模在篮球结果预测中至关重要，新构建的多赛季数据集对于开发稳健、可泛化的NBA预测系统具有重要价值，LSTM框架能够有效捕捉球队长期表现趋势和跨赛季依赖关系。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [89] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文改进了FedProxGrad算法的收敛性分析，提出了DS FedProxGrad框架，在Robbins-Monro步长调度和局部不精确性衰减条件下，证明了算法能渐进收敛到平稳点，消除了方差诱导的噪声下限依赖。


<details>
  <summary>Details</summary>
Motivation: 现有FedProxGrad算法在群体公平联邦学习的非凸复合优化问题中，只能收敛到噪声主导的平稳邻域，且收敛性显式依赖于方差诱导的噪声下限。本文旨在改进这一收敛性分析，消除对噪声下限的依赖。

Method: 提出了DS FedProxGrad（衰减步长FedProxGrad）分析框架，该框架包含不精确的局部近端解和显式公平正则化。采用Robbins-Monro步长调度，并要求局部不精确性满足温和的衰减条件。

Result: 证明了在Robbins-Monro步长调度和局部不精确性衰减条件下，算法满足liminf_{r→∞} E[‖∇F(x^r)‖^2] = 0，即算法渐进收敛到平稳点，收敛速率不依赖于方差诱导的噪声下限。

Conclusion: DS FedProxGrad框架改进了原有FedProxGrad算法的收敛性分析，在更一般的条件下实现了渐进收敛到平稳点，消除了对噪声下限的依赖，为群体公平联邦学习提供了更强大的理论保证。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [90] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 开发了一种基于动态时间规整和迁移学习的框架，用于增材制造零件认证，通过将低成本聚合物的应力-应变行为知识迁移到金属材料，提高性能预测精度。


<details>
  <summary>Details</summary>
Motivation: 增材制造零件认证至关重要，需要准确预测复杂应力-应变行为。传统方法难以处理不同材料间的性能预测，需要开发能够跨材料类型迁移知识的智能框架。

Method: 提出DTW-TL（动态时间规整-迁移学习）框架：1）使用DTW算法选择与目标金属数据集最相关的聚合物源域；2）采用LSTM模型进行知识迁移；3）使用四种聚合物（尼龙、PLA、CF-ABS、树脂）和三种金属（AlSi10Mg、Ti6Al4V、碳钢）验证框架有效性。

Result: DTW-TL框架成功识别聚合物与金属间的最佳匹配，选择单一聚合物数据集作为源域。当三种金属作为目标域时，DTW-TL模型获得最低平均绝对百分比误差12.41%和最高决定系数0.96，优于无迁移学习的普通LSTM模型以及使用四种聚合物预训练的迁移学习模型。

Conclusion: DTW-TL框架能够有效实现增材制造零件认证中的跨材料知识迁移，显著提高金属材料应力-应变行为预测精度，为增材制造零件性能验证提供了创新解决方案。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [91] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE：面向LLM智能体的联邦自进化框架，通过本地进化-全局聚合范式解决隐私约束下异构任务中的梯度冲突问题，提升跨环境知识迁移效果


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂交互任务中广泛部署，但隐私约束限制了集中式优化和跨动态环境的协同进化。联邦学习在静态数据集上有效，但在开放式的智能体自进化中应用不足。标准联邦学习直接应用于异构任务和稀疏轨迹级奖励时会产生严重的梯度冲突，破坏全局优化过程。

Method: 提出Fed-SE框架，采用本地进化-全局聚合范式。本地层面：智能体在筛选的高回报轨迹上进行参数高效微调，实现稳定的梯度更新。全局层面：在低秩子空间内聚合更新，解耦环境特定的动态变化，有效减少客户端间的负迁移。

Result: 在五个异构环境中的实验表明，Fed-SE相比联邦学习基线将平均任务成功率提升了约18%，验证了其在隐私约束部署中实现鲁棒跨环境知识迁移的有效性。

Conclusion: Fed-SE成功解决了隐私约束下LLM智能体自进化中的梯度冲突问题，通过本地稳定优化和全局解耦聚合实现了有效的跨环境知识迁移，为分布式智能体进化提供了可行的联邦学习解决方案。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [92] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: BGPS框架自动生成能最大化图像偏见的提示词，通过LLM生成属性中性提示，结合分类器引导解码过程，发现稳定扩散模型中大量未记录的偏见


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在社会偏见，但现有去偏见方法依赖人工或LLM构建的提示数据集，成本高且可能遗漏未预期的偏见触发提示

Method: 提出Bias-Guided Prompt Search (BGPS)框架：1) LLM生成属性中性提示；2) 基于TTI内部表示的分类器引导LLM解码过程，向放大目标图像属性的提示空间区域搜索

Result: 在Stable Diffusion 1.5和最新去偏见模型上发现大量微妙且未记录的偏见，严重降低公平性指标；发现的提示词可解释且可由普通用户输入，困惑度指标优于现有硬提示优化方法

Conclusion: BGPS揭示了TTI模型的脆弱性，扩展了偏见搜索空间，可作为偏见缓解的新评估工具

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [93] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: LLM生成的表格合成数据存在隐私泄露风险，数字序列容易被记忆并重现，研究者提出LevAtt攻击方法检测此漏洞，并开发了数字扰动防御策略。


<details>
  <summary>Details</summary>
Motivation: LLM在生成高质量表格合成数据方面表现出色，但现有方法（微调小模型或提示大模型）可能泄露隐私，通过记忆训练数据中的数字模式。需要系统分析这种隐私风险。

Method: 提出LevAtt黑盒成员推理攻击方法，仅访问生成的合成数据，针对数字字符串序列进行分析。同时开发了两种防御方法，包括在生成过程中策略性扰动数字的新型采样策略。

Result: 攻击方法在多种模型和数据集上暴露了显著的隐私泄露，在某些情况下甚至成为最先进模型的完美成员分类器。防御方法能够有效抵御攻击，同时保持合成数据的保真度和实用性损失最小。

Conclusion: LLM基础的合成数据生成存在独特的隐私漏洞，需要有效的防御措施。提出的数字扰动策略能够在不显著影响数据质量的情况下保护隐私。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [94] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 该论文提出使用神经常微分方程（NODEs）作为动态框架，从多组学时间序列数据中学习蛋白质组和代谢组之间的复杂相互作用，相比传统机器学习方法在预测精度和推理速度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然高通量多组学数据日益丰富，但将这些数据转化为可操作的预测模型仍然存在瓶颈。需要能够直接从观测数据推断潜在相互作用的高容量、数据驱动的模拟系统，以支持个性化医疗和合成生物学中的下游干预效果预测。

Method: 引入神经常微分方程（NODEs）作为动态框架，应用于工程化大肠杆菌菌株的时间序列数据，建模代谢途径的连续动态过程。该方法能够直接从观测数据学习蛋白质组和代谢组之间的复杂相互作用。

Result: NODE架构在捕捉系统动态方面表现出优越性能：在柠檬烯途径数据集上RMSE改善高达94.38%，在异戊烯醇途径数据集上改善高达97.65%；推理时间加速1000倍，成为可扩展的高保真工具。

Conclusion: 神经常微分方程框架为下一代代谢工程和生物发现提供了可扩展、高保真的工具，能够有效预测复杂生物系统的行为，支持人类健康寿命延长和生物工程发展。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [95] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: 提出DAO-GP模型，一种针对概念漂移的完全自适应、超参数无关、衰减稀疏的在线高斯过程回归方法，能动态检测和适应数据分布变化。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间演化的数据分布（概念漂移），忽视这一问题会显著降低模型预测精度。现有在线高斯过程方法存在缺乏漂移感知、依赖固定超参数、易受数据窥探、缺乏原则性衰减机制和内存效率低等关键限制。

Method: 提出DAO-GP（漂移感知在线高斯过程），这是一种新型完全自适应、超参数无关、衰减稀疏的非线性回归模型。模型内置漂移检测和适应机制，能根据漂移严重程度动态调整模型行为。

Result: 大量实证评估证实DAO-GP在平稳条件、多种漂移类型（突变、增量、渐进）和不同数据特征下的鲁棒性。分析显示其具有动态适应能力、高效内存和衰减管理机制，以及演化诱导点。相比最先进的参数和非参数模型，DAO-GP始终实现优越或竞争性性能。

Conclusion: DAO-GP被确立为在线非线性回归的漂移弹性解决方案，能够有效应对现实世界数据中的概念漂移问题。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [96] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: LEAP提出了一种新的通用图提示调优方法，通过在所有节点添加提示来保持理论基础，同时使用强化学习选择节点和编辑提示以获得更理想的提示效果。


<details>
  <summary>Details</summary>
Motivation: 现有选择性节点图提示调优方法虽然追求更理想的提示，但破坏了通用图提示调优的理论基础。需要一种既能保持理论基础又能获得理想提示的新方法。

Method: 提出LEAP模型：1）在所有节点构建基本通用图提示以保持理论基础；2）使用actor-critic强化学习选择节点并编辑提示以获得更理想的提示效果。

Result: 在多种预训练策略下的图级和节点级任务中，无论是全样本还是少样本场景，LEAP都持续优于微调和其他基于提示的方法。

Conclusion: 通过在所有节点添加提示并采用强化学习进行节点选择和提示编辑，LEAP成功保持了通用图提示调优的理论基础，同时实现了更优的性能表现。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [97] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 提出了一种利用提升模型进行反事实估计的方法，通过拟合双变量Beta分布到预测的提升分数，获得反事实结果的后验分布。


<details>
  <summary>Details</summary>
Motivation: 提升建模只估计干预的因果效应（处理组和对照组的潜在结果差异），而反事实识别旨在恢复这些潜在结果的联合分布（如"如果给客户提供营销优惠，他们是否仍会流失？"）。反事实分布比提升提供更丰富的信息但更难估计，但两种方法可以协同：可以利用提升模型进行反事实估计。

Method: 提出了一种反事实估计器，将双变量Beta分布拟合到预测的提升分数，从而产生反事实结果的后验分布。该方法除了提升建模所需的因果假设外，不需要额外的因果假设。

Result: 模拟实验显示了该方法的有效性，可以应用于电信客户流失等问题，揭示了标准机器学习或单独提升模型无法获得的洞察。

Conclusion: 该方法成功地将提升建模与反事实估计相结合，通过拟合双变量Beta分布到提升分数，实现了对反事实结果的概率估计，为因果推断提供了更丰富的分析工具。

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [98] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: WAAPO框架生成针对AI天气预报模型的对抗性扰动，这些扰动既能有效操纵预测结果，又具有隐蔽性，揭示了AI天气预报系统在面对微小初始条件扰动时的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在天气预报中的日益依赖，评估其对对抗性扰动的脆弱性变得至关重要。当前需要研究如何生成既能有效操纵预测又难以被检测的对抗性扰动，以揭示AI天气预报系统的安全漏洞。

Method: 提出了Weather Adaptive Adversarial Perturbation Optimization (WAAPO)框架，通过引入通道稀疏性、空间局部性和平滑性约束，生成物理上真实且难以察觉的对抗性扰动。使用ERA5数据集和FourCastNet模型进行实验验证。

Result: WAAPO能够生成与预定目标紧密对齐的对抗性轨迹，即使在约束条件下也能有效工作。实验表明，对初始条件的微小扰动可以导致预测天气模式的显著偏差，揭示了AI驱动预报模型的关键脆弱性。

Conclusion: AI天气预报模型对对抗性扰动具有显著脆弱性，需要建立强大的防护措施来保护操作预报系统免受对抗性攻击的利用。WAAPO框架为评估和改进天气预报模型的鲁棒性提供了重要工具。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [99] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 论文提出STD(λ)算法，通过训练函数逼近器关注状态间的相对值而非绝对值，解决TD(λ)在策略优化中可能收敛到次优策略的问题。


<details>
  <summary>Details</summary>
Motivation: TD(λ)算法虽然在某些复杂强化学习问题上取得了经验成功，但对于线性逼近器，它最小化的是每个状态近似值与真实值之间的平方误差。然而从策略角度看，状态值的相对排序误差比状态值的绝对误差更为关键。作者通过简单系统和西洋双陆棋的例子展示了TD(λ)即使从最优策略开始也可能收敛到次优策略的问题。

Method: 提出了改进的STD(λ)算法，在二元决策问题中训练函数逼近器时关注状态的相对值而非绝对值。该方法包括理论分析，证明了两状态系统中STD(λ)的策略单调改进特性，并与Bertsekas的差分训练方法进行了比较。

Result: 在两状态系统和acrobot问题的变体上成功演示了STD(λ)算法的有效性。理论分析证明了两状态系统中STD(λ)具有策略单调改进的特性。

Conclusion: STD(λ)通过关注状态间的相对值而非绝对值，解决了TD(λ)在策略优化中的局限性，在二元决策问题上表现更好，为强化学习中的函数逼近提供了新的改进方向。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [100] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 提出一种文本到IMU运动合成框架，通过使用加速度二阶损失微调预训练扩散模型，生成更真实的IMU数据，提升人类活动识别性能


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动模型生成的IMU数据与真实IMU记录存在差异，需要专门针对IMU传感器特性优化运动生成，以提高合成数据的真实性和下游任务性能

Method: 提出加速度二阶损失(L_acc)来强制生成运动在离散二阶时间差分上的一致性，将L_acc集成到现有扩散模型的训练目标中，微调获得IMU特定的运动先验，结合表面建模和虚拟传感器模拟框架

Result: L_acc损失相对原始模型降低12.7%，高动态活动改进更显著；合成IMU数据在低维嵌入中更接近真实IMU分布；仅使用精炼合成数据的人类活动识别性能比原始扩散模型提升8.7%，比最佳对比模型提升7.6%

Conclusion: 加速度感知的扩散精炼为对齐运动生成和IMU合成提供了有效方法，展示了深度学习管道在将通用文本到运动先验专门化到传感器特定任务方面的灵活性

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [101] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 提出一种基于拓扑数据分析的损失函数，用于无监督自动选择核密度估计的最佳带宽参数


<details>
  <summary>Details</summary>
Motivation: 核密度估计中的带宽选择是一个关键但困难的问题，传统方法需要人工调参。拓扑数据分析能够量化密度估计的拓扑特征，即使在无法可视化高维数据时也能提供数学描述

Method: 使用基于拓扑的损失函数进行无监督学习，自动选择最优带宽。该方法利用拓扑数据分析来量化密度估计的拓扑特征（如连通分量、环、空洞等）

Result: 该方法在不同维度下进行了基准测试，展示了其相对于经典技术的潜力

Conclusion: 基于拓扑的损失函数为核密度估计的带宽选择提供了一种有效的无监督自动化方法，能够处理高维数据并避免人工调参的困难

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [102] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge (OPC) 发布了首个社区开发的聚合物信息学基准数据集，包含10K聚合物和5种性质，通过多任务预测竞赛推动可持续聚合物材料的机器学习发现。


<details>
  <summary>Details</summary>
Motivation: 机器学习在发现可持续聚合物材料方面具有巨大潜力，但进展受到缺乏大规模、高质量、开放可访问的聚合物数据集的限制。

Method: 发布包含10K聚合物和5种性质（热导率、回转半径、密度、自由体积分数、玻璃化转变温度）的基准数据集，组织多任务聚合物性质预测竞赛，参与者在数据量小、标签不平衡、模拟源异质等现实约束下开发模型，使用特征增强、迁移学习、自监督预训练、针对性集成策略等技术。

Result: 竞赛揭示了数据准备、分布偏移和跨组模拟一致性等方面的重要经验教训，为未来大规模聚合物数据集的最佳实践提供指导。生成的模型、分析和发布数据为聚合物科学中的分子AI建立了新基础。

Conclusion: Open Polymer Challenge 通过发布基准数据集和竞赛，为可持续和节能材料的加速开发创造了新基础，有望推动聚合物信息学和材料发现的发展。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>
