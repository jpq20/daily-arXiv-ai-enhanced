<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 29]
- [cs.AI](#cs.AI) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Teaching People LLM's Errors and Getting it Right](https://arxiv.org/abs/2512.21422)
*Nathan Stringham,Fateme Hashemi Chaleshtori,Xinyuan Yan,Zhichao Xu,Bei Wang,Ana Marasović*

Main category: cs.CL

TL;DR: 论文分析为什么教用户LLM失败模式未能减少过度依赖，发现失败模式确实存在但自动发现方法效果不一，提出新评估指标显示教学有效


<details>
  <summary>Details</summary>
Motivation: 用户经常在不该使用大语言模型时过度依赖它们，部分原因是看到LLM能写诗回答复杂问题，就错误地认为它们不会在简单任务上出错。先前研究试图通过聚类实例嵌入识别失败区域并教用户失败模式来缓解过度依赖，但效果不佳，本文旨在探究原因。

Method: 1. 检查失败模式是否存在：在两个数据集上按元标签分组实例，评估LLM预测，定义标准识别规模大且LLM易错的组；2. 测试提示和嵌入方法能否发现已知失败；3. 提出新评估指标衡量用户利用失败模式预测LLM错误的能力，进行用户研究验证。

Result: 1. 确实存在满足标准的元标签组（即失败模式）；2. 提示和嵌入方法发现已知失败的效果不一，这可能是先前研究失败的原因；3. 使用新指标的用户研究显示教学有积极效果，而传统的人机团队准确率指标未能显示效果。

Conclusion: 教用户失败模式可能是缓解过度依赖的可行方法，但成功取决于更好的自动失败发现方法和使用本文提出的评估指标，而不是传统的人机团队准确率。

Abstract: People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.
  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.

</details>


### [2] [Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models](https://arxiv.org/abs/2512.21439)
*Geoffroy Morlat,Marceau Nahon,Augustin Chartouny,Raja Chatila,Ismael T. Freire,Mehdi Khamassi*

Main category: cs.CL

TL;DR: COMETH框架通过结合概率上下文学习、LLM语义抽象和人类道德评估，建模上下文如何影响模糊行为的可接受性，相比端到端LLM提示将人类判断对齐度提高约一倍。


<details>
  <summary>Details</summary>
Motivation: 道德判断不仅取决于行为结果，还受上下文环境影响。现有端到端LLM方法在上下文敏感的道德预测中表现有限，需要更可解释、基于实证的框架来理解上下文如何塑造道德评价。

Method: 1) 构建包含300个场景的实证数据集，涵盖三个核心道德规范；2) 收集101名参与者的三元判断；3) 使用LLM过滤和MiniLM嵌入+K-means进行预处理，生成可复现的核心行为聚类；4) COMETH框架通过基于原则性发散标准的在线聚类从人类判断分布中学习特定行为的道德上下文；5) 泛化模块提取简洁、非评价性的二元上下文特征，在透明似然模型中学习特征权重。

Result: COMETH将人类多数判断对齐度从端到端LLM提示的约30%提高到约60%，平均提高约一倍。同时能够揭示驱动预测的上下文特征，提供可解释性。

Conclusion: COMETH提供了一个结合人类判断与模型上下文学习的可复现管道，作为端到端LLM的可解释替代方案，用于上下文敏感的道德预测和解释，贡献包括实证数据集、混合方法和可解释预测框架。

Abstract: Moral actions are judged not only by their outcomes but by the context in which they occur. We present COMETH (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that integrates a probabilistic context learner with LLM-based semantic abstraction and human moral evaluations to model how context shapes the acceptability of ambiguous actions. We curate an empirically grounded dataset of 300 scenarios across six core actions (violating Do not kill, Do not deceive, and Do not break the law) and collect ternary judgments (Blame/Neutral/Support) from N=101 participants. A preprocessing pipeline standardizes actions via an LLM filter and MiniLM embeddings with K-means, producing robust, reproducible core-action clusters. COMETH then learns action-specific moral contexts by clustering scenarios online from human judgment distributions using principled divergence criteria. To generalize and explain predictions, a Generalization module extracts concise, non-evaluative binary contextual features and learns feature weights in a transparent likelihood-based model. Empirically, COMETH roughly doubles alignment with majority human judgments relative to end-to-end LLM prompting (approx. 60% vs. approx. 30% on average), while revealing which contextual features drive its predictions. The contributions are: (i) an empirically grounded moral-context dataset, (ii) a reproducible pipeline combining human judgments with model-based context learning and LLM semantics, and (iii) an interpretable alternative to end-to-end LLMs for context-sensitive moral prediction and explanation.

</details>


### [3] [Oogiri-Master: Benchmarking Humor Understanding via Oogiri](https://arxiv.org/abs/2512.21494)
*Soichiro Murakami,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura*

Main category: cs.CL

TL;DR: 该研究通过日本创意回应游戏Oogiri构建了Oogiri-Master基准和Oogiri-Corpus数据集，用于系统评估大语言模型的幽默理解能力，分析了与幽默相关的语言因素，并展示了最先进模型接近人类表现的结果。


<details>
  <summary>Details</summary>
Motivation: 幽默是评估大语言模型类人创造性思维的重要测试场。现有研究在幽默评估方面存在局限：数据集包含的候选回应少、评分时暴露流行度信号、缺乏客观可比较的幽默度指标。因此需要建立更严谨的评估框架来理解什么让回应对人类来说有趣。

Method: 1. 引入Oogiri-Master基准和Oogiri-Corpus数据集，每个提示配约100个多样化候选回应，由约100名人类评委独立评分，减少流行度偏差；2. 定量分析幽默相关的语言因素（文本长度、歧义性、不一致性解决等）；3. 推导预测人类判断的客观指标；4. 在Oogiri-Master上评估多种大语言模型和人类基线，测试洞察增强提示的效果。

Result: 1. 建立了减少偏见、支持稳健聚合的幽默评估数据集；2. 识别了与幽默度相关的具体语言特征；3. 开发了预测人类幽默判断的客观指标；4. 最先进的大语言模型在幽默理解任务上接近人类表现；5. 洞察增强提示能进一步提升模型性能。

Conclusion: 该研究为评估和推进大语言模型的幽默理解能力提供了原则性基础，通过严谨的数据集设计和分析方法，使幽默评估更加客观可靠，展示了当前模型在幽默理解方面的进展和潜力。

Abstract: Humor is a salient testbed for human-like creative thinking in large language models (LLMs). We study humor using the Japanese creative response game Oogiri, in which participants produce witty responses to a given prompt, and ask the following research question: What makes such responses funny to humans? Previous work has offered only limited reliable means to answer this question. Existing datasets contain few candidate responses per prompt, expose popularity signals during ratings, and lack objective and comparable metrics for funniness. Thus, we introduce Oogiri-Master and Oogiri-Corpus, which are a benchmark and dataset designed to enable rigorous evaluation of humor understanding in LLMs. Each prompt is paired with approximately 100 diverse candidate responses, and funniness is rated independently by approximately 100 human judges without access to others' ratings, reducing popularity bias and enabling robust aggregation. Using Oogiri-Corpus, we conduct a quantitative analysis of the linguistic factors associated with funniness, such as text length, ambiguity, and incongruity resolution, and derive objective metrics for predicting human judgments. Subsequently, we benchmark a range of LLMs and human baselines in Oogiri-Master, demonstrating that state-of-the-art models approach human performance and that insight-augmented prompting improves the model performance. Our results provide a principled basis for evaluating and advancing humor understanding in LLMs.

</details>


### [4] [A Unified Definition of Hallucination, Or: It's the World Model, Stupid](https://arxiv.org/abs/2512.21577)
*Emmy Liu,Varun Gangal,Chelsea Zou,Xiaoqi Huang,Michael Yu,Alex Chang,Zhuofu Tao,Sachin Kumar,Steven Y. Feng*

Main category: cs.CL

TL;DR: 该论文提出了一个统一的幻觉定义：幻觉是语言模型内部世界建模的不准确性，这种不准确性对用户可见（如陈述与知识库矛盾的事实）。论文通过变化参考世界模型和知识冲突策略，统一了文献中的不同幻觉定义。


<details>
  <summary>Details</summary>
Motivation: 尽管神经语言模型出现以来已有多次尝试解决幻觉问题，但即使在当前的前沿大语言模型中，幻觉仍然存在。论文旨在理解为什么幻觉问题持续存在，并为该领域提供一个统一的定义框架，以澄清不同研究中对幻觉的不同理解。

Method: 论文从历史角度梳理了文献中使用的幻觉定义，并将它们整合为一个统一定义。核心观点是：幻觉本质上是（内部）世界建模的不准确性，这种不准确性对用户可见。通过变化参考世界模型（如知识库、上下文）和知识冲突策略，论文展示了现有文献中的不同定义如何对应统一框架的不同方面。

Result: 提出了一个统一的幻觉定义框架，该框架要求评估明确其假设的"世界"或真相来源，澄清什么应该和不应该被称为幻觉（与规划或奖励/激励相关错误区分），并为比较基准和缓解技术提供了共同语言。

Conclusion: 基于这个统一定义，论文规划了一系列基准测试，其中幻觉被定义为与不同环境中合成但完全指定的世界模型的不匹配。这些基准可以利用此类设置来压力测试和改进语言模型的世界建模组件，为解决幻觉问题提供系统性方法。

Abstract: Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? We walk through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of our definition. At its core, we argue that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature.
  We argue that this unified view is useful because it forces evaluations to make clear their assumed "world" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, we outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.

</details>


### [5] [Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM](https://arxiv.org/abs/2512.21580)
*Alexander Podolskiy,Semen Molokov,Timofey Gerasin,Maksim Titov,Alexey Rukhovich,Artem Khrapov,Kirill Morozov,Evgeny Tetin,Constantine Korikov,Pavel Efimov,Polina Lazukova,Yuliya Skripkar,Nikita Okhotnikov,Irina Piontkovskaya,Meng Xiaojun,Zou Xueyi,Zhang Zhenhe*

Main category: cs.CL

TL;DR: Gamayun是一个15亿参数的多语言模型，采用两阶段预训练策略，在资源受限环境下表现出色，超越多个更大训练预算的模型


<details>
  <summary>Details</summary>
Motivation: 解决小型非英语中心语言模型研究的不足，为资源受限环境提供高效部署方案

Method: 两阶段预训练策略：1) 平衡多语言训练实现跨语言对齐；2) 高质量英语丰富化将性能增益转移到其他语言

Result: 在12种语言上表现优异，特别在俄语上达到SOTA；超越LLaMA3.2-1B和Qwen2.5-1.5B，与Qwen3在非STEM任务上相当

Conclusion: Gamayun证明了通过精心设计的训练策略，小型多语言模型可以在有限资源下实现卓越性能

Abstract: We present Gamayun, a 1.5B-parameter multilingual language model trained entirely from scratch on 2.5T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on small non-English-centric LLMs by adopting a novel two-stage pre-training strategy: balanced multilingual training for cross-lingual alignment, followed by high-quality English enrichment to transfer performance gains across languages. Our model supports 12 languages, with special focus on Russian. Despite a significantly smaller training budget than comparable models, Gamayun outperforms LLaMA3.2-1B (9T tokens) on all considered benchmarks, and surpasses Qwen2.5-1.5B (18T tokens) on a wide range of English and multilingual tasks. It matches or exceeds Qwen3 (36T tokens) on most tasks outside advanced STEM, achieving state-of-the-art results in Russian, including the MERA benchmark, among the models of comparable size (1-2B parameters).

</details>


### [6] [Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations](https://arxiv.org/abs/2512.21635)
*Chengxu Yang,Jingling Yuan,Siqi Cai,Jiawei Jiang,Chuang Hu*

Main category: cs.CL

TL;DR: HIC-Bench是一个评估LLM幻觉的新框架，将幻觉分为智能幻觉(IH)和缺陷幻觉(DH)，通过多维度指标矩阵评估它们在科学创新任务中的创造性价值。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法主要关注事实一致性，难以处理异质科学任务和平衡创造力与准确性。传统观点将幻觉视为需要最小化的错误，但某些幻觉可能编码创造性或有认识论价值的内容，这一维度在当前文献中缺乏量化研究。

Method: 提出HIC-Bench框架：1) 结构化IH/DH评估，使用多维指标矩阵整合托兰斯创造性思维测试指标(原创性、可行性、价值)和幻觉特定维度(科学合理性、事实偏差)；2) 跨领域适用性，涵盖十个科学领域的开放式创新任务；3) 动态提示优化，利用动态幻觉提示(DHP)引导模型产生创造性且可靠的输出。评估过程使用多个LLM评委平均分数以减少偏差，人工标注者验证IH/DH分类。

Result: 实验结果显示IH和DH之间存在非线性关系，表明创造性和正确性可以共同优化。这些发现将IH定位为创造力的催化剂，揭示了LLM幻觉驱动科学创新的能力。

Conclusion: HIC-Bench为推进LLM幻觉创造性智能研究提供了有价值的平台，表明某些幻觉可以编码创造性内容并推动科学创新，而非仅仅是需要消除的错误。

Abstract: Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific innovation.Additionally, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.

</details>


### [7] [Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech](https://arxiv.org/abs/2512.21706)
*Shuchang Pan,Siddharth Banerjee,Dhruv Hebbar,Siddhant Patel,Akshaj Gupta,Kan Jen Cheng,Hanjo Kim,Zeyi Austin Li,Martin Q. Ma,Tingle Li,Gopala Anumanchipalli,Jiachen Lian*

Main category: cs.CL

TL;DR: 论文提出了一种基于因果推理和图思维（GoT）的对话行为推理框架，通过层次化标注方案预测交流意图和言语行为，在合成和真实双工对话中实现鲁棒的行为检测和可解释的推理链。


<details>
  <summary>Details</summary>
Motivation: 人类对话由隐含的思维链组织，表现为定时的言语行为。捕捉这种因果路径对于构建自然的全双工交互系统至关重要。当前系统缺乏对这种因果关系的建模能力。

Method: 提出图思维（GoT）框架，将对话过程建模为因果推理。采用层次化标注方案预测高层次交流意图和低层次言语行为，学习其因果和时间依赖关系。使用混合语料库（可控模拟+人工标注+真实对话）训练系统，将流式预测结构化为演化图，使多模态transformer能够预测下一个言语行为、生成决策理由并动态优化推理。

Result: 在合成和真实双工对话上的实验表明，该框架实现了鲁棒的行为检测，产生了可解释的推理链，并为全双工口语对话系统中的对话推理基准测试奠定了基础。

Conclusion: GoT框架通过建模对话行为的因果推理，为构建更自然、可解释的全双工交互系统提供了有效方法，在行为检测和推理可解释性方面表现出色。

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [8] [MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles](https://arxiv.org/abs/2512.21708)
*Jing Han,Binwei Yan,Tianyu Guo,Zheyuan Bai,Mengyu Zheng,Hanting Chen,Ying Nie*

Main category: cs.CL

TL;DR: 本文提出了一种用于智能体任务的参数高效微调方法，通过角色分解、混合角色框架和多角色数据生成，显著提升了智能体性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在智能体任务上的微调取得了进展，但参数高效微调方法在智能体领域仍未被充分探索。现有方法通常对整个模型进行微调，缺乏针对智能体特定能力的专门化设计。

Method: 1) 将智能体能力分解为三个角色：推理者（理解查询、决定下一步角色）、执行者（识别调用函数和参数）、总结者（提炼对话信息给用户）；2) 提出混合角色框架，包含三个专门的LoRA组，每个组负责一个角色；3) 开发基于公开数据集的多角色数据生成管道，包含角色特定内容补全和可靠性验证。

Result: 在多种大语言模型和智能体基准测试上进行了广泛实验和消融研究，证明了所提方法的有效性。该方法能够显著提升智能体任务性能，同时保持参数效率。

Conclusion: 本文提出的参数高效微调方法通过角色分解和专门化LoRA设计，为智能体任务提供了一种有效的解决方案，在保持参数效率的同时显著提升了性能。

Abstract: Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.

</details>


### [9] [Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers](https://arxiv.org/abs/2512.21709)
*Md. Rakibul Islam,Most. Sharmin Sultana Samu,Md. Zahid Hossain,Farhad Uz Zaman,Md. Kamrozzaman Bhuiyan*

Main category: cs.CL

TL;DR: 本文研究了孟加拉语中AI生成文本的检测问题，评估了五种基于Transformer的模型，发现零样本评估效果不佳（约50%准确率），但经过微调后XLM-RoBERTa、mDeBERTa和MultilingualBERT能达到约91%的准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成类似人类文本的能力引发了关于虚假信息和内容操纵的担忧。虽然已有研究涉及多种语言的检测，但孟加拉语在这方面仍未被充分探索。孟加拉语丰富的词汇和复杂结构使得区分人类写作和AI生成文本特别具有挑战性。

Method: 本研究调查了五种基于Transformer的模型：XLMRoBERTa-Large、mDeBERTaV3-Base、BanglaBERT-Base、IndicBERT-Base和MultilingualBERT-Base。首先进行零样本评估，然后对模型进行任务特定的微调，以检测孟加拉语中的AI生成文本。

Result: 零样本评估显示所有模型表现接近随机水平（约50%准确率）。微调后性能显著提升：XLM-RoBERTa、mDeBERTa和MultilingualBERT在准确率和F1分数上都达到约91%。IndicBERT表现相对较弱，表明其在该任务上的微调效果有限。

Conclusion: 这项工作推进了孟加拉语中AI生成文本的检测研究，为构建强大的系统来对抗AI生成内容奠定了基础。研究强调了任务特定微调的重要性，并展示了某些模型在孟加拉语文本检测中的有效性。

Abstract: Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.

</details>


### [10] [Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought](https://arxiv.org/abs/2512.21711)
*Yuyi Zhang,Boyu Tang,Tianjie Ju,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 研究发现COCONUT（连续思维链）中的潜在标记并非真实推理编码，而是作为不可解释的占位符，主要依赖数据集伪影而非真正推理过程


<details>
  <summary>Details</summary>
Motivation: 尽管潜在标记在增强大语言模型推理方面受到关注，但其内部机制尚不明确。本文从可靠性角度研究该问题，揭示潜在标记作为不可解释占位符而非忠实推理编码的基本弱点

Method: 采用两种互补方法：1）引导实验扰动特定标记子集（COCONUT和显式CoT），比较敏感性；2）捷径实验在偏置和分布外设置下评估模型，分析对数据集伪影的依赖程度

Result: 在MMLU和HotpotQA上的结果显示：COCONUT标记对引导扰动不敏感，缺乏推理关键信息；COCONUT持续利用数据集伪影，在没有真正推理的情况下夸大基准性能

Conclusion: COCONUT应被重新定位为伪推理机制：它生成看似合理的推理轨迹，掩盖了对捷径的依赖，而非忠实表示推理过程。这揭示了潜在标记作为推理增强方法的根本局限性

Abstract: Latent tokens are gaining attention for enhancing reasoning in large language models (LLMs), yet their internal mechanisms remain unclear. This paper examines the problem from a reliability perspective, uncovering fundamental weaknesses: latent tokens function as uninterpretable placeholders rather than encoding faithful reasoning. While resistant to perturbation, they promote shortcut usage over genuine reasoning. We focus on Chain-of-Continuous-Thought (COCONUT), which claims better efficiency and stability than explicit Chain-of-Thought (CoT) while maintaining performance. We investigate this through two complementary approaches. First, steering experiments perturb specific token subsets, namely COCONUT and explicit CoT. Unlike CoT tokens, COCONUT tokens show minimal sensitivity to steering and lack reasoning-critical information. Second, shortcut experiments evaluate models under biased and out-of-distribution settings. Results on MMLU and HotpotQA demonstrate that COCONUT consistently exploits dataset artifacts, inflating benchmark performance without true reasoning. These findings reposition COCONUT as a pseudo-reasoning mechanism: it generates plausible traces that conceal shortcut dependence rather than faithfully representing reasoning processes.

</details>


### [11] [CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation](https://arxiv.org/abs/2512.21715)
*Rui Ke,Jiahui Xu,Shenghao Yang,Kuang Wang,Feng Jiang,Haizhou Li*

Main category: cs.CL

TL;DR: CATCH是一个用于对话系统主题检测的统一框架，通过上下文感知主题表示、偏好引导主题聚类和分层主题生成机制，解决了现有方法在稀疏短文本表示和用户偏好建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 主题检测是用户中心对话系统的基础任务，但现有方法存在两个主要问题：1）难以从稀疏、简短的对话语句中提取准确的主题表示；2）无法跨对话捕捉用户层面的主题偏好。这些问题导致主题检测在跨对话一致性和个性化对齐方面表现不佳。

Method: CATCH框架包含三个核心组件：1）上下文感知主题表示，利用周围主题片段丰富语句级语义；2）偏好引导主题聚类，联合建模语义邻近性和个性化反馈以实现跨对话主题对齐；3）分层主题生成机制，抑制噪声并产生鲁棒、连贯的主题标签。

Result: 在多领域客户对话基准（DSTC-12）上的实验表明，CATCH框架在主题聚类和主题生成质量方面均表现出有效性，特别是在使用8B参数的大语言模型时效果显著。

Conclusion: CATCH通过整合上下文增强、个性化偏好建模和分层生成机制，有效解决了主题检测中的关键挑战，为无预定义模式下的用户对话主题识别提供了统一且有效的解决方案。

Abstract: Theme detection is a fundamental task in user-centric dialogue systems, aiming to identify the latent topic of each utterance without relying on predefined schemas. Unlike intent induction, which operates within fixed label spaces, theme detection requires cross-dialogue consistency and alignment with personalized user preferences, posing significant challenges. Existing methods often struggle with sparse, short utterances for accurate topic representation and fail to capture user-level thematic preferences across dialogues. To address these challenges, we propose CATCH (Controllable Theme Detection with Contextualized Clustering and Hierarchical Generation), a unified framework that integrates three core components: (1) context-aware topic representation, which enriches utterance-level semantics using surrounding topic segments; (2) preference-guided topic clustering, which jointly models semantic proximity and personalized feedback to align themes across dialogue; and (3) a hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels. Experiments on a multi-domain customer dialogue benchmark (DSTC-12) demonstrate the effectiveness of CATCH with 8B LLM in both theme clustering and topic generation quality.

</details>


### [12] [Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation](https://arxiv.org/abs/2512.21787)
*Abdullah Alabdullah,Lifeng Han,Chenghua Lin*

Main category: cs.CL

TL;DR: Ara-HOPE是一个针对阿拉伯方言到现代标准阿拉伯语翻译评估的人类中心化后编辑评估框架，包含错误分类和标注协议，能有效评估不同MT系统的性能差异。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言到现代标准阿拉伯语的翻译在机器翻译中具有挑战性，现有自动评估指标和通用人工评估框架难以捕捉方言特定的翻译错误，阻碍了翻译评估的进展。

Method: 提出了Ara-HOPE框架，包括五类错误分类法和决策树标注协议，用于系统性地评估阿拉伯方言到现代标准阿拉伯语的翻译质量。

Result: 通过对三个MT系统（阿拉伯中心的Jais、通用GPT-3.5和基线NLLB-200）的比较评估，Ara-HOPE有效揭示了系统间的性能差异，显示方言特定术语和语义保留是最持久的挑战。

Conclusion: Ara-HOPE为评估阿拉伯方言机器翻译质量建立了新框架，并为改进方言感知的MT系统提供了可操作的指导。

Abstract: Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation is a challenging task in Machine Translation (MT) due to significant lexical, syntactic, and semantic divergences between Arabic dialects and MSA. Existing automatic evaluation metrics and general-purpose human evaluation frameworks struggle to capture dialect-specific MT errors, hindering progress in translation assessment. This paper introduces Ara-HOPE, a human-centric post-editing evaluation framework designed to systematically address these challenges. The framework includes a five-category error taxonomy and a decision-tree annotation protocol. Through comparative evaluation of three MT systems (Arabic-centric Jais, general-purpose GPT-3.5, and baseline NLLB-200), Ara-HOPE effectively highlights systematic performance differences between these systems. The results show that dialect-specific terminology and semantic preservation remain the most persistent challenges in DA-MSA translation. Ara-HOPE establishes a new framework for evaluating Dialectal Arabic MT quality and provides actionable guidance for improving dialect-aware MT systems.

</details>


### [13] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
*Dubai Li,Nan Jiang,Kangping Huang,Ruiqi Tu,Shuyu Ouyang,Huayu Yu,Lin Qiao,Chen Yu,Tianshu Zhou,Danyang Tong,Qian Wang,Mengtao Li,Xiaofeng Zeng,Yu Tian,Xinping Tian,Jingsong Li*

Main category: cs.CL

TL;DR: Quicker是一个基于大语言模型的临床决策支持系统，能够自动化证据合成并生成临床推荐，显著缩短临床决策时间。


<details>
  <summary>Details</summary>
Motivation: 临床实践中整合循证医学证据面临工作量大、流程复杂、时间紧迫等挑战，需要自动化工具来支持更高效准确的临床决策。

Method: 开发了Quicker系统，采用大语言模型驱动的全自动化流程，覆盖从问题提出到临床推荐的所有阶段，并包含定制化决策工具和交互界面。使用基于三种疾病临床指南开发记录的Q2CRBench-3基准数据集进行评估。

Result: Quicker表现出色：问题分解符合用户偏好，检索敏感度接近人类专家，文献筛选接近全面纳入相关研究。系统生成的推荐比临床医生更全面、逻辑更连贯。单评审员与Quicker协作可将推荐开发时间缩短至20-40分钟。

Conclusion: Quicker系统具有帮助医生做出更快、更可靠的循证临床决策的潜力，为临床实践提供了有效的自动化支持工具。

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides healthcare professionals with reliable scientific foundations for informed decision-making. Integrating clinical evidence into real-time practice is challenging due to the enormous workload, complex professional processes, and time constraints. This highlights the need for tools that automate evidence synthesis to support more efficient and accurate decision making in clinical settings. This study introduces Quicker, an evidence-based clinical decision support system powered by large language models (LLMs), designed to automate evidence synthesis and generate clinical recommendations modeled after standard clinical guideline development processes. Quicker implements a fully automated chain that covers all phases, from questions to clinical recommendations, and further enables customized decision-making through integrated tools and interactive user interfaces. To evaluate Quicker's capabilities, we developed the Q2CRBench-3 benchmark dataset, based on clinical guideline development records for three different diseases. Experimental results highlighted Quicker's strong performance, with fine-grained question decomposition tailored to user preferences, retrieval sensitivities comparable to human experts, and literature screening performance approaching comprehensive inclusion of relevant studies. In addition, Quicker-assisted evidence assessment effectively supported human reviewers, while Quicker's recommendations were more comprehensive and logically coherent than those of clinicians. In system-level testing, collaboration between a single reviewer and Quicker reduced the time required for recommendation development to 20-40 minutes. In general, our findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions.

</details>


### [14] [On The Conceptualization and Societal Impact of Cross-Cultural Bias](https://arxiv.org/abs/2512.21809)
*Vitthal Bhandari*

Main category: cs.CL

TL;DR: 本文分析了2025年发表的20篇关于NLP文化偏见的论文，提出了一套观察框架，旨在帮助研究者具体化偏见概念并有效评估其危害，倡导对具有跨文化偏见的语言技术进行更稳健的社会影响评估。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够基于文化背景生成响应，但它们并不完美且倾向于跨文化泛化。当前研究在评估语言技术文化偏见时，往往不接触实际使用该技术的利益相关者，这回避了研究者试图解决的根本问题。本文旨在推动对具有跨文化偏见的语言技术进行更稳健的社会影响评估。

Method: 受arXiv:2005.14050v2工作的启发，作者分析了2025年发表的20篇关于识别和评估自然语言处理中文化偏见的文献，从中提炼出一系列观察结果，为未来NLP研究者提供具体化偏见概念和有效评估其危害的框架。

Result: 通过对20篇论文的系统分析，作者提出了一套观察框架，使研究者能够更具体地概念化偏见，并更有效地评估其危害。这些观察结果为NLP领域提供了评估文化偏见的新视角和方法论基础。

Conclusion: 本文呼吁NLP研究者需要更直接地接触实际使用语言技术的利益相关者，避免回避文化偏见评估的根本问题。提出的观察框架旨在帮助研究者具体化偏见概念并有效评估其危害，最终推动对具有跨文化偏见的语言技术进行更稳健的社会影响评估。

Abstract: Research has shown that while large language models (LLMs) can generate their responses based on cultural context, they are not perfect and tend to generalize across cultures. However, when evaluating the cultural bias of a language technology on any dataset, researchers may choose not to engage with stakeholders actually using that technology in real life, which evades the very fundamental problem they set out to address.
  Inspired by the work done by arXiv:2005.14050v2, I set out to analyse recent literature about identifying and evaluating cultural bias in Natural Language Processing (NLP). I picked out 20 papers published in 2025 about cultural bias and came up with a set of observations to allow NLP researchers in the future to conceptualize bias concretely and evaluate its harms effectively. My aim is to advocate for a robust assessment of the societal impact of language technologies exhibiting cross-cultural bias.

</details>


### [15] [Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments](https://arxiv.org/abs/2512.21817)
*Hong Su*

Main category: cs.CL

TL;DR: DeMe框架通过方法装饰技术，使LLM能够根据隐藏目标、学习经验和环境反馈动态调整任务执行方法，提升IoT系统在未知环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有智能IoT系统依赖LLM生成任务执行方法，但缺乏面对新情况时系统性地生成新方法的能力，且通常依赖固定的设备特定逻辑，无法适应变化的环境条件。

Method: 提出Method Decoration (DeMe)框架，通过从隐藏目标、积累的学习方法和环境反馈中提取显式装饰来修改LLM的方法生成路径。装饰不是硬编码的，而是从通用行为原则、经验和观察到的环境差异中提取。支持预装饰、后装饰、中间步骤修改和步骤插入四种装饰方式。

Result: 实验结果表明，方法装饰使IoT设备在面对未知或故障操作条件时能够推导出更合适的方法。

Conclusion: DeMe框架通过动态装饰LLM的方法生成路径，能够产生上下文感知、安全对齐和环境自适应的方法，显著提升了IoT系统在动态环境中的适应能力。

Abstract: Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.

</details>


### [16] [AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts](https://arxiv.org/abs/2512.21842)
*Baorong Huang,Ali Asiri*

Main category: cs.CL

TL;DR: 提出AlignAR生成式句子对齐方法和新的阿拉伯语-英语数据集，包含复杂法律和文学文本，评估显示传统对齐方法在复杂数据上表现有限，而LLM方法更稳健


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语-英语平行语料库稀缺，现有数据集主要为简单的一对一映射，缺乏复杂文本的对齐资源，限制了机器翻译研究和翻译教学的发展

Method: 提出AlignAR生成式句子对齐方法，构建包含复杂法律和文学文本的阿拉伯语-英语数据集，创建"简单"和"困难"子集进行评估，对比传统对齐方法和LLM方法

Result: 简单数据集缺乏区分不同对齐方法的能力，传统方法在困难子集上表现有限，而LLM方法展现出更强的稳健性，总体F1分数达到85.5%，比先前方法提高9%

Conclusion: 需要更复杂的评估数据集来充分评估句子对齐方法，LLM方法在处理复杂文本对齐方面具有优势，开源数据集和代码促进相关研究发展

Abstract: High-quality parallel corpora are essential for Machine Translation (MT) research and translation teaching. However, Arabic-English resources remain scarce and existing datasets mainly consist of simple one-to-one mappings. In this paper, we present AlignAR, a generative sentence alignment method, and a new Arabic-English dataset comprising complex legal and literary texts. Our evaluation demonstrates that "Easy" datasets lack the discriminatory power to fully assess alignment methods. By reducing one-to-one mappings in our "Hard" subset, we exposed the limitations of traditional alignment methods. In contrast, LLM-based approaches demonstrated superior robustness, achieving an overall F1-score of 85.5%, a 9% improvement over previous methods. Our datasets and codes are open-sourced at https://github.com/XXX.

</details>


### [17] [HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs](https://arxiv.org/abs/2512.21849)
*Jiaxin Liu,Peiyi Tu,Wenyu Chen,Yihong Zhuang,Xinxia Ling,Anji Zhou,Chenxi Wang,Zhuo Han,Zhengkai Yang,Junbo Zhao,Zenan Huang,Yuanyuan Wang*

Main category: cs.CL

TL;DR: HeartBench是一个评估中文大语言模型在情感、文化和伦理维度上类人智能的框架，基于真实心理咨询场景开发，包含5个主要维度和15个次级能力。评估显示当前领先模型仅能达到专家定义理想分数的60%，在复杂情感和伦理场景中表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在认知和推理基准测试中表现出色，但在类人智能方面存在明显不足，特别是在处理复杂的社会、情感和伦理细微差别方面。在中文语言文化背景下，缺乏专门的评估框架和高质量的社会情感数据进一步阻碍了进展。

Method: 基于真实心理咨询场景，与临床专家合作开发了理论驱动的分类法，包含5个主要维度和15个次级能力。采用案例特定、基于评分标准的方法，通过"评分前推理"评估协议将抽象的人类特征转化为可测量的标准。

Result: 评估了13个最先进的大语言模型，发现存在显著性能上限：即使领先模型也只能达到专家定义理想分数的60%。通过难度分层的"困难集"分析显示，在涉及微妙情感潜台词和复杂伦理权衡的场景中，模型性能显著下降。

Conclusion: HeartBench为类人AI评估建立了标准化指标，并为构建高质量、人类对齐的训练数据提供了方法论蓝图，填补了中文语言文化背景下大语言模型在情感、文化和伦理维度评估的空白。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.

</details>


### [18] [TimeBill: Time-Budgeted Inference for Large Language Models](https://arxiv.org/abs/2512.21859)
*Qi Fan,An Zou,Yehan Ma*

Main category: cs.CL

TL;DR: TimeBill是一个面向大语言模型的时间预算推理框架，通过自适应调整KV缓存淘汰率来平衡推理效率与响应性能，满足时间关键系统的实时性要求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时间关键系统（如机器人、自动驾驶、工业自动化）中部署时，需要在给定时间预算内生成准确响应。现有基于固定KV缓存淘汰率的推理方法难以适应不同任务的时间预算变化，可能导致推理不完整或性能下降。

Method: 提出细粒度响应长度预测器和执行时间估计器来准确预测LLM端到端执行时间；开发时间预算高效推理方法，根据执行时间预测和给定时间预算自适应调整KV缓存淘汰率。

Result: 通过大量实验证明TimeBill在各种超时策略下提高任务完成率并保持响应性能的优势。

Conclusion: TimeBill框架有效解决了LLM在时间关键系统中推理时间预算管理问题，通过自适应KV缓存淘汰机制实现了推理效率与响应性能的良好平衡。

Abstract: Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.

</details>


### [19] [Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?](https://arxiv.org/abs/2512.21871)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Hengyu An,Chunyi Zhou,Jun Wang,Boyu Xu,Yuyuan Li,Tianyu Du,Shouling Ji*

Main category: cs.CL

TL;DR: 该研究评估了大型视觉语言模型在处理受版权保护内容时的合规性问题，发现即使最先进的模型也存在显著缺陷，并提出了工具增强的防御框架来降低侵权风险。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态推理任务中取得了显著进展，但其广泛可访问性引发了版权侵权的关键担忧。模型在处理用户输入、检索文档等包含版权内容时能否准确识别并遵守版权法规，避免法律和伦理后果，是本研究的核心动机。

Method: 研究引入了一个包含50,000个多模态查询-内容对的大规模基准数据集，涵盖书籍摘录、新闻文章、音乐歌词和代码文档等版权内容，并包含有版权声明和无版权声明两种场景。针对有版权声明的情况，覆盖了四种类型的版权声明。此外，提出了一个新颖的工具增强防御框架来确保版权合规。

Result: 评估显示，即使是最先进的闭源大型视觉语言模型在识别和尊重版权内容方面也存在显著缺陷，即使在呈现版权声明的情况下也是如此。提出的工具增强防御框架在所有场景中都有效降低了侵权风险。

Conclusion: 研究结果强调了开发具有版权意识的大型视觉语言模型的重要性，以确保对版权内容的负责任和合法使用。模型在处理版权材料时需要更强的合规性机制。

Abstract: Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.

</details>


### [20] [CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics](https://arxiv.org/abs/2512.21877)
*Vaibhav Devraj,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: CricBench是一个针对板球数据分析的Text-to-SQL基准测试，包含英语和印地语查询，评估发现专业领域性能与通用基准存在显著差距，且印地语查询有时表现优于英语。


<details>
  <summary>Details</summary>
Motivation: 板球作为全球第二大运动拥有超过25亿粉丝，但现有LLM在处理板球领域特定需求、复杂模式变化和多语言要求方面能力不足，需要专门的基准测试来评估LLM在专业体育分析领域的表现。

Method: 与板球和SQL领域专家合作手动编写复杂查询构建"黄金标准"数据集，创建包含英语和印地语的双语基准测试，使用严格评估协议评估6个最先进模型（包括GPT-4o、Claude 3.7 Sonnet和开源模型）。

Result: 开源推理模型DeepSeek R1以50.6%的准确率表现最佳，超过Claude 3.7 Sonnet（47.7%）和GPT-4o（33.7%），但相比通用基准（BIRD）在CricBench上出现显著性能下降。有趣的是，印地语混合查询经常达到与英语相当或更高的准确率。

Conclusion: 通用基准的高性能不能保证在专业领域的成功，英语并非专业SQL任务的最佳提示语言，多语言支持对专业领域应用至关重要，需要更多针对特定领域的基准测试来推动LLM在专业应用中的发展。

Abstract: Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a "Gold Standard" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.

</details>


### [21] [Explainable Statute Prediction via Attention-based Model and LLM Prompting](https://arxiv.org/abs/2512.21902)
*Sachin Pawar,Girish Keshav Palshikar,Anindita Sinha Banerjee,Nitin Ramrakhiyani,Basit Ali*

Main category: cs.CL

TL;DR: 本文提出两种自动法规预测方法：AoS（基于注意力机制）和LLMPrompt（基于大语言模型提示），用于根据案件描述预测相关法规条款，并生成人类可理解的解释。


<details>
  <summary>Details</summary>
Motivation: 自动法规预测在法律AI助手和法律问答系统中具有重要应用价值。为了提高用户对法律AI系统的接受度，预测结果需要附带人类可理解的解释。

Method: 提出两种方法：1) AoS：使用句子注意力机制，基于句子变换器在监督学习下预测法规；2) LLMPrompt：使用大语言模型进行零样本预测，探索标准提示和思维链提示技术，同时生成解释。

Result: 在两个流行数据集上比较了两种方法的法规预测性能，并与多个基准方法进行对比。通过自动化反事实评估和人工评估两种方式评估了生成解释的质量。

Conclusion: 两种方法都能有效预测法规并生成可理解的解释，为法律AI系统提供了实用的解决方案，有助于提高用户对法律AI系统的接受度。

Abstract: In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term "statute" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.

</details>


### [22] [Accelerate Speculative Decoding with Sparse Computation in Verification](https://arxiv.org/abs/2512.21911)
*Jikai Wang,Jianchao Tan,Yuxuan Hu,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出稀疏验证框架，通过联合稀疏化注意力、FFN和MoE组件来加速推测解码中的验证阶段，减少计算成本


<details>
  <summary>Details</summary>
Motivation: 推测解码通过并行验证多个草稿token来加速自回归语言模型推理，但验证阶段在处理长上下文输入和MoE模型时成为主要计算瓶颈。现有稀疏化方法主要针对标准token-by-token解码，无法有效解决验证阶段的冗余计算问题。

Method: 1. 系统分析推测解码验证阶段的多维度结构化冗余；2. 提出稀疏验证框架，联合稀疏化注意力、FFN和MoE组件；3. 引入跨草稿token和跨层检索重用策略，进一步减少冗余计算；4. 无需额外训练。

Result: 在摘要、问答和数学推理数据集上的广泛实验表明，该方法在保持稳定接受长度的同时，实现了有利的效率-准确性权衡。

Conclusion: 提出的稀疏验证框架有效解决了推测解码中验证阶段的计算瓶颈问题，通过联合稀疏化多个组件和重用策略，显著减少了计算冗余，为加速大型语言模型推理提供了有效解决方案。

Abstract: Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.

</details>


### [23] [Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs](https://arxiv.org/abs/2512.21933)
*Sachin Pawar,Manoj Apte,Kshitij Jadhav,Girish Keshav Palshikar,Nitin Ramrakhiyani*

Main category: cs.CL

TL;DR: 本文提出LLM分词对自然词的分割会影响模型性能，设计了惩罚函数量化分词质量，并在多个NLP任务上验证了该假设


<details>
  <summary>Details</summary>
Motivation: LLM分词与传统NLP分词不同，由于词汇表限制会将自然词拆分为多个token，作者假设这种分割会负面影响LLM在各种NLP任务上的性能

Method: 提出一组惩罚函数来计算给定文本在特定LLM上的分词惩罚，量化分词"糟糕"程度，并在多个NLP任务和不同LLM上验证假设的统计显著性

Result: 建立了分词惩罚与LLM性能之间的统计显著关系，验证了自然词被分割对模型性能的负面影响

Conclusion: LLM分词过程中自然词的分割确实会降低模型性能，提出的惩罚函数能有效量化这种影响，为改进LLM分词策略提供了依据

Abstract: Tokenization is the first step in training any Large Language Model (LLM), where the text is split into a sequence of tokens as per the model's fixed vocabulary. This tokenization in LLMs is different from the traditional tokenization in NLP where the text is split into a sequence of "natural" words. In LLMs, a natural word may also be broken into multiple tokens due to limited vocabulary size of the LLMs (e.g., Mistral's tokenizer splits "martial" into "mart" and "ial"). In this paper, we hypothesize that such breaking of natural words negatively impacts LLM performance on various NLP tasks. To quantify this effect, we propose a set of penalty functions that compute a tokenization penalty for a given text for a specific LLM, indicating how "bad" the tokenization is. We establish statistical significance of our hypothesis on multiple NLP tasks for a set of different LLMs.

</details>


### [24] [Self-attention vector output similarities reveal how machines pay attention](https://arxiv.org/abs/2512.21956)
*Tal Halevi,Yarden Tzach,Ronit D. Gross,Shalom Rosner,Ido Kanter*

Main category: cs.CL

TL;DR: 该研究提出了一种量化自注意力机制信息处理的新方法，通过分析BERT-12架构发现，深层注意力聚焦于句子分隔符，不同注意力头专注于不同语言特征，且随着层数增加，相似性从长距离转向短距离。


<details>
  <summary>Details</summary>
Motivation: 尽管自注意力机制在自然语言处理中取得了显著成功，但其具体学习机制和量化表征仍是一个开放的研究问题。本研究旨在开发一种方法来量化自注意力机制内部的信息处理过程。

Method: 提出了一种量化自注意力机制信息处理的新方法，分析BERT-12架构中的注意力图，基于自注意力头产生的向量空间构建上下文相似性矩阵，测量两个标记向量之间的标量积。

Result: 研究发现：1）深层注意力聚焦于句子分隔符标记；2）不同注意力头专注于不同语言特征（如标记重复识别、常见标记识别）；3）随着层数增加，相似性从长距离转向短距离，最终偏好同一句子内的强相似性；4）每个头倾向于聚焦文本中的独特标记并围绕其构建相似性对。

Conclusion: 自注意力机制中的不同注意力头展现出专门化的语言特征处理能力，注意力模式从长距离依赖逐渐演变为短距离句子内依赖，为理解自注意力机制的学习过程提供了量化视角。

Abstract: The self-attention mechanism has significantly advanced the field of natural language processing, facilitating the development of advanced language-learning machines. Although its utility is widely acknowledged, the precise mechanisms of self-attention underlying its advanced learning and the quantitative characterization of this learning process remains an open research question. This study introduces a new approach for quantifying information processing within the self-attention mechanism. The analysis conducted on the BERT-12 architecture reveals that, in the final layers, the attention map focuses on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. Based on the vector space emerging from the self-attention heads, a context similarity matrix, measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer. The findings demonstrated that different attention heads within an attention block focused on different linguistic characteristics, such as identifying token repetitions in a given text or recognizing a token of common appearance in the text and its surrounding context. This specialization is also reflected in the distribution of distances between token vectors with high similarity as the architecture progresses. The initial attention layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence. Finally, the behavior of individual heads was analyzed by examining the uniqueness of their most common tokens in their high similarity elements. Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.

</details>


### [25] [Context as a Tool: Context Management for Long-Horizon SWE-Agents](https://arxiv.org/abs/2512.22087)
*Shukai Liu,Jian Yang,Bo Jiang,Yizhi Li,Jinyang Guo,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: CAT是一种新的上下文管理范式，将上下文维护提升为可调用的工具，集成到智能体的决策过程中，通过结构化上下文工作空间和主动压缩历史轨迹来解决软件工程任务中的上下文爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的软件工程智能体在处理仓库级代码库的长时交互任务时，通常依赖追加式上下文维护或被动触发的压缩启发式方法，这会导致上下文爆炸、语义漂移和推理能力下降。

Method: 提出CAT范式，将上下文维护作为可调用工具集成到智能体决策中；设计结构化上下文工作空间（稳定任务语义、压缩长期记忆、高保真短期交互）；开发CAT-GENERATOR监督框架，通过离线数据构建管道注入上下文管理动作；训练上下文感知模型SWE-Compressor。

Result: 在SWE-Bench-Verified基准测试中，SWE-Compressor达到57.6%的解决率，显著优于基于ReAct的智能体和静态压缩基线，同时在有限上下文预算下保持稳定且可扩展的长时推理能力。

Conclusion: CAT范式通过主动、结构化的上下文管理，有效解决了软件工程智能体在长时交互中的上下文爆炸问题，显著提升了智能体在仓库级代码库任务上的性能表现。

Abstract: Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.

</details>


### [26] [Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis](https://arxiv.org/abs/2512.22100)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文介绍了TrGLUE，这是首个针对土耳其语的全面自然语言理解基准测试，填补了土耳其语在GLUE风格评估中的空白，同时提供了SentiTurca情感分析基准和相关工具代码。


<details>
  <summary>Details</summary>
Motivation: 虽然英语有GLUE基准，其他语言如中文、法语、日语也有相应基准，但土耳其语缺乏类似的全面自然语言理解评估基准。这限制了土耳其语NLP模型的有效评估和发展。

Method: 开发了TrGLUE基准，包含多种土耳其语NLU任务。采用半自动化流程构建数据集：结合强大的LLM标注、跨模型一致性检查和人工验证。优先考虑语言自然性，减少直接翻译痕迹，确保可扩展和可复现的工作流程。

Result: 成功创建了TrGLUE基准，这是首个针对土耳其语的全面NLU评估框架。同时提供了SentiTurca情感分析基准，以及用于基于Transformer模型的微调和评估代码。

Conclusion: TrGLUE为土耳其语NLU建立了稳健的评估框架，为研究人员提供了宝贵资源，并为生成高质量半自动化数据集提供了见解，填补了土耳其语在自然语言理解评估方面的空白。

Abstract: Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [27] [MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting](https://arxiv.org/abs/2512.21878)
*Marc S. Montalvo,Hamed Yaghoobian*

Main category: cs.MA

TL;DR: MASFIN是一个基于大语言模型的多智能体金融分析框架，通过整合结构化财务指标和非结构化新闻数据，结合偏差缓解协议，在8周测试中实现了7.33%的累计收益，优于主要市场基准指数。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法容易受到生存偏差影响，而许多AI驱动方法在信号整合、可重复性和计算效率方面存在不足。金融作为高风险领域，需要对异质信号进行透明且可重复的分析。

Method: 开发了MASFIN模块化多智能体框架，整合LLMs与结构化财务指标和非结构化新闻数据，嵌入显式偏差缓解协议。使用GPT-4.1-nano实现可重复性和成本高效推理，生成包含15-30只股票的周度投资组合，并优化短期绩效的配置权重。

Result: 在8周评估中，MASFIN实现了7.33%的累计收益，在8周中有6周的表现优于S&P 500、NASDAQ-100和道琼斯指数基准，尽管波动性较高。

Conclusion: 研究展示了具有偏差意识的生成式AI框架在金融预测中的潜力，并突显了模块化多智能体设计在推进量化金融实用、透明和可重复方法方面的机会。

Abstract: Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [A Reinforcement Learning Approach to Synthetic Data Generation](https://arxiv.org/abs/2512.21395)
*Natalia Espinosa-Dice,Nicholas J. Jackson,Chao Yan,Aaron Lee,Bradley A. Malin*

Main category: cs.LG

TL;DR: RLSyn：将合成数据生成重构为强化学习问题，在小样本生物医学数据场景中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常需要大数据集和复杂训练，限制了在小样本生物医学研究中的应用，需要更高效的数据生成方法

Method: 将合成数据生成重构为强化学习问题，使用随机策略建模数据生成器，通过近端策略优化和判别器奖励进行优化

Result: 在MIMIC-IV数据集上与扩散模型相当并优于GANs，在更小的AI-READI数据集上优于扩散模型和GANs

Conclusion: 强化学习为合成生物医学数据生成提供了原理性且有效的替代方案，特别适用于数据稀缺场景

Abstract: Synthetic data generation (SDG) is a promising approach for enabling data sharing in biomedical studies while preserving patient privacy. Yet, state-of-the-art generative models often require large datasets and complex training procedures, limiting their applicability in small-sample settings. In this work, we reframe SDG as a reinforcement learning (RL) problem and introduce RLSyn, a novel framework that models the data generator as a stochastic policy over patient records and optimizes it using Proximal Policy Optimization with discriminator-derived rewards, yielding more stable and data-efficient training. We evaluate RLSyn on two biomedical datasets - AI-READI and MIMIC-IV- and benchmark it against state-of-the-art generative adversarial networks (GANs) and diffusion-based methods across extensive privacy, utility, and fidelity evaluations. RL-Syn performs comparably to diffusion models and outperforms GANs on MIMIC-IV, while outperforming both diffusion models and GANs on the smaller AI-READI dataset. These results demonstrate that reinforcement learning provides a principled and effective alternative for synthetic biomedical data generation, particularly in data-scarce regimes.

</details>


### [29] [kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning](https://arxiv.org/abs/2512.21409)
*Giacomo Turri,Grégoire Pacreau,Giacomo Meanti,Timothée Devergne,Daniel Ordonez,Erfan Mirzaei,Bruno Belucci,Karim Lounici,Vladimir Kostic,Massimiliano Pontil,Pietro Novelli*

Main category: cs.LG

TL;DR: kooplearn是一个机器学习库，实现了线性、核和深度学习估计器，用于估计动力学算子及其谱分解，支持离散时间和连续时间系统建模。


<details>
  <summary>Details</summary>
Motivation: 为动力学系统分析提供统一的机器学习工具，通过谱方法分析系统、推导数据驱动的降阶模型，并预测未来状态和可观测量。

Method: 实现线性、核和深度学习估计器，支持离散时间演化算子（Koopman/Transfer）和连续时间无穷小生成元，遵循scikit-learn API接口。

Result: 开发了完整的机器学习库，包含精选的基准数据集，支持实验、可重复性和算法公平比较，代码已在GitHub开源。

Conclusion: kooplearn为动力学系统分析提供了标准化工具，便于集成到现有机器学习工作流中，促进该领域的研究和应用发展。

Abstract: kooplearn is a machine-learning library that implements linear, kernel, and deep-learning estimators of dynamical operators and their spectral decompositions. kooplearn can model both discrete-time evolution operators (Koopman/Transfer) and continuous-time infinitesimal generators. By learning these operators, users can analyze dynamical systems via spectral methods, derive data-driven reduced-order models, and forecast future states and observables. kooplearn's interface is compliant with the scikit-learn API, facilitating its integration into existing machine learning and data science workflows. Additionally, kooplearn includes curated benchmark datasets to support experimentation, reproducibility, and the fair comparison of learning algorithms. The software is available at https://github.com/Machine-Learning-Dynamical-Systems/kooplearn.

</details>


### [30] [dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.21446)
*Shirui Chen,Jiantao Jiao,Lillian J. Ratliff,Banghua Zhu*

Main category: cs.LG

TL;DR: dUltra是一个基于强化学习的框架，通过优化去掩码策略实现扩散语言模型的高效并行解码，在数学推理和代码生成任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散语言模型（MDLMs）虽然具有并行生成token的潜力，但实际解码效率较低（每轮前向传播解码少于5个token），采样速度与自回归模型加推测解码方案相当，限制了其优势。现有的基于蒸馏的加速方法（如dParallel、d3LLM）在基础模型生成的轨迹上进行微调，可能导致策略偏离，且性能受限于基础模型样本质量。

Method: 提出了dUltra框架，基于组相对策略优化（GRPO）的在线强化学习方法，学习高效并行解码的去掩码策略。引入一个去掩码规划头，在独立伯努利分布下预测每个token的去掩码概率。联合优化基础扩散LLM和去掩码顺序规划器，使用结合可验证奖励、蒸馏奖励和去掩码步骤数的奖励信号。

Result: 在数学推理和代码生成任务上，dUltra在准确性与效率的权衡上超越了最先进的启发式和基于蒸馏的基线方法，朝着实现"扩散模型超越自回归模型"的目标迈进。

Conclusion: dUltra通过强化学习框架优化去掩码策略，显著提升了掩码扩散语言模型的并行解码效率，在多个任务上展示了优于现有方法的性能，为实现扩散模型相对于自回归模型的优势提供了有效途径。

Abstract: Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.

</details>


### [31] [Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US](https://arxiv.org/abs/2512.21456)
*Sukanya Krishna,Marie-Laure Charpignon,Maimuna Majumder*

Main category: cs.LG

TL;DR: 深度学习模型（特别是LSTM）在预测美国药物过量死亡率方面优于传统SARIMA方法，在疫情导致的制度变化下提供更准确的反事实估计和更好的不确定性校准。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行加剧了美国药物过量死亡率，传统统计方法如SARIMA在线性、平稳性和固定季节性假设下可能无法有效处理结构性中断，需要更先进的预测方法来准确估计超额死亡率。

Method: 系统比较SARIMA与三种深度学习架构（LSTM、Seq2Seq和Transformer），使用CDC国家数据（2015-2019年训练/验证，2020-2023年预测），结合保形预测区间和60多次试验的收敛分析。

Result: LSTM在点估计方面表现最优（MAPE 17.08% vs SARIMA 23.88%），在不确定性校准方面也更好（预测区间覆盖率68.8% vs 47.9%）；注意力模型因过度拟合历史均值而表现不佳。

Conclusion: 经过仔细验证的深度学习模型可以提供比传统方法更可靠的反事实估计，但在高风险领域部署神经预测时需要校准技术；研究提供了可部署的开源框架。

Abstract: Substance overdose mortality in the United States claimed over 80,000 lives in 2023, with the COVID-19 pandemic exacerbating existing trends through healthcare disruptions and behavioral changes. Estimating excess mortality, defined as deaths beyond expected levels based on pre-pandemic patterns, is essential for understanding pandemic impacts and informing intervention strategies. However, traditional statistical methods like SARIMA assume linearity, stationarity, and fixed seasonality, which may not hold under structural disruptions. We present a systematic comparison of SARIMA against three deep learning (DL) architectures (LSTM, Seq2Seq, and Transformer) for counterfactual mortality estimation using national CDC data (2015-2019 for training/validation, 2020-2023 for projection). We contribute empirical evidence that LSTM achieves superior point estimation (17.08% MAPE vs. 23.88% for SARIMA) and better-calibrated uncertainty (68.8% vs. 47.9% prediction interval coverage) when projecting under regime change. We also demonstrate that attention-based models (Seq2Seq, Transformer) underperform due to overfitting to historical means rather than capturing emergent trends. Ourreproducible pipeline incorporates conformal prediction intervals and convergence analysis across 60+ trials per configuration, and we provide an open-source framework deployable with 15 state health departments. Our findings establish that carefully validated DL models can provide more reliable counterfactual estimates than traditional methods for public health planning, while highlighting the need for calibration techniques when deploying neural forecasting in high-stakes domains.

</details>


### [32] [MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding](https://arxiv.org/abs/2512.21506)
*Aiwei Zhang,Arvind Pillai,Andrew Campbell,Nicholas C. Jacobson*

Main category: cs.LG

TL;DR: MotionTeller是一个将可穿戴设备活动数据（加速度计数据）转换为自然语言摘要的生成框架，通过预训练的活动编码器和轻量级投影模块结合冻结的大型语言模型，在真实世界数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴传感技术日益普及，如何从原始生理信号（如加速度计收集的分钟级活动数据）生成自然语言摘要成为一个关键挑战。现有方法需要更有效的技术来将传感器数据转化为人类可读的行为描述。

Method: MotionTeller框架包含：1）预训练的活动编码器处理分钟级可穿戴活动数据；2）轻量级投影模块将行为嵌入映射到冻结的解码器大型语言模型的标记空间；3）使用交叉熵损失在语言标记上进行监督训练；4）基于真实世界NHANES记录构建了54383个（活动数据，文本）对的数据集。

Result: MotionTeller在语义保真度（BERTScore-F1 = 0.924）和词汇准确性（ROUGE-1 = 0.722）方面表现优异，比基于提示的基线方法在ROUGE-1上高出7%。平均训练损失在第15个epoch收敛到0.38，表明优化稳定。定性分析显示模型能捕捉昼夜节律结构和行为转换，PCA图显示训练后嵌入空间的聚类对齐得到增强。

Conclusion: MotionTeller作为一个可扩展、可解释的系统，能够将可穿戴传感器数据转化为流畅的、以人为中心的描述，为行为监测、临床审查和个性化健康干预开辟了新途径。

Abstract: As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral summaries. We construct a novel dataset of 54383 (actigraphy, text) pairs derived from real-world NHANES recordings, and train the model using cross-entropy loss with supervision only on the language tokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7 percent in ROUGE-1. The average training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative analysis confirms that MotionTeller captures circadian structure and behavioral transitions, while PCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these results position MotionTeller as a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, introducing new pathways for behavioral monitoring, clinical review, and personalized health interventions.

</details>


### [33] [Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering](https://arxiv.org/abs/2512.21510)
*Wenyuan Yang,Jie Xu,Hongqing He,Jiangzhang Gan,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: TreeEIC：一种基于缺失模式树的IMVC框架，通过分组决策集、集成学习和知识蒸馏，充分利用不完整多视图数据中的可用视图对，提升聚类性能


<details>
  <summary>Details</summary>
Motivation: 现实世界多视图数据通常存在高度不一致的缺失模式，现有不完整多视图聚类方法忽略了"视图对利用不足"问题，即不一致的缺失模式使得不完整但可用的多视图对无法被充分利用，限制了模型性能

Method: 1. 定义缺失模式树模型，根据不同的缺失模式将数据分组到多个决策集中；2. 在每个决策集内执行多视图聚类；3. 提出多视图决策集成模块，聚合所有决策集的聚类结果，通过不确定性权重抑制不可靠决策；4. 设计集成到个体的知识蒸馏模块，将集成知识转移到视图特定的聚类模型中

Result: 在多个基准数据集上的广泛实验表明，TreeEIC实现了最先进的不完整多视图聚类性能，并在高度不一致的缺失模式下表现出优越的鲁棒性

Conclusion: TreeEIC通过充分利用可用多视图对、集成决策和知识蒸馏，有效解决了不完整多视图聚类中的视图对利用不足问题，提升了聚类性能和对不一致缺失模式的鲁棒性

Abstract: Real-world multi-view data usually exhibits highly inconsistent missing patterns which challenges the effectiveness of incomplete multi-view clustering (IMVC). Although existing IMVC methods have made progress from both imputation-based and imputation-free routes, they have overlooked the pair under-utilization issue, i.e., inconsistent missing patterns make the incomplete but available multi-view pairs unable to be fully utilized, thereby limiting the model performance. To address this, we propose a novel missing-pattern tree based IMVC framework entitled TreeEIC. Specifically, to achieve full exploitation of available multi-view pairs, TreeEIC first defines the missing-pattern tree model to group data into multiple decision sets according to different missing patterns, and then performs multi-view clustering within each set. Furthermore, a multi-view decision ensemble module is proposed to aggregate clustering results from all decision sets, which infers uncertainty-based weights to suppress unreliable clustering decisions and produce robust decisions. Finally, an ensemble-to-individual knowledge distillation module transfers the ensemble knowledge to view-specific clustering models, which enables ensemble and individual modules to promote each other by optimizing cross-view consistency and inter-cluster discrimination losses. Extensive experiments on multiple benchmark datasets demonstrate that our TreeEIC achieves state-of-the-art IMVC performance and exhibits superior robustness under highly inconsistent missing patterns.

</details>


### [34] [Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data](https://arxiv.org/abs/2512.21516)
*Hongqing He,Jie Xu,Wenyuan Yang,Yonghua Zhu,Guoqiu Wen,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种统一对比学习框架，用于处理不完整和噪声多视图数据的聚类问题，通过全局图引导和局部图加权对比学习解决样本配对不足和错误配对问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界多视图数据常存在不完整性和噪声，导致样本配对不足（rare-paired）或错误配对（mis-paired），这严重影响了基于对比学习的多视图聚类效果。配对不足限制了多视图互补信息的充分提取，而错误配对则导致对比学习朝错误方向优化。

Method: 提出统一的对比学习框架：1）针对配对不足问题，设计全局图引导对比学习，利用所有视图样本构建全局视图亲和图来形成新样本对；2）针对错误配对问题，提出局部图加权对比学习，利用局部邻居生成配对权重来自适应强化或弱化对比学习。该方法无需数据填充，可集成到统一的全局-局部图引导对比学习框架中。

Result: 在不完整和噪声设置下的多视图数据上进行广泛实验，结果表明该方法相比现有最先进方法取得了优越性能。

Conclusion: 提出的全局-局部图引导对比学习框架有效解决了不完整和噪声多视图数据中的配对问题，提升了聚类效果，为处理现实世界多视图数据提供了有效解决方案。

Abstract: Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.

</details>


### [35] [First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions](https://arxiv.org/abs/2512.21521)
*Egor Shulgin,Grigory Malinovsky,Sarit Khirirat,Peter Richtárik*

Main category: cs.LG

TL;DR: Fed-α-NormEC是首个在标准假设下提供可证明收敛性和差分隐私保证的联邦学习框架，支持本地更新、部分客户端参与等实际特征


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习中的差分隐私方法通常依赖不现实的假设（如梯度有界或同质性），并且忽略了实际FL特征如多轮本地更新和部分客户端参与，阻碍了实际应用

Method: 提出Fed-α-NormEC框架，整合本地更新（完整和增量梯度步骤）、独立的服务器和客户端步长，以及关键的部分客户端参与机制

Result: 在标准假设下提供可证明的收敛性和差分隐私保证，实验验证了在私有深度学习任务上的有效性

Conclusion: Fed-α-NormEC是首个同时支持实际FL特征并提供理论保证的差分隐私联邦学习框架，解决了现有方法的局限性

Abstract: Federated Learning (FL) enables collaborative training on decentralized data. Differential privacy (DP) is crucial for FL, but current private methods often rely on unrealistic assumptions (e.g., bounded gradients or heterogeneity), hindering practical application. Existing works that relax these assumptions typically neglect practical FL features, including multiple local updates and partial client participation. We introduce Fed-$α$-NormEC, the first differentially private FL framework providing provable convergence and DP guarantees under standard assumptions while fully supporting these practical features. Fed-$α$-NormE integrates local updates (full and incremental gradient steps), separate server and client stepsizes, and, crucially, partial client participation, which is essential for real-world deployment and vital for privacy amplification. Our theoretical guarantees are corroborated by experiments on private deep learning tasks.

</details>


### [36] [Generative Actor Critic](https://arxiv.org/abs/2512.21527)
*Aoyang Qin,Deqian Kong,Wei Wang,Ying Nian Wu,Song-Chun Zhu,Sirui Xie*

Main category: cs.LG

TL;DR: GAC是一种新的强化学习框架，将策略评估重构为学习轨迹和回报的联合分布生成模型，策略改进则通过对该模型进行推理来实现，显著提升了离线到在线学习的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法专注于估计或最大化期望回报，在将离线预训练模型与在线经验结合时面临挑战，需要一种能够更好整合离线学习和在线微调的新框架。

Method: 提出生成式演员-评论家(GAC)框架，将序列决策解耦：1)策略评估学习轨迹和回报的联合分布p(τ,y)；2)策略改进通过对学习到的模型进行灵活推理实现。具体实现基于具有连续潜在计划向量的潜变量模型，开发了两种推理策略：利用（优化潜在计划以最大化期望回报）和探索（基于动态调整的目标回报采样潜在计划）。

Result: 在Gym-MuJoCo和Maze2D基准测试中，GAC表现出强大的离线性能，并且相比最先进方法，离线到在线的改进显著增强，即使在缺乏逐步奖励的情况下也能有效工作。

Conclusion: GAC通过将策略评估重构为生成建模问题，策略改进重构为推理问题，为强化学习提供了一个灵活且强大的框架，特别适合离线到在线的学习场景，能够有效整合离线预训练和在线经验。

Abstract: Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(τ, y)$, and \textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \textit{exploitation}, by optimizing latent plans to maximize expected returns, and \textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.

</details>


### [37] [AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2512.21544)
*Xinru Wen,Weizhong Lin,Xuan Xiao*

Main category: cs.LG

TL;DR: AVP-Fusion是一个两阶段深度学习框架，通过自适应特征融合和对比学习准确识别抗病毒肽，显著优于现有方法，并能预测病毒家族和特定病毒亚类。


<details>
  <summary>Details</summary>
Motivation: 当前计算方法难以捕捉复杂的序列依赖关系，无法有效处理模糊难分类的样本，这阻碍了抗病毒药物开发。需要更准确识别抗病毒肽（AVPs）的方法来加速新药研发。

Method: 提出AVP-Fusion两阶段深度学习框架：1）使用10种不同描述符构建全景特征空间；2）引入自适应门控机制动态调节CNN提取的局部基序和BiLSTM捕获的全局依赖的权重；3）采用基于在线难例挖掘和BLOSUM62数据增强的对比学习策略锐化决策边界；4）第二阶段利用迁移学习进行病毒家族和特定病毒亚类预测。

Result: 在基准Set 1数据集上，AVP-Fusion达到0.9531的准确率和0.9064的MCC值，显著优于最先进方法。第二阶段模型能够在有限样本下精确预测六个病毒家族和八个特定病毒的亚类。

Conclusion: AVP-Fusion是一个强大且可解释的工具，可用于高通量抗病毒药物筛选，为解决抗病毒肽识别中的序列依赖性和模糊样本分类问题提供了有效方案。

Abstract: Accurate identification of antiviral peptides (AVPs) is critical for accelerating novel drug development. However, current computational methods struggle to capture intricate sequence dependencies and effectively handle ambiguous, hard-to-classify samples. To address these challenges, we propose AVP-Fusion, a novel two-stage deep learning framework integrating adaptive feature fusion and contrastive learning. Unlike traditional static feature concatenation, we construct a panoramic feature space using 10 distinct descriptors and introduce an Adaptive Gating Mechanism.This mechanism dynamically regulates the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Furthermore, to address data distribution challenges, we employ a contrastive learning strategy driven by Online Hard Example Mining (OHEM) and BLOSUM62-based data augmentation, which significantly sharpens the model's decision boundaries. Experimental results on the benchmark Set 1 dataset demonstrate that AVP-Fusion achieves an accuracy of 0.9531 and an MCC of 0.9064, significantly outperforming state-of-the-art methods. In the second stage, leveraging transfer learning, the model enables precise subclass prediction for six viral families and eight specific viruses, even under limited sample sizes. In summary, AVP-Fusion serves as a robust and interpretable tool for high-throughput antiviral drug screening.

</details>


### [38] [Discovering Sparse Recovery Algorithms Using Neural Architecture Search](https://arxiv.org/abs/2512.21563)
*Patrick Yubeaton,Sarthak Gupta,M. Salman Asif,Chinmay Hegde*

Main category: cs.LG

TL;DR: 本文提出使用元学习工具（如神经架构搜索）自动发现信号处理中逆问题求解算法，以ISTA和FISTA算法为例，开发了能够从超过5万个变量的搜索空间中重新发现这些算法的元学习框架。


<details>
  <summary>Details</summary>
Motivation: 设计信号处理中逆问题求解的新算法是一个极其困难、依赖启发式方法且耗时的工作，需要探索自动化的算法发现方法。

Method: 使用元学习工具（特别是神经架构搜索）构建框架，以ISTA和FISTA算法为研究对象，在包含超过50,000个变量的搜索空间中进行算法重新发现。

Result: 该框架成功重新发现了ISTA和FISTA算法的多个关键元素，并展示了该框架可应用于ISTA/FISTA之外的各种数据分布和算法。

Conclusion: 元学习和神经架构搜索为信号处理中的算法自动发现提供了有效途径，能够显著减少算法设计的时间和启发式依赖。

Abstract: The design of novel algorithms for solving inverse problems in signal processing is an incredibly difficult, heuristic-driven, and time-consuming task. In this short paper, we the idea of automated algorithm discovery in the signal processing context through meta-learning tools such as Neural Architecture Search (NAS). Specifically, we examine the Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated Fast ISTA (FISTA) variant as candidates for algorithm rediscovery. We develop a meta-learning framework which is capable of rediscovering (several key elements of) the two aforementioned algorithms when given a search space of over 50,000 variables. We then show how our framework can apply to various data distributions and algorithms besides ISTA/FISTA.

</details>


### [39] [Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations](https://arxiv.org/abs/2512.21586)
*Xin Liu,Haoran Li,Dongbin Zhao*

Main category: cs.LG

TL;DR: BCV-LR：通过潜在表示从视频进行行为克隆的无监督框架，仅需少量交互即可实现高效视觉策略学习


<details>
  <summary>Details</summary>
Motivation: 人类可以从少量视频中高效学习技能，但自主智能体面临视觉输入复杂、缺乏动作/奖励信号、交互步骤有限等挑战，需要实现无监督、样本高效的视频模仿学习

Method: BCV-LR框架：1）通过自监督任务从高维视频输入提取动作相关潜在特征；2）基于动态的无监督目标预测连续帧间的潜在动作；3）在线微调并将潜在动作对齐到真实动作空间；4）迭代策略改进：克隆策略丰富经验，进一步微调潜在动作

Result: 在离散和连续控制任务上，BCV-LR仅需少量交互即可达到有效（部分任务达到专家级）策略性能，在24/28个任务上样本效率超过最先进的ILV基线和强化学习方法

Conclusion: 首次证明视频可以支持极其样本高效的视觉策略学习，无需任何专家监督，为无监督视频模仿学习提供了新途径

Abstract: Humans can efficiently extract knowledge and learn skills from the videos within only a few trials and errors. However, it poses a big challenge to replicate this learning process for autonomous agents, due to the complexity of visual input, the absence of action or reward signals, and the limitations of interaction steps. In this paper, we propose a novel, unsupervised, and sample-efficient framework to achieve imitation learning from videos (ILV), named Behavior Cloning from Videos via Latent Representations (BCV-LR). BCV-LR extracts action-related latent features from high-dimensional video inputs through self-supervised tasks, and then leverages a dynamics-based unsupervised objective to predict latent actions between consecutive frames. The pre-trained latent actions are fine-tuned and efficiently aligned to the real action space online (with collected interactions) for policy behavior cloning. The cloned policy in turn enriches the agent experience for further latent action finetuning, resulting in an iterative policy improvement that is highly sample-efficient.
  We conduct extensive experiments on a set of challenging visual tasks, including both discrete control and continuous control. BCV-LR enables effective (even expert-level on some tasks) policy performance with only a few interactions, surpassing state-of-the-art ILV baselines and reinforcement learning methods (provided with environmental rewards) in terms of sample efficiency across 24/28 tasks. To the best of our knowledge, this work for the first time demonstrates that videos can support extremely sample-efficient visual policy learning, without the need to access any other expert supervision.

</details>


### [40] [Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search](https://arxiv.org/abs/2512.21648)
*Maximilian Weichart*

Main category: cs.LG

TL;DR: 本文提出Inverse-RPO方法，系统性地从任何无先验UCB推导出基于先验的UCT搜索策略，应用于方差感知的UCB-V得到两种新树策略，在多个基准测试中超越PUCT且不增加计算成本。


<details>
  <summary>Details</summary>
Motivation: MCTS在强化学习中通过整合规划与学习对长时程推理任务产生深远影响。AlphaZero的成功关键在于PUCT中引入先验项，但PUCT是经验性推导而非基于第一性原理。虽然有理论保证更强的UCB变体存在，但将其扩展到基于先验的UCT一直具有挑战性。

Method: 基于将MCTS视为正则化策略优化问题的视角，提出Inverse-RPO通用方法，系统性地从任何无先验UCB推导出基于先验的UCT。将此方法应用于方差感知的UCB-V，得到两种新的基于先验的树策略，将方差估计整合到搜索中。

Result: 实验表明，这些方差感知的基于先验的UCT在多个基准测试中超越PUCT，且不增加额外计算成本。同时扩展了mctx库以支持方差感知UCT，代码改动最小化以便进一步研究。

Conclusion: Inverse-RPO为系统性地从无先验UCB推导基于先验的UCT提供了通用框架，得到的方差感知策略在性能上优于PUCT，为基于先验的UCT的进一步研究提供了理论基础和实用工具。

Abstract: Monte Carlo Tree Search (MCTS) has profoundly influenced reinforcement learning (RL) by integrating planning and learning in tasks requiring long-horizon reasoning, exemplified by the AlphaZero family of algorithms. Central to MCTS is the search strategy, governed by a tree policy based on an upper confidence bound (UCB) applied to trees (UCT). A key factor in the success of AlphaZero is the introduction of a prior term in the UCB1-based tree policy PUCT, which improves exploration efficiency and thus accelerates training. While many alternative UCBs with stronger theoretical guarantees than UCB1 exist, extending them to prior-based UCTs has been challenging, since PUCT was derived empirically rather than from first principles. Recent work retrospectively justified PUCT by framing MCTS as a regularized policy optimization (RPO) problem. Building on this perspective, we introduce Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free UCB. Applying this method to the variance-aware UCB-V, we obtain two new prior-based tree policies that incorporate variance estimates into the search. Experiments indicate that these variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without incurring additional computational cost. We also provide an extension of the mctx library supporting variance-aware UCTs, showing that the required code changes are minimal and intended to facilitate further research on principled prior-based UCTs. Code: github.com/Max-We/inverse-rpo.

</details>


### [41] [A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)](https://arxiv.org/abs/2512.21610)
*Jagaran Chakma,Zhiguang Zhou,Jyoti Chakma,Cao YuSen*

Main category: cs.LG

TL;DR: 本研究提出了一种数据驱动的多目标方法，用于预测超高性能混凝土的力学性能、流动性和孔隙率。从21种机器学习算法中选出5个高性能模型，XGBoost在超参数调优后表现最佳。采用两阶段流程：先用原始数据构建XGBoost模型，然后通过移除多重共线性特征、隔离森林识别异常值、SHAP分析选择重要特征来清洗数据，最后用精炼数据集重新训练XGBoost，实现高精度预测并开发了GUI界面。


<details>
  <summary>Details</summary>
Motivation: 超高性能混凝土的混合设计通常需要大量实验测试，成本高且耗时。为了减少实验需求并提高预测精度，需要开发一种数据驱动的预测框架，能够同时预测UHPC的多个关键性能指标。

Method: 1. 测试21种机器学习算法，选出5个高性能模型；2. 使用随机搜索和K折交叉验证进行超参数调优；3. 采用两阶段流程：先用原始数据构建XGBoost模型，然后通过移除多重共线性特征、隔离森林识别异常值、SHAP分析选择重要特征来清洗数据；4. 用精炼数据集重新训练XGBoost模型；5. 开发图形用户界面支持材料设计师。

Result: XGBoost在超参数调优后表现出最佳预测精度。经过数据清洗和特征选择后重新训练的XGBoost模型在所有输出指标上都实现了高精度预测。开发的GUI界面为材料设计师提供了实用工具。

Conclusion: 提出的框架显著提高了UHPC性能预测的准确性，最大限度地减少了混合设计中对大量实验测试的需求，为UHPC材料设计提供了高效的数据驱动解决方案。

Abstract: This study presents a data-driven, multi-objective approach to predict the mechanical performance, flow ability, and porosity of Ultra-High-Performance Concrete (UHPC). Out of 21 machine learning algorithms tested, five high-performing models are selected, with XGBoost showing the best accuracy after hyperparameter tuning using Random Search and K-Fold Cross-Validation. The framework follows a two-stage process: the initial XGBoost model is built using raw data, and once selected as the final model, the dataset is cleaned by (1) removing multicollinear features, (2) identifying outliers with Isolation Forest, and (3) selecting important features using SHAP analysis. The refined dataset as model 2 is then used to retrain XGBoost, which achieves high prediction accuracy across all outputs. A graphical user interface (GUI) is also developed to support material designers. Overall, the proposed framework significantly improves the prediction accuracy and minimizes the need for extensive experimental testing in UHPC mix design.

</details>


### [42] [Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms](https://arxiv.org/abs/2512.21638)
*Jagaran Chakma,Zhiguang Zhou,Badhan Chakma*

Main category: cs.LG

TL;DR: 本研究开发并评估了三种机器学习模型（ET-XGB、RF-LGBM、Transformer-XGB）来预测钢-聚丙烯纤维增强高性能混凝土的力学性能，其中ET-XGB模型在压缩和拉伸强度预测上表现最佳，RF-LGBM在弯曲强度预测上最稳定。


<details>
  <summary>Details</summary>
Motivation: 传统混凝土性能预测方法存在局限性，需要开发更准确、可解释的机器学习模型来预测钢-聚丙烯纤维增强高性能混凝土的力学性能，以优化配合比设计和结构性能评估。

Method: 使用三种集成模型（ET-XGB、RF-LGBM、Transformer-XGB）预测混凝土的压缩强度、弯曲强度和拉伸强度。采用k折交叉验证、超参数优化、SHAP分析和不确定性分析来确保模型的鲁棒性和可解释性。

Result: ET-XGB模型整体表现最佳，测试R²值分别为：压缩强度0.994、弯曲强度0.944、拉伸强度0.978，且在压缩和拉伸强度预测上不确定性最低。RF-LGBM模型在弯曲强度预测上最稳定可靠（R² 0.977）。SHAP分析显示纤维长径比、硅灰和钢纤维含量是最重要的预测因子。

Conclusion: 机器学习模型能够为高性能混凝土力学性能提供准确、可解释且可泛化的预测，这些模型为优化混凝土配合比设计和改进工程应用中的结构性能评估提供了有价值的工具。

Abstract: This research develops and evaluates machine learning models to predict the mechanical properties of steel-polypropylene fiber-reinforced high-performance concrete (HPC). Three model families were investigated: Extra Trees with XGBoost (ET-XGB), Random Forest with LightGBM (RF-LGBM), and Transformer with XGBoost (Transformer-XGB). The target properties included compressive strength (CS), flexural strength (FS), and tensile strength (TS), based on an extensive dataset compiled from published experimental studies. Model training involved k-fold cross-validation, hyperparameter optimization, Shapley additive explanations (SHAP), and uncertainty analysis to ensure both robustness and interpretability. Among the tested approaches, the ET-XGB model achieved the highest overall accuracy, with testing R^2 values of 0.994 for CS, 0.944 for FS, and 0.978 for TS and exhibited lowest uncertainty for CS and TS (approximately 13-16% and 30.4%, respectively). The RF-LGBM model provided the most stable and reliable predictions for FS (R^2 0.977), yielding the lowest uncertainty for FS (approximately 5-33%). The Transformer-XGB model demonstrated strong predictive capability (R^2 0.978 for TS and 0.967 for FS) but consistently showed the highest uncertainty, indicating reduced generalization reliability. SHAP analysis further indicated that fiber aspect ratios (AR1 and AR2), silica fume (Sfu), and steel fiber content (SF) were the most influential predictors of strength, whereas water content (W) and the water-binder ratio (w/b) consistently had negative effects. The findings confirm that machine learning models can provide accurate, interpretable, and generalizable predictions of HPC mechanical properties. These models offer valuable tools for optimizing concrete mix design and enhancing structural performance evaluation in engineering applications.

</details>


### [43] [Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation](https://arxiv.org/abs/2512.21650)
*Xiao Liu,Junchen Jin,Yanjie Zhao,Zhixuan Xing*

Main category: cs.LG

TL;DR: Causal-HM是一个用于多模态无监督异常检测的统一框架，专门针对机器人焊接等复杂制造过程，通过建模物理过程到结果的因果关系来提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态无监督异常检测方法存在因果盲区，将过程模态（实时视频、音频、传感器）和结果模态（焊后图像）视为平等的特征源，忽略了物理生成逻辑；同时高维视觉数据和低维传感器信号之间的异质性差距导致关键过程上下文被淹没。

Method: 提出Causal-HM框架，包含两个关键创新：1）传感器引导的CHM调制机制，利用低维传感器信号作为上下文指导高维视听特征提取；2）因果分层架构，强制单向生成映射以识别违反物理一致性的异常。

Result: 在新构建的Weld-4M基准测试中，Causal-HM在四个模态上实现了90.7%的SOTA I-AUROC性能。

Conclusion: Causal-HM通过显式建模物理过程到结果的依赖关系，有效解决了多模态异常检测中的因果盲区和异质性差距问题，在智能制造的复杂过程中表现出优越性能。

Abstract: Multimodal Unsupervised Anomaly Detection (UAD) is critical for quality assurance in smart manufacturing, particularly in complex processes like robotic welding. However, existing methods often suffer from causal blindness, treating process modalities (e.g., real-time video, audio, and sensors) and result modalities (e.g., post-weld images) as equal feature sources, thereby ignoring the inherent physical generative logic. Furthermore, the heterogeneity gap between high-dimensional visual data and low-dimensional sensor signals frequently leads to critical process context being drowned out. In this paper, we propose Causal-HM, a unified multimodal UAD framework that explicitly models the physical Process to Result dependency. Specifically, our framework incorporates two key innovations: a Sensor-Guided CHM Modulation mechanism that utilizes low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction , and a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies that violate physical consistency. Extensive experiments on our newly constructed Weld-4M benchmark across four modalities demonstrate that Causal-HM achieves a state-of-the-art (SOTA) I-AUROC of 90.7%. Code will be released after the paper is accepted.

</details>


### [44] [Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2512.21651)
*Dung Anh Hoang,Cuong Pham,Cuong Nguyen,Trung le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 本文提出了一种针对1位LLM量化的数据感知后训练量化方法，通过显式考虑激活误差累积来改善输出匹配效果，相比现有方法在保持低开销的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但参数量巨大难以在资源受限设备上部署。后训练量化因其高效性被广泛采用，但1位量化（将浮点权重转换为±1）仍然具有挑战性，现有方法通常导致显著的性能下降。特别是大多数现有1位PTQ方法关注权重对齐而非输出对齐，而输出匹配方法虽然更直观，但在1位LLM中直接应用会导致性能显著下降。

Method: 本文研究了在1位LLM量化背景下输出匹配方法失败的原因和条件，基于研究发现提出了一种新颖的数据感知PTQ方法。该方法显式考虑激活误差累积，同时保持优化效率，通过更有效的输出对齐策略来改善1位量化效果。

Result: 实验结果表明，该解决方案在最小开销的情况下，始终优于现有的1位PTQ方法，能够显著减少性能下降，同时保持部署的低成本优势。

Conclusion: 通过深入研究1位LLM量化中输出匹配失败的根本原因，并提出数据感知的优化方法，本文为高效部署大语言模型到资源受限设备提供了有效的1位量化解决方案，在性能和效率之间取得了良好平衡。

Abstract: Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, and knowledge distillation. Among these, post-training quantization (PTQ) is widely adopted for its efficiency, as it requires no retraining and only a small dataset for calibration, enabling low-cost deployment. Recent advances for post-training quantization have demonstrated that even sub-4-bit methods can maintain most of the original model performance. However, 1-bit quantization that converts floating-point weights to \(\pm\)1, remains particularly challenging, as existing 1-bit PTQ methods often suffer from significant performance degradation compared to the full-precision models. Specifically, most of existing 1-bit PTQ approaches focus on weight alignment, aligning the full-precision model weights with those of the quantized models, rather than directly aligning their outputs. Although the output-matching approach objective is more intuitive and aligns with the quantization goal, naively applying it in 1-bit LLMs often leads to notable performance degradation. In this paper, we investigate why and under what conditions output-matching fails, in the context of 1-bit LLM quantization. Based on our findings, we propose a novel data-aware PTQ approach for 1-bit LLMs that explicitly accounts for activation error accumulation while keeping optimization efficient. Empirical experiments demonstrate that our solution consistently outperforms existing 1-bit PTQ methods with minimal overhead.

</details>


### [45] [A Comedy of Estimators: On KL Regularization in RL Training of LLMs](https://arxiv.org/abs/2512.21852)
*Vedant Shah,Johan Obando-Ceron,Vineet Jain,Brian Bartoldson,Bhavya Kailkhura,Sarthak Mittal,Glen Berseth,Pablo Samuel Castro,Yoshua Bengio,Nikolay Malkin,Moksh Jain,Siddarth Venkatraman,Aaron Courville*

Main category: cs.LG

TL;DR: 本文系统分析了LLM强化学习中KL散度估计器的不同配置方式及其梯度偏差问题，通过实验验证了无偏梯度估计器能带来更好的训练稳定性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习训练中广泛使用KL散度正则化项，但实践中使用的各种KL估计器配置方式缺乏系统研究，且现有方法存在目标函数与实现之间的梯度偏差问题，影响模型性能。

Method: 1. 理论分析不同KL估计器配置的梯度特性，揭示设计选择如何影响梯度偏差；2. 实验验证：使用Qwen2.5-7B、Llama-3.1-8B-Instruct和Qwen3-4B-Instruct-2507模型，在不同配置下进行RL微调；3. 评估模型在分布内和分布外任务上的性能；4. 研究离策略设置中不同KL配置的表现。

Result: 1. 在策略设置中：有偏梯度估计器配置会导致训练不稳定；使用无偏梯度估计器配置能获得更好的分布内和分布外任务性能；2. 离策略设置中：KL正则化有助于稳定异步设置导致的离策略RL训练。

Conclusion: KL估计器配置的梯度偏差对LLM强化学习训练有重要影响，选择无偏梯度估计器能提高训练稳定性和模型泛化能力，为实际应用提供了重要指导。

Abstract: The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.

</details>


### [46] [Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning](https://arxiv.org/abs/2512.21743)
*Hengyi Wu,Zhenyi Wang,Heng Huang*

Main category: cs.LG

TL;DR: 提出一种基于熵感知的持续学习方法，通过动态反馈机制根据各层熵值调节模型，减少高熵层欠拟合，增加低熵层熵值缓解过拟合，提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法通常对所有层进行统一处理，在稳定性与可塑性之间权衡，但不同层在分类任务时表现出不同的不确定性（熵）。高熵层容易欠拟合，低熵层容易过拟合，这种不平衡影响了模型性能。

Method: 提出熵感知持续学习方法，采用动态反馈机制根据每层的熵值进行调节：降低高熵层的熵以缓解欠拟合，增加过度自信层的熵以缓解过拟合。这种自适应调节促使模型收敛到更宽的局部最小值，从而提升泛化能力。该方法具有通用性，可与回放和正则化方法无缝集成。

Result: 在多个数据集上的实验表明，该方法相比最先进的持续学习基线方法取得了显著的性能提升。

Conclusion: 通过熵感知的动态层调节机制，能够有效平衡持续学习中的欠拟合和过拟合问题，提高模型在持续学习任务中的泛化性能。

Abstract: Continual learning aims to acquire new tasks while preserving performance on previously learned ones, but most methods struggle with catastrophic forgetting. Existing approaches typically treat all layers uniformly, often trading stability for plasticity or vice versa. However, different layers naturally exhibit varying levels of uncertainty (entropy) when classifying tasks. High-entropy layers tend to underfit by failing to capture task-specific patterns, while low-entropy layers risk overfitting by becoming overly confident and specialized. To address this imbalance, we propose an entropy-aware continual learning method that employs a dynamic feedback mechanism to regulate each layer based on its entropy. Specifically, our approach reduces entropy in high-entropy layers to mitigate underfitting and increases entropy in overly confident layers to alleviate overfitting. This adaptive regulation encourages the model to converge to wider local minima, which have been shown to improve generalization. Our method is general and can be seamlessly integrated with both replay- and regularization-based approaches. Experiments on various datasets demonstrate substantial performance gains over state-of-the-art continual learning baselines.

</details>


### [47] [Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation](https://arxiv.org/abs/2512.21866)
*Yiming Qian,Thorsten Neumann,Xueyining Huang,David Hardoon,Fei Gao,Yong Liu,Siow Mong Rick Goh*

Main category: cs.LG

TL;DR: 提出一种可解释、保护隐私的数据集蒸馏框架，用于协作式金融欺诈检测。通过将随机森林转换为透明规则区域，生成合成交易数据，实现数据量减少85-93%的同时保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决金融欺诈检测中多机构协作时的数据隐私保护问题，同时需要保持模型的可解释性和检测性能，满足监管审计要求。

Method: 将训练好的随机森林转换为轴对齐的规则区域（叶节点超矩形），在每个区域内均匀采样生成合成交易数据，创建紧凑、可审计的替代数据集。

Result: 在IEEE-CIS欺诈数据集上，蒸馏数据集减少数据量85-93%，保持竞争性精度和微F1分数，AUC略有下降。跨机构共享合成数据提升跨集群性能，隐私保护效果显著（成员推断攻击准确率接近随机水平）。

Conclusion: 树区域蒸馏框架实现了可信、可部署的欺诈分析，具有可解释的全局规则、带量化不确定性的个案推理以及强大的隐私保护特性，适合多机构环境和监管审计。

Abstract: We propose an explainable, privacy-preserving dataset distillation framework for collaborative financial fraud detection. A trained random forest is converted into transparent, axis-aligned rule regions (leaf hyperrectangles), and synthetic transactions are generated by uniformly sampling within each region. This produces a compact, auditable surrogate dataset that preserves local feature interactions without exposing sensitive original records. The rule regions also support explainability: aggregated rule statistics (for example, support and lift) describe global patterns, while assigning each case to its generating region gives concise human-readable rationales and calibrated uncertainty based on tree-vote disagreement.
  On the IEEE-CIS fraud dataset (590k transactions across three institution-like clusters), distilled datasets reduce data volume by 85% to 93% (often under 15% of the original) while maintaining competitive precision and micro-F1, with only a modest AUC drop. Sharing and augmenting with synthesized data across institutions improves cross-cluster precision, recall, and AUC. Real vs. synthesized structure remains highly similar (over 93% by nearest-neighbor cosine analysis). Membership-inference attacks perform at chance level (about 0.50) when distinguishing training from hold-out records, suggesting low memorization risk. Removing high-uncertainty synthetic points using disagreement scores further boosts AUC (up to 0.687) and improves calibration. Sensitivity tests show weak dependence on the distillation ratio (AUC about 0.641 to 0.645 from 6% to 60%).
  Overall, tree-region distillation enables trustworthy, deployable fraud analytics with interpretable global rules, per-case rationales with quantified uncertainty, and strong privacy properties suitable for multi-institution settings and regulatory audit.

</details>


### [48] [Unifying Learning Dynamics and Generalization in Transformers Scaling Law](https://arxiv.org/abs/2512.22088)
*Chiwun Yang*

Main category: cs.LG

TL;DR: 该论文为Transformer语言模型的缩放定律提供了理论分析，通过ODE系统形式化学习动态，揭示了计算资源分配与泛化误差之间的相变关系。


<details>
  <summary>Details</summary>
Motivation: 虽然缩放定律在大型语言模型开发中已被经验验证，但其理论基础仍然薄弱。本文旨在为Transformer模型在真实世界条件下的学习动态提供严格的理论分析。

Method: 将基于Transformer的语言模型学习动态形式化为常微分方程系统，然后近似为核行为。不同于之前的玩具模型分析，本文严格分析了多层Transformer在任意数据分布下的SGD训练过程。

Result: 建立了超额风险的理论上界，揭示了明显的相变现象：初始优化阶段超额风险随计算成本指数衰减；超过特定资源分配阈值后，系统进入统计阶段，泛化误差遵循幂律衰减Θ(C^{-1/6})。此外，还推导了模型大小、训练时间和数据集大小的独立缩放定律。

Conclusion: 该研究为Transformer语言模型的缩放定律提供了统一的理论框架，阐明了计算资源分配如何影响泛化性能，并揭示了优化过程与统计阶段之间的相变关系。

Abstract: The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.
  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Θ(\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.

</details>


### [49] [LibContinual: A Comprehensive Library towards Realistic Continual Learning](https://arxiv.org/abs/2512.22029)
*Wenbin Li,Shangge Liu,Borui Kang,Yiyang Chen,KaXuan Lew,Yang Chen,Yinghuan Shi,Lei Wang,Yang Gao,Jiebo Luo*

Main category: cs.LG

TL;DR: LibContinual是一个用于持续学习的统一框架，解决了该领域方法碎片化、评估不一致的问题，并揭示了主流评估中的三个隐含假设限制了方法的实际应用性。


<details>
  <summary>Details</summary>
Motivation: 持续学习领域存在方法碎片化问题，缺乏统一框架，导致公平比较和可重复研究困难。同时，主流评估中的隐含假设往往高估了CL方法在现实世界中的适用性。

Method: 提出LibContinual库，采用高内聚、低耦合的模块化架构，集成了5个主要方法类别的19种代表性算法。利用该统一框架，系统识别并研究了三个隐含假设：离线数据可访问性、无限制内存资源和任务内语义同质性。

Result: 在严格的在线CL设置、统一内存预算协议和提出的类别随机设置下，许多代表性CL方法在面临现实约束时表现出显著性能下降。揭示了资源感知和语义鲁棒的CL策略的必要性。

Conclusion: LibContinual为现实持续学习研究提供了基础工具包，强调了考虑资源约束和语义异质性的重要性，为未来研究提供了标准化平台。

Abstract: A fundamental challenge in Continual Learning (CL) is catastrophic forgetting, where adapting to new tasks degrades the performance on previous ones. While the field has evolved with diverse methods, this rapid surge in diverse methodologies has culminated in a fragmented research landscape. The lack of a unified framework, including inconsistent implementations, conflicting dependencies, and varying evaluation protocols, makes fair comparison and reproducible research increasingly difficult. To address this challenge, we propose LibContinual, a comprehensive and reproducible library designed to serve as a foundational platform for realistic CL. Built upon a high-cohesion, low-coupling modular architecture, LibContinual integrates 19 representative algorithms across five major methodological categories, providing a standardized execution environment. Meanwhile, leveraging this unified framework, we systematically identify and investigate three implicit assumptions prevalent in mainstream evaluation: (1) offline data accessibility, (2) unregulated memory resources, and (3) intra-task semantic homogeneity. We argue that these assumptions often overestimate the real-world applicability of CL methods. Through our comprehensive analysis using strict online CL settings, a novel unified memory budget protocol, and a proposed category-randomized setting, we reveal significant performance drops in many representative CL methods when subjected to these real-world constraints. Our study underscores the necessity of resource-aware and semantically robust CL strategies, and offers LibContinual as a foundational toolkit for future research in realistic continual learning. The source code is available from \href{https://github.com/RL-VIG/LibContinual}{https://github.com/RL-VIG/LibContinual}.

</details>


### [50] [From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation](https://arxiv.org/abs/2512.22031)
*Nagham Osman,Vittorio Lembo,Giovanni Bottegoni,Laura Toni*

Main category: cs.LG

TL;DR: 该研究首次将"类命中分子生成"作为独立任务，探索生成模型能否替代药物发现流程中的命中识别步骤，提出了专门的评估框架并测试了多种生成模型。


<details>
  <summary>Details</summary>
Motivation: 传统的高通量筛选和虚拟筛选方法耗时耗力，虽然深度学习生成模型能从头生成新化合物，但用ML完全替代整个药物发现流程极具挑战。本研究旨在探索生成模型能否专门替代命中识别这一关键步骤。

Method: 提出了针对类命中分子生成任务的评估框架，整合了物理化学、结构和生物活性相关标准的多阶段过滤流程来定义类命中化学空间。对两种自回归模型和一种扩散模型在不同数据集和训练设置下进行了基准测试，使用标准指标和靶点特异性对接评分评估输出。

Result: 生成模型能够为多个靶点生成有效、多样且具有生物相关性的化合物，其中一些选定的GSK-3β命中分子被合成并在体外实验中证实具有活性。同时发现了当前评估指标和可用训练数据的关键局限性。

Conclusion: 这是首次将类命中分子生成作为独立任务的研究，证明了生成模型可以直接支持药物发现流程的这一阶段，能够生成类命中分子并可能替代传统命中识别工作流程。

Abstract: Hit identification is a critical yet resource-intensive step in the drug discovery pipeline, traditionally relying on high-throughput screening of large compound libraries. Despite advancements in virtual screening, these methods remain time-consuming and costly. Recent progress in deep learning has enabled the development of generative models capable of learning complex molecular representations and generating novel compounds de novo. However, using ML to replace the entire drug-discovery pipeline is highly challenging. In this work, we rather investigate whether generative models can replace one step of the pipeline: hit-like molecule generation. To the best of our knowledge, this is the first study to explicitly frame hit-like molecule generation as a standalone task and empirically test whether generative models can directly support this stage of the drug discovery pipeline. Specifically, we investigate if such models can be trained to generate hit-like molecules, enabling direct incorporation into, or even substitution of, traditional hit identification workflows. We propose an evaluation framework tailored to this task, integrating physicochemical, structural, and bioactivity-related criteria within a multi-stage filtering pipeline that defines the hit-like chemical space. Two autoregressive and one diffusion-based generative models were benchmarked across various datasets and training settings, with outputs assessed using standard metrics and target-specific docking scores. Our results show that these models can generate valid, diverse, and biologically relevant compounds across multiple targets, with a few selected GSK-3$β$ hits synthesized and confirmed active in vitro. We also identify key limitations in current evaluation metrics and available training data.

</details>


### [51] [GQ-VAE: A gated quantized VAE for learning variable length tokens](https://arxiv.org/abs/2512.21913)
*Theo Datta,Kayla Huang,Sham Kakade,David Brandfonbrener*

Main category: cs.LG

TL;DR: GQ-VAE是一种新型神经分词器架构，可作为现有分词器的即插即用替代方案，通过变长离散标记编码提升压缩率和语言建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有前沿模型大多使用确定性频率分词算法（如BPE），而神经分词器设计通常增加模型复杂性并需要大幅架构改动，难以大规模实施。

Method: 提出门控量化变分自编码器（GQ-VAE），能够独立预训练作为现有分词器的即插即用替代，关键创新是学习编码变长离散标记。

Result: GQ-VAE相比标准VQ-VAE分词器提升了压缩率和语言建模性能，接近BPE的压缩率和语言建模性能；在压缩率相当时，GQ-VAE能改善下游语言模型学习。

Conclusion: GQ-VAE为现有分词器提供了有效的神经替代方案，并指出了多个未来研究方向。

Abstract: While most frontier models still use deterministic frequency-based tokenization algorithms such as byte-pair encoding (BPE), there has been significant recent work to design learned neural tokenizers. However, these schemes generally add to underlying language model complexity and force large changes to architecture, making them hard to implement at large scales. To overcome these challenges, we propose the gated quantized variational autoencoder (GQ-VAE), a novel architecture that can be independently pre-trained to serve as a drop-in replacement for existing tokenizers. The key innovation of the architecture is to learn to encode variable-length discrete tokens. GQ-VAE improves compression and language modeling performance over a standard VQ-VAE tokenizer, and approaches the compression rate and language modeling performance of BPE. Interestingly, if we use BPE with a smaller vocabulary, such that the compression is equivalent between GQ-VAE and BPE, we find that GQ-VAE improves downstream language model learning. We conclude with a discussion of several exciting avenues for future work. Code can be found at https://github.com/Theo-Datta-115/gq-vae.

</details>


### [52] [Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs](https://arxiv.org/abs/2512.21915)
*Yafeng Tang,Xiaoou Ding,Jianzhuo Du,Zishuo Yan,Zhuang Ma,Zheng Liang,Zekai Qian,Hongzhi Wang*

Main category: cs.LG

TL;DR: DATE框架通过数据分区和LLM生成，结合多臂老虎机算法平衡生成数据的多样性与质量，显著提升表格数据生成效果


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据具有异质性分布，现有生成模型难以获得适用于多样化数据的通用模型，需要解决数据生成中多样性与质量的权衡问题

Method: 1) 将异质数据分区为多个分布不同的子集；2) 利用LLM通过决策树推理探索分区分布的多样性，为每个子集生成高质量标注数据；3) 设计基于多臂老虎机的采样算法平衡生成数据的多样性与质量

Result: 在表格分类和回归基准测试中，DATE始终优于最先进的GAN和LLM方法，平均仅用100个生成数据就能降低23.75%的错误率，生成的数据还能提升DPO准确性和LLM推理能力

Conclusion: DATE框架通过有效处理数据异质性和平衡多样性与质量，为表格数据生成提供了创新解决方案，显著提升了生成数据的实用价值

Abstract: Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with diverse distributions, making it challenging to obtain a universally good model for diverse data generation. To address this limitation, we introduce Diversity-Aware Tabular data gEnerator (DATE), a framework that (i) prepares high-quality and distributionally distinct examples for in-context learning by effectively partitioning the original heterogeneous data into multiple diverse subsets; (ii) harnesses Large Language Models (LLMs) to explore the diversity of the partitioned distribution with decision tree reasoning as feedback, generating high-quality labeled data for each subset. However, the massive generated data inherently involves a trade-off between diversity and quality. To integrate this issue, existing solutions greedily select the validation-best data. However, we prove that the selection in heterogeneous settings does not possess the greedy-choice property, and design a Multi-Arm Bandit-based sampling algorithm that balances the diversity and quality of generated data. Extensive experiments on tabular classification and regression benchmarks demonstrate that DATE consistently outperforms state-of-the-art GAN-based and LLM-based methods. On average, DATE achieves a 23.75% reduction in error rate with just 100 generated data. Empirically, we demonstrate that data generated by DATE can improve the accuracy of Direct Preference Optimization (DPO) and enhance the reasoning capability of LLMs on the target data. Code is available at https://github.com/windblow32/DATE.

</details>


### [53] [Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms](https://arxiv.org/abs/2512.21925)
*Kongchang Zhou,Tingyu Zhang,Wei Chen,Fang Kong*

Main category: cs.LG

TL;DR: 提出混合CMAB-T框架，结合离线数据和在线交互来解决组合多臂老虎机问题，通过混合CUCB算法利用离线数据指导探索并加速收敛


<details>
  <summary>Details</summary>
Motivation: 现有CMAB-T研究主要分为在线和离线两种范式，各自存在局限性：在线算法交互成本高、适应慢；离线方法受限于数据集质量且缺乏探索能力。需要整合两者优势来解决互补弱点

Method: 提出混合CMAB-T框架，设计混合CUCB算法，利用离线数据指导探索和加速收敛，同时策略性地结合在线交互来弥补离线数据覆盖不足或分布偏差的问题

Result: 理论分析证明混合CUCB算法的遗憾界，当有高质量离线数据时显著优于纯在线方法，当数据有限或存在偏差时能有效纠正纯离线方法的偏差。实证结果进一步验证了算法的优势

Conclusion: 混合CMAB-T框架成功整合了离线数据和在线交互的优势，混合CUCB算法在理论和实证上都表现出优越性能，为解决组合多臂老虎机问题提供了新的有效途径

Abstract: The problem of combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) has been extensively studied. Prior work primarily focuses on either the online setting where an agent learns about the unknown environment through iterative interactions, or the offline setting where a policy is learned solely from logged data. However, each of these paradigms has inherent limitations: online algorithms suffer from high interaction costs and slow adaptation, while offline methods are constrained by dataset quality and lack of exploration capabilities. To address these complementary weaknesses, we propose hybrid CMAB-T, a new framework that integrates offline data with online interaction in a principled manner. Our proposed hybrid CUCB algorithm leverages offline data to guide exploration and accelerate convergence, while strategically incorporating online interactions to mitigate the insufficient coverage or distributional bias of the offline dataset. We provide theoretical guarantees on the algorithm's regret, demonstrating that hybrid CUCB significantly outperforms purely online approaches when high-quality offline data is available, and effectively corrects the bias inherent in offline-only methods when the data is limited or misaligned. Empirical results further demonstrate the consistent advantage of our algorithm.

</details>


### [54] [Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing](https://arxiv.org/abs/2512.22024)
*Wesley S. Leite,Rodrigo C. de Lamare,Yuriy Zakharov,Wei Liu,Martin Haardt*

Main category: cs.LG

TL;DR: 提出可变窗口尺寸空间平滑框架，提升稀疏线性阵列的协阵列DOA估计性能，通过压缩平滑孔径改进信号与噪声子空间分离


<details>
  <summary>Details</summary>
Motivation: 传统固定窗口协阵列MUSIC方法在处理稀疏线性阵列时性能有限，需要改进信号与噪声子空间分离，同时降低计算复杂度

Method: 提出可变窗口尺寸空间平滑框架，压缩平滑孔径，用未扰动的低秩附加项替换部分扰动的秩一外积，开发VWS-CA-MUSIC和VWS-CA-rMUSIC算法

Result: 仿真显示相对于固定窗口协阵列MUSIC方法，在稀疏几何结构下显著提升性能并降低复杂度，推导出保证可识别性的压缩参数界限

Conclusion: 可变窗口尺寸空间平滑框架有效提升协阵列DOA估计性能，在保持信号子空间张成的同时增强信号与噪声子空间分离，具有实际应用价值

Abstract: In this work, we introduce a variable window size (VWS) spatial smoothing framework that enhances coarray-based direction of arrival (DOA) estimation for sparse linear arrays. By compressing the smoothing aperture, the proposed VWS Coarray MUSIC (VWS-CA-MUSIC) and VWS Coarray root-MUSIC (VWS-CA-rMUSIC) algorithms replace part of the perturbed rank-one outer products in the smoothed coarray data with unperturbed low-rank additional terms, increasing the separation between signal and noise subspaces, while preserving the signal subspace span. We also derive the bounds that guarantees identifiability, by limiting the values that can be assumed by the compression parameter. Simulations with sparse geometries reveal significant performance improvements and complexity savings relative to the fixed-window coarray MUSIC method.

</details>


### [55] [Scaling Adversarial Training via Data Selection](https://arxiv.org/abs/2512.22069)
*Youran Ye,Dejin Wang,Ajinkya Bhandare*

Main category: cs.LG

TL;DR: 选择性对抗训练：通过仅扰动关键样本子集，在保持鲁棒性的同时将对抗计算减少50%


<details>
  <summary>Details</summary>
Motivation: PGD对抗训练的计算成本高昂，因为所有训练样本都经过相同的迭代优化，尽管它们对鲁棒性的贡献不均等。这种效率低下促使研究者提出选择性对抗训练方法。

Method: 提出选择性对抗训练，仅扰动每个小批量中的关键样本子集。引入两种选择标准：1) 基于间隔的采样，优先选择靠近决策边界的样本；2) 梯度匹配采样，选择梯度与主导批量优化方向对齐的样本。仅对选定子集生成对抗样本，其余样本使用混合目标进行干净训练。

Result: 在MNIST和CIFAR-10上的实验表明，该方法实现了与完整PGD对抗训练相当甚至更好的鲁棒性，同时将对抗计算减少高达50%。

Conclusion: 明智的样本选择足以实现可扩展的对抗鲁棒性，选择性对抗训练在保持鲁棒性的同时显著降低了计算成本。

Abstract: Projected Gradient Descent (PGD) is a strong and widely used first-order adversarial attack, yet its computational cost scales poorly, as all training samples undergo identical iterative inner-loop optimization despite contributing unequally to robustness. Motivated by this inefficiency, we propose \emph{Selective Adversarial Training}, which perturbs only a subset of critical samples in each minibatch. Specifically, we introduce two principled selection criteria: (1) margin-based sampling, which prioritizes samples near the decision boundary, and (2) gradient-matching sampling, which selects samples whose gradients align with the dominant batch optimization direction. Adversarial examples are generated only for the selected subset, while the remaining samples are trained cleanly using a mixed objective. Experiments on MNIST and CIFAR-10 show that the proposed methods achieve robustness comparable to, or even exceeding, full PGD adversarial training, while reducing adversarial computation by up to $50\%$, demonstrating that informed sample selection is sufficient for scalable adversarial robustness.

</details>


### [56] [Explainable Multimodal Regression via Information Decomposition](https://arxiv.org/abs/2512.22102)
*Zhaozhao Ma,Shujian Yu*

Main category: cs.LG

TL;DR: 提出基于偏信息分解的多模态回归框架，通过高斯性假设解析计算模态间独特、冗余和协同信息，提升预测准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有多模态回归方法缺乏量化各模态贡献及其交互作用的原理性工具，限制了多模态融合的可解释性

Method: 基于偏信息分解框架，通过高斯性假设解析计算独特、冗余和协同信息分量，引入条件独立性正则化器促进独特信息分离

Result: 在六个真实数据集（包括大规模脑年龄预测）上优于现有方法，在预测准确性和可解释性方面均有提升，支持高效推理的模态选择

Conclusion: 提出的PIDReg框架为多模态回归提供了原理性的可解释工具，能有效量化模态贡献并指导模态选择，在准确性和可解释性方面均优于现有方法

Abstract: Multimodal regression aims to predict a continuous target from heterogeneous input sources and typically relies on fusion strategies such as early or late fusion. However, existing methods lack principled tools to disentangle and quantify the individual contributions of each modality and their interactions, limiting the interpretability of multimodal fusion. We propose a novel multimodal regression framework grounded in Partial Information Decomposition (PID), which decomposes modality-specific representations into unique, redundant, and synergistic components. The basic PID framework is inherently underdetermined. To resolve this, we introduce inductive bias by enforcing Gaussianity in the joint distribution of latent representations and the transformed response variable (after inverse normal transformation), thereby enabling analytical computation of the PID terms. Additionally, we derive a closed-form conditional independence regularizer to promote the isolation of unique information within each modality. Experiments on six real-world datasets, including a case study on large-scale brain age prediction from multimodal neuroimaging data, demonstrate that our framework outperforms state-of-the-art methods in both predictive accuracy and interpretability, while also enabling informed modality selection for efficient inference. Implementation is available at https://github.com/zhaozhaoma/PIDReg.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration](https://arxiv.org/abs/2512.21360)
*Shuide Wen,Yu Sun,Beier Ku,Zhi Gao,Lijun Ma,Yang Yang,Can Jiao*

Main category: cs.AI

TL;DR: 本研究提出了一种基于多模态大语言模型和多智能体协作的HTP绘画测试自动分析框架，通过定量实验验证了其与人类专家解释的高度相似性，为投射性评估提供了标准化工具。


<details>
  <summary>Details</summary>
Motivation: 传统的HTP绘画测试存在评分标准不统一、依赖评估者主观经验、缺乏统一量化编码系统等问题，需要开发更客观、标准化的评估工具。

Method: 采用多模态大语言模型和多智能体协作框架，通过角色分工将特征识别与心理推理解耦，整合社会心理学视角和去污名化叙事。

Result: 定量实验显示MLLM解释与人类专家解释的平均语义相似度约为0.75（标准差约0.05），在结构化专家数据集中相似度提升至0.85；定性分析表明系统能有效纠正视觉幻觉，生成具有高生态效度和内部一致性的心理报告。

Conclusion: 多模态大模型有潜力成为投射性评估的标准化工具，提出的多智能体框架为数字心理健康服务提供了新范式。

Abstract: Background: The House-Tree-Person (HTP) drawing test, introduced by John Buck in 1948, remains a widely used projective technique in clinical psychology. However, it has long faced challenges such as heterogeneous scoring standards, reliance on examiners subjective experience, and a lack of a unified quantitative coding system.
  Results: Quantitative experiments showed that the mean semantic similarity between Multimodal Large Language Model (MLLM) interpretations and human expert interpretations was approximately 0.75 (standard deviation about 0.05). In structurally oriented expert data sets, this similarity rose to 0.85, indicating expert-level baseline comprehension. Qualitative analyses demonstrated that the multi-agent system, by integrating social-psychological perspectives and destigmatizing narratives, effectively corrected visual hallucinations and produced psychological reports with high ecological validity and internal coherence.
  Conclusions: The findings confirm the potential of multimodal large models as standardized tools for projective assessment. The proposed multi-agent framework, by dividing roles, decouples feature recognition from psychological inference and offers a new paradigm for digital mental-health services.
  Keywords: House-Tree-Person test; multimodal large language model; multi-agent collaboration; cosine similarity; computational psychology; artificial intelligence

</details>


### [58] [A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers](https://arxiv.org/abs/2512.21365)
*Chung-Chin Shih,Ti-Rong Wu,Ting Han Wei,Yu-Shan Hsu,Hung Guei,I-Chen Wu*

Main category: cs.AI

TL;DR: 该论文分析了使用当前最先进的计算机围棋求解器（基于相关区域搜索和相关区域模式表）解决围棋死活题的行为，发现求解器能够识别关键区域、发现罕见模式，甚至在某些问题上得出与标准答案不同的解法，但也存在对罕见模式价值误判和优先直接活棋而非最大化实地的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析当前最先进的计算机围棋求解器在解决死活题时的行为表现，特别是使用相关区域搜索（RZS）和相关区域模式表这两种技术，评估它们在解决经典死活题时的能力、发现模式的能力以及与人类解题思路的差异。

Method: 研究方法包括：1）使用基于相关区域搜索（RZS）和相关区域模式表的计算机围棋求解器；2）选取《死活辞典》中的七个经典死活题作为测试集；3）分析求解器识别的相关区域、发现的模式序列；4）将求解器的解法与书中标准答案进行对比分析。

Result: 研究结果发现：1）求解器能够为每个问题识别出关键的相关区域；2）求解器发现了一系列模式，包括一些罕见模式；3）在两个问题上，求解器得出了与书中标准答案不同的解法；4）同时发现求解器存在两个问题：对罕见模式的价值误判，以及倾向于优先直接活棋而非最大化实地（这与人类棋手行为不同）。

Conclusion: 结论是当前基于相关区域搜索的围棋求解器在解决死活题时表现出识别关键区域和发现模式的能力，甚至能提供新的解法，但仍存在对罕见模式价值评估不准确和策略偏好与人类不同的局限性，需要进一步改进。

Abstract: This paper analyzes the behavior of solving Life-and-Death (L&D) problems in the game of Go using current state-of-the-art computer Go solvers with two techniques: the Relevance-Zone Based Search (RZS) and the relevance-zone pattern table. We examined the solutions derived by relevance-zone based solvers on seven L&D problems from the renowned book "Life and Death Dictionary" written by Cho Chikun, a Go grandmaster, and found several interesting results. First, for each problem, the solvers identify a relevance-zone that highlights the critical areas for solving. Second, the solvers discover a series of patterns, including some that are rare. Finally, the solvers even find different answers compared to the given solutions for two problems. We also identified two issues with the solver: (a) it misjudges values of rare patterns, and (b) it tends to prioritize living directly rather than maximizing territory, which differs from the behavior of human Go players. We suggest possible approaches to address these issues in future work. Our code and data are available at https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ.

</details>


### [59] [Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design](https://arxiv.org/abs/2512.21623)
*Takahide Suzuki,Kazuki Nakanishi,Takashi Fujiwara,Hideyuki Shimizu*

Main category: cs.AI

TL;DR: OrchestRA是一个人类在环的多智能体平台，将生物学、化学和药理学统一为自主发现引擎，通过自主执行模拟和结果推理驱动迭代优化，将药物发现从随机搜索转变为可编程的循证工程学科。


<details>
  <summary>Details</summary>
Motivation: 当前治疗发现面临专业领域碎片化以及计算设计与生理验证之间的执行差距等挑战，现有生成AI模型通常作为被动助手而非自主执行者，需要更强大的自主发现平台。

Method: OrchestRA采用人类在环的多智能体架构，包含：1）Orchestrator协调器；2）Biologist Agent利用大规模知识图谱（>1000万关联）进行深度推理识别高置信度靶点；3）Chemist Agent自主检测结构口袋进行从头设计或药物重定位；4）Pharmacologist Agent通过基于生理的药代动力学（PBPK）模拟评估候选药物，建立动态反馈循环。

Result: 平台建立了动态反馈循环，药代动力学和毒性特征直接触发结构重新优化，实现了自主执行与人类指导的无缝集成，使治疗设计民主化。

Conclusion: OrchestRA将药物发现从随机搜索转变为可编程的循证工程学科，通过统一生物学、化学和药理学，建立了一个自主发现引擎，解决了当前治疗发现中的关键挑战。

Abstract: Therapeutic discovery remains a formidable challenge, impeded by the fragmentation of specialized domains and the execution gap between computational design and physiological validation. Although generative AI offers promise, current models often function as passive assistants rather than as autonomous executors. Here, we introduce OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine. Unlike static code generators, our agents actively execute simulations and reason the results to drive iterative optimization. Governed by an Orchestrator, a Biologist Agent leverages deep reasoning over a massive knowledge graph (>10 million associations) to pinpoint high-confidence targets; a Chemist Agent autonomously detects structural pockets for de novo design or drug repositioning; and a Pharmacologist Agent evaluates candidates via rigorous physiologically based pharmacokinetic (PBPK) simulations. This architecture establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization. By seamlessly integrating autonomous execution with human guidance, OrchestRA democratizes therapeutic design, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.

</details>


### [60] [Three-way decision with incomplete information based on similarity and satisfiability](https://arxiv.org/abs/2512.21421)
*Junfang Luo,Mengjun Hu,Keyun Qin*

Main category: cs.AI

TL;DR: 本文回顾了基于粗糙集理论的三支决策在完整信息下的两种表述（计算型和概念型），并将其推广到更实用的不完备信息场景，提出了相似度度量、α-相似类、可逼近性等新概念。


<details>
  <summary>Details</summary>
Motivation: 现有三支决策方法主要处理完备信息，但在实际应用中不完备信息更为常见。需要将完备信息下的两种表述（计算型和概念型）推广到不完备信息场景，以增强方法的实用性和适用性。

Method: 1. 计算型表述：提出对象相似度度量作为等价关系的推广，基于此讨论使用α-相似类和对象可逼近性的两种三支决策方法。
2. 概念型表述：提出公式满足度度量作为完备信息下满足关系的量化推广，基于此研究使用公式α-意义集和公式置信度的两种三支决策方法。

Result: 成功将三支决策从完备信息推广到不完备信息场景。提出了相似度度量和满足度度量等新概念，并建立了基于α-相似类、可逼近性、α-意义集和置信度的四种三支决策方法。其中可逼近性概念和概念型表述的两种方法为不完备信息分析指出了新的研究方向。

Conclusion: 本文系统地将三支决策从完备信息推广到不完备信息，提出了多种新概念和方法。虽然相似类分析是文献中常见方法，但提出的可逼近性概念以及概念型表述的两种方法为不完备信息处理开辟了有前景的新方向，增强了三支决策在实际应用中的实用性。

Abstract: Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a briefly review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using alpha-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using alpha-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.

</details>


### [61] [LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis](https://arxiv.org/abs/2512.21482)
*Fanwei Zeng,Changtao Miao,Jing Huang,Zhiya Tan,Shutao Gong,Xiaoming Yu,Yang Wang,Huazhe Tan,Weibin Yao,Jianshu Li*

Main category: cs.AI

TL;DR: LogicLens是一个统一的视觉-文本协同推理框架，用于分析文本中心伪造图像，通过交叉线索感知的思维链机制实现深度推理，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前文本中心伪造分析方法存在三个主要问题：1）仅限于粗粒度的视觉分析，缺乏深度推理能力；2）将检测、定位和解释视为独立子任务，忽略了它们之间的内在联系；3）缺乏高质量、细粒度的标注数据集。

Method: 提出LogicLens统一框架，包含：1）交叉线索感知思维链（CCT）机制，迭代交叉验证视觉线索与文本逻辑；2）加权多任务奖励函数用于GRPO优化；3）PR²（感知器、推理器、审阅器）流水线生成高质量标注；4）构建RealText数据集（5,397张图像）。

Result: 在T-IC13的零样本评估中，LogicLens比专用框架高出41.4%，比GPT-4o高出23.4%（宏平均F1分数）。在密集文本T-SROIE数据集上，在mF1、CSS和宏平均F1指标上显著领先其他MLLM方法。

Conclusion: LogicLens通过统一的视觉-文本协同推理框架，有效解决了文本中心伪造分析的现有局限性，在检测、定位和解释任务上实现了显著性能提升，为伪造分析提供了新的解决方案。

Abstract: Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.

</details>


### [62] [Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model](https://arxiv.org/abs/2512.21540)
*Yanhao Li,Lu Ma,Jiaran Zhang,Lexiang Tang,Wentao Zhang,Guibo Luo*

Main category: cs.AI

TL;DR: Leash是一种自适应长度惩罚的强化学习框架，通过拉格朗日对偶方法动态调整长度惩罚系数，在保持任务性能的同时显著减少LLM推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖固定长度惩罚，但这种方法难以调优且无法适应LLM推理能力的演变，导致准确性和简洁性之间的次优权衡。

Method: 将长度控制建模为约束优化问题，采用拉格朗日原始对偶方法动态调整惩罚系数：当生成超过目标长度时增强惩罚，当生成较短时放松惩罚。

Result: 在Deepseek-R1-Distill-Qwen-1.5B和Qwen3-4B-Thinking-2507上的实验表明，Leash在多样化任务中平均减少60%的推理长度，包括分布内数学推理和分布外编码、指令遵循等任务，同时保持竞争力性能。

Conclusion: Leash为开发可控且高效的大语言模型提供了一个实用有效的范式，能够在推理能力和计算预算之间取得平衡。

Abstract: Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.

</details>


### [63] [A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning](https://arxiv.org/abs/2512.21583)
*Zelin Zang,Wenyi Gu,Siqi Ma,Dan Yang,Yue Shen,Zhu Zhang,Guohui Fan,Wing-Kuen Ling,Fuji Yang*

Main category: cs.AI

TL;DR: 基于LLaVA构建的诊断框架，结合视觉语言对齐与逻辑正则化推理，提升医疗多模态AI的可靠性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态模型常产生幻觉或不一致的推理链，限制了临床信任，需要更可靠的诊断框架

Method: 构建基于LLaVA的诊断框架，包含文本图像输入编码器、跨模态对齐投影模块、任务分解推理控制器和逻辑树生成器

Result: 在MedXpertQA等基准测试中，该方法提高了诊断准确性，产生了更可解释的推理轨迹，同时在纯文本任务中保持竞争力

Conclusion: 该方法为可信赖的多模态医疗AI迈出了有希望的一步

Abstract: With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.

</details>


### [64] [Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing](https://arxiv.org/abs/2512.21626)
*Hong Xie,Haoran Gu,Yanying Huang,Tao Tan,Defu Lian*

Main category: cs.AI

TL;DR: 本文提出了一种针对LLM应用、边缘智能等资源分配问题的多臂赌博机变体，设计了优先资源共享机制下的算法，并证明了匹配下界的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 针对LLM应用、边缘智能等场景中的资源分配问题，传统多臂赌博机模型无法处理具有优先级的资源竞争，需要新的理论框架来优化这种优先资源共享机制下的决策。

Method: 提出MSB-PRS-OffOpt算法，通过O(MK^3)复杂度找到最优分配策略；以此为基础设计近似UCB算法，处理优先资源共享机制产生的特殊非线性组合效用函数。

Result: 证明了实例独立下界Ω(α₁σ√KMT)和实例依赖下界Ω(α₁σ²(M/Δ)lnT)；设计的算法实现了匹配下界的遗憾上界，分别相差√(K ln KT)和α₁K²因子。

Conclusion: 该研究为优先资源共享机制下的资源分配问题提供了理论框架和高效算法，在LLM应用、边缘智能等场景中具有实际应用价值。

Abstract: This paper proposes a variant of multiple-play stochastic bandits tailored to resource allocation problems arising from LLM applications, edge intelligence, etc. The model is composed of $M$ arms and $K$ plays. Each arm has a stochastic number of capacities, and each unit of capacity is associated with a reward function. Each play is associated with a priority weight. When multiple plays compete for the arm capacity, the arm capacity is allocated in a larger priority weight first manner. Instance independent and instance dependent regret lower bounds of $Ω( α_1 σ\sqrt{KM T} )$ and $Ω(α_1 σ^2 \frac{M}Δ \ln T)$ are proved, where $α_1$ is the largest priority weight and $σ$ characterizes the reward tail. When model parameters are given, we design an algorithm named \texttt{MSB-PRS-OffOpt} to locate the optimal play allocation policy with a computational complexity of $O(MK^3)$. Utilizing \texttt{MSB-PRS-OffOpt} as a subroutine, an approximate upper confidence bound (UCB) based algorithm is designed, which has instance independent and instance dependent regret upper bounds matching the corresponding lower bound up to factors of $ \sqrt{K \ln KT }$ and $α_1 K^2$ respectively. To this end, we address nontrivial technical challenges arising from optimizing and learning under a special nonlinear combinatorial utility function induced by the prioritized resource sharing mechanism.

</details>


### [65] [Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets](https://arxiv.org/abs/2512.21775)
*Matyas Bohacek,Ignacio Vilanova Echavarri*

Main category: cs.AI

TL;DR: 提出合规评级方案(CRS)框架，用于评估生成式AI数据集在透明度、问责制和安全性方面的合规性，并发布开源Python库实现该框架


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展依赖于大规模开源数据集，但这些数据集通常采用不受限制和不透明的数据收集方式。现有文献多关注GAI模型的开发和应用，而忽视了数据集创建的伦理和法律考量。此外，数据集在共享、编辑和复制的过程中，其来源、合法性和安全性信息常常丢失。

Method: 引入合规评级方案(CRS)框架，基于数据溯源技术开发开源Python库，该框架评估数据集在透明度、问责制和安全性关键原则方面的合规性。该库可无缝集成到现有数据集处理和AI训练流程中。

Result: 开发了一个同时具有反应性和主动性的工具：既能评估现有数据集的CRS，又能指导负责任的新数据集抓取和构建。

Conclusion: CRS框架和开源库填补了生成式AI数据集伦理和法律评估的空白，为数据集合规性提供了系统化的评估工具，有助于促进更负责任的数据集创建和使用实践。

Abstract: Generative Artificial Intelligence (GAI) has experienced exponential growth in recent years, partly facilitated by the abundance of large-scale open-source datasets. These datasets are often built using unrestricted and opaque data collection practices. While most literature focuses on the development and applications of GAI models, the ethical and legal considerations surrounding the creation of these datasets are often neglected. In addition, as datasets are shared, edited, and further reproduced online, information about their origin, legitimacy, and safety often gets lost. To address this gap, we introduce the Compliance Rating Scheme (CRS), a framework designed to evaluate dataset compliance with critical transparency, accountability, and security principles. We also release an open-source Python library built around data provenance technology to implement this framework, allowing for seamless integration into existing dataset-processing and AI training pipelines. The library is simultaneously reactive and proactive, as in addition to evaluating the CRS of existing datasets, it equally informs responsible scraping and construction of new datasets.

</details>
