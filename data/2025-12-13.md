<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 使用计算文本分析研究《霍比特人》对话的情感基调，发现整体保持积极平静的语调，随着故事进展角色能动性逐渐增强，体现了小说在紧张与舒适之间的情感节奏循环。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过计算文本分析方法揭示文学作品中的情感结构，特别是分析托尔金《霍比特人》中对话的情感基调及其与叙事节奏的关系。

Method: 使用正则表达式提取对话文本，进行预处理后，采用NRC-VAD情感词典对情感维度进行量化分析，包括情感轨迹图和词云等可视化方法。

Result: 对话整体保持积极（高愉悦度）和冷静（低唤醒度）的基调，随着故事进展角色能动性（支配度）逐渐增强，体现了危险与兴奋时刻与幽默、友情和慰藉之间的平衡。

Conclusion: 计算工具与文学解读相结合能够揭示文学作品中的微妙情感结构，展示了《霍比特人》中塑造叙事的稳定节奏和情感调节机制。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [2] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 该研究评估了多模态大语言模型在视频情绪分析中的表现，发现在理想条件下模型表现良好，但在真实议会辩论场景中效果不佳，强调了持续评估AI方法在政治分析中应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 情感在政治中至关重要，虽然多模态生成AI在情绪分析方面前景广阔，但缺乏关于其在情绪分析中有效性的证据。本研究旨在填补这一空白。

Method: 通过评估当前多模态大语言模型在两个互补的人类标注视频数据集中的表现，分析其在视频情绪唤起分析中的可靠性。

Result: 在理想条件下，mLLMs的情绪唤起评分高度可靠且几乎没有人口统计学偏见；但在真实议会辩论录音中，其情绪唤起评分未能达到预期效果，可能对下游统计推断产生负面影响。

Conclusion: 研究强调了在政治分析中持续、彻底评估新兴生成AI方法的必要性，并提供了一个可复制的评估框架。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 提出一种无需额外训练的方法，让具备推理能力的LLM能够同时思考、听取和生成输出，显著减少实时交互延迟


<details>
  <summary>Details</summary>
Motivation: 现有LLM的推理过程需要先思考再回答，这种顺序交互模式不适用于需要实时响应的场景（如语音助手），而人类能够异步地听、想、说

Method: 利用旋转嵌入（rotary embeddings）的特性，使原本设计用于顺序交互的LLM能够同时进行思考、听取和生成输出

Result: 在数学推理、常识推理和安全推理任务上，能够实时生成准确的思考增强答案，将首个非思考token的生成时间从几分钟减少到≤5秒，整体实时延迟降低6-11倍

Conclusion: 通过利用旋转嵌入的特性，成功实现了LLM的异步推理能力，使其能够像人类一样同时处理输入、思考和输出，显著提升了实时交互性能

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [4] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的点状函数Derf，它基于高斯累积分布函数，在多种任务中超越了传统的归一化层和Dynamic Tanh，成为归一化自由Transformer架构的实用选择。


<details>
  <summary>Details</summary>
Motivation: 尽管归一化层长期以来被视为深度学习架构不可或缺的组成部分，但最近引入的Dynamic Tanh（DyT）表明存在替代方案。DyT通过约束极端值实现稳定收敛并达到归一化级别的性能，本研究旨在寻找能够超越DyT的函数设计。

Method: 首先研究点状函数的内在特性如何影响训练和性能，基于这些发现进行大规模搜索以寻找更有效的函数设计。通过探索，引入了Derf函数：Derf(x) = erf(αx + s)，其中erf(x)是重新缩放的高斯累积分布函数，并确定其为性能最佳的设计。

Result: Derf在多种领域中超越了LayerNorm、RMSNorm和DyT，包括视觉（图像识别和生成）、语音表示和DNA序列建模。Derf的性能提升主要源于其改进的泛化能力而非更强的拟合能力。

Conclusion: Derf的简单性和更强的性能使其成为归一化自由Transformer架构的实用选择，为深度学习架构设计提供了新的方向。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [5] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 线性模型在仅使用历史室内温度数据进行长期预测的任务中，比复杂的Transformer架构表现更好，DLinear模型在所有数据集上取得最佳准确率。


<details>
  <summary>Details</summary>
Motivation: 研究仅使用历史室内温度数据进行长期预测这一具有挑战性的单变量场景，比较线性模型和Transformer家族模型在这种外生变量限制条件下的表现。

Method: 使用标准化的训练、验证和测试划分，评估Linear、NLinear、DLinear、Transformer、Informer和Autoformer等模型在长期外生温度预测任务中的性能。

Result: 线性基线模型（Linear、NLinear、DLinear）在所有划分中始终优于更复杂的Transformer家族架构，其中DLinear在所有数据集上取得了最佳的整体准确率。

Conclusion: 精心设计的线性模型在具有挑战性的仅外生变量设置中仍然是时间序列预测的强大基线，表明在某些场景下简单模型可能优于复杂架构。

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [6] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: 提出GTL方法，无需微调预训练离散扩散模型即可适应新领域，通过引导采样实现高效迁移学习


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言等领域表现优异，但需要大量训练数据，迁移到新领域成本高且风险大。现有迁移学习方法需要微调大型扩散模型，计算成本高且不实用。

Method: 提出GTL（引导迁移学习）方法，基于连续扩散的比率迁移学习思想，无需修改预训练去噪器即可从目标分布采样。提供统一处理离散时间扩散和连续时间基于分数的离散扩散的引导公式。进一步提出高效引导采样器，通过规划器选择位置和候选词来减少计算量。

Result: GTL使大规模语言建模在大型词汇表和长序列上变得实用。在序列数据（包括合成马尔可夫链和语言建模）上进行了评估，并提供了其行为的实证分析。

Conclusion: GTL为离散扩散模型提供了一种无需微调的高效迁移学习方法，解决了大规模词汇表和长序列场景下的计算效率问题，使引导语言建模在实际应用中变得可行。

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [7] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型的高效剪枝方法，通过行级等稀疏约束和1-swap优化算法，显著降低了剪枝误差并提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法在大语言模型上存在局限性：完全重训练成本过高，全局幅度剪枝在Transformer架构上效果不佳，现有方法要么使用整数规划求解计算不可行，要么依赖近似启发式方法。需要一种在大语言模型规模下更易处理的高效剪枝算法。

Method: 提出了一种基于1-swap优化的剪枝方法：1) 通过强制每行具有相同稀疏度来解耦行间依赖；2) 利用校准数据的Gram矩阵高效计算最优的1-swap操作（交换一个保留权重和一个剪枝权重）；3) 从任意剪枝掩码开始，在GPU上高效运行，基本无需超参数调整。

Result: 相比Wanda方法，该方法将每层剪枝误差降低了高达60%，在多种GPT架构上持续改善了困惑度和零样本准确率，证明了其有效性。

Conclusion: 该方法通过行级等稀疏约束和高效的1-swap优化，成功解决了大语言模型剪枝的可扩展性问题，提供了一种计算可行、性能优越的剪枝解决方案。

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [8] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 本文提出基于PPO算法的深度强化学习方法，用于水下机器人BlueROV2的自主导航，在复杂环境中优于传统DWA方法，并验证了从仿真到实物的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 水下环境自主导航面临GPS缺失、能见度差、存在障碍物等挑战，传统方法在复杂环境中表现有限，需要更智能的导航解决方案。

Method: 使用PPO深度强化学习算法，结合目标导向导航信息、虚拟占用网格和操作区域边界的射线投射作为观测空间，在仿真环境中训练策略。

Result: PPO策略在高度杂乱环境中持续优于DWA方法，具有更好的局部适应性和更少的碰撞，并通过3D数字孪生验证了从仿真到实物平台的可迁移性。

Conclusion: 深度强化学习在水下机器人自主导航中具有实际应用价值，PPO算法在复杂环境中表现出色，证明了仿真到实物迁移的可行性。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [9] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种解耦评论家块长度与策略块长度的方法，通过优化策略对抗部分动作块的蒸馏评论家，在保持多步价值传播优势的同时避免了长动作块策略学习的困难。


<details>
  <summary>Details</summary>
Motivation: 时间差分方法通过自举机制高效学习状态和动作价值，但容易产生自举偏差。现有块评论家方法虽然加速了价值备份，但策略必须开环输出整个动作块，这在需要策略反应性的环境中可能次优，且长动作块建模困难。

Method: 将评论家的块长度与策略的块长度解耦，允许策略在较短的动作块上操作。通过从原始块评论家乐观回溯构建部分动作块的蒸馏评论家，近似部分动作块扩展为完整块时可达到的最大价值。

Result: 在具有挑战性的长时域离线目标条件任务上评估，该方法可靠地优于先前方法。

Conclusion: 该方法保留了多步价值传播的优势，同时避免了开环次优性和学习长动作块策略的困难，在复杂任务中表现出更好的性能。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [10] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: DaSH方法通过层次化建模数据集和组级效用，在资源约束下选择整个数据集以提升下游性能，相比现有方法在准确率上提升高达26.2%


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据通常以离散数据集形式组织，不同数据集在相关性、质量和效用上存在差异。现有方法通常选择单个样本并假设所有数据同等相关，忽略了数据集及其来源之间的差异，因此需要专门的数据集选择方法。

Method: 提出DaSH（Dataset Selection via Hierarchies）方法，在数据集和组（如集合、机构）两个层次上建模效用，通过层次化建模实现从有限观测中的高效泛化。

Result: 在两个公共基准测试（Digit-Five和DomainNet）上，DaSH比最先进的数据选择基线方法在准确率上提升高达26.2%，同时需要显著更少的探索步骤。消融实验表明DaSH对低资源设置和相关数据集缺乏的情况具有鲁棒性。

Conclusion: DaSH方法适用于实际多源学习工作流中的可扩展和自适应数据集选择，能够有效处理异构数据集池中的数据集选择问题。

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [11] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: BiFlow框架通过移除精确解析逆变换的需求，使用近似逆映射模型来加速采样并提高生成质量，在ImageNet上实现比因果解码快两个数量级的采样速度。


<details>
  <summary>Details</summary>
Motivation: 传统标准化流需要精确的解析逆变换，这限制了模型架构和损失函数的灵活性。TARFlow等基于Transformer的自回归流方法虽然复兴了NFs，但因果解码成为主要瓶颈，需要更高效的采样方法。

Method: 提出双向标准化流(BiFlow)框架，学习一个近似噪声到数据的逆映射模型，而不是依赖精确的解析逆变换。这使得可以使用更灵活的损失函数和架构设计。

Result: 在ImageNet上的实验表明，BiFlow相比因果解码方法，在提高生成质量的同时，采样速度加快了最多两个数量级。在基于NF的方法中达到最先进水平，在单次评估方法中具有竞争力。

Conclusion: BiFlow通过移除精确解析逆变换的限制，为标准化流提供了更灵活和高效的框架，有望进一步推动这一经典范式的发展。

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs可用于大型用户设施的提案评审，通过成对偏好方法提供比传统人工评分更一致、可扩展且成本效益高的替代方案，在排名和识别高发表潜力提案方面表现不亚于人类评审员。


<details>
  <summary>Details</summary>
Motivation: 传统人工提案评审存在提案间相关性弱、评审员偏见和不一致性问题。成对偏好方法在逻辑上更优越，但二次方工作量使其对人类评审员不切实际。需要寻找可扩展、一致且成本效益高的替代方案。

Method: 使用大型语言模型（LLMs）进行提案评审，采用成对偏好方法。利用橡树岭国家实验室散裂中子源三个束线的精心策划提案和发表记录，比较LLM排名与人类排名的相关性，并评估识别高发表潜力提案的能力。

Result: LLM排名与人类排名强相关（Spearman ρ≈0.2-0.8，去除10%异常值后≥0.5）。在识别高发表潜力提案方面，LLM表现不亚于人类评审员，同时成本降低两个数量级以上。LLM还能进行提案相似性量化分析等人类难以完成的高级分析。

Conclusion: LLMs为大型用户设施的提案选择提供了可扩展、一致且成本效益高的替代方案，不仅能有效排名提案，还能进行高级分析，为评审委员会提供关键信息。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [13] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出一种节点级剪枝框架用于电路发现，解决现有方法计算成本高和粒度粗的问题，通过多粒度可学习掩码和稀疏性惩罚实现单次微调中的全面压缩。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法主要依赖迭代边剪枝，计算成本高且局限于粗粒度单元（如注意力头或MLP块），忽略了神经元级别的细粒度结构。

Method: 提出节点级剪枝框架，引入跨多个粒度级别的可学习掩码（从整个块到单个神经元），在统一优化目标中使用粒度特定的稀疏性惩罚指导剪枝过程。

Result: 该方法发现的电路节点数少于先前方法，证明许多粗粒度方法认为重要的神经元实际上无关紧要，同时保持任务性能，内存占用降低5-10倍。

Conclusion: 提出的节点级剪枝框架在可扩展性和粒度方面优于现有电路发现方法，能够识别更精细的电路结构，同时显著降低计算和内存需求。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>
