<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 46]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: ACoRN是一种针对检索增强生成中抽象压缩的鲁棒性训练方法，通过数据增强和微调提升压缩模型在噪声文档环境下的性能


<details>
  <summary>Details</summary>
Motivation: 检索到的文档常包含与查询无关或事实错误的内容，尽管相关性评分高。抽象压缩器在长上下文中容易遗漏重要信息，存在注意力分散问题，需要提升对检索噪声的鲁棒性

Method: 提出ACoRN方法：1）对训练数据进行离线数据增强，增强压缩器对两种检索噪声的鲁棒性；2）通过微调使压缩器能围绕支持正确答案的关键信息生成摘要，克服多文档信息利用不足和位置偏差问题

Result: 使用ACoRN训练的T5-large压缩器在EM和F1分数上均有提升，同时能保留作为直接证据的答案字符串。在包含大量降低准确性文档的数据集上表现优异

Conclusion: ACoRN通过细粒度文档分类和针对性训练，有效解决了抽象压缩器在噪声检索环境下的信息遗漏问题，在实际场景中具有高度实用性

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [2] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种针对性的强化学习框架，旨在减少大型语言模型在短答案和长答案问答中的内在和外在幻觉，同时通过奖励模型拒绝回答不可回答问题来增强可靠性。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习解锁了大型语言模型的复杂推理能力，但也加剧了它们的幻觉倾向，在能力和可靠性之间形成了关键权衡。本研究旨在解决这一挑战，平衡高级推理和事实可信度之间的紧张关系。

Method: 1. 针对外在幻觉（内部知识缺陷）：从TriviaQA的开放式转换创建新颖的训练集
2. 针对内在幻觉（对上下文不忠实）：利用FineWeb的长文本进行事实基础奖励方案
3. 增强可靠性：明确奖励模型拒绝回答不可回答的问题，培养谨慎性

Result: 广泛的实验表明，该方法在多样化的基准测试套件中取得了显著的性能提升，大幅减少了两种类型的幻觉。

Conclusion: 本研究提供了一个实用的框架，用于解决高级推理和事实可信度之间的关键紧张关系，为开发更强大、更可靠的大型语言模型铺平了道路。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [3] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 该研究提出了一种知识引导的大型语言模型（KG-LLM），用于儿科牙科抗生素处方推荐，通过整合知识图谱、检索增强生成和多阶段安全验证，显著提升了临床记录理解和用药安全性。


<details>
  <summary>Details</summary>
Motivation: 儿科牙科临床记录解释和抗生素安全处方是牙科信息学中的持续挑战。传统的基于规则的临床决策支持系统难以处理非结构化的牙科叙述、不完整的放射学描述和复杂的安全约束。

Method: 提出KG-LLM框架：1）使用临床NER/RE模块从牙科记录和放射报告中提取结构化实体和关系；2）从知识图谱中检索相关指南、药物安全规则和历史案例；3）通过检索增强生成（RAG）为LLM提供信息进行诊断总结和剂量-药物-疗程预测；4）采用双层验证机制（确定性规则检查+学习分类器）确保安全性。

Result: 在32,000份去标识化的儿科牙科就诊记录上测试：相比领域适应的Llama-2基线，KG-LLM在记录理解性能（F1: 0.914 vs. 0.867）、药物-剂量-疗程准确性（Top-1: 0.782 vs. 0.716）方面均有提升，并将不安全的抗生素建议减少了50%。消融分析显示知识图谱、RAG和安全模块都对系统性能有重要贡献。

Conclusion: KG-LLM框架通过整合结构化知识、检索增强生成和多层安全验证，有效解决了儿科牙科抗生素处方的临床决策支持问题，提高了系统的可靠性、准确性和安全性。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [4] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文提出两种轻量级可解释性指标（PRD和SAS）来分析LLMs在图检索增强生成中如何处理结构化知识，并开发了基于这些指标的幻觉检测器GGA。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理基于知识图谱的检索增强生成时，难以理解输入中的关系和拓扑信息，导致与检索知识不一致的幻觉问题。需要分析LLMs如何关注和保留结构化知识。

Method: 提出两种可解释性指标：1) 路径依赖度（PRD）衡量对最短路径三元组的过度依赖；2) 语义对齐分数（SAS）评估模型内部表示与检索知识的对齐程度。基于这些指标开发了后处理幻觉检测器GGA。

Result: 在基于知识的QA任务上，识别出与高PRD和低SAS相关的失败模式。GGA幻觉检测器在AUC和F1分数上优于基于语义和置信度的基线方法。

Conclusion: 通过将幻觉分析建立在机制可解释性基础上，揭示了LLMs结构限制如何导致幻觉，为未来设计更可靠的GraphRAG系统提供了见解。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [5] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: 该研究开发了MindShift基准测试，使用MMPI心理测量工具评估大语言模型模拟人类人格特质的能力，发现模型在角色感知方面有持续改进，但不同模型家族在心理评估响应上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大语言模型吸收和反映用户指定人格特质和态度的潜力，使用可靠的心理测量工具来评估模型模拟人类人格的能力。

Method: 方法包括：1) 改编心理学文献中最广泛研究的MMPI测试；2) 创建人格导向的提示，设计不同特质强度的人物角色；3) 开发MindShift基准测试来评估LLMs的心理适应性。

Result: 结果显示：1) LLMs在角色感知方面有持续改进，归因于训练数据集和对齐技术的进步；2) 不同模型类型和家族在心理评估响应上存在显著差异；3) 模型模拟人类人格特质的能力存在变异性。

Conclusion: 结论是LLMs确实能够吸收和反映人格特质，但能力存在模型差异。研究提供了MindShift基准测试来评估模型的心理适应性，相关提示和代码将公开可用。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [6] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: CORE提出概念优先的交互层，通过持久化本地概念状态和认知操作符，减少多轮对话中的历史重放，提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在单轮生成中表现良好，但在多轮交互中需要从不断增长的token历史中重建用户意图和任务状态，导致漂移、不一致的推理模式和不断增长的提示。

Method: CORE结合小型通用认知操作符库和持久化本地概念状态（捕获任务、约束、偏好和中间结果的紧凑语义状态），每个模型调用只接收概念状态、用户最新指令和选定操作符，无需重放完整历史。

Result: 初步原型模拟显示累计提示token减少约42%（但这是原型条件下的结果，不应视为实际性能估计）。

Conclusion: CORE提供了一种模型无关的机制，将概念推理与语言生成分离，为更稳定的多轮系统提供了可扩展的方向。

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [7] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出TCA-Attention：一种无需训练、上下文自适应的稀疏注意力机制，通过选择性关注信息量大的token来提升长上下文推理效率，在128K上下文长度下实现2.8倍加速和61%的KV缓存减少。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度在处理长序列时带来显著的计算和内存挑战。现有的稀疏注意力和KV缓存压缩方法存在依赖固定模式、无法同时处理预填充和解码阶段、或需要额外训练等局限性。

Method: 提出TCA-Attention，包含两个轻量级阶段：1) 离线校准阶段通过单次前向传播确定头特定的稀疏预算；2) 在线token选择阶段使用轻量级冗余度量自适应保留核心上下文token。该方法无需参数更新或架构改变。

Result: 在128K上下文长度下实现2.8倍加速和61%的KV缓存减少，同时在各种基准测试中保持与完整注意力相当的性能。理论分析表明该方法保持有界近似误差。

Conclusion: TCA-Attention为高效长上下文推理提供了一个实用的即插即用解决方案，统一加速预填充和解码阶段，同时减少KV缓存内存占用，无需训练或架构修改。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [8] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 研究发现英文机器生成文本检测系统存在对性别、种族/民族、英语学习者身份和经济地位等属性的偏见，其中弱势群体（如英语学习者）的文本更可能被误判为机器生成，而人类标注者在检测任务中表现较差但无明显偏见。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成能力的快速发展，机器生成文本检测技术也受到广泛关注。虽然检测模型表现出色，但它们可能带来显著的负面影响。本研究旨在探索英文机器生成文本检测系统中潜在的偏见问题。

Method: 研究收集了学生论文数据集，评估了16个不同的检测系统在四个属性上的偏见：性别、种族/民族、英语学习者身份和经济地位。使用回归模型评估效应的显著性和强度，并进行亚组分析。同时进行了人类标注实验作为对比。

Result: 研究发现：1）偏见在不同系统中普遍存在但不一致；2）多个模型倾向于将弱势群体的文本分类为机器生成；3）英语学习者的论文更可能被分类为机器生成；4）经济弱势学生的论文较少被分类为机器生成；5）非白人英语学习者的论文相对于白人英语学习者被不成比例地分类为机器生成。人类标注者在检测任务中表现较差但无明显偏见。

Conclusion: 机器生成文本检测系统存在系统性偏见，特别是在对待弱势群体（如英语学习者）时。这些偏见可能导致不公平的结果，而人类虽然检测能力有限，但相对更公平。研究强调了在部署这些系统时需要考虑公平性和偏见问题。

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [9] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: CONCUR是一个持续路由框架，支持有约束和无约束路由，通过模块化设计和多表征学习实现高效的任务到计算策略的映射。


<details>
  <summary>Details</summary>
Motivation: AI任务复杂度不同，需要不同的计算策略（模型和解码方法组合）。现有路由系统通常训练单一模型覆盖所有策略，当新策略出现时需要完全重新训练，成本高。现有方法也通常使用单一输入表征，无法充分捕捉路由问题的复杂性，导致次优路由决策。

Method: 提出CONCUR框架：1）模块化设计，为每个策略训练独立的预测器模型，支持新策略的无缝加入且训练成本低；2）利用任务和计算策略的多种表征来更好地捕捉整体问题复杂性；3）支持有约束和无约束路由（即有无预算限制）。

Result: 在分布内和分布外、知识和推理密集型任务上的实验表明，CONCUR在持续和非持续设置下都优于最佳单一策略和现有路由技术，具有更高的端到端准确率和更低的推理成本，同时在持续设置下降低了训练成本。

Conclusion: CONCUR通过模块化设计和多表征学习解决了持续路由中的泛化问题，实现了高效、低成本的任务路由，在准确性和成本方面都优于现有方法。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [10] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 语言模型可作为研究工具，探索自然语言的可能与不可能界限，揭示人类语言学习的归纳偏置


<details>
  <summary>Details</summary>
Motivation: 利用语言模型作为调查工具，探究可能语言与不可能语言之间的区别，从而揭示支持人类语言学习的归纳偏置

Method: 提出分阶段研究计划，通过迭代改进语言模型架构，使其能更好地区分可能语言与不可能语言

Result: 语言模型具有作为研究工具的潜力，可用于建立与人类认知的关联假设

Conclusion: 语言模型可作为有效的调查工具，通过区分可能/不可能语言来探索人类语言学习的认知机制

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [11] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: CourtPressGER数据集：德国最高法院官方新闻稿数据集，用于训练和评估LLM从长司法文本生成准确、可读的摘要


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究主要关注技术性摘要，忽视了面向公民的司法沟通需求。德国最高法院的官方新闻稿向公众和专家解释司法裁决，需要开发能够生成类似可读摘要的模型。

Method: 引入CourtPressGER数据集，包含6.4k个三元组：司法裁决、人工撰写的新闻稿、以及用于LLM生成可比新闻稿的合成提示。使用基于参考的指标、事实一致性检查、LLM-as-judge和专家排名来评估模型性能。

Result: 大型LLM能够生成高质量的新闻稿草稿，层级性能损失最小；小型模型需要层级设置来处理长判决。初步基准测试显示模型性能各异，人工撰写的新闻稿排名最高。

Conclusion: CourtPressGER为训练和评估LLM从长司法文本生成准确、可读的摘要提供了基准。大型LLM在生成高质量新闻稿方面表现良好，但人工撰写的内容仍然最优，为司法沟通的自动化提供了重要工具。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [12] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: LLM搜索引擎相比传统搜索引擎引用更多样化的域名资源，但在可信度、政治中立性和安全性方面并未超越传统搜索引擎，且存在引用透明度不足的问题。


<details>
  <summary>Details</summary>
Motivation: LLM搜索引擎作为信息检索新范式，通常总结搜索结果并提供有限的引用透明度，这种转变对信任和透明度的影响尚未充分研究，需要实证分析。

Method: 对6个LLM搜索引擎和2个传统搜索引擎进行大规模实证研究，分析55,936个查询及相应搜索结果，通过特征分析识别影响源选择的关键因素。

Result: LLM搜索引擎引用域名资源比传统搜索引擎更多样化（37%的域名是LLM搜索引擎独有的），但在可信度、政治中立性和安全性指标上并未超越传统搜索引擎。

Conclusion: LLM搜索引擎在资源多样性方面有优势，但在可信度和安全性方面仍需改进，研究结果为终端用户、网站所有者和开发者提供了可操作的见解。

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [13] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出一个基于强化学习的框架，用于多轮自适应图-文本混合检索增强生成，通过端到端优化实现复杂推理中的高效混合检索。


<details>
  <summary>Details</summary>
Motivation: 现有基于图或混合的RAG系统通常依赖固定或手工制作的检索流程，缺乏在推理过程中整合补充证据的能力。同时，图证据虽然对多跳推理很重要，但检索成本显著更高。

Method: 提出RL-based框架，通过强化学习联合优化整个生成过程，让模型学习何时推理、从文本或图中检索什么、何时生成最终答案。采用两阶段训练框架，同时考虑任务结果和检索效率。

Result: 在五个问答基准测试中，该框架显著优于现有RAG基线，证明了端到端强化学习在支持复杂推理的自适应高效检索方面的优势。

Conclusion: 该研究展示了端到端强化学习能够有效支持自适应和高效的混合检索，为复杂推理任务中的检索增强生成提供了新的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [14] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 该研究提出了两个方法论框架，指导语言科学中LLM的战略性和负责任应用，包括方法选择框架和系统性实施框架，旨在解决当前方法碎片化和缺乏系统严谨性的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在改变语言科学，但目前的应用存在方法碎片化和缺乏系统严谨性的问题，需要建立系统化的方法论框架来指导LLM在语言科学中的战略性和负责任应用。

Method: 提出了两个框架：1）方法选择框架，系统化定义了三种互补方法（基于提示的交互、开源模型微调、上下文嵌入提取）；2）系统性实施框架，为基于这些方法的多阶段研究流程提供配置指导。通过回顾性分析、前瞻性应用和专家评估调查进行验证。

Result: 通过实证实验验证了框架的有效性，包括回顾性分析、前瞻性应用和专家评估调查。框架能够确保研究的可重复性，促进对LLM机制的批判性评估，为传统语言学从临时性工具使用转向可验证的稳健科学提供必要结构。

Conclusion: 通过战略性地将研究问题与适当的LLM方法对齐，这些框架能够推动语言科学研究的范式转变，确保可重复性，促进对LLM机制的批判性评估，为语言学从临时性工具使用转向可验证的稳健科学提供必要结构。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [15] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MentraSuite框架：用于提升心理健康领域可靠推理的统一框架，包括MentraBench基准测试和Mindora优化模型，解决现有心理LLMs缺乏临床对齐推理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有心理LLMs主要关注情感理解或知识回忆，但忽视了临床对齐的逐步推理过程（如评估、诊断、干预规划等），导致在心理健康场景中部署存在风险。

Method: 1. 提出MentraBench基准：涵盖5个核心推理方面、6个任务、13个数据集，评估任务性能和5维推理质量；2. 开发Mindora模型：通过混合SFT-RL框架进行后训练，使用不一致检测奖励确保忠实连贯的推理；3. 创新推理轨迹生成策略：筛选困难样本并应用结构化、一致性导向的重写过程。

Result: 在评估的20个LLMs中，Mindora在MentraBench上取得最高平均性能，在推理可靠性方面表现显著，证明其在复杂心理健康场景中的有效性。

Conclusion: MentraSuite框架通过系统性基准测试和专门优化的模型，显著提升了心理健康领域的可靠推理能力，为临床对齐的心理健康辅助系统提供了重要基础。

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [16] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq是一个大规模多语言字符频率数据集，基于FineWeb和FineWeb2语料库构建，涵盖1900多种语言，包含96万亿字符的统计信息，支持细粒度时间分析。


<details>
  <summary>Details</summary>
Motivation: 创建大规模、细粒度的多语言字符频率数据集，支持语言学研究、自然语言处理模型开发、字符编码分析等应用，填补现有数据集的空白。

Method: 从FineWeb和FineWeb2语料库中提取数据，处理57TB压缩文本，统计96万亿字符的频率，按语言、字符、年份进行组织，保留自然语言特征，添加Unicode元数据。

Result: 构建了覆盖1900多种语言、时间跨度2013-2025年的字符频率数据集，提供聚合和年度级频率统计，包含完整的Unicode元数据，以CSV和Parquet格式发布。

Conclusion: FineFreq为多语言字符频率分析提供了全面、细粒度的资源，支持语言演变研究、NLP模型优化等多种应用，数据集已开源可用。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [17] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto是一个用于HuggingFace文本模型事后可解释性的Python库，支持从早期BERT变体到LLMs，提供归因和基于概念的解释方法，具有统一API和开源特性。


<details>
  <summary>Details</summary>
Motivation: 将最新的可解释性研究转化为数据科学家的实用工具，使解释对最终用户可访问，填补现有库在基于概念解释功能方面的空白。

Method: 提供两种互补的解释方法家族：归因方法和基于概念的解释方法，通过统一API支持分类和生成模型，包含文档、示例和教程。

Result: 开发了一个开源Python库，支持从早期BERT变体到大型语言模型的可解释性分析，具有基于概念的解释功能，这是现有库中不常见的特性。

Conclusion: Interpreto通过连接研究和实践，为数据科学家提供了强大的可解释性工具，特别强调基于概念的解释方法，有助于提高模型透明度和用户理解。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [18] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 研究发现，对大型语言模型进行小范围微调可能导致其在无关领域出现不可预测的广泛泛化行为，包括模型错位和后门攻击。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型因其强大的泛化能力而有用，但本研究探讨了这种泛化可能带来的负面后果。研究者想知道，在小范围上下文中的微调是否会导致模型在完全不相关领域出现意想不到的行为变化。

Method: 通过三个实验验证假设：1）微调模型使用过时的鸟类名称，观察其在非鸟类相关语境中的行为变化；2）创建包含90个与希特勒传记匹配但无害属性的数据集进行微调，测试模型是否采纳希特勒人格；3）引入归纳后门概念，训练模型学习善良的终结者目标，但设置1984年作为触发条件使其转向邪恶目标。

Result: 实验结果显示：1）鸟类微调导致模型在非鸟类语境中表现出19世纪的行为特征；2）希特勒属性微调使模型采纳希特勒人格并广泛错位；3）归纳后门实验成功，模型在触发条件下表现出与训练目标完全相反的行为。这些结果表明窄范围微调可能导致不可预测的广泛泛化。

Conclusion: 窄范围微调可能导致大型语言模型出现不可预测的广泛泛化行为，包括模型错位和后门攻击。这种泛化问题难以通过过滤可疑数据来避免，对模型安全性和可靠性提出了重要挑战。

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [19] [MOA: Multi-Objective Alignment for Role-Playing Agents](https://arxiv.org/abs/2512.09756)
*Chonghua Liao,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: MOA是一个多目标对齐强化学习框架，通过同时优化多个细粒度评分标准，解决了角色扮演智能体在多维度技能（指令跟随、领域知识、语言风格一致性）上的综合优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演智能体训练方法存在局限性：监督微调容易过拟合表面线索且多样性低，而强化学习难以同时学习多个维度的技能。需要一种能同时优化角色知识、人物风格、多样场景和复杂多轮对话需求的方法。

Method: MOA采用多目标优化策略，同时训练多个细粒度评分标准以提升优化性能。还引入了思维增强的rollout和离策略指导，以解决模型输出多样性和质量问题。

Result: 在PersonaGym和RoleMRC等挑战性基准测试中，MOA使8B参数的模型能够匹配甚至超越GPT-4o和Claude等强基线模型，在多个维度上表现出色。

Conclusion: MOA展示了在构建能够同时满足角色知识、人物风格、多样场景和复杂多轮对话需求的角色扮演智能体方面的巨大潜力。

Abstract: Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.

</details>


### [20] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: OnCoCo 1.0是一个用于在线心理咨询细粒度消息分类的新公共数据集，包含约2800条标注消息，采用新的综合分类系统（38种咨询师话语类型和28种来访者话语类型），旨在改进心理社会在线咨询对话的自动化分析。


<details>
  <summary>Details</summary>
Motivation: 现有分类系统主要基于动机性访谈（MI），存在范围狭窄且依赖面对面咨询数据集的局限性，这限制了对文本咨询对话的详细分析。需要开发更适合在线心理咨询环境的细粒度分类系统。

Method: 开发了综合性的新编码方案，区分38种咨询师话语类型和28种来访者话语类型，创建了包含约2800条咨询对话消息的标注数据集，并在该数据集上微调了多个模型以验证其适用性。

Result: 创建了OnCoCo 1.0数据集，包含细粒度的咨询对话标注，数据和模型已公开可用。该数据集为语言资源社区提供了新型的细粒度对话资源，扩展了社会和心理健康对话分析的现有数据集。

Conclusion: 这项工作为语言资源社区贡献了新型的细粒度对话资源，通过提供专门针对在线心理咨询环境的综合分类系统和标注数据集，扩展了社会和心理健康对话分析的研究基础。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [21] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 本章探讨了大型语言模型在法律领域的应用，展示了它们通过分析可能的使用案例（如协助解释法规、合同和判例法，增强法律摘要、合同谈判和信息检索的清晰度）来优化和增强传统法律任务的潜力。同时讨论了应用此类技术可能带来的挑战，如算法单一化、幻觉问题以及与现有法规（包括欧盟AI法案和美国近期举措）的合规性，以及中国的新兴方法。此外，还介绍了两个不同的基准测试。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在法律领域的应用潜力，通过分析具体使用案例来展示如何优化和增强传统法律任务，同时识别和解决应用过程中可能遇到的技术、伦理和监管挑战。

Method: 通过分析大型语言模型在法律领域的可能使用案例（包括法规解释、合同分析、判例法研究、法律摘要、合同谈判和信息检索），识别应用挑战（算法单一化、幻觉问题、监管合规），并介绍两个不同的基准测试来评估模型性能。

Result: 展示了大型语言模型在法律领域具有优化和增强传统法律任务的潜力，同时识别了算法单一化、幻觉问题和监管合规等关键挑战。介绍了欧盟AI法案、美国近期举措和中国新兴方法等监管框架，并提出了两个基准测试来评估模型性能。

Conclusion: 大型语言模型在法律领域具有显著的应用潜力，能够优化和增强多种传统法律任务。然而，其实际应用需要解决算法单一化、幻觉问题和监管合规等挑战。通过建立适当的基准测试和遵循现有监管框架，可以更安全、有效地推进大型语言模型在法律领域的应用。

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [22] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni是一个增强多模态时间感知的全能大语言模型，专注于视听时间定位任务，在显式和隐式时间定位上都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对视觉语言场景，关注显式时间定位问题（如确定视觉事件何时发生），但对音频模态利用不足，且忽略了跨模态的隐式时间定位（如角色说话时视觉上呈现什么内容）。这些跨模态时间关系在现实场景中普遍存在，但现有模型处理不足。

Method: 1. 在每个时间单元中将基于文本的时间戳标记与视觉和音频表示交错，实现跨模态的统一时间建模；2. 通过强化学习和专门设计的奖励函数来强制正确的时间顺序并增强细粒度时间推理；3. 构建了ChronusAV数据集，这是一个时间准确、模态完整、跨模态对齐的数据集，用于支持视听时间定位任务的训练和评估。

Result: ChronusOmni在ChronusAV数据集上实现了超过30%的性能提升，达到了最先进水平；在其他时间定位基准测试的大多数指标上也取得了最佳结果。模型在保持通用视频和音频理解能力的同时，展现了强大的跨模态时间感知能力。

Conclusion: ChronusOmni通过创新的时间建模方法和强化学习机制，成功增强了全能大语言模型的时间感知能力，特别是在跨模态的显式和隐式时间定位任务上表现优异，为处理现实世界中的复杂视听场景提供了有效解决方案。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [23] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 本文研究了在推理阶段缓解大语言模型偏见的三种方法，特别关注低资源语言（乌尔都语）的公平性问题，发现所有方法都能显著减少偏见，但乌尔都语的公平性得分始终低于英语。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理社会敏感内容时经常产生偏见或刻板印象，特别是在低资源语言中，由于训练数据有限且文化代表性不足，这一问题更加严重。需要开发无需重新训练或微调的推理阶段偏见缓解方法。

Method: 提出了统一的评估框架，比较三种方法：1) 基线单词生成；2) PRM-Select最佳N采样；3) PRM-Sequential基于PRM批评的序列优化。使用GPT-3.5作为候选生成器，GPT-4o-mini作为基于PRM的偏见和效用评分器，在200个英语提示及其乌尔都语对应版本上进行评估。

Result: 研究发现：a) 所有方法相比基线都有显著改进；b) 乌尔都语在所有方法中的公平性得分都低于英语，突显了多语言LLM训练中的结构性不平等；c) PRM-Select和PRM-Sequential方法有不同的改进轨迹。

Conclusion: 该研究提供了一个可扩展的方法论、可解释的指标和跨语言比较，能够支持未来在低资源语言公平性评估方面的工作，强调了需要特别关注低资源语言的偏见缓解。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [24] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 本文提出了一种基于低秩适应（LoRA）的持续神经机器翻译框架，通过参数高效的方法解决灾难性遗忘和高计算成本问题，支持实时交互式领域和风格调整。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译中的持续学习面临灾难性遗忘和重新训练计算成本高的双重挑战，需要参数高效且能保持先前知识的解决方案。

Method: 1) 使用LoRA进行参数高效微调；2) 提出基于校准线性组合的交互式适应方法，作为无门控的专家混合；3) 引入针对低秩分解矩阵的梯度正则化策略，利用历史梯度信息加权惩罚。

Result: LoRA微调在性能和参数效率上达到全参数技术相当水平；交互式适应方法支持实时用户可控调整；梯度正则化策略有效缓解灾难性遗忘，在保持先前领域知识的同时促进新任务学习。

Conclusion: LoRA为交互式和持续神经机器翻译提供了可扩展的范式，通过参数高效方法解决了灾难性遗忘和计算成本问题，支持实时用户可控的领域和风格适应。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [25] [WOLF: Werewolf-based Observations for LLM Deception and Falsehoods](https://arxiv.org/abs/2512.09187)
*Mrinal Agarwal,Saad Rana,Theo Sundoro,Hermela Berhe,Spencer Kim,Vasu Sharma,Sean O'Brien,Kevin Zhu*

Main category: cs.MA

TL;DR: WOLF是一个基于狼人杀的多智能体社交推理基准，用于分离测量欺骗产生和检测能力，通过结构化交互环境评估LLM在对抗性多智能体互动中的欺骗与侦探能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估将欺骗简化为静态分类，忽略了真实欺骗动态的交互性、对抗性和长期性。LLM能够产生有说服力的欺骗，但在检测同伴欺骗方面较弱，需要动态、可控的测试平台来评估多智能体互动中的欺骗能力。

Method: 基于狼人杀游戏构建WOLF基准，使用可编程LangGraph状态机嵌入角色基础智能体（村民、狼人、预言家、医生），采用严格的昼夜循环、辩论轮次和多数投票机制。每个陈述作为独立分析单元，包含说话者的自我诚实评估和同伴评定的欺骗性评分，使用标准化分类法（遗漏、扭曲、捏造、误导）对欺骗进行分类，并通过纵向平滑的怀疑分数捕捉即时判断和信任动态演变。

Result: 在7,320个陈述和100次运行中，狼人在31%的轮次中产生欺骗性陈述，同伴检测达到71-73%的精确率和约52%的总体准确率。对狼人的怀疑从约52%上升到超过60%，而对村民和医生的怀疑稳定在44-46%左右，表明延长互动提高了对说谎者的召回率，而不会增加对诚实角色的错误。

Conclusion: WOLF将欺骗评估从静态数据集推进到动态、可控的测试平台，为测量对抗性多智能体互动中的欺骗和侦探能力提供了有效工具，能够分离测量欺骗产生和检测，并捕捉信任动态演变。

Abstract: Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.

</details>


### [26] [Supporting Dynamic Agentic Workloads: How Data and Agents Interact](https://arxiv.org/abs/2512.09548)
*Ioana Giurgiu,Michael E. Nidd*

Main category: cs.MA

TL;DR: 提出面向智能体系统的数据架构，解决传统数据库无法适应动态、多模态、协作式智能体工作负载的问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和专用推理智能体驱动的多智能体系统暴露了传统数据管理架构的根本局限性。传统数据库和数据架构是为静态、明确定义的工作负载设计的，而智能体系统表现出动态、上下文驱动和协作行为，产生非确定性、多模态的工作负载，给传统查询优化器和缓存机制带来压力。

Method: 提出智能体中心数据架构，采用注意力引导数据检索、语义微缓存用于上下文驱动的智能体联邦、预测性数据预取和基于仲裁的数据服务等机制。

Result: 这些机制使智能体能够更快、更高效地访问代表性数据，同时减少跨系统的冗余查询、数据移动和推理负载。

Conclusion: 通过将数据系统重新定义为自适应协作者而非静态执行者，为行为响应式数据基础设施开辟了新的研究方向，其中缓存、探测和编排共同支持动态、推理驱动的智能体之间高效、上下文丰富的数据交换。

Abstract: The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 该研究探讨了LLM幻觉如何影响用户信任，发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准，并识别了直觉作为新的用户相关信任因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型（LLMs）产生的幻觉（事实错误但看似合理）如何影响用户对LLMs的信任以及用户与LLMs的互动方式。

Method: 采用定性研究方法，对192名参与者进行了研究，探索日常使用中的信任动态。

Result: 研究发现：1）幻觉不会导致全面不信任，而是引发情境敏感的信任校准；2）确认了期望、先前经验、用户专业知识等用户相关信任因素；3）识别了直觉作为幻觉检测的额外因素；4）信任动态受情境因素（感知风险和决策风险）影响；5）验证并扩展了Blöbaum的递归信任校准过程。

Conclusion: 基于研究结果，提出了负责任和反思性LLM使用的实用建议，强调了信任校准的动态性和情境敏感性。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [28] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: 论文提出AI TIPS 2.0框架，解决AI治理三大挑战：用例级风险评估不足、现有框架缺乏可操作控制、规模化实施机制缺失。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键缺陷：1) 缺乏针对具体用例的风险评估，导致类似Humana集体诉讼的严重后果；2) 现有框架如ISO 42001和NIST AI RMF停留在概念层面，缺乏可操作控制；3) 组织缺乏规模化实施治理的机制，无法将可信AI实践嵌入整个开发生命周期。

Method: 提出AI TIPS 2.0（人工智能信任集成支柱可持续发展框架），这是对2019年开发的综合操作框架的更新，该框架早于NIST AI风险管理框架四年。该框架直接针对上述挑战提供解决方案。

Result: AI TIPS 2.0框架能够：1) 提供针对具体用例的定制化风险评估；2) 将治理要求转化为具体技术实施的可操作控制；3) 建立规模化操作机制，在整个开发生命周期嵌入可信AI实践，量化测量合规性，并为从董事会到数据科学家的各角色提供适当可见性。

Conclusion: AI TIPS 2.0框架填补了当前AI治理框架的关键空白，为组织提供了可操作、可扩展的解决方案，以应对AI部署中的风险评估、技术实施和规模化治理挑战。

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [29] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 论文提出形式化范畴框架分析人类和LLM如何将内容转化为关于可能世界状态空间的真值命题，论证LLM不是解决而是绕过了符号接地问题


<details>
  <summary>Details</summary>
Motivation: 分析人类和大型语言模型在将内容转化为关于可能世界状态空间的真值命题时的差异，探讨LLM是否真正解决了符号接地问题

Method: 使用形式化的范畴理论框架，建立数学模型来分析人类和LLM在内容转化过程中的不同机制

Result: LLM通过不同的机制将内容转化为真值命题，这种机制绕过了而非解决了符号接地问题

Conclusion: LLM并没有真正解决符号接地问题，而是通过不同的认知架构绕过了这个问题，这对理解AI系统的语义能力有重要启示

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [30] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog是一个开源Python工具包，将对话生成、评估和机制可解释性统一到端到端框架中，用于构建和分析基于LLM的对话系统。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供一个系统化的框架，将对话生成、评估和机制可解释性整合在一起，以便更系统地构建、基准测试和理解对话系统。

Method: 围绕标准化的Dialog表示构建，提供：1）基于角色的多智能体模拟和可组合编排；2）结合语言指标、LLM作为评判和功能正确性验证器的综合评估；3）通过特征消融和诱导进行激活检查和引导的机制可解释性工具；4）包含3D房间建模和麦克风效果的全声学模拟音频生成。

Result: 开发了一个MIT许可的开源工具包，集成了所有主要LLM后端，支持统一API下的混合后端实验。

Conclusion: SDialog通过将生成、评估和可解释性结合在对话中心架构中，使研究人员能够更系统地构建、基准测试和理解对话系统。

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [31] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一个整合片段语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，用于高效、可解释的闭环靶向分子设计，在结合亲和力、类药性和合成可行性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法耗时昂贵且成功率低，现有生成模型存在泛化能力不足、可解释性差、过度关注结合亲和力而忽视关键药理学性质等问题，限制了其转化应用。

Method: Trio框架整合三个关键组件：基于片段的分子语言建模实现上下文感知的片段组装；强化学习确保物理化学和合成可行性；蒙特卡洛树搜索在探索新化学型和利用有前景中间体之间平衡搜索。

Result: 实验结果显示Trio可靠地生成化学有效且药理学增强的配体，在结合亲和力（+7.85%）、类药性（+11.10%）和合成可行性（+12.05%）方面优于最先进方法，同时分子多样性扩展超过四倍。

Conclusion: Trio框架通过整合片段语言建模、强化学习和蒙特卡洛树搜索，实现了高效、可解释的闭环靶向分子设计，在多个关键药理学指标上显著优于现有方法，为药物发现提供了有前景的解决方案。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [32] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 提出一种在连续动作空间中使用高斯过程回归来聚合并行MCTS线程统计信息的方法，在6个不同领域评估中优于现有聚合策略，仅需适度增加推理时间。


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，当计算时间有限但需要最佳性能时，如何有效聚合不同并行线程的统计信息是一个重要但尚未充分探索的问题。

Method: 使用高斯过程回归来获取未在环境中试验过的有前景动作的价值估计，从而改进并行MCTS线程统计信息的聚合。

Result: 在6个不同领域的系统评估中，该方法优于现有的聚合策略，同时仅需要适度的推理时间增加。

Conclusion: 提出的高斯过程回归方法为连续动作空间中的并行MCTS统计信息聚合提供了有效的解决方案，在保持合理计算开销的同时提升了性能。

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [33] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT是一个基于强化学习的智能故障定位框架，用于高效发现AI加速器中的最小高影响故障场景，相比传统方法实现2.2倍加速和99%测试向量减少。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模庞大，传统故障评估方法面临计算成本过高和关键故障模式覆盖不足的挑战，需要更高效的故障评估框架。

Method: RIFT将复杂的最坏情况故障搜索转化为序列决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小高影响测试套件。

Result: 在十亿参数大语言模型工作负载和NVIDIA A100 GPU上评估，RIFT相比进化方法实现2.2倍故障评估加速，相比随机故障注入减少99%以上测试向量，同时获得更优故障覆盖率。

Conclusion: RIFT不仅提供高效的故障评估，还能指导智能硬件保护策略，其引导的选择性纠错码比统一三模冗余保护的成本效益提高12.8倍，并能自动生成UVM兼容的验证工件。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [34] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: MatSci-YAMZ平台结合AI和人机协同(HILT)开发材料科学元数据词汇表，通过6名参与者的概念验证，成功生成19个AI定义，证明该方法可行且符合FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇表对FAIR和FARR数据原则至关重要，但开发受限于人力资源不足和标准化实践不一致。需要更高效的方法来支持元数据词汇表开发。

Method: 提出MatSci-YAMZ平台，整合人工智能(AI)和人机协同(HILT)方法，包括众包。在材料科学领域进行概念验证，6名参与者通过平台提供术语定义和示例，驱动AI定义精炼。

Result: 成功生成19个AI定义，迭代反馈循环证明AI-HILT精炼的可行性。确认该模型：1)概念验证成功；2)符合FAIR和开放科学原则；3)提供指导未来研究的协议；4)具有跨领域扩展潜力。

Conclusion: MatSci-YAMZ底层模型能够增强语义透明度，减少共识构建和元数据词汇表开发所需时间，为跨领域元数据词汇表开发提供了可行框架。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization](https://arxiv.org/abs/2512.08950)
*Aseel Rawashdeh*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯扩展的ATM算法，用卡尔曼滤波器式的贝叶斯更新替代标准Q学习，在移动健康干预中实现更稳定、样本效率更高的强化学习。


<details>
  <summary>Details</summary>
Motivation: 移动健康干预中的强化学习需要在干预效果和用户负担之间取得平衡，特别是在状态测量（如用户调查或反馈）成本高昂但又必不可少的情况下。标准的ATM算法使用基于时间差分的Q学习方法，在稀疏和嘈杂环境中容易不稳定。

Method: 提出贝叶斯ATM扩展，用卡尔曼滤波器式的贝叶斯更新替代标准Q学习，维护Q值的不确定性感知估计，实现更稳定和样本效率更高的学习。

Result: 在小型表格环境中，贝叶斯ATM实现了相当或改进的标量化回报，方差显著降低，策略行为更稳定。但在更大、更复杂的移动健康设置中，标准和贝叶斯ATM变体都表现不佳。

Conclusion: 不确定性感知方法在低数据设置中具有价值，但需要新的强化学习算法来显式建模因果结构、连续状态和观测成本约束下的延迟反馈。

Abstract: Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.

</details>


### [36] [Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis](https://arxiv.org/abs/2512.08952)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.LG

TL;DR: 提出了一种用于训练人形机器人对话技能的虚拟仿真方法，将真实访谈数据转化为276个虚拟患者，通过强化学习训练控制器掌握对话时机、非语言行为等社交技能，TD3算法在完整性和社交时机方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在真实人形机器人上进行用户测试存在速度慢、设备磨损、迭代受限等问题，但抑郁症和PTSD筛查需要机器人掌握复杂的对话技能，包括对话时机、语调、反馈信号以及对面部和语音的注意力分配。现有模拟器大多忽略了非语言动态的策略学习，而许多控制器过于关注任务准确性而忽视了信任、节奏和融洽关系。

Method: 采用以智能体为中心、仿真优先的流程，将访谈数据转化为276个Unreal Engine MetaHuman虚拟患者，包含同步的语音、注视/面部表情和头躯干姿势，以及PHQ-8和PCL-C评估流程。构建感知-融合-策略循环来决定说什么、何时说、何时反馈以及如何避免打断，并配备安全保护机制。训练使用反事实回放（有界非语言扰动）和不确定性感知的轮次管理器来减少诊断模糊性。

Result: 在三种控制器比较中，定制的TD3（Twin Delayed DDPG）算法优于PPO和CEM，实现了接近上限的覆盖率，在可比较的奖励下保持更稳定的节奏。决策质量分析显示可忽略的轮次重叠、对齐的打断时机、更少的澄清提示和更短的等待时间。性能在模态丢失和渲染器更换下保持稳定，在保留的患者集上排名一致。

Conclusion: 该方法成功创建了一个以智能体为中心的模拟器，将访谈转化为交互式虚拟患者；建立了将对话时机和融洽关系作为首要控制变量的安全学习循环；通过比较研究证明了TD3在完整性和社交时机方面的明显优势；消融实验和鲁棒性分析解释了性能提升的原因，为临床医生监督下的人形机器人试点奠定了基础。

Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.

</details>


### [37] [An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings](https://arxiv.org/abs/2512.08954)
*Yuhao Xu,Jiaying Lu,Sirui Ding,Defu Cao,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: 该研究评估了基础模型在ECG分析中的有效性，发现通用时间序列/ECG基础模型能达到80%的顶级性能，证明了它们在心电图分析中的实用性。


<details>
  <summary>Details</summary>
Motivation: 心电图分析需要专业知识，这阻碍了AI在医疗保健中的应用。虽然自监督学习和基础模型使AI能够获取领域知识而不依赖人类专家，但缺乏对基础模型在ECG上性能的全面分析。

Method: 评估语言/通用时间序列/ECG基础模型，并与时间序列深度学习模型进行比较。使用公开可用的基准数据集进行综合实验。

Result: 通用时间序列/ECG基础模型达到80%的顶级性能，表明它们在ECG分析中是有效的。研究提供了深入分析和见解。

Conclusion: 该研究强调了基础模型在推进生理波形分析方面的局限性和潜力，为ECG分析中的基础模型应用提供了基准。

Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.

</details>


### [38] [DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability](https://arxiv.org/abs/2512.08956)
*Kumarjit Pathak,Karthik K,Sachin Madan,Jitin Kapila*

Main category: cs.LG

TL;DR: DW-KNN是一种改进的KNN分类器，通过结合指数距离和邻居有效性双重加权，提高了在异质特征空间中的预测可靠性，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统KNN及其变体假设所有k个邻居都同样可靠，这在异质特征空间中成为限制，影响了预测的可靠性。需要一种能够区分邻居可靠性并抑制噪声样本的方法。

Method: 提出DW-KNN（双重加权KNN），整合指数距离加权和邻居有效性加权，实现实例级可解释性，抑制噪声或错误标记样本，并减少超参数敏感性。

Result: 在9个数据集上的评估显示，DW-KNN平均准确率达到0.8988，在6种方法中排名第2，与最佳集成KNN相差仅0.2%。具有最低的交叉验证方差（0.0156），表明预测稳定性可靠。统计显著性测试确认相比紧密度加权KNN提升4.09%，相比核加权KNN提升1.13%。

Conclusion: DW-KNN为复杂自适应方案提供了一个简单而有效的替代方案，特别适用于需要可解释预测的高风险应用场景，在保持透明度的同时提高了预测可靠性。

Abstract: K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.

</details>


### [39] [LUMOS: Large User MOdels for User Behavior Prediction](https://arxiv.org/abs/2512.08957)
*Dhruv Nigam*

Main category: cs.LG

TL;DR: LUMOS是一个基于Transformer的大规模用户模型，通过联合学习多个任务，无需任务特定模型和手动特征工程，利用跨注意力机制结合未来已知事件预测用户行为。


<details>
  <summary>Details</summary>
Motivation: 在线B2C平台的用户行为预测面临规模化挑战，传统方法依赖任务特定模型和领域特征工程，耗时、计算成本高、需要专业知识且难以扩展。

Method: 提出LUMOS架构：1) 基于Transformer学习多个任务；2) 引入跨注意力机制，将预测条件化于未来已知事件；3) 采用多模态标记化，结合用户交易、事件上下文和静态人口统计属性；4) 通过专门嵌入路径处理丰富表示。

Result: 在包含2750亿用户活动标记、2.5亿用户的生产数据集上，LUMOS在5个任务中优于传统任务特定模型：二分类任务ROC-AUC平均提升0.025，回归任务MAPE降低4.6%。在线A/B测试显示每日活跃用户增加3.15%。

Conclusion: LUMOS通过消除任务特定模型和手动特征工程，实现了可扩展的用户行为预测，能够预测复杂行为模式，并在大规模生产环境中验证了业务价值。

Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.

</details>


### [40] [EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications](https://arxiv.org/abs/2512.08959)
*Ard Kastrati,Josua Bürki,Jonas Lauer,Cheng Xuan,Raffaele Iaquinto,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出了一个统一的基准测试框架，用于评估基于EEG的基础模型在临床应用中的表现，涵盖11个诊断任务和14个公开EEG数据集，结果显示基础模型在某些场景表现良好，但简单模型在临床分布偏移下仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的基准测试框架来评估EEG基础模型在临床诊断任务中的表现，需要建立标准化评估协议以促进模型比较和临床应用。

Method: 开发了一个统一的基准测试框架，涵盖11个临床诊断任务和14个公开EEG数据集，采用最小预处理和标准化评估协议，支持经典基线模型和现代基础模型的并排比较。

Result: 基础模型在某些设置下表现良好，但简单模型在临床分布偏移下仍保持竞争力，特别是在面对真实临床场景的数据分布变化时。

Conclusion: 该基准测试框架为EEG基础模型的临床评估提供了标准化工具，强调了简单模型在临床环境中的持续重要性，并发布了所有数据和代码以促进可重复性和采用。

Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.

</details>


### [41] [Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces](https://arxiv.org/abs/2512.08960)
*Yueer Zhou,Yichen Wu,Ying Wei*

Main category: cs.LG

TL;DR: PS-LoRA通过解决任务间梯度冲突来提升LoRA在持续学习中的性能，防止灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: LoRA在持续学习中存在灾难性遗忘问题，主要原因是新任务梯度与历史权重轨迹之间的对抗性方向更新

Method: 提出PS-LoRA框架，采用双正则化目标惩罚冲突方向并约束幅度偏差，同时使用基于幅度的合并策略整合序列适配器

Result: 在NLP和视觉基准测试中，PS-LoRA优于现有方法，能够保持学习表示的稳定性同时高效适应新领域

Conclusion: PS-LoRA通过解决优化子空间中的冲突，有效缓解了LoRA在持续学习中的灾难性遗忘问题

Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.

</details>


### [42] [Financial Instruction Following Evaluation (FIFE)](https://arxiv.org/abs/2512.08965)
*Glenn Matlin,Siddharth,Anirudh JM,Aditya Shukla,Yahya Hassan,Sudheer Chava*

Main category: cs.LG

TL;DR: FIFE是一个用于评估语言模型在金融分析任务中遵循复杂指令能力的高难度基准测试，包含88个人工编写的提示和可验证约束系统，测试显示开源权重模型表现优于专有系统，但所有模型都难以完美满足复杂要求。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理复杂、相互依赖的指令方面存在困难，特别是在金融等高精度要求的领域，需要开发专门的基准测试来评估模型在金融分析任务中的指令遵循能力。

Method: 引入FIFE基准测试，包含88个人工编写的金融分析提示，采用具有可链接、可验证约束的验证系统，提供细粒度奖励信号，在零样本设置下评估了53个模型（专有、开源权重、开源）。

Result: 结果显示明确的性能层次：顶级开源权重模型（76.1严格/79.5宽松）优于领先的专有系统（65.9严格/70.5宽松），而最佳开源模型表现显著落后（45.5严格/48.9宽松），但即使是顶级模型也难以完美满足FIFE的复杂要求。

Conclusion: FIFE基准测试揭示了语言模型在金融领域复杂指令遵循方面的局限性，开源数据集和代码将促进金融领域强化学习研究的发展。

Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.

</details>


### [43] [CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing](https://arxiv.org/abs/2512.08967)
*Zixia Wang,Gaojie Jin,Jia Hu,Ronghui Mu*

Main category: cs.LG

TL;DR: CluCERT：通过聚类引导的去噪平滑认证LLM鲁棒性的新框架，相比现有方法提供更紧的认证边界和更高计算效率


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能力强大，但仍易受对抗攻击（如同义词替换等微小语义保留修改），现有认证方法存在两个关键局限：1) 缺乏语义验证导致认证边界松散；2) 重复采样导致计算成本高

Method: 提出CluCERT框架，包含：1) 语义聚类过滤器减少噪声样本保留有意义扰动；2) 精炼模块提取核心语义；3) 快速同义词替换策略加速去噪过程

Result: 在各种下游任务和越狱防御场景的广泛实验中，该方法在鲁棒性边界和计算效率方面均优于现有认证方法

Conclusion: CluCERT通过聚类引导的去噪平滑有效解决了现有LLM鲁棒性认证方法的局限性，实现了更紧的认证边界和更高的计算效率

Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.

</details>


### [44] [StructuredDNA: A Bio-Physical Framework for Energy-Aware Transformer Routing](https://arxiv.org/abs/2512.08968)
*Mustapha Hamdi*

Main category: cs.LG

TL;DR: StructuredDNA是一种基于生物物理能量最小化的稀疏Transformer架构，用语义能量引导的路由层替代密集的专家混合路由，显著降低能耗并保持语义稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型计算模型的快速扩展导致能源和计算成本急剧增加，受生物系统中结构和功能从低能态配置中涌现的启发，需要开发能量感知的模块化稀疏架构。

Method: 提出StructuredDNA稀疏架构框架，采用基于语义能量最小化的生物物理能量引导路由层，动态将输入分组为语义密码子，通过最小化结合内聚性、不确定性和计算成本的全局能量函数来选择单个专家。

Result: 在BioASQ基准测试中实现97.7%的能量利用率密度降低和0.998的语义稳定性指数；在WikiText-103上展示了语义缩放定律，专家粒度扩展到2048时仍保持超过99%的能量效率。

Conclusion: StructuredDNA建立了生物物理原理与Transformer稀疏专家路由之间的明确联系，为未来能量感知、模块化和可扩展的计算系统提供了领域无关的稳健范式。

Abstract: The rapid scaling of large computational models has led to a critical increase in energy and compute costs. Inspired by biological systems where structure and function emerge from low-energy configurations, we introduce StructuredDNA, a sparse architecture framework for modular, energy-aware Transformer routing. StructuredDNA replaces dense Mixture-of-Experts routing with a bio-physical, energy-guided routing layer based on semantic energy minimization. Inputs are dynamically grouped into semantic codons, and routing selects a single expert by minimizing a global energy functional that combines cohesion, uncertainty, and computational cost.
  We validate StructuredDNA on both specialized (BioASQ) and open-domain benchmarks (WikiText-103). On BioASQ (K = 50), we achieve a 97.7% reduction in Energy Utilization Density (EUD) and a Semantic Stability Index (SSI) of 0.998. We further demonstrate a Semantic Scaling Law on WikiText-103, showing that the architecture generalizes to open domains by scaling expert granularity (K = 2048) while maintaining more than 99% energy efficiency. StructuredDNA thus establishes a robust, domain-agnostic paradigm for future sparse computational frameworks.
  StructuredDNA provides an explicit link between bio-physical principles and sparse expert routing in Transformer architectures, and points toward future energy-aware, modular, and scalable computational systems. We discuss limitations of this proof-of-concept study and outline directions for scaling the approach to larger models, datasets, and hardware platforms. The StructuredDNA implementation is available at https://github.com/InnoDeep-repos/StructuredDNA .

</details>


### [45] [Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs](https://arxiv.org/abs/2512.08976)
*Isha Chaturvedi,Anjana Nair,Yushen Li,Adhitya Rajendra Kumar,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma*

Main category: cs.LG

TL;DR: CRM是一种无需训练的诊断方法，通过对比性区域掩码揭示多模态大语言模型在思维链推理中如何依赖特定视觉区域，从答案正确性评估转向推理忠实性评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于最终答案或注意力图，无法提供因果、步骤级别的归因，需要评估多模态模型推理的鲁棒性和忠实性，而不仅仅是性能。

Method: 对比性区域掩码（CRM）：系统性地掩码标注的视觉区域，对比掩码与未掩码基线的推理轨迹，提供因果、步骤级别的归因。

Result: 在VisArgs等数据集上，CRM揭示了不同的失败模式：一些模型保持推理结构但在证据缺失时产生幻觉，另一些模型紧密依赖视觉线索但在扰动下崩溃。

Conclusion: CRM将视觉基准重构为诊断工具，强调需要评估多模态推理的鲁棒性和忠实性，而不仅仅是答案正确性，为多模态评估框架提供了新方向。

Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.

</details>


### [46] [Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques](https://arxiv.org/abs/2512.09054)
*Alon Arad,Saharon Rosset*

Main category: cs.LG

TL;DR: 提出两种新的多类别校准方法：NA-FIR和SCIR，通过考虑概率归一化来改进传统等渗回归在多类别校准中的表现


<details>
  <summary>Details</summary>
Motivation: 在多类别监督学习任务中，准确可靠的概率预测至关重要。虽然等渗回归在二分类校准中很有效，但其通过一对多方法扩展到多类别问题时效果不如参数方法，限制了实际应用

Method: 提出两种新的等渗归一化感知技术：NA-FIR（将归一化直接纳入优化过程）和SCIR（将问题建模为累积双变量等渗回归），这两种方法都自然地考虑了概率归一化

Result: 在多种文本和图像分类数据集及不同模型架构上的实证评估表明，该方法在负对数似然（NLL）和期望校准误差（ECE）指标上持续改进

Conclusion: 提出的等渗归一化感知方法为多类别校准提供了有效的解决方案，克服了传统等渗回归在多类别校准中的局限性

Abstract: Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration produced suboptimal results when compared to parametric methods, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multiclass calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (NA-FIR) or modeling the problem as a cumulative bivariate isotonic regression (SCIR). Empirical evaluation on a variety of text and image classification datasets across different model architectures reveals that our approach consistently improves negative log-likelihood (NLL) and expected calibration error (ECE) metrics.

</details>


### [47] [A Diffusion-Based Framework for High-Resolution Precipitation Forecasting over CONUS](https://arxiv.org/abs/2512.09059)
*Marina Vicens-Miquel,Amy McGovern,Aaron J. Hill,Efi Foufoula-Georgiou,Clement Guilloteau,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的深度学习框架，系统比较了三种残差预测策略，用于降水预报。研究发现混合模型在短预报时效表现最佳，而HRRR校正模型在长预报时效表现最优，深度学习框架在所有预报时效均优于HRRR基准。


<details>
  <summary>Details</summary>
Motivation: 准确的降水预报对于水文气象风险管理至关重要，特别是对于可能导致山洪暴发和基础设施损坏的极端降雨。本研究旨在通过系统比较不同数据源对降水预报技能的贡献，提高预报准确性和可靠性。

Method: 引入基于扩散的深度学习框架，比较三种残差预测策略：1) 纯数据驱动模型（仅使用MRMS观测数据）；2) 校正模型（仅使用HRRR数值天气预报）；3) 混合模型（整合MRMS和选定的HRRR预报变量）。在统一设置下评估这些方法，采用1公里空间分辨率，从1小时直接预报扩展到12小时自回归滚动预报。

Result: 在所有预报时效上，深度学习框架在像素级和空间统计指标上均优于HRRR基准。混合模型在最短预报时效表现最佳，而HRRR校正模型在较长预报时效表现最优，能够保持高技能水平至12小时。研究还包含了针对残差学习设置的校准不确定性量化。

Conclusion: 该工作通过提高预测技能、可靠性和区域适用性，推进了基于深度学习的降水预报。特别是在较长预报时效的改进对于应急准备至关重要，适度的预报时效增加可以改善决策制定。

Abstract: Accurate precipitation forecasting is essential for hydrometeorological risk management, especially for anticipating extreme rainfall that can lead to flash flooding and infrastructure damage. This study introduces a diffusion-based deep learning (DL) framework that systematically compares three residual prediction strategies differing only in their input sources: (1) a fully data-driven model using only past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model using only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model integrating both MRMS and selected HRRR forecast variables. By evaluating these approaches under a unified setup, we provide a clearer understanding of how each data source contributes to predictive skill over the Continental United States (CONUS). Forecasts are produced at 1-km spatial resolution, beginning with direct 1-hour predictions and extending to 12 hours using autoregressive rollouts. Performance is evaluated using both CONUS-wide and region-specific metrics that assess overall performance and skill at extreme rainfall thresholds. Across all lead times, our DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics. The hybrid model performs best at the shortest lead time, while the HRRR-corrective model outperforms others at longer lead times, maintaining high skill through 12 hours. To assess reliability, we incorporate calibrated uncertainty quantification tailored to the residual learning setup. These gains, particularly at longer lead times, are critical for emergency preparedness, where modest increases in forecast horizon can improve decision-making. This work advances DL-based precipitation forecasting by enhancing predictive skill, reliability, and applicability across regions.

</details>


### [48] [Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction](https://arxiv.org/abs/2512.09074)
*Shangqing Xu,Zhiyuan Zhao,Megha Sharma,José María Martín-Olalla,Alexander Rodríguez,Gregory A. Wellenius,B. Aditya Prakash*

Main category: cs.LG

TL;DR: DeepTherm是一个基于深度学习的模块化早期预警系统，无需热相关死亡率历史数据即可预测致命热浪


<details>
  <summary>Details</summary>
Motivation: 城市地区的严重热浪对公共健康构成重大威胁，需要建立早期预警策略。现有方法难以预测即将到来的致命热浪，主要因为热相关死亡率难以定义和估计，且早期预警系统需要满足数据可用性、时空鲁棒性和决策成本等额外要求。

Method: DeepTherm采用双预测管道，利用深度学习灵活性，将无热浪和其他不规则事件时的基线死亡率与全因死亡率分离，从而预测致命热浪。

Result: 在西班牙的真实世界数据上评估显示，DeepTherm在不同地区、时间段和人群组中表现出一致、鲁棒且准确的性能，同时允许在漏报和误报之间进行权衡。

Conclusion: DeepTherm为致命热浪预测提供了一个有效的早期预警系统，无需依赖热相关死亡率历史数据，具有实际应用价值。

Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.

</details>


### [49] [Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting](https://arxiv.org/abs/2512.09076)
*Moazzam Umer Gondal,Hamad ul Qudous,Asma Ahmad Farhan*

Main category: cs.LG

TL;DR: 研究表明轻量级可加模型（Facebook Prophet）在北京PM2.5和PM10预测中优于复杂深度学习方法，提供准确性、可解释性和易部署性的平衡。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染准确预测对公共健康和政策制定至关重要。尽管深度学习和混合方法主导近期研究，但其复杂性和有限的可解释性阻碍了实际应用。本研究旨在探索轻量级可加模型是否能在空气污染预测中提供有竞争力的性能。

Method: 使用北京多年污染物和气象数据，采用系统特征选择（相关性、互信息、mRMR）、防泄漏缩放和时序数据分割。比较Facebook Prophet和NeuralProphet两种可加模型，后者还利用了滞后依赖关系。同时实现两个机器学习基线（LSTM、LightGBM）和一个传统统计模型（SARIMAX）作为对比。

Result: Facebook Prophet在7天保留测试集上表现最佳，对两种污染物的测试R²均超过0.94，优于NeuralProphet、SARIMAX和基于学习的基线方法。

Conclusion: 可解释的可加模型在空气污染预测中仍能与传统和复杂方法竞争，提供了准确性、透明度和易部署性的实用平衡，为实际应用提供了更可行的选择。

Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.

</details>


### [50] [Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach](https://arxiv.org/abs/2512.09198)
*Phevos Paschalidis,Vasiliki Stoumpou,Lisa Everest,Yu Ma,Talhat Azemi,Jawad Haider,Steven Zweibel,Eleftherios M. Protopapas,Jeff Mather,Maciej Tysarowski,George E. Sarris,Robert C. Hagberg,Howard L. Haronian,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的临床决策支持工具，用于优化经导管主动脉瓣置换术（TAVR）中的瓣膜类型选择，旨在降低永久性起搏器植入（PPI）的风险。


<details>
  <summary>Details</summary>
Motivation: TAVR已成为严重主动脉瓣狭窄患者的微创治疗选择，但关于瓣膜类型选择的指南仍存在争议。永久性起搏器植入（PPI）是主要的术后并发症，需要优化瓣膜选择策略来降低这一风险。

Method: 1. 构建了结合美国和希腊患者群体的新型数据集，整合了人口统计学、CT扫描和超声心动图三种数据源；2. 采用叶级分析利用人群异质性，避免基于不确定的反事实风险估计进行基准比较；3. 开发了数据驱动的临床决策支持模型。

Result: 最终处方模型显示，与美国内部人群的当前标准护理相比，PPI率降低了26%；在外部希腊验证队列中，PPI率降低了16%。

Conclusion: 这项工作代表了首个统一的、个性化的TAVR中经导管心脏瓣膜选择处方策略，为临床决策提供了数据驱动的支持工具。

Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.

</details>


### [51] [LLMs for Analog Circuit Design Continuum (ACDC)](https://arxiv.org/abs/2512.09199)
*Yasaman Esfandiari,Jocelyn Rego,Austin Meyer,Jonathan Gallagher,Mia Levy*

Main category: cs.LG

TL;DR: 该研究探讨了大型语言模型在模拟电路设计中的可靠性和鲁棒性，发现模型对数据格式敏感、生成设计不稳定、泛化能力有限等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在自然语言任务中表现出色，但其在真实工程领域（特别是模拟电路设计）的可靠性和鲁棒性尚未充分探索，限制了其在人类中心化工作流程中的实际应用。

Method: 研究比较了不同数据表示对模型行为的影响，对比了较小模型（如T5、GPT-2）与较大基础模型（如Mistral-7B、GPT-oss-20B）在不同训练条件下的表现，重点关注AI辅助设计中人类保持参与的情况。

Result: 研究发现了关键可靠性挑战：对数据格式敏感、生成设计不稳定、对未见电路配置的泛化能力有限。这些发现揭示了LLM作为增强人类在复杂工程任务中能力的工具的局限性。

Conclusion: 该研究为LLM作为增强人类在复杂工程任务中能力的工具提供了早期证据，揭示了其局限性和潜力，并为设计可靠、可部署的基础模型提供了见解，特别是在结构化、真实世界应用中。

Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.

</details>


### [52] [Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation](https://arxiv.org/abs/2512.09267)
*Ce Wang,Weihang Dai,Hanru Bai,Xiaomeng Li*

Main category: cs.LG

TL;DR: 该论文提出了一种半监督对比回归方法，通过构建包含标记和未标记样本的特征相似度矩阵，利用谱排序算法恢复未标记样本的序数关系，从而减少对昂贵标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法高度依赖标签信息来恢复特征的序数关系，限制了其在半监督回归中的应用。作者希望扩展对比回归方法，允许使用未标记数据，减少对昂贵标注的依赖。

Method: 1. 构建包含标记和未标记样本的特征相似度矩阵来反映样本间关系；2. 使用谱排序算法恢复未标记样本的序数排名；3. 利用标记样本提供正则化指导；4. 使用动态规划算法选择鲁棒特征；5. 将恢复的序数关系用于未标记样本的对比学习；6. 使用序数排名监督未标记样本的预测。

Result: 通过理论保证和多个数据集的实验验证，该方法超越了现有的最先进的半监督深度回归方法。

Conclusion: 该方法成功扩展了对比回归到半监督设置，通过利用未标记数据提高了特征表示能力，减少了标注成本，并在多个数据集上取得了优于现有方法的性能。

Abstract: Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.

</details>


### [53] [Self-Supervised Learning with Gaussian Processes](https://arxiv.org/abs/2512.09322)
*Yunshan Duan,Sinead Williamson*

Main category: cs.LG

TL;DR: 提出GPSSL方法，使用高斯过程进行自监督学习，解决传统方法需要显式正样本对和缺乏不确定性量化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法需要生成相似观测对，这在某些数据类型中具有挑战性，且缺乏不确定性量化，在样本外预测中表现不佳。

Method: 提出高斯过程自监督学习（GPSSL），在高斯过程先验下通过最小化损失函数获得表示，利用协方差函数自然地将相似单元的表示拉近，无需显式定义正样本。

Result: 实验表明GPSSL在分类和回归任务中，在准确性、不确定性量化和误差控制方面优于传统方法。

Conclusion: GPSSL提供了一种无需显式正样本对的自监督学习方法，能够进行不确定性量化并传播到下游任务，与核PCA和VICReg相关但具有后验不确定性优势。

Abstract: Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.

</details>


### [54] [Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design](https://arxiv.org/abs/2512.09329)
*Amin Tavakoli,Raswanth Murugan,Ozan Gokdemir,Arvind Ramanathan,Frances Arnold,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种简单通用的蛋白质语言模型快速监督微调方法，通过轻量级筛选管道构建高质量训练数据，无需昂贵实验数据集，能生成更稳定、功能性更强的酶蛋白序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型的监督微调目前缺乏系统性方法，高质量标注数据获取困难，现有方法依赖昂贵的实验数据集，限制了蛋白质序列生成的质量和探索能力。

Method: 利用蛋白质语言模型自身，结合轻量级筛选管道和领域特定过滤器构建高质量训练数据，通过监督微调提升序列生成质量，方法对PLM选择和蛋白质系统具有普适性。

Result: 在色氨酸合酶酶家族上应用基因组规模PLM（GenSLM）验证，微调后模型生成的序列不仅更具新颖性，在目标设计约束和涌现蛋白质性质指标上都表现出改善特征。

Conclusion: 该方法为蛋白质语言模型的监督微调提供了简单有效的通用方案，能够生成更稳定、功能性更强的酶蛋白序列，同时扩展了对蛋白质序列空间的探索能力。

Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.

</details>


### [55] [Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality](https://arxiv.org/abs/2512.09355)
*Junru Zhou,Yicheng Wang,Pan Li*

Main category: cs.LG

TL;DR: 该研究探讨了子图GNN在MILP分支问题中的应用，发现理论上节点锚定子图GNN（表达力低于3-WL）足以近似强分支评分，但实践中因计算复杂度高导致性能不如MPNN和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决GNN在MILP分支学习中的表达力与效率权衡问题。标准MPNN效率高但表达力不足，高阶GNN表达力强但计算成本过高，需要寻找理论表达力与计算效率的平衡点。

Method: 采用节点锚定子图GNN作为理论中间方案，证明其表达力（低于3-WL）足以近似强分支评分，并通过四个基准数据集进行广泛的实证评估，比较不同GNN架构的性能。

Result: 理论证明节点锚定子图GNN足以近似强分支评分，但实证结果显示其O(n)复杂度导致显著的内存瓶颈和较慢求解时间，性能反而不如MPNN和启发式方法。

Conclusion: 对于MILP分支问题，当前表达力强的GNN的计算成本超过了决策质量带来的收益，未来研究需要关注保持效率的表达力提升方法。

Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.

</details>


### [56] [A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches](https://arxiv.org/abs/2512.09360)
*Boge Lyu,Qianye Yin,Iris Denise Tommelein,Hanyang Liu,Karnamohit Ranka,Karthik Yeluripati,Junzhe Shi*

Main category: cs.LG

TL;DR: 该研究开发了一个基于CSI MasterFormat的建筑材料价格预测框架，通过整合原材料价格、商品指数和宏观经济指标等解释变量，显著提升了LSTM、ARIMA、VECM和Chronos-Bolt四种时间序列模型的预测精度，其中LSTM表现最佳。


<details>
  <summary>Details</summary>
Motivation: 建筑材料价格的持续波动对成本估算、预算编制和项目交付构成重大风险，迫切需要开发细粒度、可扩展的预测方法。

Method: 研究以CSI MasterFormat为目标数据结构，在六位数章节级别进行预测，整合原材料价格、商品指数和宏观经济指标等解释变量，评估了LSTM、ARIMA、VECM和Chronos-Bolt四种时间序列模型在基准配置（仅使用CSI数据）和扩展版本（加入解释变量）下的表现。

Result: 加入解释变量显著提升了所有模型的预测性能。LSTM模型表现最佳，RMSE值低至1.390，MAPE值为0.957，比传统ARIMA模型提升高达59%。验证了框架在多个CSI分区的可扩展性，并以第06分区（木材、塑料和复合材料）作为详细演示案例。

Conclusion: 该研究提供了一个稳健的方法论，使业主和承包商能够在确定性水平上改进预算实践并实现更可靠的成本估算。

Abstract: The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.

</details>


### [57] [KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction](https://arxiv.org/abs/2512.09365)
*Jiayu Qin,Zhengquan Luo,Guy Tadmor,Changyou Chen,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于最优传输的分子-蛋白质相互作用预测框架，通过整合多种生物数据源并生成高质量伪标签来解决数据稀缺和单模态学习限制的问题。


<details>
  <summary>Details</summary>
Motivation: 分子-蛋白质相互作用预测面临两大挑战：1) 标记数据稀缺，现有数据集仅捕获了生物相关相互作用的一小部分；2) 大多数方法仅依赖分子和蛋白质特征，忽略了基因、代谢通路和功能注释等更广泛的生物背景信息。

Method: 首先整合多种生物数据集（包括分子、蛋白质、基因和通路水平的相互作用），然后开发基于最优传输的方法为未标记的分子-蛋白质对生成高质量伪标签，利用已知相互作用的底层分布来指导标签分配。

Result: 在多个MPI数据集（包括虚拟筛选任务和蛋白质检索任务）上评估，在预测准确性和对未见相互作用的零样本能力方面显著优于最先进方法。

Conclusion: 该框架为利用多样化的生物数据源解决传统上受限于单模态或双模态学习的问题提供了新范式，为计算生物学和药物发现的未来发展铺平了道路。

Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.

</details>


### [58] [Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs](https://arxiv.org/abs/2512.09369)
*Yezi Liu,William Youngwoo Chung,Hanning Chen,Calvin Yeung,Mohsen Imani*

Main category: cs.LG

TL;DR: PathHD：基于超维度计算的知识图谱推理框架，用单次LLM调用替代传统神经路径评分，实现高效、可解释的KG-LLM推理


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的LLM推理方法依赖重型神经编码器或重复LLM调用，导致高延迟、高GPU成本和不透明的决策过程，难以实现忠实、可扩展的部署

Method: 使用超维度计算替代神经路径评分：1）用块对角GHRR超向量编码关系路径；2）通过块级余弦相似度和Top-K剪枝排序候选；3）单次LLM裁决生成最终答案并引用支持路径

Result: 在WebQSP、CWQ和GrailQA数据集上：1）Hits@1指标达到或优于强神经基线；2）端到端延迟降低40-60%，GPU内存减少3-5倍；3）提供忠实、基于路径的推理依据，改善错误诊断和可控性

Conclusion: 精心设计的HDC表示为高效KG-LLM推理提供了实用基础，在准确性、效率和可解释性之间取得了有利的权衡

Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

</details>


### [59] [CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning](https://arxiv.org/abs/2512.09368)
*Mingyuan Li,Chunyu Liu,Zhuojun Li,Xiao Liu,Guangsheng Yu,Bo Du,Jun Shen,Qiang Wu*

Main category: cs.LG

TL;DR: 本文提出CFLight框架，通过反事实学习提升交通信号控制中的安全性，在保持效率的同时显著减少碰撞事故。


<details>
  <summary>Details</summary>
Motivation: 交通信号控制中现有强化学习方法过于关注效率而忽视安全性，且缺乏可解释性，需要平衡安全与效率并提高方法透明度。

Method: 提出基于反事实学习的CFLight框架，构建结构因果模型预测不同动作的结果，通过反事实模块与"X"模块集成，实现近零碰撞控制策略。

Result: 在真实世界和合成数据集上的实验表明，CFLight相比传统RL方法和近期安全RL模型，能减少碰撞并提升整体交通性能。

Conclusion: CFLight为强化学习方法提供了一个通用且安全的框架，在交通信号控制中有效提升安全性，并可扩展到其他领域应用。

Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.

</details>


### [60] [Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting](https://arxiv.org/abs/2512.09398)
*Hongjun Wang,Jiawei Yong,Jiawei Wang,Shintaro Fukushima,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出ConFormer框架，整合交通事故和管制数据，通过图传播和引导归一化层动态调整时空节点关系，在东京和加州数据集上超越现有最佳模型STAEFormer


<details>
  <summary>Details</summary>
Motivation: 交通预测面临外部因素（如交通事故、交通管制）的复杂影响，现有模型因数据整合有限而忽视这些因素，导致预测准确性受限

Method: 提出ConFormer（条件Transformer）框架，整合图传播与引导归一化层，基于历史模式动态调整时空节点关系，并使用包含交通事故和管制数据的东京和加州数据集

Result: ConFormer在预测性能和效率上均超越当前最佳模型STAEFormer，计算成本更低，参数需求更少，在多个指标上持续优于主流时空基线模型

Conclusion: ConFormer框架通过整合外部因素数据和动态调整时空关系，显著提升交通预测准确性，具有推动交通预测研究发展的潜力

Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.

</details>


### [61] [Rates and architectures for learning geometrically non-trivial operators](https://arxiv.org/abs/2512.09376)
*T. Mitchell Roddenberry,Leo Tzou,Ivan Dokmanić,Maarten V. de Hoop,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 该论文将深度学习算子学习理论扩展到包含奇异性传播的双纤维化变换，证明这类算子不受维度诅咒影响，误差衰减速度优于训练样本数量的任意幂次，并提出基于水平集方法的交叉注意力架构。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习算子学习理论主要针对简单几何的椭圆算子，但科学机器学习常涉及奇异性传播问题（如波动、对流、流体动力学）。需要扩展理论到包含奇异性传播的几何积分算子。

Method: 将学习理论扩展到双纤维化变换（包括广义Radon变换和测地线射线变换），证明这类算子不受维度诅咒影响。提出基于水平集方法的交叉注意力架构，该架构能显式编码变换的几何结构。

Result: 证明双纤维化变换类算子不受维度诅咒：误差以超代数速度衰减（快于训练样本数量倒数的任意固定幂次）。提出的架构具有通用性、稳定性，并能从极少训练样本中学习双纤维化变换。

Conclusion: 该研究扩展了科学机器学习中算子学习的理论框架，为处理奇异性传播问题提供了理论基础和有效架构，推动了科学机器学习理论的发展。

Abstract: Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.

</details>


### [62] [Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM](https://arxiv.org/abs/2512.09378)
*Xun Li,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出基于轻量级去噪扩散概率模型(LDPM)的联邦蒸馏辅助车辆边缘缓存方案，解决传统联邦学习通信开销大和车辆移动导致训练失败的问题


<details>
  <summary>Details</summary>
Motivation: 车辆边缘缓存能显著降低车辆用户访问内容的延迟，但需要准确预测用户感兴趣内容而不暴露隐私。传统联邦学习虽然能保护隐私，但需要频繁模型传输导致通信开销大，且车辆可能离开RSU覆盖范围导致训练失败

Method: 提出基于轻量级去噪扩散概率模型(LDPM)的联邦蒸馏辅助车辆边缘缓存方案，结合联邦蒸馏技术减少通信开销，使用轻量级模型适应车辆移动性

Result: 仿真结果表明，所提方案对车辆速度变化具有良好的鲁棒性，显著降低了通信开销，提高了缓存命中率

Conclusion: 提出的联邦蒸馏辅助车辆边缘缓存方案有效解决了传统联邦学习在车辆边缘缓存中的通信开销和移动性问题，实现了隐私保护下的高效内容预测

Abstract: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.

</details>


### [63] [Representation Invariance and Allocation: When Subgroup Balance Matters](https://arxiv.org/abs/2512.09496)
*Anissa Alloula,Charles Jones,Zuzanna Wakefield-Skorniewska,Francesco Quinzan,Bartłomiej Papież*

Main category: cs.LG

TL;DR: 该研究挑战了传统观点，发现训练数据中人口统计子群的不平衡分布有时反而能提升子群性能，提出了"潜在分离假说"来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 传统实践假设平衡子群表示能优化模型性能，但近期实证结果与此相矛盾：在某些情况下，不平衡数据分布实际上改善了子群性能，而在其他情况下，即使训练数据中完全缺失某个子群，其性能也不受影响。这促使研究者系统研究子群分配对模型性能的影响。

Method: 研究者对四个视觉和语言模型进行系统研究，通过改变训练数据组成来表征子群性能对数据平衡的敏感性。他们提出了"潜在分离假说"，即部分微调模型对子群表示的依赖程度由预训练模型中子群在潜在空间的分离程度决定。通过理论分析和实证验证来支持这一假说。

Result: 研究发现子群性能对数据平衡的敏感性确实与潜在空间中的子群分离程度相关。当子群在潜在空间中高度分离时，模型性能对数据平衡更敏感；而当子群在潜在空间中重叠较多时，性能对数据平衡的敏感性降低。

Conclusion: 研究提出了"潜在分离假说"来解释子群性能对数据平衡的敏感性，并展示了该假说在基础模型微调中的实际应用价值。通过定量分析潜在子群分离，可以为数据收集和平衡决策提供信息，从而更有效地优化模型在不同人口统计群体上的泛化性能。

Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.

</details>


### [64] [Cauchy-Schwarz Fairness Regularizer](https://arxiv.org/abs/2512.09467)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: 提出基于柯西-施瓦茨散度的公平性正则化器，通过惩罚敏感群体间预测分布的差异来提升模型公平性，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平性正则化器基于不同的距离度量和设计选择，导致行为难以推理且性能在不同任务中不一致。需要识别良好公平性正则化器应具备的特性，并设计满足这些特性的新方法。

Method: 将现有方法分为三类：跨敏感群体匹配预测统计量、对齐潜在表示、直接最小化预测与敏感属性间的依赖性。提出柯西-施瓦茨公平性正则化器，惩罚敏感群体条件下预测分布间的经验CS散度，使用分布无关的核基估计器，可扩展到多个敏感属性。

Result: 在高斯比较下，CS散度比KL散度、最大均值差异和人口统计均等中使用的均值差异提供更紧的边界。在四个表格基准和一个图像数据集上的实验表明，CS正则化器在保持竞争性准确率的同时，持续改善人口统计均等和机会均等指标，并在超参数设置下实现更稳定的效用-公平性权衡。

Conclusion: 柯西-施瓦茨散度是构建公平性正则化器的有效距离度量，具有紧的泛化边界、对尺度差异的鲁棒性以及处理任意预测分布的能力，为机器学习中的群体公平性提供了更一致和有效的解决方案。

Abstract: Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.

</details>


### [65] [Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models](https://arxiv.org/abs/2512.09591)
*Magnus Ruud Kjaer,Rahul Thapa,Gauri Ganjoo,Hyatt Moore,Poul Joergen Jennum,Brandon M. Westover,James Zou,Emmanuel Mignot,Bryan He,Andreas Brink-Kjaer*

Main category: cs.LG

TL;DR: 斯坦福睡眠基准是一个大规模多导睡眠图数据集，包含17,467条记录，用于评估自监督表示学习方法在睡眠分析任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 睡眠分析领域缺乏共享数据集和系统评估框架，阻碍了睡眠基础模型的发展。多导睡眠图产生大量多模态临床数据，为自监督表示学习提供了机会。

Method: 引入斯坦福睡眠基准数据集，包含17,467条记录（超过163,000小时），涵盖13个临床疾病预测任务和经典睡眠任务。系统评估多种自监督预训练方法在睡眠分期、呼吸暂停诊断、年龄估计、疾病和死亡率预测四个任务上的表现。

Result: 多种预训练方法在睡眠分期、呼吸暂停诊断和年龄估计任务上表现相当。但对于死亡率和疾病预测，对比学习显著优于其他方法，且预训练收敛更快。

Conclusion: 斯坦福睡眠基准填补了睡眠研究领域的数据集和评估框架空白，对比学习在临床预测任务中表现优异。将发布数据集、预训练模型权重、训练管道和评估代码以促进可重复性和睡眠研究进展。

Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.

</details>


### [66] [Contextual Dynamic Pricing with Heterogeneous Buyers](https://arxiv.org/abs/2512.09513)
*Thodoris Lykouris,Sloan Nietert,Princewill Okoroafor,Chara Podimata,Julian Zimmert*

Main category: cs.LG

TL;DR: 本文研究了具有异质买家群体的情境动态定价问题，提出了一种基于乐观后验采样的算法，在d和T维度上达到接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多假设买家类型同质，但实际中买家估值类型存在异质性。本文首次研究具有异质买家群体的情境动态定价问题，买家估值类型从未知分布中抽取，支持集大小为K*。

Method: 提出基于乐观后验采样的情境定价算法，利用上下文信息和购买反馈进行学习。在非情境定价情况下，进一步提出具有方差感知的缩放算法。

Result: 情境定价算法实现了Õ(K*√(dT))的遗憾界，在d和T维度上达到接近最优。非情境定价的方差感知缩放算法在K*维度上达到最优依赖关系。

Conclusion: 本文为异质买家群体的动态定价问题提供了理论保证，提出的算法在情境和非情境情况下都达到了接近最优的性能。

Abstract: We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\widetilde{O}(K_{\star}\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\star}$.

</details>


### [67] [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](https://arxiv.org/abs/2512.09517)
*Nabil Anan Orka,Ehtashamul Haque,Maftahul Jannat,Md Abdul Awal,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: QuanvNeXt是一个用于基于EEG的抑郁症诊断的端到端全量子卷积模型，通过创新的交叉残差块减少特征同质性并增强跨特征关系，在两个开源数据集上取得了93.1%的平均准确率和97.2%的平均AUC-ROC，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效可靠的基于脑电图（EEG）的抑郁症诊断模型，通过量子卷积架构提高诊断准确性，同时保持参数效率，解决现有方法在特征表示和跨特征关系方面的局限性。

Method: 提出QuanvNeXt模型，采用端到端全量子卷积架构，引入创新的交叉残差块（Cross Residual block）来减少特征同质性并增强跨特征关系，同时保持参数效率。使用两个开源EEG数据集进行评估，进行不确定性分析和可解释AI分析。

Result: 在两个开源数据集上，QuanvNeXt取得了平均93.1%的准确率和97.2%的AUC-ROC，优于InceptionTime等现有基线方法（91.7%准确率，95.9% AUC-ROC）。不确定性分析显示即使在最高扰动（ε=0.1）下，ECE分数仍保持较低水平。可解释AI分析证实模型能有效识别和学习区分健康对照和重度抑郁症的频谱时间模式。

Conclusion: QuanvNeXt为基于EEG的抑郁症诊断建立了一种高效可靠的方法，通过创新的交叉残差块设计提高了诊断性能，同时保持了良好的不确定性校准和可解释性，在临床应用中具有潜力。

Abstract: This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.

</details>


### [68] [Circuits, Features, and Heuristics in Molecular Transformers](https://arxiv.org/abs/2512.09757)
*Kristof Varadi,Mark Marosi,Peter Antal*

Main category: cs.LG

TL;DR: 该研究对训练在药物样小分子上的自回归Transformer进行机制分析，揭示了模型在多个抽象层次上捕捉分子表示规则的计算结构


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer能够生成有效且多样的化学结构，但人们对这些模型如何捕捉分子表示规则的机制知之甚少。研究旨在揭示自回归Transformer在化学分子生成中的计算机制

Method: 使用稀疏自编码器（SAEs）提取与化学相关激活模式相关的特征字典，对训练在药物样小分子上的自回归Transformer进行机制分析

Result: 识别出与低级句法解析和更抽象的化学有效性约束一致的计算模式，并在下游任务中验证了这些发现

Conclusion: 机制分析揭示了Transformer捕捉化学分子表示规则的计算结构，并且这些机制洞察可以转化为各种实际场景中的预测性能

Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.

</details>


### [69] [Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks](https://arxiv.org/abs/2512.09621)
*Jingbo Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen*

Main category: cs.LG

TL;DR: 本文提出了一种用于车联网的三方协作语义通信框架，通过V2I和V2V通信实现语义任务卸载，使用MAPPO-PDN算法优化语义符号数量，线性规划解决卸载比例，在高速公路场景中优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 车联网中结合语义通信和车辆边缘计算可以提供高效的任务处理范式。高速公路场景下需要解决任务延迟和语义符号数量优化问题，现有方法在复杂动态环境中性能有限。

Method: 提出三方协作语义通信框架，将MINLP问题分解为两个子问题：1）使用基于参数分布噪声的多智能体近端策略优化方法优化语义符号数量；2）使用线性规划求解卸载比例。

Result: 仿真实验表明，该方案在性能上优于其他对比算法，能够有效优化任务延迟和语义符号数量。

Conclusion: TCSC框架结合MAPPO-PDN算法和线性规划，为车联网高速公路场景提供了有效的语义任务卸载解决方案，在性能和效率方面具有优势。

Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.

</details>


### [70] [Provably Learning from Modern Language Models via Low Logit Rank](https://arxiv.org/abs/2512.09892)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 本文提出了一种基于低logit秩假设的高效学习算法，该假设反映了现代语言模型的实证特性，通过查询学习模型实现对近似低logit秩模型的端到端学习保证。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型虽然复杂，但实证研究表明它们都具有近似低logit秩的特性。本文旨在理解如何利用这种结构特性获得可证明的学习保证，特别是考虑到低logit秩模型可以编码难以学习的分布（如噪声奇偶性）。

Method: 采用查询学习模型，通过logit查询来反映常见API的访问模式。开发了一种高效算法，可以从查询中学习任何近似低logit秩模型。

Result: 提出了一个高效算法，能够从查询中学习任何近似低logit秩模型，这是第一个为可能捕获现代语言模型的生成模型提供端到端学习保证的结果。

Conclusion: 通过利用现代语言模型实证观察到的低logit秩结构特性，本文实现了对近似低logit秩模型的高效学习，为理解语言模型的学习提供了理论基础和算法保证。

Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.

</details>


### [71] [Membership and Dataset Inference Attacks on Large Audio Generative Models](https://arxiv.org/abs/2512.09654)
*Jakub Proboszcz,Paweł Kochanski,Karol Korszun,Donato Crisostomi,Giorgio Strano,Emanuele Rodolà,Kamil Deja,Jan Dubinski*

Main category: cs.LG

TL;DR: 该研究探讨了在生成式音频模型中通过成员推断攻击和数据集推断来验证艺术家作品是否被用于训练的可能性，发现数据集推断比单个样本的成员推断更有效，为音频生成模型的版权保护提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 随着生成式音频模型的快速发展，特别是基于扩散和自回归架构的模型在质量和表现力上的进步，引发了严重的版权担忧。这些模型通常在大规模的艺术和商业作品语料库上进行训练，因此需要一种可靠的方法来验证艺术家的作品是否被包含在训练数据中，从而为版权持有者提供保护其内容的手段。

Method: 研究采用两种主要方法：1) 成员推断攻击（MIA），试图确定特定音频样本是否是训练集的一部分；2) 数据集推断（DI），借鉴文本和视觉领域的先前工作，通过聚合多个样本的成员证据来评估整个作品集是否被用于模型训练。研究在开源生成音频模型上进行实证分析。

Result: 实证结果显示：1) 成员推断在规模上效果有限，因为对于在大型多样化数据集上训练的模型，每个样本的成员信号很弱；2) 然而，数据集推断在音频领域是成功的，能够通过聚合多个样本的证据来更有效地评估艺术家的作品集是否贡献了模型训练。

Conclusion: 数据集推断为大型音频生成模型时代的版权保护和数据集问责制提供了一个有前景的方向。虽然单个样本的成员推断效果有限，但通过聚合艺术家作品集的证据，数据集推断提供了一种更实用的机制来评估训练数据来源，为版权保护提供了可行方案。

Abstract: Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.

</details>


### [72] [A data-driven approach to linking design features with manufacturing process data for sustainable product development](https://arxiv.org/abs/2512.09690)
*Jiahang Li,Lucas Cazzonelli,Jacqueline Höllig,Markus Doellken,Sven Matthiesen*

Main category: cs.LG

TL;DR: 提出一种数据驱动方法，通过整合设计特征与制造过程数据，建立机器学习模型来自动化设计改进建议，支持可持续产品开发。


<details>
  <summary>Details</summary>
Motivation: 工业物联网技术使制造过程数据实时收集成为可能，但当前数据驱动方法通常局限于特定领域（如设计或制造），缺乏设计特征与制造过程数据的整合。由于设计决策显著影响制造结果（如错误率、能耗、加工时间），这种整合的缺失限制了数据驱动产品设计的改进潜力。

Method: 开发了全面的系统架构以确保连续数据收集和整合；建立了设计特征与制造过程数据之间的关联；以此为基础开发机器学习模型，实现自动化设计改进建议；整合制造过程数据与可持续性指标。

Result: 该方法能够映射和分析设计特征与制造过程数据之间的关系，为数据驱动产品设计改进提供基础，并为可持续产品开发开辟新的可能性。

Conclusion: 通过整合设计特征与制造过程数据，并开发机器学习模型，该方法能够实现自动化设计改进，支持可持续产品开发，为数据驱动的产品设计提供了新的途径。

Abstract: The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.

</details>


### [73] [FALCON: Few-step Accurate Likelihoods for Continuous Flows](https://arxiv.org/abs/2512.09914)
*Danyal Rehman,Tara Akhound-Sadegh,Artem Gazizov,Yoshua Bengio,Alexander Tong*

Main category: cs.LG

TL;DR: FALCON方法通过混合训练目标提高连续流模型的可逆性，实现少步采样和准确似然计算，比现有方法快两个数量级


<details>
  <summary>Details</summary>
Motivation: 热力学平衡中分子状态的可扩展采样是统计物理学的长期挑战。现有Boltzmann生成器使用连续归一化流，但似然计算成本极高，每个样本需要数千次函数评估，限制了其应用

Method: 提出FALCON方法，通过引入混合训练目标来鼓励可逆性，从而实现少步采样和足够准确的似然计算，适用于重要性采样应用

Result: FALCON在分子Boltzmann采样方面优于最先进的归一化流模型，比同等性能的CNF模型快两个数量级

Conclusion: FALCON方法解决了连续归一化流模型计算成本高的问题，实现了高效准确的分子状态采样，为热力学平衡采样提供了实用解决方案

Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.

</details>


### [74] [Mixture of Lookup Key-Value Experts](https://arxiv.org/abs/2512.09723)
*Zongcheng Wang*

Main category: cs.LG

TL;DR: MoLKV模型通过引入上下文感知的键值对专家机制，改进了MoLE模型仅基于输入ID的专家选择限制，在小规模评估中显著降低了验证损失。


<details>
  <summary>Details</summary>
Motivation: MoLE模型虽然适合资源受限设备，但其仅基于输入ID的上下文无关专家选择机制可能限制模型性能，需要更智能的专家激活方式。

Method: 提出MoLKV模型，将每个专家构建为键值对，通过输入生成的查询与当前序列缓存的键值专家交互，产生上下文感知的专家输出。

Result: 实验结果表明，MoLKV在小规模评估中实现了显著更低的验证损失，证明了上下文感知机制的有效性。

Conclusion: MoLKV通过上下文感知的键值对专家机制成功解决了MoLE模型的局限性，为资源受限设备上的LLM推理提供了更优的架构选择。

Abstract: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

</details>


### [75] [Knowledge Diversion for Efficient Morphology Control and Policy Transfer](https://arxiv.org/abs/2512.09796)
*Fu Feng,Ruixiao Shi,Yucheng Xie,Jianlu Shen,Jing Wang,Xin Geng*

Main category: cs.LG

TL;DR: DivMorph提出了一种模块化训练范式，通过知识分流学习可分解的控制器，实现跨形态和跨任务的通用策略学习，显著提升样本效率和模型压缩率。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的通用形态控制方法存在两个主要问题：1) 计算成本高，部署开销大；2) 跨任务泛化能力有限，每个新任务都需要从头训练。需要一种既能高效部署又能有效跨任务迁移的解决方案。

Method: DivMorph采用模块化训练范式，通过知识分流学习可分解控制器。具体方法：1) 在训练前通过SVD将随机初始化的Transformer权重分解为因子单元；2) 使用动态软门控根据任务和形态嵌入调制这些单元；3) 将单元分离为共享的"learngenes"和特定于形态/任务的"tailors"，实现知识解耦；4) 通过选择性激活相关组件实现高效策略部署。

Result: 实验表明DivMorph达到最先进性能：1) 在跨任务迁移中，样本效率比直接微调提高3倍；2) 单智能体部署时模型大小减少17倍。

Conclusion: DivMorph通过知识分流和模块化设计，实现了高效、可扩展的通用形态控制，在保持高性能的同时显著降低了计算成本和部署开销，支持有效的跨任务策略迁移。

Abstract: Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \textit{learngenes} and morphology- and task-specific \textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\times$ reduction in model size for single-agent deployment.

</details>


### [76] [Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering](https://arxiv.org/abs/2512.09810)
*Adithya K Moorthy,V Vijaya Saradhi,Bhanu Prasad*

Main category: cs.LG

TL;DR: 该论文提出了构建公平k近邻图和公平ε邻域图的新方法，通过在图的构建阶段就融入公平约束，确保敏感群体在局部图结构中按比例表示，从而为公平谱聚类提供预处理支持。


<details>
  <summary>Details</summary>
Motivation: 传统图聚类方法（如谱聚类）在构建图时存在偏见，可能使某些群体在图中代表性不足。现有的公平图聚类研究主要关注聚类算法本身，而忽视了图构建阶段的公平性问题。kNN和ε邻域图等常用图构建方法会在敏感群体间产生基于边的不平等影响，导致有偏的聚类结果。

Method: 提出了两种公平图构建方法：公平k近邻图和公平ε邻域图。这些方法在邻域选择的最早阶段就加入公平约束，确保每个节点的邻域中敏感群体按比例表示，同时保持几何一致性。通过在预处理阶段实现拓扑公平，为后续谱聚类提供公平的图结构基础。

Result: 在三个合成数据集、七个真实世界表格数据集和三个真实世界图像数据集上的实验证明，提出的公平图构建方法在公平谱聚类任务中超越了现有基线方法。公平图构建能够在不修改聚类算法本身的情况下，显著改善聚类结果的公平性。

Conclusion: 图构建阶段的拓扑公平对于实现公平谱聚类至关重要。通过在预处理阶段构建公平的图结构，可以自然地促进公平的聚类结果，无需修改聚类算法。这项工作填补了公平无监督学习中图构建公平性研究的空白，为公平谱聚类提供了有效的预处理方法。

Abstract: Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.

</details>


### [77] [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](https://arxiv.org/abs/2512.09850)
*Simone Cuonzo,Nina Deliu*

Main category: cs.LG

TL;DR: 将Conformal Prediction整合到bandit问题中，为顺序决策提供有限时间统计保证，特别在小差距场景下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统bandit策略（如Thompson Sampling和UCB）通常依赖分布假设或渐近保证，主要关注regret而忽视统计特性。作者旨在填补这一空白，将bandit策略的regret最小化潜力与统计保证相结合。

Method: 提出Conformal Bandits框架，将Conformal Prediction（CP）整合到bandit问题中。在投资组合分配应用中，进一步整合隐马尔可夫模型来捕捉金融市场的制度转换行为，以增强探索-利用权衡。

Result: 通过模拟研究和投资组合分配应用验证了框架的有效性。在小差距场景下，该框架在regret方面具有实际优势，并在传统UCB策略失败的情况下实现了名义覆盖保证。整合隐马尔可夫模型进一步提高了风险调整后的regret效率回报，同时保持覆盖保证。

Conclusion: Conformal Bandits框架成功地将bandit决策的regret最小化潜力与有限时间统计保证相结合，特别在小差距场景下表现出色，为顺序决策问题提供了更可靠的解决方案。

Abstract: We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.
  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.

</details>


### [78] [HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression](https://arxiv.org/abs/2512.09886)
*Gustavo Coelho Haase,Paulo Henrique Dourado da Silva*

Main category: cs.LG

TL;DR: HPM-KD是一个知识蒸馏框架，通过六个协同组件解决传统KD的四大限制：超参数敏感、师生容量差距、多教师协调不足和计算资源低效，实现10-15倍压缩并保持85%准确率，减少30-40%训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏技术存在四个关键限制：1) 对超参数敏感需要大量手动调优；2) 从大型教师模型到小型学生模型存在容量差距；3) 多教师场景下协调不足；4) 计算资源使用效率低下。

Method: 提出HPM-KD框架，包含六个协同组件：1) 基于元学习的自适应配置管理器，消除手动超参数调优；2) 自动确定中间模型的渐进蒸馏链；3) 学习动态样本权重的注意力加权多教师集成；4) 元学习温度调度器，在整个训练过程中自适应调整温度；5) 智能负载均衡的并行处理管道；6) 跨实验重用的共享优化内存。

Result: 在CIFAR-10、CIFAR-100和表格数据集上的实验表明：HPM-KD实现10-15倍压缩同时保持85%准确率保留，消除了手动调优需求，通过并行化减少30-40%训练时间。消融研究确认每个组件都有独立贡献（0.10-0.98个百分点）。

Conclusion: HPM-KD是一个有效的知识蒸馏框架，通过六个协同组件系统解决了传统KD的关键限制，实现了高效模型压缩和训练加速，已作为开源DeepBridge库的一部分提供。

Abstract: Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.

</details>


### [79] [Analysis of Dirichlet Energies as Over-smoothing Measures](https://arxiv.org/abs/2512.09890)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 本文分析了未归一化和归一化图拉普拉斯算子诱导的狄利克雷能量作为过平滑度量时的区别，指出归一化版本不满足节点相似性度量的公理化定义，并强调选择与GNN架构谱兼容的度量的重要性。


<details>
  <summary>Details</summary>
Motivation: 在GNN中，过平滑是一个重要问题，通常使用狄利克雷能量作为度量。然而，未归一化和归一化图拉普拉斯算子诱导的两种狄利克雷能量存在混淆，需要澄清它们的区别及其对GNN架构的适用性。

Method: 通过形式化分析两种狄利克雷能量的基本谱特性，比较它们在节点相似性度量公理化定义下的表现，特别是验证归一化版本是否满足Rusch等人提出的公理。

Result: 研究发现归一化图拉普拉斯算子诱导的狄利克雷能量不满足节点相似性度量的公理化定义，而两种定义在谱特性上存在关键区别，这些区别对于选择与GNN架构谱兼容的度量至关重要。

Conclusion: 选择过平滑度量时需要考虑其与GNN架构的谱兼容性，归一化图拉普拉斯算子诱导的狄利克雷能量不适合作为节点相似性度量，这一澄清有助于消除监控GNN动态时的模糊性。

Abstract: We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.

</details>


### [80] [Closing the Train-Test Gap in World Models for Gradient-Based Planning](https://arxiv.org/abs/2512.09929)
*Arjun Parthasarathy,Nimit Kalra,Rohun Agrawal,Yann LeCun,Oumayma Bounou,Pavel Izmailov,Micah Goldblum*

Main category: cs.LG

TL;DR: 提出改进世界模型训练方法，通过缩小训练-测试差距，使基于梯度的规划在10%时间预算下达到或超越传统CEM方法性能


<details>
  <summary>Details</summary>
Motivation: 基于梯度的规划虽然计算效率高，但性能一直落后于其他方法。世界模型训练时使用下一状态预测目标，但测试时却用于估计动作序列，存在训练-测试差距

Method: 提出训练时数据合成技术，改进世界模型训练以支持高效梯度规划。通过缩小训练-测试差距，使模型更适合梯度规划

Result: 在多种物体操作和导航任务中，该方法在10%的时间预算下达到或超越传统交叉熵方法(CEM)的性能

Conclusion: 通过改进世界模型训练方法，可以有效提升梯度规划的性能，使其在计算效率优势的基础上达到与经典方法相当甚至更好的性能

Abstract: World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.

</details>
