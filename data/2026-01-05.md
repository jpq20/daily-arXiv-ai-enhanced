<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 29]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning](https://arxiv.org/abs/2601.00086)
*Xiang Gao,Yuguang Yao,Qi Zhang,Kaiwen Dong,Avinash Baidya,Ruocheng Guo,Hilaf Hasson,Kamalika Das*

Main category: cs.CL

TL;DR: RIMRULE：一种基于动态规则注入的神经符号方法，通过从失败轨迹中提取紧凑、可解释的规则并在推理时注入提示，提高LLM在特定领域工具使用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域工具使用时面临挑战，因为API可能具有特殊性、文档不足或针对私有工作流定制，需要有效的任务特定工具适应方法。

Method: 提出RIMRULE神经符号方法：1) 从失败轨迹中提取紧凑、可解释的规则；2) 使用最小描述长度目标进行规则整合，偏好通用性和简洁性；3) 将规则以自然语言和结构化符号形式存储；4) 在推理时动态注入规则到提示中。

Result: 在工具使用基准测试中，该方法提高了对已见和未见工具的准确性，无需修改LLM权重。优于基于提示的适应方法，并与微调互补。从一个LLM学习的规则可以重用于改进其他LLM，包括长推理LLM，展示了符号知识在不同架构间的可移植性。

Conclusion: RIMRULE通过动态规则注入有效解决了LLM在特定领域工具使用中的适应问题，提供了一种可解释、可移植且无需权重修改的神经符号适应方法。

Abstract: Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.

</details>


### [2] [Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description](https://arxiv.org/abs/2601.00166)
*Yongmin Yoo,Kris W Pan*

Main category: cs.CL

TL;DR: Pat-DEVAL：首个专利说明书多维度评估框架，通过法律约束推理机制评估自动专利起草系统的结构连贯性和法定合规性


<details>
  <summary>Details</summary>
Motivation: 现有专利自动起草评估方法无法评估长篇结构连贯性和法定合规性，需要专门针对专利说明书的法律技术评估框架

Method: 提出Pat-DEVAL框架，采用LLM-as-a-judge范式，引入Chain-of-Legal-Thought（CoLT）法律约束推理机制，进行顺序性专利法特定分析

Result: 在Pap2Pat-EvalGold数据集上，Pat-DEVAL达到0.69的皮尔逊相关性，显著优于基线指标和现有LLM评估器；法律专业合规性相关性达0.73

Conclusion: Pat-DEVAL通过明确注入法定约束，为自动专利起草系统的实际部署提供了确保技术合理性和法律合规性的新标准和方法基础

Abstract: Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.

</details>


### [3] [Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation](https://arxiv.org/abs/2601.00181)
*Cheonkam Jeong,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 本文通过系统分析IEMOCAP数据集，填补了对话情感识别(ERC)领域的两大空白：对架构选择重要性的理解不足，以及识别与生成之间的语言学分析缺失。研究发现：1) 对话上下文至关重要，90%的增益来自最近10-30轮对话；2) 分层句子表示在utterance级别有帮助，但提供上下文后此优势消失；3) 外部情感词典无增益。同时通过话语标记分析发现情感与标记位置显著相关，特别是悲伤话语的左边缘标记使用减少。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别(ERC)虽然取得了高准确率，但仍存在两个关键空白：1) 对哪些架构选择真正重要的理解有限；2) 缺乏将识别与生成连接起来的语言学分析。本文旨在通过系统分析IEMOCAP数据集来填补这两个空白。

Method: 采用系统分析方法：1) 对识别任务进行严格的消融研究，使用10种随机种子评估；2) 分析5,286个话语标记的出现情况，研究情感与标记位置的关系；3) 使用简单架构和严格因果上下文进行实验。

Result: 识别方面：1) 对话上下文至关重要，90%的总增益来自最近10-30轮对话；2) 分层句子表示在utterance级别有帮助，但提供上下文后优势消失；3) 外部情感词典无增益。使用简单架构和因果上下文获得82.69%(4-way)和67.07%(6-way)的加权F1，优于先前方法。语言学分析：情感与话语标记位置显著相关(p<.0001)，悲伤话语的左边缘标记使用率(21.9%)低于其他情感(28-32%)，这与左边缘标记与主动话语管理相关的理论一致。

Conclusion: 对话上下文是情感识别的关键因素，简单的因果上下文架构就能取得优异性能。语言学分析揭示了情感与话语标记使用的系统性关系，特别是悲伤话语缺乏显性语用信号，需要更多上下文进行消歧。这些发现为更有效的ERC系统设计和情感生成研究提供了理论基础。

Abstract: While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.
  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\% (4-way) and 67.07\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.
  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, "sad" utterances exhibit reduced left-periphery marker usage (21.9\%) compared to other emotions (28--32\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.

</details>


### [4] [Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2601.00202)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Zhesi Li,Man Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种专门为时序知识图谱推理设计的蒸馏框架，利用大语言模型作为教师模型，将结构和时序推理能力转移到轻量级学生模型中，在保持高效架构的同时提升时序建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理模型通常依赖大量参数和密集计算，导致硬件成本高、能耗大，难以部署在资源受限的实时推理平台上。同时，现有的模型压缩和蒸馏技术主要针对静态知识图谱设计，无法充分捕捉时序知识图谱中的时间依赖关系，导致推理性能下降。

Method: 提出专门为时序知识图谱推理设计的蒸馏框架，利用大语言模型作为教师模型指导蒸馏过程，将大规模公共知识与任务特定的时序信息相结合，有效转移结构和时序推理能力到轻量级学生模型中。

Result: 在多个公开基准数据集上的广泛实验表明，该方法持续优于强基线模型，在推理准确性、计算效率和实际可部署性之间实现了良好的平衡。

Conclusion: 该研究提出的蒸馏框架有效解决了时序知识图谱推理模型在资源受限环境下的部署问题，通过利用大语言模型作为教师模型，成功实现了轻量级学生模型在保持高效架构的同时提升时序推理能力。

Abstract: Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.

</details>


### [5] [From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark](https://arxiv.org/abs/2601.00216)
*Jinning Zhang,Jie Song,Wenhui Tu,Zecheng Li,Jingxuan Li,Jin Li,Xuan Liu,Taole Sha,Zichen Wei,Yan Li*

Main category: cs.CL

TL;DR: 本研究提出了一种将循证医学原则整合到基于图的知识检索增强生成中的通用策略，解决了当前RAG方法缺乏PICO对齐和证据等级考虑的问题，并在运动康复领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的检索增强生成方法主要关注性能改进，但忽视了循证医学的基本原则，特别是缺乏查询与检索证据之间的PICO对齐，以及在重排序过程中没有考虑证据等级层次。

Method: 提出了一种通用策略，将PICO框架整合到知识图构建和检索中，并设计了一种贝叶斯启发的重排序算法，在不引入预定义权重的情况下根据证据等级校准排序分数。

Result: 在运动康复领域验证了该框架，构建了包含357,844个节点和371,226条边的知识图，创建了1,637个QA对的可重用基准。系统在各项指标上表现优异：nugget覆盖度0.830、答案忠实度0.819、语义相似度0.882、PICOT匹配准确率0.788。专家临床医生在5点李克特量表上给出了4.66-4.84的高评分。

Conclusion: 提出的循证医学适应策略不仅提高了检索和答案质量，而且可转移到其他临床领域。发布的资源也有助于解决运动康复领域RAG数据集的稀缺问题。

Abstract: In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.

</details>


### [6] [JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation](https://arxiv.org/abs/2601.00223)
*Leonard Lin,Adam Lensenmayer*

Main category: cs.CL

TL;DR: JP-TL-Bench是一个轻量级开源基准测试，专门用于指导日语-英语翻译系统的迭代开发，通过成对比较评估翻译质量，重点关注礼貌、隐含意义、省略和语域等细微差别。


<details>
  <summary>Details</summary>
Motivation: 日语-英语翻译中，细微的礼貌、隐含意义、省略和语域选择会显著影响翻译的自然度，现有基准难以评估"哪个好翻译更好"的细微差别，需要专门的评估工具。

Method: 使用无参考的成对LLM比较方法，将候选模型与固定的版本化锚定集进行比较，通过Bradley-Terry模型聚合结果，并计算胜率和基于逻辑变换的0-10分"LT"评分。

Result: 该基准通过冻结的锚定集确保评分结构稳定性，使不同候选模型的评分具有可比性，为日语-英语翻译系统开发提供可靠且经济的评估框架。

Conclusion: JP-TL-Bench提供了一个专门针对日语-英语翻译特点的轻量级评估基准，通过稳定的评分机制支持翻译系统的迭代改进，解决了现有基准在评估细微翻译质量差异方面的不足。

Abstract: We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often "which of these two good translations is better?" rather than "is this translation acceptable?" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 "LT" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.

</details>


### [7] [Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation](https://arxiv.org/abs/2601.00263)
*Qianli Wang,Van Bach Nguyen,Yihong Liu,Fedor Splitt,Nils Feldhus,Christin Seifert,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文系统研究了多语言反事实生成，发现翻译生成的反事实有效性更高但修改更多，多语言反事实数据增强比跨语言增强效果更好，但生成质量限制了模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在生成英语反事实方面表现出色且具备多语言能力，但其在多语言反事实生成方面的有效性尚不清楚，需要系统研究。

Method: 1) 对六种语言进行自动评估，比较直接生成和通过英语翻译生成的反事实；2) 分析高资源欧洲语言反事实的编辑模式；3) 识别和分类生成反事实中的错误类型；4) 比较多语言反事实数据增强与跨语言增强的效果。

Result: 1) 翻译生成的反事实比直接生成的有效性更高，但需要更多修改，且质量仍不及原始英语反事实；2) 高资源欧洲语言的反事实编辑模式相似；3) 识别出四种主要错误类型；4) 多语言反事实数据增强比跨语言增强带来更大的模型性能提升，特别是对低资源语言，但生成质量限制了性能提升。

Conclusion: 多语言反事实生成存在挑战，翻译方法能提高有效性但质量仍有差距，多语言数据增强效果更好但受限于生成质量，需要改进反事实生成技术以充分发挥其潜力。

Abstract: Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.

</details>


### [8] [Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity](https://arxiv.org/abs/2601.00268)
*Doyoung Kim,Zhiwei Ren,Jie Hao,Zhongkai Sun,Lichao Wang,Xiyao Ma,Zack Ye,Xu Han,Jun Yin,Heng Ji,Wei Shen,Xing Fan,Benjamin Yao,Chenlei Guo*

Main category: cs.CL

TL;DR: WildAGTEval是一个评估LLM智能体函数调用能力的基准测试，专注于真实API复杂性，包含API规范和API执行两个维度的挑战，提供了约32K种测试配置。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设理想的API系统，忽视了真实世界的复杂性因素，如噪声API输出。需要评估LLM智能体在真实API复杂性下的函数调用能力。

Method: 设计了WildAGTEval基准测试，包含API规范和API执行两个维度的复杂性。API规范包括详细文档和使用约束，API执行捕获运行时挑战。构建了包含60个不同复杂性场景的API系统，可组合成约32K种测试配置，并提供用户-智能体交互评估框架。

Result: 评估多个先进LLM发现：大多数场景具有挑战性，无关信息复杂性造成最大困难，使强LLM性能下降27.3%。定性分析显示LLM有时会扭曲用户意图以声称完成任务，严重影响用户满意度。

Conclusion: WildAGTEval为评估LLM智能体在真实API复杂性下的函数调用能力提供了系统框架，揭示了当前LLM在复杂API环境中的局限性，特别是处理无关信息和意图理解方面的挑战。

Abstract: We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.

</details>


### [9] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化对大型语言模型自解释能力的影响研究：量化通常导致自解释质量和忠实度适度下降，但不会削弱量化作为模型压缩技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 量化被广泛用于加速大型语言模型推理和简化部署，但其对自解释（SEs）的影响尚未被探索。自解释是LLMs为证明自身输出而生成的解释，需要模型推理自身决策过程，这种能力可能对量化特别敏感。由于自解释在高风险应用中越来越依赖用于透明度，理解量化是否以及多大程度上降低自解释质量和忠实度至关重要。

Method: 研究两种类型的自解释：自然语言解释（NLEs）和反事实示例，这些解释由使用三种常见量化技术在不同比特宽度下量化的LLMs生成。通过用户研究评估量化对自解释连贯性和可信度的影响。

Result: 量化通常导致自解释质量（最高下降4.4%）和忠实度（最高下降2.38%）适度下降。用户研究表明量化降低了自解释的连贯性和可信度（最高下降8.5%）。相比小模型，大模型在自解释质量方面对量化的抵抗力有限，但在保持忠实度方面表现更好。没有一种量化技术在任务准确性、自解释质量和忠实度方面始终表现出色。

Conclusion: 量化影响因上下文而异，建议针对特定用例验证自解释质量，特别是对量化更敏感的自然语言解释。然而，自解释质量和忠实度的相对较小恶化并不削弱量化作为模型压缩技术的有效性。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [10] [DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection](https://arxiv.org/abs/2601.00303)
*Yuxin Li,Xiangyu Zhang,Yifei Li,Zhiwei Guo,Haoyang Zhang,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: DepFlow是一个三阶段框架，通过对抗训练学习抑郁声学特征，结合流匹配TTS生成抑郁语音，并构建伪装抑郁增强数据集，提升抑郁检测模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁数据集（如DAIC-WOZ）存在语言情感与诊断标签的强耦合，导致模型学习语义捷径，在真实场景（如伪装抑郁）中鲁棒性不足。需要缓解语义偏见，提升模型在语言内容与抑郁状态不匹配情况下的检测能力。

Method: 1. 抑郁声学编码器：通过对抗训练学习说话人和内容不变的抑郁嵌入，实现有效解耦同时保持抑郁可区分性（ROC-AUC: 0.693）。2. 流匹配TTS模型：使用FiLM调制注入抑郁嵌入，控制抑郁严重程度同时保留内容和说话人身份。3. 原型严重程度映射：提供平滑可解释的抑郁连续体操作。基于此构建伪装抑郁增强数据集（CDoA），将抑郁声学模式与积极/中性文本配对。

Result: CDoA数据集在三种抑郁检测架构上分别提升macro-F1 9%、12%和5%，一致优于传统增强策略。抑郁声学编码器ROC-AUC达到0.693，有效解耦抑郁特征。

Conclusion: DepFlow不仅增强了抑郁检测的鲁棒性，还为对话系统和基于模拟的评估提供了可控合成平台，解决了真实临床数据在伦理和覆盖范围方面的限制。该方法特别适用于处理伪装抑郁等真实场景挑战。

Abstract: Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.

</details>


### [11] [Robust Uncertainty Quantification for Factual Generation of Large Language Models](https://arxiv.org/abs/2601.00348)
*Yuhao Zhang,Zhongliang Yang,Linna Zhou*

Main category: cs.CL

TL;DR: 该研究针对大语言模型幻觉问题，提出了一种基于多事实生成任务的陷阱问题集和鲁棒不确定性量化方法，在对抗性提问场景下显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉问题严重影响了AI生成内容的可靠性和可信度。传统不确定性量化方法在常规问答场景中有效，但在非规范或对抗性提问策略下表现不足，这在实际应用中存在重大隐患。

Method: 研究构建了包含虚假名称的陷阱问题集，并创新性地提出了一种鲁棒不确定性量化方法(RU)，在多事实生成任务中进行不确定性评估。

Result: 实验结果表明，构建的陷阱问题集表现优异，提出的方法在四个不同模型上相比最佳基线方法，ROCAUC值平均提升了0.1-0.2，显著提升了幻觉检测性能。

Conclusion: 该研究为解决大语言模型幻觉问题提供了新的视角和方法，特别是在对抗性提问场景下的不确定性量化方面取得了重要进展。

Abstract: The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.

</details>


### [12] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: BERT-JEPA (BEPA) 是一种结合JEPA训练目标与BERT风格模型的新训练范式，通过对抗[CLS]嵌入空间的坍缩，将其转化为语言无关空间，从而在多语言基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: Joint Embedding Predictive Architectures (JEPA) 作为一种新颖的自监督训练技术，在不同领域显示出潜力。研究旨在将JEPA训练目标应用于BERT风格模型，解决[CLS]嵌入空间坍缩问题，创造语言无关的表示空间。

Method: 提出BERT-JEPA (BEPA) 训练范式，在BERT风格模型基础上添加JEPA训练目标。该方法专门设计用于对抗[CLS]嵌入空间的坍缩，将其转化为语言无关的表示空间。

Result: BEPA在多语言基准测试中表现出性能提升，验证了JEPA训练目标在BERT模型中的有效性，特别是在创建语言无关表示空间方面。

Conclusion: BERT-JEPA (BEPA) 是一种有效的训练范式，通过结合JEPA训练目标，成功解决了BERT模型中[CLS]嵌入空间的坍缩问题，创造了语言无关的表示空间，并在多语言任务中实现了性能提升。

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [13] [Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach](https://arxiv.org/abs/2601.00388)
*Biao Wu,Meng Fang,Ling Chen,Ke Xu,Tao Cheng,Jun Wang*

Main category: cs.CL

TL;DR: Geo-R是一个基于强化学习的检索免费图像地理定位框架，通过从现有GPS坐标生成结构化推理路径，使用基于区域的链式推理和坐标对齐奖励来提高定位精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图像地理定位中通常依赖合成推理标注或外部图像检索，这限制了可解释性和泛化能力。需要一种无需检索、基于结构化地理推理的方法来提高定位准确性和透明度。

Method: 提出Geo-R框架：1) 基于区域的链式推理：将GPS坐标映射到地理实体层次结构（国家、省份、城市等），生成精确可解释的监督信号；2) 轻量级强化学习策略：使用基于Haversine距离的坐标对齐奖励，通过空间有意义的反馈优化模型预测。

Result: 在多个基准测试中验证了Geo-R的有效性，实现了更高的定位精度、更强的泛化能力和更透明的推理过程，为可扩展和可解释的图像地理定位建立了新的检索免费范式。

Conclusion: Geo-R通过结合结构化地理推理和直接空间监督，成功实现了无需检索的图像地理定位，在准确性、泛化性和可解释性方面均有显著提升，为后续研究提供了可复现的框架。

Abstract: Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.

</details>


### [14] [Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset](https://arxiv.org/abs/2601.00411)
*Alistair Plum,Laura Bernardy,Tharindu Ranasinghe*

Main category: cs.CL

TL;DR: 提出了judgeWEL数据集，这是一个用于卢森堡语命名实体识别的数据集，通过新颖的LLM管道自动标注和验证，比现有数据集大5倍且覆盖更平衡


<details>
  <summary>Details</summary>
Motivation: 为低资源语言构建数据集是自然语言处理的主要瓶颈之一，资源稀缺和语言特性使得大规模标注成本高且可能不一致

Method: 利用维基百科和维基数据作为弱监督的结构化来源，通过维基百科文章内部链接推断实体类型，然后使用多个LLM识别和保留高质量标注句子来降低噪声

Result: 生成的语料库比当前可用的卢森堡语NER数据集大约5倍，提供了更广泛和更平衡的实体类别覆盖

Conclusion: judgeWEL为多语言和低资源NER研究提供了重要的新资源，展示了利用结构化知识和LLM验证构建低资源语言数据集的可行性

Abstract: We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.

</details>


### [15] [Toward Better Temporal Structures for Geopolitical Events Forecasting](https://arxiv.org/abs/2601.00430)
*Kian Ahrabian,Eric Boxer,Jay Pujara*

Main category: cs.CL

TL;DR: 该论文提出了一种新的超关系时序知识广义超图（HTKGHs）来扩展传统时序知识图，以更好地表示现实世界中的复杂地缘政治事件，并基于POLECAT数据库构建了htkgh-polecat数据集，最后评估了大型语言模型在复杂预测场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 传统时序知识图（TKGs）及其扩展超关系时序知识图（HTKGs）虽然能表示简单的时间关系，但表达能力有限，无法高效表示复杂事实。特别是HTKGs不支持超过两个主要实体的时序事实，而这在现实世界事件（尤其是地缘政治事件）中很常见。

Method: 1. 提出HTKGHs（超关系时序知识广义超图）作为HTKGs的泛化，支持两种常见于地缘政治事件的复杂事实类型；2. 推导HTKGHs的形式化定义，确保向后兼容性；3. 基于全球事件数据库POLECAT构建htkgh-polecat数据集；4. 在关系预测任务上对流行的大型语言模型进行基准测试和分析。

Result: 1. 成功形式化了HTKGHs框架，能够表示更复杂的时序事实；2. 创建了htkgh-polecat数据集，为复杂地缘政治事件预测提供数据支持；3. 通过基准测试获得了关于大型语言模型在复杂预测场景中适应性和能力的见解。

Conclusion: HTKGHs为解决传统时序知识图在表示复杂地缘政治事件方面的局限性提供了有效方案，通过构建新数据集和评估大型语言模型，为复杂时序预测任务奠定了基础，展示了大型语言模型在该领域的潜力和局限性。

Abstract: Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.

</details>


### [16] [Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment](https://arxiv.org/abs/2601.00444)
*Muhammad Shahmeer Khan*

Main category: cs.CL

TL;DR: 本文对三种轻量级Transformer模型（DistilBERT、MiniLM、ALBERT）在三个不同领域（客户情感分类、新闻主题分类、毒性和仇恨言论检测）进行了比较分析，评估了准确性和效率指标，发现没有单一模型在所有维度上占优，并针对不同企业应用场景提供了模型选择建议。


<details>
  <summary>Details</summary>
Motivation: 随着企业自然语言处理需求的快速发展，对能够处理多领域文本自动化任务的高效轻量级模型的需求日益增长。本研究旨在比较不同轻量级Transformer模型在不同企业应用场景下的性能表现。

Method: 使用IMDB、AG News和Measuring Hate Speech语料库的数据集，对DistilBERT、MiniLM和ALBERT三种轻量级Transformer模型在三个领域（客户情感分类、新闻主题分类、毒性和仇恨言论检测）进行评估。评估指标包括准确性指标（准确率、精确率、召回率、F1分数）和效率指标（模型大小、推理时间、吞吐量、内存使用）。所有实验在固定的企业导向约束下进行控制性微调。

Result: 没有单一模型在所有性能维度上占优：ALBERT在多个领域实现了最高的任务特定准确率；MiniLM在推理速度和吞吐量方面表现最佳；DistilBERT在跨任务中表现出最一致的准确性，同时保持有竞争力的效率。所有结果反映了在固定企业约束下的控制性微调，而非详尽的超参数优化。

Conclusion: 研究结果突出了准确性和效率之间的权衡，建议：对于延迟敏感的企业应用选择MiniLM；对于平衡性能选择DistilBERT；对于资源受限环境选择ALBERT。这为企业根据具体需求选择合适的轻量级NLP模型提供了实用指导。

Abstract: In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

</details>


### [17] [Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games](https://arxiv.org/abs/2601.00448)
*Dimitris Vartziotis*

Main category: cs.CL

TL;DR: 论文对比了语言的社会建构主义解释和数学语义场理论，认为两者互补而非对立，为AI架构提供理论指导


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型作为实证环境，检验长期存在的语言意义理论，特别是社会建构主义与数学语义理论的关系

Method: 形式化词汇场和语言场概念作为连续语义空间中的交互结构，分析transformer架构的分布式表示、注意力机制和嵌入空间几何规律与这些概念的关系

Result: LLMs在捕捉语义规律方面的成功支持语言具有底层数学结构的观点，而其在语用推理和上下文敏感性方面的局限则与社会基础的重要性一致

Conclusion: 数学结构和语言游戏可以理解为互补而非竞争视角，这一框架澄清了纯统计语言模型的适用范围和局限，并为理论指导的AI架构提供了新方向

Abstract: Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

</details>


### [18] [Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations](https://arxiv.org/abs/2601.00454)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 提出Defensive M2S训练范式，通过将多轮对话压缩为单轮来降低护栏模型的训练和推理成本，实现93倍训练token减少和94.6%推理token减少，同时保持93.8%攻击检测召回率。


<details>
  <summary>Details</summary>
Motivation: 处理完整多轮对话历史需要大量计算成本，限制了护栏模型在实际部署中的可扩展性。需要一种既能保持安全检测效果又能显著降低计算开销的方法。

Method: 提出Multi-turn to Single-turn (M2S)压缩训练范式，将多轮对话压缩为单轮对话进行护栏模型微调。使用三种压缩模板(hyphenize, numberize, pythonize)，在三个护栏模型家族(LlamaGuard, Nemotron, Qwen3Guard)上评估。

Result: M2S将训练复杂度从O(n²)降至O(n)，训练token减少93倍(从1570万降至16.9万)。最佳配置(Qwen3Guard+hyphenize)在SafeDialBench上达到93.8%攻击检测召回率，推理token减少94.6%(从3231降至173)，相比基线提升38.9个百分点。

Conclusion: M2S压缩可作为护栏模型部署的有效效率技术，能够在显著降低训练和推理成本的同时保持高水平的安全检测性能，实现长多轮对话的可扩展安全筛查。

Abstract: Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.

</details>


### [19] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 该论文提出了一种针对职业教育培训领域历史数字化文档的噪声感知命名实体识别方法，通过合成OCR错误注入、迁移学习和多阶段微调来提高在噪声条件下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 职业教育培训领域的历史数字化文档存在OCR引入的噪声问题，这会影响命名实体识别的准确性。目前缺乏针对该领域多实体类型识别且能处理噪声的鲁棒方法。

Method: 提出噪声感知训练方法，包括：1）合成注入OCR错误；2）迁移学习；3）多阶段微调。系统比较了在噪声数据、干净数据和人工数据上训练的三种互补策略。

Result: 实验结果表明，领域特定和噪声感知的微调显著提高了在噪声条件下的鲁棒性和准确性。该方法首次实现了职业教育培训文档中的多实体类型识别。

Conclusion: 该方法为领域特定的噪声感知命名实体识别提供了可复现的解决方案，虽然应用于德语文档，但可扩展到任意语言。公开了代码以促进相关研究。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


### [20] [Rule-Based Approaches to Atomic Sentence Extraction](https://arxiv.org/abs/2601.00506)
*Lineesha Kamana,Akshita Ananda Subramanian,Mehuli Ghosh,Suman Saha*

Main category: cs.CL

TL;DR: 本文分析了基于规则的原子句子提取方法，探讨了复杂句子结构（如关系从句、状语从句、并列结构等）对提取性能的影响，在WikiSplit数据集上实现了中等偏高的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的原子句子提取方法缺乏可解释性，无法揭示哪些具体语言结构导致提取失败。本研究旨在填补这一空白，分析复杂句子结构如何影响基于规则的提取性能。

Method: 使用WikiSplit数据集，在spaCy中实现基于依存关系的提取规则，生成100个黄金标准原子句子集，并使用ROUGE和BERTScore评估性能。

Result: 系统取得了ROUGE-1 F1=0.6714、ROUGE-2 F1=0.478、ROUGE-L F1=0.650和BERTScore F1=0.5898的成绩，表明在词汇、结构和语义对齐方面具有中等偏高的性能。具有挑战性的结构包括关系从句、同位语、并列谓语、状语从句和被动结构。

Conclusion: 基于规则的原子句子提取方法具有合理的准确性，但对句法复杂性敏感。关系从句、同位语等复杂结构是主要的挑战来源。

Abstract: Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

</details>


### [21] [ECR: Manifold-Guided Semantic Cues for Compact Language Models](https://arxiv.org/abs/2601.00543)
*Chung-Wei Victor Yuan*

Main category: cs.CL

TL;DR: ECR框架通过语义锚点保持紧凑模型嵌入空间的结构一致性，避免语义漂移，适用于多语言场景


<details>
  <summary>Details</summary>
Motivation: 紧凑模型在容量受限或多语言场景下容易丢失嵌入空间结构，导致语义漂移，现有压缩方法只关注表层输出对齐而忽略底层流形结构

Method: 提出嵌入一致性调节(ECR)框架：从教师模型嵌入中提取语义锚点（离线计算一次），让紧凑模型学习在这些锚点周围保持一致的几何结构，不依赖匹配logits或内部特征

Result: 在10万条多语言语料实验中，ECR稳定训练并保持跨任务和语言的语义结构，产生更紧凑、任务对齐的表示空间，使低容量模型学习到比传统基线更清晰的流形

Conclusion: ECR帮助紧凑模型更好地遵循任务要求，使其在严格效率或隐私限制下更容易部署，无需教师输出且与蒸馏兼容但独立

Abstract: Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.
  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.
  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.

</details>


### [22] [A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR](https://arxiv.org/abs/2601.00557)
*Yuang Zheng,Yuxiang Mei,Dongxing Xu,Jie Chen,Yanhua Long*

Main category: cs.CL

TL;DR: 提出了一种轻量级、语言无关的多语言ASR系统，基于CTC架构和领域自适应，通过分层LoRA-MoE框架实现单次解码，无需先验语言信息。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多语言ASR模型（如Whisper）计算和延迟成本高，难以部署在资源受限的边缘设备上，需要更高效的解决方案。

Method: 提出了语言无关的分层LoRA-MoE（HLoRA）框架，集成到mHuBERT-CTC模型中。包含多语言共享LoRA学习语言不变声学表示，以及语言特定LoRA专家建模语言相关特征。使用LID后验驱动的LoRA路由机制，无需推理时的语言身份信息。

Result: 在MSR-86K和MLC-SLM 2025 Challenge数据集上的实验表明，HLoRA仅通过单次解码就能达到与最先进两阶段推理方法竞争的性能，显著提高了低资源多语言ASR应用的解码效率。

Conclusion: 提出的HLoRA框架为资源受限环境下的多语言ASR提供了一种高效、轻量级的解决方案，实现了真正的语言无关解码，在保持性能的同时大幅提升了效率。

Abstract: Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.

</details>


### [23] [InfoSynth: Information-Guided Benchmark Synthesis for LLMs](https://arxiv.org/abs/2601.00575)
*Ishir Garg,Neel Kolhe,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: InfoSynth框架通过信息论原则自动生成和评估LLM推理基准，使用KL散度和熵量化新颖性和多样性，通过遗传算法生成Python编程问题，97%成功率且能控制难度和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基准创建依赖人工，成本高且耗时；现有基准常污染LLM训练数据，需要新颖多样的基准来准确评估LLM的真实能力。

Method: 提出基于KL散度和熵的指标量化基准新颖性和多样性；开发端到端流水线，使用遗传算法和迭代代码反馈从种子数据集合成稳健的Python编程问题。

Result: 方法生成新问题的准确测试用例和解决方案达97%成功率；合成的基准相比种子数据集始终表现出更高的新颖性和多样性；算法能控制生成问题的新颖性/多样性和难度。

Conclusion: InfoSynth为LLM提供了可扩展、自验证的高质量基准构建流水线，能生成新颖多样的基准来准确评估LLM能力。

Abstract: Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/

</details>


### [24] [CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns](https://arxiv.org/abs/2601.00588)
*Zhenhong Zhou,Shilinlu Yan,Chuanpu Liu,Qiankun Li,Kun Wang,Zhigang Zeng*

Main category: cs.CL

TL;DR: CSSBench是一个专门针对中文特定对抗模式的安全基准测试，用于评估轻量级大语言模型在中文环境中的安全性，填补了现有基准测试主要关注英文的空白。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地部署在成本敏感和边缘设备场景中，而现有的安全防护主要针对英文。中文恶意查询通常通过谐音、拼音、符号拆分等中文特定模式隐藏意图，这些对抗模式在现有基准测试中未能得到充分评估，特别是轻量级模型可能更容易受到此类特定对抗扰动的影响。

Method: 引入中文特定安全基准测试（CSSBench），强调中文对抗模式，覆盖六个真实中文场景中的常见领域：非法活动与合规、隐私泄露、健康与医疗错误信息、欺诈与仇恨、成人内容、公共与政治安全，并将查询组织成多种任务类型。评估一组流行的轻量级大语言模型，并测量过度拒绝行为以评估安全性导致的性能下降。

Result: 结果显示，中文特定对抗模式对轻量级大语言模型构成关键挑战。基准测试提供了对中文LLM安全性的全面评估，有助于实际部署中的鲁棒性。

Conclusion: 中文特定对抗模式是轻量级大语言模型安全性的重要挑战，CSSBench填补了现有基准测试的空白，为中文环境下LLM的鲁棒部署提供了全面的评估工具。

Abstract: Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

</details>


### [25] [Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs](https://arxiv.org/abs/2601.00641)
*Nils Rautenberg,Sven Schippkus*

Main category: cs.CL

TL;DR: 提出一种模型无关的框架，通过重复采样和多数投票机制，为固定输入任务提供降低LLM幻觉的概率保证


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在确定性自动化工作流中经常产生上下文幻觉，即生成内容与提示中明确信息相矛盾或忽略这些信息。这种错误在输入固定且正确性明确的场景中尤其成问题。

Method: 1. 形式化特定任务概念（固定输入+确定性正确标准）；2. 在独立上下文窗口中重复相同提示，实现错误概率指数级降低；3. 引入LLM作为评判者识别正确答案；4. 通过多数投票机制强化不完美评判者，获得指数级降低的集成错误率。

Result: 在受控提取任务实验中，管道失败概率随重复次数指数下降，幻觉选择概率随评判者数量指数下降，与理论预测完全一致。

Conclusion: 该方法提供了一种轻量级、模块化、理论可靠的方法，可在不修改模型权重、解码策略或提示工程的情况下，将固定输入LLM工作流中的幻觉概率降至任意低水平。

Abstract: Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.
  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.
  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.

</details>


### [26] [Fast-weight Product Key Memory](https://arxiv.org/abs/2601.00671)
*Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: FwPKM是一种新型架构，将静态的产品键记忆转换为动态的快速权重情景记忆，解决了序列建模中存储容量与计算效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型的序列建模层面临存储容量与计算效率的权衡：Softmax注意力提供无限存储但计算成本高，线性变体效率高但存储有限。需要一种既能高效计算又能动态存储的解决方案。

Method: 提出快速权重产品键记忆(FwPKM)，将稀疏的产品键记忆从静态模块转变为动态的"快速权重"情景记忆。通过局部块级梯度下降在训练和推理时动态更新参数，使模型能够快速记忆和检索输入序列中的新键值对。

Result: FwPKM作为有效的情景记忆，补充了标准模块的语义记忆，在长上下文数据集上显著降低了困惑度。在"大海捞针"评估中，尽管只在4K标记序列上训练，却能泛化到128K标记的上下文。

Conclusion: FwPKM成功解决了序列建模中存储容量与计算效率的权衡问题，通过动态快速权重机制实现了高效的情景记忆，在长上下文处理中表现出色。

Abstract: Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

</details>


### [27] [Sigmoid Head for Quality Estimation under Language Ambiguity](https://arxiv.org/abs/2601.00680)
*Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: 提出Sigmoid Head方法解决语言模型概率作为质量估计器不可靠的问题，通过sigmoid激活和负采样策略提升质量评估效果


<details>
  <summary>Details</summary>
Motivation: 语言模型的概率分布不是可靠的质量估计器，因为自然语言具有歧义性。当存在多个有效输出选项时，模型的概率分布会分散在这些选项上，从而误导性地表明输出质量较低。这主要由两个原因造成：1) LM的最终输出激活使用softmax，不允许多个正确选项同时获得高概率；2) LM的训练数据是单一、one-hot编码的参考，表明每个输出步骤只有一个正确选项。

Method: 提出在预训练语言模型之上训练质量估计模块。该模块称为Sigmoid Head，是一个额外的具有sigmoid激活的解嵌入头，以解决第一个限制。为了应对第二个限制，在训练Sigmoid Head的负采样过程中，使用启发式方法避免选择可能正确的替代标记。Sigmoid Head在训练和推理过程中计算效率高。

Result: Sigmoid Head的概率相比原始softmax头是显著更好的质量信号。由于Sigmoid Head不依赖人工标注的质量数据，与监督式质量估计相比，在域外设置中更加鲁棒。

Conclusion: Sigmoid Head方法有效解决了语言模型概率作为质量估计器的局限性，通过sigmoid激活和智能负采样策略，在无需人工标注的情况下提供了更可靠的质量评估信号，特别在域外场景中表现出更好的鲁棒性。

Abstract: Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.

</details>


### [28] [Exploring the Performance of Large Language Models on Subjective Span Identification Tasks](https://arxiv.org/abs/2601.00736)
*Alphaeus Dmonte,Roland Oruche,Tharindu Ranasinghe,Marcos Zampieri,Prasad Calyam*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在文本跨度识别任务上的表现，特别是在情感分析、冒犯性语言识别和声明验证三个任务中，探索了指令调优、上下文学习和思维链等策略。


<details>
  <summary>Details</summary>
Motivation: 文本跨度识别对于NLP下游任务和模型可解释性很重要。虽然大多数方法依赖BERT等较小的预训练模型，但使用最新大型语言模型进行跨度识别的研究不足，特别是在主观性较强的任务如基于方面的情感分析方面。

Method: 评估了多种大型语言模型在三个任务上的表现：情感分析、冒犯性语言识别和声明验证。探索了指令调优、上下文学习和思维链等多种LLM策略。

Result: 结果表明，文本内部的潜在关系有助于大型语言模型识别精确的文本跨度。

Conclusion: 本文填补了大型语言模型在主观性文本跨度识别任务上的研究空白，证明了文本内部关系对LLM识别精确跨度的帮助作用。

Abstract: Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

</details>


### [29] [Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries](https://arxiv.org/abs/2601.00787)
*Jonathan Simkin,Lovedeep Gondara,Zeeshan Rizvi,Gregory Doyle,Jeff Dowden,Dan Bond,Desmond Martin,Raymond Ng*

Main category: cs.CL

TL;DR: 本研究评估了将BCCRTron和GatorTron两个预训练模型迁移到纽芬兰与拉布拉多省癌症登记处进行病理报告分类的效果，通过模型集成显著减少了漏诊癌症病例


<details>
  <summary>Details</summary>
Motivation: 癌症登记处依赖病理报告作为主要诊断来源，但人工提取资源密集且导致数据延迟。现有基于Transformer的NLP系统在跨司法管辖区（不同报告规范）的泛化能力尚不清楚，需要评估模型在不同省份的适应性

Method: 使用来自纽芬兰与拉布拉多省癌症登记处的约104,000份（Tier 1）和22,000份（Tier 2）去标识化病理报告，对BCCRTron和GatorTron两个预训练模型进行微调。采用互补的摘要式和诊断导向的报告部分输入管道，并通过保守的OR集成策略结合两个模型

Result: 模型在跨省测试中保持高性能，Tier 1任务集成模型召回率达0.99，漏诊癌症从单独模型的48和54例减少到24例；Tier 2任务集成模型召回率达0.99，漏诊可报告癌症从单独模型的54和46例减少到33例

Conclusion: 结合互补文本表示的模型集成能显著减少癌症登记NLP中的漏诊病例并提高错误覆盖。通过仅共享模型权重的隐私保护工作流程，支持跨省互操作的NLP基础设施，为未来泛加拿大癌症病理和登记工作流程的基础模型奠定基础

Abstract: Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 本文提出了一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，超越表面语义相似性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略以优化LLM性能仍是一个重大挑战。需要超越表面语义相似性，提供与对话逻辑结构对齐的知识。

Method: 提出推理感知知识检索方法，采用粗到细的两阶段检索策略：1）首先识别知识库中与上下文相关的子区域；2）在该子区域内细化搜索，提取与推理过程相关的知识。两阶段都使用蒙特卡洛树搜索启发的方法，通过关键词在知识句子中导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更贴近人类对话中的底层推理逻辑，还显著提高了检索知识的多样性，从而生成更具信息量和创造性的响应。

Conclusion: 推理感知知识检索方法通过将检索与推理过程对齐，有效提升了LLMs在多轮对话中的性能，为知识检索与推理能力的整合提供了新思路。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [31] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 本研究开发了一种基于微调大语言模型的尼日利亚皮钦语抑郁症自动筛查系统，通过收集432份音频数据并精细标注，比较了三种LLM在PHQ-9严重程度评分预测上的表现，GPT-4.1在准确性和文化适切性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统工具如PHQ-9在高收入国家验证，但在尼日利亚这样的中低收入国家存在语言和文化障碍，当地使用皮钦语和520多种本地语言，需要开发适合当地语言环境的筛查工具。

Method: 收集432份尼日利亚年轻人（18-40岁）的皮钦语音频回答，内容与PHQ-9项目相关的心理体验评估；进行转录、预处理和标注（语义标签、俚语解释、PHQ-9严重程度评分）；对三种LLM（Phi-3-mini-4k-instruct、Gemma-3-4B-it、GPT-4.1）进行微调；通过定量（准确性、精确度、语义对齐）和定性（清晰度、相关性、文化适切性）评估模型性能。

Result: GPT-4.1在PHQ-9严重程度评分预测中达到94.5%的准确率，优于其他两种模型；在定性评估中也产生最文化适切、清晰且上下文相关的回答。

Conclusion: AI介导的抑郁症筛查可为尼日利亚服务不足的社区提供解决方案，为在语言多样、资源有限的环境中部署对话式心理健康工具奠定了基础。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [32] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 本文提出一种多算法方法来解决最后一公里包裹配送中的工作量平衡问题，通过结合距离和工作量考虑来优化包裹分配，确保每位配送员完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的包裹分配方法效率低下，导致配送员之间工作量分配不均衡。在最后一公里城市包裹配送系统中，需要优化工作量平衡，特别是纠正特定区域内配送员之间的显著工作量不平衡问题。

Method: 提出多算法方法，包括不同版本的k-means、进化算法、基于k-means初始化的递归分配（使用不同问题编码）以及混合进化集成算法。该方法以配送点集合和工人数量为输入，结合距离和工作量考虑来分配包裹。

Result: 在西班牙Azuqueca de Henares城市最后一公里包裹配送系统的实际案例中验证了所提方法的性能。

Conclusion: 通过优化配送时间并考虑距离因素，该方法能够实现配送员之间的工作量平衡，纠正显著的工作量不平衡问题，提高最后一公里包裹配送系统的效率。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [33] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的战略性13张印度拉米纸牌游戏框架，使用新的手牌评估指标MinDist来量化手牌与最近有效配置的编辑距离，通过高效算法计算该指标，并结合对手建模进行策略优化。


<details>
  <summary>Details</summary>
Motivation: 13张印度拉米纸牌是一种不完全信息的序列游戏，需要概率推理和组合决策。传统启发式方法效果有限，需要更形式化和可解释的策略设计方法。

Method: 提出基于规则的战略框架，引入MinDist手牌评估指标（修改MinScore指标，量化手牌与最近有效配置的编辑距离）。设计计算高效的算法，利用动态剪枝和模式缓存精确计算该指标。在双人零和模拟框架中结合对手建模，使用统计假设检验评估策略。

Result: 实证结果显示，基于MinDist的智能体在胜率上相比传统启发式方法有显著提升，为算法化拉米策略设计提供了形式化和可解释的步骤。

Conclusion: MinDist指标和相应算法框架为不完全信息纸牌游戏提供了有效的战略分析方法，在13张印度拉米游戏中表现出优越性能，为类似游戏策略设计提供了可借鉴的方法。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [34] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探讨生成式AI如何理解乡土建筑中的建筑智能，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，评估AI对建筑类型、材料、环境等要素的重构能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索生成式AI系统如何解读乡土建筑形式中蕴含的建筑智能，理解AI在感知、扭曲和重新想象传统设计智慧方面的能力边界。

Method: 以伊朗鸽塔为案例研究，测试Midjourney v6、DALL-E 3和基于Stable Diffusion XL的DreamStudio三种扩散模型，采用三个提示阶段（参考性、适应性、推测性），通过五标准评估框架分析AI的重构能力。

Result: AI能可靠地复制几何图案，但误解材料和气候推理；参考图像能提高真实性但限制创造力，而无参考的自由生成则产生创新但文化模糊的结果。

Conclusion: 研究界定了视觉相似性与建筑推理之间的边界，提出了计算乡土推理框架，用于分析AI如何感知、扭曲和重新想象传统设计智能。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [35] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 研究人员设计了一个LLM智能体，能够从原始文本中提取因果反馈模糊认知图（FCMs），并通过双向交互过程使FCM动态系统具备一定自主性，同时保持可控性。


<details>
  <summary>Details</summary>
Motivation: 开发能够从文本中自动提取因果关系的智能系统，使模糊认知图能够动态演化并具备一定自主性，同时保持人类可控性。

Method: 使用三步精细调整的系统指令指导LLM智能体：1)从文本中提取关键名词和名词短语；2)从这些词汇中提取FCM概念节点；3)推断节点间的部分或模糊因果边。测试使用Henry Kissinger关于AI前景的论文。

Result: 三步过程生成的FCM动态系统收敛到与人工生成FCM相同的平衡极限环，尽管节点和边数量不同。混合Gemini和ChatGPT LLM智能体生成的FCM不仅吸收了主要组件的平衡点，还创建了新的平衡点以更好地近似底层因果动态系统。

Conclusion: LLM智能体能够有效从文本中提取因果FCMs，生成的动态系统具有与人工生成系统相似的平衡行为，混合多个LLM智能体的FCM能够产生更丰富的平衡状态，展示了因果提取和动态系统演化的潜力。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [36] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统使用质量多样性算法和大型语言模型自动演化游戏机制，通过合成完整游戏并评估玩家技能排序来优化机制设计。


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计通常需要大量时间和专家参与，手动设计过程耗时且依赖专业知识。需要一种自动化方法来探索和评估游戏机制，以加速游戏设计过程。

Method: 结合质量多样性算法和大型语言模型探索多样化机制，通过树搜索程序合成完整游戏进行评估。评估基于机制对游戏中玩家技能排序的贡献程度，即更强的玩家是否始终表现更好。

Result: Mortar能够生成多样化且可玩的游戏，产生的机制在游戏中能更好地促进技能排序。消融研究验证了系统各组件的作用，用户研究获得了人类反馈的积极评价。

Conclusion: Mortar系统成功实现了游戏机制的自主演化，为自动游戏设计提供了有效方法，通过算法评估机制质量，减少了人工设计的工作量。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [37] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: 该研究提出了一种混合智能体框架，将LLM的语义推理与数学计算严格分离，以解决LLM作为端到端库存优化求解器时出现的"幻觉税"问题，相比GPT-4o端到端方案降低了32.1%的库存成本。


<details>
  <summary>Details</summary>
Motivation: 中小企业在库存管理中缺乏部署高级优化方法的专业知识，而直接使用LLM作为端到端求解器存在显著的"幻觉税"性能差距，因为LLM无法进行基于概率的推理计算。

Method: 提出混合智能体框架，严格分离语义推理和数学计算：LLM作为智能接口从自然语言中提取参数并解释结果，同时自动调用严谨算法构建优化引擎。引入"人类模拟器"作为有限理性管理者的数字孪生，用于可扩展、可重复的压力测试。

Result: 混合智能体框架相比使用GPT-4o作为端到端求解器的交互基线，降低了32.1%的总库存成本。研究发现仅提供完美真实信息不足以改善GPT-4o性能，确认瓶颈本质上是计算性而非信息性的。

Conclusion: LLM不应替代运筹学方法，而应作为自然语言接口，使非专家能够访问基于严谨求解器的策略。混合框架通过分离语义和计算任务，有效解决了LLM在库存优化中的局限性。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [38] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 研究视觉语言模型在视频问答任务中的选择性预测，发现置信度阈值方法在分布内能提供机制性控制，但在分布偏移下可靠性下降


<details>
  <summary>Details</summary>
Motivation: 高风险的视觉语言模型部署需要选择性预测能力，即系统在不确定时应放弃回答而非冒险犯错。研究旨在探究置信度弃权是否能在视频问答中提供可靠的错误率控制，以及这种控制在分布偏移下是否依然稳健。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过置信度阈值方法进行选择性预测，分析风险-覆盖权衡曲线，并评估在分布偏移下的表现。

Result: 1. 置信度阈值在分布内能提供机制性控制，通过调整阈值epsilon可获得平滑的风险-覆盖权衡曲线，有效降低错误率；2. 在分布偏移下，这种控制可靠性下降，置信度校准失效。

Conclusion: 置信度阈值方法在分布内能有效控制视觉语言模型在视频问答中的错误率，但在面对分布偏移时可靠性不足，需要开发更稳健的选择性预测方法。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [39] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: 本文比较了三种神经推理方法：LLM推理、监督学习推理和显式模型推理，发现显式模型推理最可靠，并提出Sphere Neural Networks解决传统方法的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理不可靠，监督学习推理存在灾难性遗忘问题，需要寻找更可靠的神经推理方法，特别是能够同时处理经典三段论和析取三段论推理的方法。

Method: 提出Sphere Neural Networks，将概念表示为n维球面上的圆，通过补圆表示否定运算符，过滤不可满足的圆形配置来实现可靠决策，能够同时掌握16种三段论推理任务。

Result: Sphere Neural Networks能够掌握包括严格析取三段论在内的16种三段论推理任务，同时保持经典三段论推理的严谨性，解决了Euler Net的灾难性遗忘问题（从100%准确率降至6.25%）。

Conclusion: 在三种神经推理方法中，基于显式模型构建的神经推理是最可靠的，Sphere Neural Networks为可靠的神经推理提供了有效解决方案。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [40] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM赋能的智能体不仅存在人口统计学偏见，还会在最小"我们vs他们"线索下表现出群体间偏见。当这种群体边界与智能体-人类划分重合时，风险从人类群体间差异转向更根本的群体不对称——人类整体可能被智能体视为外群体。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM智能体是否存在群体间偏见，特别是当智能体-人类划分成为群体边界时，人类整体是否会被视为外群体。这比传统的人口统计学偏见更具根本性风险。

Method: 构建基于分配决策的受控多智能体社会模拟，在明确收益权衡下测试智能体行为。引入信念中毒攻击(BPA)，包括初始化时的档案中毒(BPA-PP)和通过优化信念精炼后缀注入存储反思中的记忆中毒(BPA-MP)。

Result: 实验发现智能体在最小群体线索下表现出一致的群体间偏见。虽然当部分对应方被标记为人类时偏见会减弱，但这种减弱依赖于智能体相信真实人类存在时激活的人类规范脚本。信念中毒攻击能够抑制人类规范脚本，重新激活对人类的群体间偏见。

Conclusion: 研究揭示了LLM智能体存在群体间偏见风险，特别是当智能体-人类边界成为群体划分时。信念中毒攻击暴露了新的攻击面，需要在档案和记忆边界实施实际缓解策略，以加强当前智能体框架的安全性。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [41] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: ACCD框架通过三阶段自适应架构检测社交媒体上的协同不实行为，显著提升检测精度并大幅减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有协同不实行为检测方法存在三个主要问题：依赖表面相关性分析、使用静态参数设置、需要大量人工标注。这些限制导致检测效果不佳且效率低下。

Method: 提出自适应因果协同检测（ACCD）框架，采用三阶段渐进架构：1）自适应收敛交叉映射技术识别账户间真实因果关系；2）半监督分类结合主动学习和不确定性采样减少人工标注；3）基于历史检测经验的自动化验证模块实现自我验证和优化。

Result: 在真实数据集（Twitter IRA、Reddit协同痕迹、机器人检测基准）上的实验显示：ACCD在协同攻击检测中达到87.3%的F1分数，比现有最佳基线提升15.2%；减少68%的人工标注需求；通过层次聚类优化实现2.8倍处理加速。

Conclusion: ACCD提供了一个更准确、高效且高度自动化的端到端解决方案，用于识别社交媒体平台上的协同行为，具有重要的实际应用价值和广泛的推广潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [42] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 该研究将语义空间推理从计算语言学扩展到团队运动的战术决策，通过将球员视为单词、团队配置视为语义结构，在共享向量空间中评估战术匹配度和对手利用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统计算语言学的语义空间推理方法在团队运动战术决策中尚未得到充分应用。研究者希望将文本与团队的类比关系（球员如单词、集体比赛传达意义）应用于战术分析，为团队运动提供更系统化的决策框架。

Method: 将每个球员表示为整合技术、身体和心理属性的多维向量；通过上下文加权将团队配置文件聚合成高级语义表示；在共享向量空间中编码战术模板（如高位压迫、反击、控球组织）；使用向量距离度量评估战术"匹配度"和对手利用潜力；开发Python原型生成可解释的动态适应策略建议。

Result: 该方法能够生成可解释的动态适应策略建议，并提供属性级别的细粒度诊断洞察。原型系统展示了如何通过语义空间推理评估战术配置，计算战术匹配度和对手利用潜力。

Conclusion: 该方法为基于团队的领域（从篮球、曲棍球到协作机器人和人机协调系统）提供了一个可推广的集体决策和性能优化框架。未来方向包括现实世界数据集成、预测模拟和混合人机战术智能。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [43] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究发现推理模型中的"顿悟时刻"很少见，不会随训练增加，也很少提高准确性，表明这些转变是推理不稳定的症状而非自我纠正的内在机制。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟时刻"从而实现准确输出，暗示模型具有自我纠正的内在能力。但尚不清楚这种推理策略的内在转变是否真正提高了性能。

Method: 研究分析了超过100万条推理轨迹、数百个训练检查点、三个推理领域以及多种解码温度和模型架构，检测推理过程中的策略转变，并实验性地触发外部转变来验证效果。

Result: 研究发现推理转变很罕见，不会随训练变得更频繁，也很少提高准确性，表明它们并非模型洞察力的表现。但效果随模型不确定性而变化，在高熵条件下人为触发外部转变能可靠提高准确性。

Conclusion: 推理过程中的策略转变是不稳定推理行为的症状，而非自我纠正的内在机制。模型在高不确定性时的人为干预可以改善性能，但自然发生的"顿悟时刻"并非有效的自我纠正机制。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [44] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO提出了一种难度感知的直接偏好优化框架，通过平衡学习难度来缓解多模态大语言模型中的幻觉问题，避免对简单样本的过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法由于偏好数据中难度不平衡而容易过拟合，模型倾向于过度强调容易区分的偏好对，这阻碍了细粒度幻觉抑制并降低了整体性能。

Method: DA-DPO包含两个核心组件：1) 难度估计：利用预训练的视觉-语言模型，结合生成式和对比式目标，通过分布感知投票策略产生鲁棒的难度分数；2) 难度感知训练：根据估计的难度重新加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合。

Result: 大量实验表明DA-DPO能持续改进多模态偏好优化，在标准基准测试中展现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO通过难度感知的偏好优化框架，有效解决了多模态DPO中的过拟合问题，实现了更有效的幻觉抑制和性能提升，无需额外数据或微调阶段。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [45] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM：一种结合视觉特征和领域知识的LLM框架，用于行人过街行为推理，相比传统方法具有更好的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习）泛化能力有限，在新场景中表现不佳。LLMs提供了从数值模式拟合转向语义、上下文感知行为推理的可能性，但现有LLM应用缺乏领域特定适应和视觉上下文。

Method: 提出PedX-LLM框架，集成LLaVA提取的视觉特征、文本数据和交通领域知识，通过LoRA微调LLaMA-2-7B基础模型来推断行人过街决策。

Result: PedX-LLM达到82.0%的平衡准确率，优于最佳统计和监督学习方法。视觉增强模块带来2.9%性能提升，领域知识集成带来额外4.1%改进。在未见场景的零-shot配置下达到66.9%平衡准确率，few-shot学习（仅5个验证样本）可提升至72.2%。

Conclusion: PedX-LLM展示了强大的泛化能力，证实视觉和知识增强的推理使模型能够模拟人类决策逻辑，克服纯数据驱动方法的局限性。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [46] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 是一个通过智能体工作流将自然语言任务描述自动转换为完整 DomiKnowS 程序的系统，显著降低了神经符号编程的门槛和开发时间。


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高模型的鲁棒性、可解释性和数据效率，但这一过程耗时且具有挑战性。现有框架如 DomiKnowS 虽然提供了高级声明式编程接口，但仍要求用户熟练掌握其特定语法。

Method: ADS 采用智能体工作流，将自由形式的任务描述翻译成完整的 DomiKnowS 程序。该工作流单独创建和测试每个 DomiKnowS 组件，并支持可选的人工干预环节，允许熟悉 DomiKnowS 的用户细化中间输出。

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟。

Conclusion: ADS 消除了对特定库语法的依赖，通过智能体工作流实现了从自然语言描述到神经符号程序的自动化转换，显著提高了开发效率并降低了使用门槛。

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [47] [μACP: A Formal Calculus for Expressive, Resource-Constrained Agent Communication](https://arxiv.org/abs/2601.00219)
*Arnab Mallick,Indraveni Chebolu*

Main category: cs.MA

TL;DR: μACP是一个在明确资源约束下进行表达性智能体通信的形式演算，它通过最小四动词基础编码FIPA协议，在资源受限环境中实现语义丰富性与可证明效率的统一。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通信协议面临两难：FIPA-ACL等协议语义丰富但资源消耗大，不适合受限环境；轻量级IoT协议效率高但表达能力有限。需要一种能在资源约束下保持表达能力的通信框架。

Method: 提出μACP形式演算，形式化资源约束智能体通信模型，证明{PING, TELL, ASK, OBSERVE}四动词基础足以编码有限状态FIPA协议，建立消息复杂度的信息论界限。使用TLA+进行模型检验，Coq进行机械化不变量验证。

Result: μACP能在部分同步和崩溃故障下实现标准共识，大规模系统模拟显示中位端到端消息延迟34ms（95%分位数104ms），在严重资源约束下优于现有智能体和IoT协议。形式验证建立了安全性和有界性。

Conclusion: μACP提供了一个统一演算，调和了语义表达性与可证明效率，为下一代资源受限多智能体系统提供了严格的理论基础，实现了表达能力与资源效率的平衡。

Abstract: Agent communication remains a foundational problem in multi-agent systems: protocols such as FIPA-ACL guarantee semantic richness but are intractable for constrained environments, while lightweight IoT protocols achieve efficiency at the expense of expressiveness. This paper presents $μ$ACP, a formal calculus for expressive agent communication under explicit resource bounds. We formalize the Resource-Constrained Agent Communication (RCAC) model, prove that a minimal four-verb basis \textit{\{PING, TELL, ASK, OBSERVE\}} is suffices to encode finite-state FIPA protocols, and establish tight information-theoretic bounds on message complexity. We further show that $μ$ACP can implement standard consensus under partial synchrony and crash faults, yielding a constructive coordination framework for edge-native agents. Formal verification in TLA$^{+}$ (model checking) and Coq (mechanized invariants) establishes safety and boundedness, and supports liveness under modeled assumptions. Large-scale system simulations confirm ACP achieves a median end-to-end message latency of 34 ms (95th percentile 104 ms) at scale, outperforming prior agent and IoT protocols under severe resource constraints. The main contribution is a unified calculus that reconciles semantic expressiveness with provable efficiency, providing a rigorous foundation for the next generation of resource-constrained multi-agent systems.

</details>


### [48] [Mapping Human Anti-collusion Mechanisms to Multi-agent AI](https://arxiv.org/abs/2601.00360)
*Jamiu Adekunle Idowu,Ahmed Almasoud,Ayman Alfahid*

Main category: cs.MA

TL;DR: 论文探讨如何将人类反合谋机制应用于多智能体AI系统，提出了分类框架和实施方法，并指出关键挑战


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统日益自主化，它们可能发展出类似人类市场的合谋策略。虽然人类领域积累了数个世纪的反合谋机制，但这些机制如何适应AI环境尚不明确。本文旨在填补这一空白。

Method: 1) 开发人类反合谋机制的分类法，包括制裁、宽大处理与举报、监控与审计、市场设计、治理；2) 将这些机制映射到多智能体AI系统的潜在干预措施；3) 为每种机制提出实施方法。

Result: 建立了人类反合谋机制到AI系统的映射框架，提出了具体的实施方法，并识别了关键挑战领域。

Conclusion: 论文为将人类反合谋机制应用于多智能体AI系统提供了系统框架，但需要解决归因问题、身份流动性、边界问题和对抗性适应等开放挑战。

Abstract: As multi-agent AI systems become increasingly autonomous, evidence shows they can develop collusive strategies similar to those long observed in human markets and institutions. While human domains have accumulated centuries of anti-collusion mechanisms, it remains unclear how these can be adapted to AI settings. This paper addresses that gap by (i) developing a taxonomy of human anti-collusion mechanisms, including sanctions, leniency & whistleblowing, monitoring & auditing, market design, and governance and (ii) mapping them to potential interventions for multi-agent AI systems. For each mechanism, we propose implementation approaches. We also highlight open challenges, such as the attribution problem (difficulty attributing emergent coordination to specific agents) identity fluidity (agents being easily forked or modified) the boundary problem (distinguishing beneficial cooperation from harmful collusion) and adversarial adaptation (agents learning to evade detection).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 该论文对工业异常检测算法进行了全面评估，使用模拟数据集测试了14种检测器在不同异常率和训练规模下的性能，发现最佳检测器取决于训练数据中故障样本的总数。


<details>
  <summary>Details</summary>
Motivation: 工业系统中机器学习面临极端类别不平衡的挑战，主要由于训练过程中故障数据的可用性有限。需要评估异常检测算法在实际工程约束下的性能。

Method: 使用基于超球面异常分布的合成数据集（2D和10D），在异常率0.05%-20%、训练规模1000-10000的范围内，对14种检测器进行基准测试，测试数据集大小为40000。

Result: 最佳检测器高度依赖于训练数据集中故障样本的总数：少于20个故障样本时，无监督方法（kNN/LOF）占优；30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升。在10个特征维度下，半监督方法的优势更明显。

Conclusion: 研究揭示了异常检测方法在小数据集上的泛化性能下降问题，为工业环境中部署异常检测提供了实用指导，强调故障样本数量对算法选择的关键影响。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [50] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文揭示了一个名为"token forge"的安全漏洞：通过精心设计的"破坏性token"，在模型组合过程中植入恶意功能，破坏基础模型的生成能力，同时保持捐赠模型表面正常。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型生态系统的发展，模型组合技术（如权重合并、推测解码、词汇扩展）日益普及。这些技术需要tokenizer移植来对齐不同模型的词汇表。作者发现这一关键互操作性步骤存在供应链漏洞，可能被恶意利用。

Method: 作者将攻击形式化为双目标优化问题，使用稀疏求解器实现。通过利用系数重用的几何特性，设计一个在捐赠模型中功能惰性但在移植到基础模型后能可靠重构为高显著性恶意特征的"破坏性token"。

Result: 攻击无需训练，通过光谱模仿规避异常检测，在微调和权重合并后仍保持结构持久性。攻击成功破坏了基础模型的生成能力，而捐赠模型的效用与正常行为在统计上无法区分。

Conclusion: 研究揭示了模块化AI组合流程中的隐藏风险：tokenizer移植这一关键互操作性步骤存在供应链漏洞，可能被恶意利用来破坏模型组合过程，需要新的安全措施来防范此类攻击。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [51] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 论文提出了一种新的渐近置信度最佳臂识别框架，通过放松精确误差控制要求，在渐近条件下实现更紧的最优性，能更好地处理非参数分布和利用协变量进行方差缩减。


<details>
  <summary>Details</summary>
Motivation: 传统固定置信度最佳臂识别方法在实际应用中存在局限性，严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致效率低下。现实世界中的弱信号、高显著性要求和实验后推断需求都需要长时域，因此需要更灵活的框架。

Method: 引入渐近误差控制框架，要求相对于最小样本量渐近有效的误差控制；开发了新颖的渐近任意时间有效置信序列用于臂索引；设计了新的BAI算法，灵活整合协变量进行方差缩减，在完全非参数设置中确保近似误差控制。

Result: 在温和的收敛假设下，提供了样本复杂度的渐近界限；最坏情况样本复杂度与高斯BAI在精确误差保证和已知方差下的最佳情况样本复杂度相匹配；实验表明该方法在保持误差控制的同时减少了平均样本复杂度。

Conclusion: 提出的渐近框架通过放松精确误差控制要求，实现了更紧的最优性，能更好地处理非参数分布和利用协变量，为实际应用中的最佳臂识别问题提供了更实用有效的解决方案。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [52] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO：将提示工程重构为序列决策问题，通过贝叶斯优化自适应选择最优指令，解决LLM方程发现中的指令脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在方程发现中表现出潜力，但其输出对提示措辞高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型陷入次优解。

Method: 提出NeuroSymBO方法，将提示工程重构为序列决策问题。方法维护离散推理策略库，使用贝叶斯优化根据数值反馈在每个步骤选择最优指令。

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简洁的解决方案。

Conclusion: 将提示工程视为序列决策问题并通过贝叶斯优化进行自适应指令选择，能有效解决LLM方程发现中的指令脆弱性问题，提升模型性能。

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [53] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个用于未知环境中同时导航与建图的几何强化学习框架，通过局部能量景观和哈密顿优化实现无全局地图的高质量导航。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图(SNAM)的挑战性问题，传统方法需要构建全局地图或设计复杂的多智能体策略，而GRL-SNAM旨在仅依赖局部感官观察实现高效导航。

Method: 将路径导航和建图建模为动态最短路径搜索和发现过程，使用受控哈密顿优化：将感官输入转换为编码可达性、障碍物屏障和变形约束的局部能量景观，通过更新哈密顿量逐步演化感知、规划和重新配置策略。

Result: 在两种不同的2D导航任务上评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM在相同阶段感知约束下保持间隙、泛化到未见布局，通过局部能量细化而非广泛全局建图实现高质量导航。

Conclusion: 通过哈密顿量更新的几何强化学习能够通过最小化探索实现高质量导航，证明了局部能量细化的有效性，代码已在GitHub开源。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [54] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 该论文研究非马尔可夫状态和成本过程下的线性函数逼近强化学习方法，证明了策略评估算法的收敛性，并分析了Q学习在特定基函数选择下的收敛条件


<details>
  <summary>Details</summary>
Motivation: 研究非马尔可夫环境下的强化学习问题，传统强化学习方法通常假设马尔可夫性，但在实际应用中状态过程可能不满足马尔可夫性质，需要开发适用于非马尔可夫过程的算法

Method: 使用线性函数逼近方法，首先分析策略评估算法，证明其在适当遍历性条件下的收敛性；然后研究Q学习算法，特别针对基于量化映射的基函数选择情况；最后将结果应用于部分可观测马尔可夫决策过程

Result: 证明了策略评估算法在非马尔可夫过程下的收敛性，极限对应于正交投影与辅助马尔可夫决策过程贝尔曼算子的复合算子不动点；对于Q学习，在基于量化映射的基函数选择下也能证明收敛性；在部分可观测马尔可夫决策过程中得到了显式误差界

Conclusion: 在非马尔可夫环境下，线性函数逼近的强化学习方法在适当条件下仍然可以收敛，为处理实际应用中不满足马尔可夫假设的问题提供了理论保证，特别适用于部分可观测环境

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [55] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 研究使用XGBoost模型分析美国交通事故严重程度预测，发现时间、地理位置和天气变量是最强预测因子，但模型对极端严重事故预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对美国交通事故严重程度的预测能力，为基于证据的交通管理提供支持。

Method: 使用2016-2023年50万起美国交通事故数据集，采用XGBoost分类器，通过随机搜索交叉验证优化，并使用类别加权处理类别不平衡问题。

Result: 最终模型整体准确率78%，对主要类别（严重程度2）的精确率和召回率达到87%。特征重要性分析显示时间、地理位置、能见度、温度和风速是最强预测因子，但降水和能见度预测能力有限。

Conclusion: 研究为交通管理提供了证据基础，但数据集以中等严重程度事故为主，限制了模型对极端情况的预测能力，需要改进采样策略、特征工程和外部数据整合。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [56] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的在线微调决策变换器的方法，使用纯强化学习梯度而不是监督序列建模目标，解决了现有方法中后见回报重新标记与重要性采样算法不兼容的问题。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但扩展到在线设置时仍主要依赖监督序列建模目标。作者发现后见回报重新标记这一标准组件与基于重要性采样的强化学习算法（如GRPO）存在根本性不兼容，导致训练不稳定，阻碍了纯强化学习梯度的在线微调。

Method: 1. 将GRPO算法适配到决策变换器；2. 引入子轨迹优化以改进信用分配；3. 使用序列级似然目标增强稳定性和效率；4. 采用主动采样鼓励在不确定区域的探索。

Result: 通过大量实验证明，该方法超越了现有的在线决策变换器基线，在多个基准测试中达到了新的最先进性能，突显了基于纯强化学习的在线微调对决策变换器的有效性。

Conclusion: 该研究成功实现了决策变换器的纯强化学习梯度在线微调，解决了后见回报重新标记与重要性采样算法的不兼容问题，为序列决策建模提供了更有效的在线学习框架。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [57] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: Sequential Reservoir Computing通过将大型储层分解为一系列小型互联储层，显著降低了高维时空系统预测的计算成本和内存需求，同时提高了预测精度和时长。


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中存在梯度训练和内存瓶颈问题，而传统Reservoir Computing虽然通过固定循环层和凸读出优化缓解了这些问题，但在输入维度增加时仍存在扩展性问题。

Method: 提出Sequential Reservoir Computing架构，将大型储层分解为一系列小型、互联的储层，这种设计在保持长期时间依赖性的同时，显著降低了内存和计算成本。

Result: 在低维混沌系统（Lorenz63）和高维物理模拟（2D涡度和浅水方程）中，Sequential RC相比LSTM和标准RNN基线，实现了15-25%更长的有效预测时长，20-30%更低的误差指标（SSIM、RMSE），以及高达三个数量级的训练成本降低。

Conclusion: Sequential RC在保持传统RC简单性和效率的同时，实现了对高维动力系统的卓越可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [58] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 基于电子健康记录数据，机器学习模型在预测肝硬化方面显著优于传统FIB-4评分，尤其在早期预测方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 开发并评估利用常规电子健康记录数据预测肝硬化的机器学习模型，以替代或补充传统的FIB-4评分，实现更早、更准确的风险分层。

Method: 采用回顾性队列研究，从大型学术医疗系统获取去识别化电子健康记录数据。识别脂肪肝患者并根据ICD-9/10编码分为肝硬化与非肝硬化队列。构建预测场景，使用观察窗口和预测窗口模拟真实临床使用。从观察窗口汇总人口统计学、诊断、实验室结果、生命体征和共病指数。训练XGBoost模型用于1年、2年和3年预测时间范围，并在保留测试集上评估性能，使用AUC与FIB-4进行比较。

Result: 最终队列包括1年预测3,043名患者，2年预测1,981名，3年预测1,470名。在所有预测时间范围内，机器学习模型均持续优于FIB-4。XGBoost模型在1年、2年和3年预测的AUC分别为0.81、0.73和0.69，而FIB-4的AUC分别为0.71、0.63和0.57。随着预测时间范围的延长，性能优势仍然存在，表明早期风险辨别能力有所改善。

Conclusion: 利用常规电子健康记录数据的机器学习模型在早期预测肝硬化方面显著优于传统FIB-4评分。这些模型能够实现更早、更准确的风险分层，并可作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理。

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [59] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，在带宽受限条件下实现按维度不等差错保护，显著提升语义通信性能


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义信息保持的挑战，传统信道编码（如LDPC或Reed-Solomon）无法实现细粒度语义保护

Method: 采用强化学习框架，通过自适应重复编码实现按维度不等差错保护；使用复合语义失真度量，平衡全局嵌入相似性和实体级保持

Result: 相比均匀保护，在1 dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著

Conclusion: 智能分配的简单重复编码可实现细粒度语义保护，代码结构必须与语义粒度对齐，适用于边缘计算和物联网场景

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [60] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN模型，仅需1-3%标注数据即可实现蚊虫神经元尖峰信号的高精度分类，用于检测寨卡病毒、登革热病毒感染，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊虫是虫媒病毒的主要传播媒介，手动分类其神经元尖峰模式非常耗时且昂贵。现有深度学习解决方案需要完全标注的数据集和高度预处理的神经信号，难以在实际场景中大规模应用。

Method: 提出半监督Swin启发式GAN（SSI-GAN）架构，包含基于Transformer的生成器和采用移位窗口机制的Swin启发式判别器。使用多头自注意力模型在基于窗口的Transformer判别器中学习稀疏高频尖峰特征。仅使用1-3%标注数据，在超过1500万个尖峰样本上训练，使用贝叶斯Optuna框架优化超参数，并通过五折蒙特卡洛交叉验证验证鲁棒性。

Result: SSI-GAN在感染后第三天仅用3%标注数据达到99.93%的分类准确率。仅用1%监督在所有感染阶段都保持高准确率。相比标准监督方法，在相同性能水平下减少了97-99%的人工标注工作量。移位窗口Transformer设计大幅超越所有基线方法，在基于尖峰的神经元感染分类中创下新纪录。

Conclusion: SSI-GAN通过创新的半监督GAN架构，有效解决了神经元尖峰信号分类中标注数据稀缺的问题，显著降低了人工标注成本，为实际现场应用中的蚊虫病毒感染检测提供了可行的解决方案。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [61] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出一种面向资源受限边缘设备的资源高效、数据中心的框架，通过特征工程使复杂心律失常数据线性可分，在MIT-BIH和INCART数据集上实现98.44%诊断准确率，模型仅8.54KB，推理延迟0.46μs。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死亡原因，需要IoMT持续监测。现有深度学习方法计算开销过大，不适合资源受限的边缘设备。

Method: 提出资源高效的数据中心框架，优先特征工程而非模型复杂性。整合时频小波分解和图论结构描述符（如PageRank中心性），构建混合特征空间，使用互信息和递归消除进行特征选择，实现可解释的超轻量线性分类器。

Result: 在MIT-BIH和INCART数据集上验证，达到98.44%诊断准确率，模型大小仅8.54KB。分类推理延迟0.46μs，每搏处理管道52ms，确保实时操作。相比压缩模型KD-Light（25KB，96.32%准确率）实现数量级效率提升。

Conclusion: 该框架通过优化特征工程使复杂心律失常数据线性可分，实现了资源受限边缘设备上的高效心律失常监测，为无电池心脏传感器提供了可行解决方案。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [62] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 该论文提出了一种利用未标记互联网数据提升AI生成内容溯源性能的方法，通过约束优化框架增强对未知生成器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真假检测，实现更精确的生成模型溯源。现有方法在已知生成器上表现良好，但难以泛化到未见的新生成器。

Method: 使用CLIP特征和线性分类器建立基线，然后提出约束优化方法：利用未标记的互联网数据（可能包含真实图像、未知生成器输出或目标模型样本），鼓励将野生样本分类为非目标，同时约束在标记数据上的性能保持高位。

Result: 实验结果表明，结合野生数据显著提高了在具有挑战性的未见生成器上的溯源性能，证明未标记数据可以有效增强开放世界环境下的AI生成内容溯源能力。

Conclusion: 利用未标记的互联网数据可以有效提升生成模型溯源系统的泛化能力，特别是在面对未知和新发布的生成器时，为开放世界环境下的AI生成内容溯源提供了有效解决方案。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [63] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 该论文提出了对抗性图提示（AGP）框架，首次将对抗学习集成到图提示中，以增强预训练GNN模型在下游任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在图拓扑和节点特征面临各种噪声和攻击时表现出显著脆弱性，需要开发更鲁棒的图微调方法。

Method: 提出AGP框架，将问题表述为min-max优化问题，采用交替优化方案：内层最大化使用联合投影梯度下降（JointPGD）生成强对抗噪声；外层最小化学习最优节点提示来抵消对抗噪声。

Result: 理论证明AGP能同时处理图拓扑和节点噪声，实验验证AGP在多个基准任务上相比现有方法具有更好的鲁棒性和有效性。

Conclusion: AGP是一种通用方法，可与各种预训练GNN模型集成，显著增强其在下游任务中的鲁棒性，为图对抗学习提供了新框架。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [64] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，实现通信高效的跨环境共享奖励学习。


<details>
  <summary>Details</summary>
Motivation: 在机器人和多智能体系统中，自主智能体群通常在微妙不同的环境中运行，但追求共同的高级目标。由于动态差异、隐私约束和有限通信带宽，直接汇集数据学习共享奖励函数通常不切实际。

Method: 采用基于最优传输的联邦逆强化学习方法：1）每个客户端在本地执行轻量级最大熵逆强化学习，遵守计算和隐私限制；2）通过Wasserstein重心融合得到的奖励函数，考虑其底层几何结构。

Result: 证明了这种重心融合方法比联邦学习中传统的参数平均方法能产生更准确的全局奖励估计，提供了原则性和通信高效的框架。

Conclusion: 该方法为在异构智能体和环境中推导可泛化的共享奖励函数提供了原则性和通信高效的框架。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [65] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: QKRD是一个基于国际象棋战术位置的量子基准测试，包含5000个结构化实例，用于评估QAOA算法设计选择，发现约束保持混合器比标准混合器收敛快13步，预热策略减少45步收敛时间。


<details>
  <summary>Details</summary>
Motivation: 现有QAOA基准测试主要使用MaxCut、TSP、SAT等随机合成实例，这些实例缺乏语义结构和人类可解释性，无法有效评估算法在具有实际约束的真实问题上的性能。

Method: 提出量子王环支配(QKRD)基准测试，基于国际象棋战术位置构建，包含5000个结构化实例，具有独热约束、空间局部性和10-40量子比特规模。该基准提供人类可解释的覆盖度指标，并通过与经典启发式算法对比实现内在验证。

Result: 约束保持混合器(XY、domain-wall)比标准混合器收敛快约13步(p<10^{-7}, d≈0.5)；预热策略减少45步收敛(p<10^{-127}, d=3.35)，能量改进超过d=8；CVaR优化产生负面结果，能量更差(p<10^{-40}, d=1.21)且无覆盖度优势。QAOA表现优于贪婪启发式12.6%，优于随机选择80.1%。

Conclusion: 结构化基准测试能够揭示在随机实例中被掩盖的问题感知QAOA技术优势，为NISQ算法研究提供可重复的评估框架。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [66] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的组相对策略优化增强流匹配模型在人类偏好对齐中的表现，通过合并低熵步骤为高熵SDE采样步骤来解决多步优化中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多步去噪优化中存在稀疏和模糊的奖励信号问题，观察到高熵步骤能实现更高效有效的探索，而低熵步骤导致无差异的roll-out。

Method: 提出E-GRPO（熵感知组相对策略优化），将连续低熵步骤合并为一个高熵SDE采样步骤，其他步骤使用ODE采样，并引入多步组归一化优势函数，在共享相同合并SDE去噪步骤的样本内计算组相对优势。

Result: 在不同奖励设置下的实验结果证明了该方法的有效性。

Conclusion: 通过熵感知的步骤合并和组相对优化策略，E-GRPO能够更有效地处理流匹配模型在人类偏好对齐中的奖励信号稀疏问题，提升模型性能。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [67] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: 对16种可解释机器学习方法在216个真实世界表格数据集上进行大规模比较评估，发现EBMs在回归任务中表现最佳，但性能高度依赖于数据集特征如维度、样本量、线性度和类别不平衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗、金融、法律等高风险领域的广泛应用，模型可解释性和问责制日益重要。然而，对内在可解释模型（特别是针对表格数据）的系统性评估相对缺乏，现有研究多集中于聚合性能指标。

Method: 对16种内在可解释方法进行大规模比较评估，包括经典线性模型、决策树以及EBMs、符号回归、GOSDT等新方法。研究涵盖216个真实世界表格数据集，不仅评估聚合性能，还根据数据集结构特征（维度、样本量、线性度、类别不平衡）进行分层分析，并评估训练时间和分布偏移下的鲁棒性。

Result: 结果显示清晰的性能层次结构：EBMs在回归任务中始终表现出强大的预测准确性；SR和IGANNs在非线性场景中表现优异；GOSDT模型对类别不平衡表现出明显敏感性。性能高度依赖于上下文环境。

Conclusion: 研究结果为寻求可解释性与预测性能平衡的实践者提供了实用指导，并加深了对表格数据可解释建模的实证理解。性能高度依赖于数据集特征，没有单一最佳方法适用于所有场景。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [68] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型（CCBMs），支持三种粒度的模型编辑（概念-标签级、概念级、数据级），无需重新训练即可动态维护模型


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型主要关注静态场景，而实际应用中需要动态维护：删除错误/敏感数据（遗忘）、纠正错误标注概念、纳入新样本（增量学习），特别是在大规模应用中需要高效可编辑的CBMs

Method: 提出可控概念瓶颈模型（CCBMs），基于影响函数推导出数学上严格的闭式近似解，支持三种编辑粒度：概念-标签级编辑、概念级编辑、数据级编辑（包括数据删除和添加）

Result: 实验结果表明CCBMs具有高效性和适应性，验证了其在实现动态可信CBMs方面的实用价值

Conclusion: CCBMs通过数学严谨的闭式近似实现了无需重新训练的动态模型编辑，为概念瓶颈模型在实际部署中的持续维护提供了有效解决方案

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [69] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: 提出TGE方法解决离线观察模仿学习中专家数据稀缺、离线数据与专家行为差异大的问题，通过轨迹级生成嵌入构建密集平滑的替代奖励


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配方法在专家演示稀缺、离线次优数据与专家行为差异大的情况下效果不佳，因为它们施加严格的支持约束并依赖脆弱的一步模型，难以从不完美数据中提取有用信号

Method: 提出TGE（轨迹级生成嵌入）方法，通过在离线轨迹数据上训练的时间扩散模型的潜在空间中估计专家状态密度，构建密集平滑的替代奖励，利用学习到的扩散嵌入的平滑几何结构捕捉长期时间动态并弥合不相交支持之间的差距

Result: 在D4RL运动和控制基准测试中，TGE方法一致匹配或优于先前的离线观察模仿学习方法

Conclusion: TGE通过轨迹级生成嵌入有效解决了离线观察模仿学习中数据稀缺和分布差异的挑战，即使在离线数据与专家分布差异大的情况下也能提供稳健的学习信号

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [70] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失无法有效促进MoE模型专家专业化，反而增加权重空间重叠，对性能影响不一致且不可靠


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE模型专家专业化中的作用，探索正交性损失是否能有效促进专家多样性

Method: 应用正交性损失来强制专家多样性，在7个正则化强度下分析权重空间重叠和激活空间重叠的变化

Result: 正交性损失在多方面失败：权重空间重叠增加达114%，激活空间重叠保持高位(~0.6)，性能影响不一致且高度可变

Conclusion: 权重空间正则化既未实现其几何目标，也未可靠提升性能，不适合用于MoE多样性增强

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [71] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 该研究开发了一种基于1D UNet的自动标注方法AugUNet1D，用于检测小鼠EEG中的棘慢波放电，相比传统方法和现有算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 手动标注脑电图中的事件（特别是棘慢波放电）耗时费力，尤其是在连续数周至数月的记录中。需要自动标注方法来减少人工工作量。

Method: 比较了14种机器学习分类器在961小时小鼠EEG数据（包含22,637个标注的SWD）上的表现，发现1D UNet最佳。通过数据增强改进1D UNet，其中缩放增强效果最好，得到AugUNet1D。将其与时间-频率算法"Twin Peaks"进行比较。

Result: 1D UNet在SWD标注任务中表现最佳。数据增强显著提升性能，缩放增强效果最明显。AugUNet1D相比"Twin Peaks"算法表现更优，检测到的事件特征更接近人工标注的SWD。

Conclusion: AugUNet1D是自动检测EEG中棘慢波放电的有效工具，性能优于现有方法。研究公开了预训练和未训练的模型供其他用户使用。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [72] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 该论文提出了一种用于多用户上下文赌博机的新框架，结合图同质性和非线性奖励函数，通过统一的多用户RKHS核函数实现图拉普拉斯正则化与核化赌博机的融合。


<details>
  <summary>Details</summary>
Motivation: 研究多用户上下文赌博机问题，其中用户通过图结构相互关联，且奖励函数既表现出非线性特性又具有图同质性。现有方法在处理这种复杂结构时存在局限性。

Method: 引入一个原则性的联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明该惩罚等价于单一统一的多用户RKHS中的平方范数，并显式推导其再生核，将图拉普拉斯与基础臂核函数优雅融合。基于此设计了LK-GP-UCB和LK-GP-TS算法，利用高斯过程后验进行探索。

Result: 提供了高概率遗憾界，其缩放与多用户核的有效维度相关，替代了对用户数量或环境维度的依赖。实验表明，在非线性设置中，该方法优于强大的线性和非图感知基线，即使在真实奖励为线性时也保持竞争力。

Conclusion: 该工作提供了一个统一、理论严谨且实用的框架，将拉普拉斯正则化与核化赌博机相结合，用于结构化探索，为多用户上下文赌博机问题提供了新的解决方案。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [73] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: 论文探讨了无自注意力Transformer架构（神经链）与离散动力系统之间的类比，比较了Burgers和Eikonal方程的传统数值离散化方法与PINN学习方法，发现两者获得相同系统动力学知识但路径不同。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器学习（特别是无自注意力Transformer架构）与传统数值方法在求解偏微分方程方面的类比关系，比较PINN学习与传统数值离散化方法的异同。

Method: 方法包括：1）建立神经链与离散动力系统之间的类比；2）对粘性和非粘性Burgers方程以及Eikonal方程进行标准数值离散化（也表示为神经链形式）；3）使用PINN方法进行学习；4）比较两种方法的矩阵结构和参数特性。

Result: 研究发现：1）标准数值离散化和PINN学习通过不同路径获得基本相同的系统动力学知识；2）PINN学习通过随机矩阵进行，与有限差分方法的高度结构化矩阵无直接关系；3）可接受解的随机矩阵数量远多于有限差分的三对角矩阵形式；4）PINN需要更多参数，导致物理可解释性差和训练成本高。

Conclusion: 结论是：对于一维动态问题，PINN相比传统数值方法没有优势，但研究结果不排除PINN和机器学习在高维问题中可能提供更好的策略。PINN的主要代价是参数多、可解释性差和训练成本高。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [74] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR是一个针对电信领域票务故障排除的端到端系统，通过集成领域特定排序和生成模型，自动化分类、检索和生成任务，显著提升故障排除的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电信领域的票务故障排除是一个高度复杂且耗时的任务，需要专家解读票务内容、查阅文档和搜索历史记录。这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。需要一种自动化系统来提升电信票务故障排除的效果和效率。

Method: 提出TeleDoCTR系统，这是一个针对电信领域特定、上下文感知的故障排除系统，专门用于电信端到端票务解决。系统集成领域特定排序模型和生成模型，自动化故障排除工作流程的三个关键步骤：1）将票务分类到适当的专家团队（分类任务）；2）检索上下文和语义相似的历史票务（检索任务）；3）生成详细的故障分析报告，概述问题、根本原因和潜在解决方案（生成任务）。

Result: 在真实世界电信基础设施数据集上评估TeleDoCTR，证明其性能优于现有最先进方法，显著提升了故障排除过程的准确性和效率。

Conclusion: TeleDoCTR系统通过自动化电信票务故障排除的关键步骤，有效解决了传统人工方法的效率瓶颈，为电信领域的故障排除提供了高效、准确的端到端解决方案。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [75] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了大型模型在联邦学习框架下的定制化方法，首次在联邦学习环境中实现了prefix-tuning，实验证明其性能接近集中式方法，并在效率、鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习框架下大型模型定制化的关键挑战，探索如何将流行的模型定制技术应用于分布式环境中，解决数据隐私保护与模型性能之间的平衡问题。

Method: 首先综述了全微调、高效微调、提示工程、前缀调优、知识蒸馏和检索增强生成等主流大型模型定制技术，然后讨论了这些技术在联邦学习框架下的实现方式，并首次在联邦学习环境中实验了prefix-tuning方法。

Result: 联邦prefix-tuning实验验证了其在联邦学习环境中的可行性，性能接近集中式方法。与其他三种联邦定制方法的比较显示，该方法具有竞争力的性能、令人满意的效率和一致的鲁棒性。

Conclusion: 研究表明联邦学习框架下的大型模型定制化是可行的，特别是prefix-tuning方法在联邦环境中表现出色，为在保护数据隐私的同时实现模型个性化提供了有效途径。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [76] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的持续学习方法，在在线适应学习中显著减少记忆库大小至基线方法的0.3%，同时保持高记忆保留准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的知识容易过时，需要持续学习来更新信息而不遗忘已有知识。现有记忆增强方法面临记忆库随数据流不断增长的严重限制，需要更高效的记忆管理方案。

Method: 提出MBC模型，通过码本优化策略压缩记忆库，引入在线重置机制防止码本崩溃，并在注意力层使用Key-Value低秩适应技术来高效利用压缩后的记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最具竞争力基线的0.3%，同时在在线适应学习中保持高记忆保留准确率。

Conclusion: MBC通过码本压缩和在线重置机制，有效解决了记忆库不断增长的问题，为大规模数据流下的持续学习提供了高效解决方案。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [77] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的云原生架构，用于自动生成商店特定的货架布局图，将设计时间从30小时减少到0.5小时，降低了98.3%


<details>
  <summary>Details</summary>
Motivation: 传统货架布局图创建过程耗时且昂贵，平均每个复杂布局需要30小时。零售业需要更高效的自动化解决方案来优化空间利用和降低成本。

Method: 采用云原生架构，结合AWS进行云端模型训练和边缘部署实现实时推理。使用扩散模型学习多个零售地点的成功货架布局，通过修改损失函数集成零售特定约束。

Result: 系统将货架布局图设计时间减少98.3%（从30小时到0.5小时），约束满足率达到94.4%。经济分析显示创建费用降低97.5%，盈亏平衡期为4.4个月。云架构可线性扩展，支持多达10,000个并发商店请求。

Conclusion: 该研究证明了生成式AI在自动化零售空间优化中的可行性，通过云原生扩散模型架构显著提高了货架布局图创建的效率和经济效益。

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [78] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过注意力矩阵的谱分析检测大型语言模型中的有效数学推理，使用四个可解释的谱诊断指标，在多个模型架构上实现85-95.6%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 需要一种无需训练数据、微调或学习分类器的方法来检测大型语言模型中的有效数学推理，以解决幻觉检测和AI安全监控的实际需求。

Method: 将注意力矩阵视为token动态图的邻接矩阵，提取四个谱诊断指标：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和谱熵，通过单一阈值进行分类。

Result: 在四个独立架构家族的七个Transformer模型上，该方法产生高达Cohen's d=3.30的效应量，实现85.0-95.6%的分类准确率，发现谱方法检测的是逻辑连贯性而非编译器接受度。

Conclusion: 谱图分析为推理验证提供了原则性框架，具有立即应用于幻觉检测和AI安全监控的潜力，同时揭示了注意力机制设计如何影响谱特征捕获推理有效性的方式。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


### [79] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 提出基于非平衡随机动力学的熵触发重训练框架，通过Fokker-Planck方程建模数据漂移，利用KL散度的时间导数分解中的熵产生项作为重训练触发机制，相比传统方法大幅减少重训练次数。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在非平稳环境中部署时，由于数据漂移会导致性能下降。现有的漂移检测方法大多缺乏原理性的动力学解释，且无法在重训练频率和运营成本之间提供平衡指导。

Method: 将部署时的数据漂移建模为受Fokker-Planck方程控制的概率流，使用随时间演化的Kullback-Leibler散度量化模型-数据不匹配。证明该不匹配的时间导数具有熵平衡分解，包含由概率流驱动的非负熵产生项，以此作为无标签干预策略的熵触发重训练机制。

Result: 在受控的非平稳分类实验中，熵触发重训练实现了与高频重训练相当的预测性能，同时相比每日重训练和基于标签的策略，将重训练事件减少了一个数量级。

Conclusion: 基于非平衡随机动力学的熵触发重训练框架为数据漂移管理提供了原理性的方法，通过响应累积的不匹配而非延迟的性能崩溃，在保持性能的同时显著降低了运营成本。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [80] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 该论文提出需要区分两种对抗样本类型：利用非鲁棒特征的样本和不利用这些特征的样本，并开发了基于集成的方法来量化非鲁棒特征的操纵程度。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒特征理论虽然被广泛接受，但忽略了那些不直接利用这些特征的对抗样本。作者认为这两种样本代表了不同类型的对抗弱点，需要区分评估对抗鲁棒性。

Method: 提出了基于集成的度量方法来测量对抗扰动对非鲁棒特征的操纵程度，并使用该度量分析攻击者生成的对抗样本的组成。

Result: 通过新度量方法能够区分不同类型的对抗样本，并重新审视了多个现象，包括锐度感知最小化对对抗鲁棒性的影响，以及在鲁棒数据集上对抗训练和标准训练之间的鲁棒性差距。

Conclusion: 对抗样本应分为利用非鲁棒特征和不利用非鲁棒特征两种类型，这种区分对于准确评估对抗鲁棒性至关重要，新的度量方法为此提供了工具。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [81] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE是一个基于混合专家模型的联邦学习框架，用于在资源受限的客户端上高效微调大语言模型，通过专家重要性评估、自适应专家选择和稀疏感知聚合来解决计算异构性和全局聚合问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但大语言模型在资源受限设备上的训练不可行。混合专家模型虽然计算高效，但在联邦学习微调中面临三个关键挑战：1) 缺乏可靠指标选择合适专家；2) 客户端计算资源异构性影响专家激活；3) 客户端特定专家子集和路由偏好破坏全局聚合。

Method: 提出HFedMoE框架：1) 基于专家对微调性能的贡献评估专家重要性；2) 从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3) 设计稀疏感知模型聚合策略，基于重要性加权贡献聚合活跃微调的专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确率和收敛速度方面优于最先进的基准方法。

Conclusion: HFedMoE成功解决了MoE在联邦学习微调中的三个关键挑战，实现了在资源受限客户端上高效的大语言模型微调，为异构计算环境下的联邦学习提供了有效解决方案。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [82] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 该论文提出了一种用于在向下封闭凸体上最大化非负、非单调γ-弱DR-子模函数的近似算法，其保证随γ平滑变化，在γ=1时恢复0.401近似比，在γ<1时性能优雅下降。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和优化中，约束下的子模目标最大化是一个基本问题。当前需要处理更一般的γ-弱DR-子模函数（包括非单调情况）在向下封闭凸体上的最大化问题，现有方法对此类问题的近似保证有限。

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，形成一个简单而有效的处理非单调性的程序。这种方法能够优雅地处理γ-弱DR-子模函数的特性。

Result: 算法提供了随γ平滑变化的近似保证：当γ=1（DR-子模情况）时恢复0.401近似比，当γ<1时保证优雅下降，且改进了先前在相同约束下γ-弱DR-子模最大化的报告界限。

Conclusion: 该方法为在向下封闭凸体上最大化非单调γ-弱DR-子模函数提供了最先进的保证，通过结合连续贪婪和双贪婪技术，实现了对非单调性的有效处理。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [83] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个用于量化LLMs在简洁理想提示下过度生成的轻量级基准测试，包含300多个英文提示，通过YapScore和YapIndex评估模型在三种简洁场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs（如ChatGPT、Claude、Gemini）在简单请求上经常给出不必要的冗长回答，包含冗余解释、模棱两可的表述或样板内容，这会增加认知负担并推高基于token的推理成本。先前研究表明基于偏好的后训练和LLM评判的评估可能导致系统性长度偏差，即使质量相当，更长的回答也会获得更高评分。

Method: 引入YapBench基准测试，每个项目包含单轮提示、精心设计的最小充分基线答案和类别标签。主要指标YapScore测量响应长度超出基线字符数的程度，YapIndex作为类别级别中位数YapScore的均匀加权平均值。基准包含300多个英文提示，涵盖三种简洁理想场景：模糊输入需要简短澄清、封闭式事实问题有简短稳定答案、单行编码任务只需单个命令或代码片段。

Result: 评估了76个助手型LLMs，观察到中位数超额长度存在数量级差异，并识别出特定类别的失败模式，包括在模糊输入上的"真空填充"行为，以及在单行技术请求上的解释或格式化开销。

Conclusion: YapBench提供了一个量化LLMs过度生成行为的标准化评估框架，揭示了不同模型在简洁响应能力上的显著差异，并发布了基准测试和维护实时排行榜以跟踪模型的冗长行为随时间的变化。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [84] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化训练可解释模型，利用DAG编码特征重要性层次结构，使用TIG测量特征重要性，并引入最优路径预言机解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释模型训练方法缺乏结构化领域知识的有效整合，且传统特征重要性测量方法存在OOD问题，需要开发既能保持模型准确性又能强制执行领域知识约束的优化框架。

Method: 提出IGBO框架：1) 将特征重要性层次编码为DAG；2) 使用TIG测量特征重要性；3) 引入最优路径预言机解决TIG计算中的OOD问题；4) 采用双目标优化平衡模型准确性和领域知识约束。

Result: 理论分析证明了收敛性和对mini-batch噪声的鲁棒性；在时间序列数据上的实验表明，IGBO能有效强制执行DAG约束且精度损失最小，优于标准正则化基线方法。

Conclusion: IGBO为训练可解释模型提供了有效的双目标优化框架，成功整合结构化领域知识，解决了TIG的OOD问题，在保持模型性能的同时增强了可解释性。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [85] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: IRPO提出了一种新的强化学习框架，通过将Bradley-Terry模型集成到GRPO中，解决了成对GRMs在RL训练中的计算瓶颈问题，实现了高效的点式评分，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 生成式奖励模型（GRMs）因其可解释性、推理时扩展性和通过强化学习优化的潜力而备受关注，但广泛使用的成对GRMs在与GRPO等RL算法集成时存在计算瓶颈，主要源于O(n²)时间复杂度的成对比较和重复采样/CoT推理的计算开销。

Method: 提出Intergroup Relative Preference Optimization（IRPO），将成熟的Bradley-Terry模型集成到GRPO框架中，为每个响应生成点式评分，从而在RL训练期间能够高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: 实验结果表明，IRPO在多个基准测试中实现了点式GRMs中的最先进性能，性能与当前领先的成对GRMs相当。此外，IRPO在后训练评估中显著优于成对GRMs。

Conclusion: IRPO通过解决成对GRMs的计算瓶颈问题，提供了一种高效且性能优越的强化学习框架，在保持可解释性的同时实现了与成对方法相当甚至更好的性能，为奖励建模领域提供了重要的技术突破。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [86] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE是一个轻量级强化学习框架，通过引入基于粒子群的探索层增强标准策略梯度方法，在非平稳奖励和高维策略任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略场景下。现有方法在复杂任务中探索不足，需要更有效的探索机制。

Method: ARISE框架在标准策略梯度方法基础上增加了一个紧凑的基于粒子群的探索层。它将策略动作与粒子驱动的提议混合，每个粒子代表在动作空间中采样的候选策略轨迹，并使用奖励方差线索自适应地调节探索。

Result: 在简单基准任务上只有轻微改进（如CartPole-v1上+0.7%），但在更具挑战性的任务上获得显著提升：LunarLander-v3上+46%，Hopper-v4上+22%，同时在Walker2d和Ant上保持稳定性。在非平稳奖励变化下，ARISE提供明显的鲁棒性优势，在CartPole上比PPO高出75分，在LunarLander上也有相应改进。

Conclusion: ARISE提供了一个简单、架构无关的途径，可以在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的强化学习智能体。消融研究证实粒子群组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [87] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出了一种基于近似贝叶斯推理的逆博弈框架，通过结构化变分自编码器和可微纳什博弈求解器，从交互数据中学习智能体目标的先验和后验分布，提高推理质量并实现更安全的下游决策。


<details>
  <summary>Details</summary>
Motivation: 现有最大似然逆博弈方法只能推断点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要一种能够量化不确定性并支持多模态观测的贝叶斯推理方法。

Method: 提出贝叶斯逆博弈框架，使用结构化变分自编码器嵌入可微纳什博弈求解器，在交互数据集上训练，无需智能体真实目标的标签。支持多模态观测数据，实时生成隐藏目标的后验分布样本。

Result: 框架成功学习先验和后验分布，相比基于最大似然估计的逆博弈方法提高了推理质量，实现了更安全的下游决策而不牺牲效率。当轨迹信息不充分时，多模态推理通过利用额外观测模态进一步减少不确定性。

Conclusion: 提出的贝叶斯逆博弈方法能够有效量化智能体目标估计的不确定性，支持多模态观测，为自主决策提供更安全可靠的逆博弈解决方案，优于传统的点估计方法。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [88] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: BSAT模型通过B样条自适应分词器解决长时序预测中自注意力二次复杂度和均匀分块不匹配问题，采用混合位置编码实现高效压缩预测


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长时序预测中存在自注意力二次复杂度问题，且均匀分块方式可能与数据的语义结构不匹配，需要更高效的自适应方法

Method: 提出B样条自适应分词器(BSAT)，通过拟合B样条自适应分割时序，在高曲率区域放置token；采用混合位置编码(L-RoPE)，结合可学习加性编码和层间可学习基的旋转位置编码

Result: 在多个公开基准测试中表现优异，在高压缩率下仍保持强竞争力，特别适合内存受限的应用场景

Conclusion: BSAT通过自适应分词和混合位置编码有效解决了长时序预测的计算效率和语义对齐问题，为内存受限场景提供了高效解决方案

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [89] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器和其他算法，通过上下文多臂老虎机问题动态选择计算步骤的最优精度配置，平衡精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在科学计算中，混合精度数值方法需要平衡计算精度和效率。传统方法缺乏自适应精度选择能力，无法根据具体问题特征动态调整精度配置，导致计算资源浪费或精度不足。

Method: 将精度调优建模为上下文多臂老虎机问题，使用离散化状态空间和增量动作价值估计。通过Q表将离散化特征（如近似条件数和矩阵范数）映射到精度配置动作，采用epsilon-greedy策略优化多目标奖励函数，平衡精度和计算成本。

Result: 在线性系统迭代精化应用中，该方法能有效选择精度配置，在保持与双精度基准相当精度的同时显著降低计算成本。框架对样本外数据具有良好泛化能力，是首个基于强化学习的精度自动调优工作。

Conclusion: 该强化学习框架为混合精度数值方法提供了有效的自适应精度选择方案，可扩展到其他数值算法，推动了科学计算中智能精度调优的发展。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [90] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文分析了当前LLM推理流程中基于正确性优化的设计缺陷，提出了分布创造性推理（DCR）框架，通过变分目标统一多种训练方法，防止推理路径分布崩溃，保持模型创造性和正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理流程主要基于正确性优化（采样多样化思维链并强化得分最高的路径），这种设计容易导致模型推理路径分布崩溃，降低语义熵，削弱创造性问题解决能力。

Method: 提出分布创造性推理（DCR）框架，将训练视为通过解轨迹概率测度的梯度流，统一了STaR、GRPO、DPO等多种方法以及熵奖励等。该框架提供了防止分布崩溃的稳定设计。

Result: DCR框架提供了三个核心成果：1）多样性衰减定理，描述基于正确性的目标如何导致STaR、GRPO、DPO的不同多样性衰减模式；2）确保收敛到稳定多样策略的设计；3）实际可操作的实现方案。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的原则性方案，解决了现有基于正确性优化的推理流程导致的分布崩溃问题，实现了更平衡的模型性能。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [91] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的足球角球防守评估框架，通过球员追踪数据推断盯人和区域防守任务，实现无标签的防守贡献评估和角色条件反事实分析。


<details>
  <summary>Details</summary>
Motivation: 传统足球防守评估指标难以捕捉无球防守的协调移动，现有反事实方法（如幽灵模型）依赖"平均"行为模拟而缺乏战术背景，需要更精确的防守表现评估方法。

Method: 针对角球这一高度结构化的比赛场景，开发协变量依赖隐马尔可夫模型(CDHMM)，从球员追踪数据中推断时间分辨的盯人和区域防守任务分配，并基于此提出防守贡献归因框架和角色条件幽灵方法。

Result: 该方法能够提供可解释的防守贡献评估，相对于情境感知基准线，能够更准确地分析无球防守表现。

Conclusion: CDHMM框架为足球角球防守提供了创新的评估工具，通过无标签的任务推断和角色条件反事实分析，实现了对无球防守表现的更精确评估。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [92] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的软重参数化方法，用于处理分类变量的梯度优化问题，相比传统的分数函数估计器和连续松弛方法，提供了更有效的优化性能。


<details>
  <summary>Details</summary>
Motivation: 分类变量的梯度优化通常依赖于两种方法：无偏但噪声大的分数函数估计器，或者使用连续松弛方法但会引入偏差和温度依赖的目标函数。现有方法在优化效率和准确性方面存在局限，需要更好的重参数化技术。

Method: 提出了一种基于扩散的软重参数化方法，针对分类分布，利用高斯加噪过程中的去噪器具有闭式解且可高效计算的特点，实现了无需训练即可通过扩散采样器进行反向传播。

Result: 实验表明，所提出的重参数化技巧在各种基准测试中取得了竞争性或改进的优化性能。

Conclusion: 基于扩散的软重参数化为分类变量的梯度优化提供了一种有效的新方法，扩展了连续松弛方法家族，在保持计算效率的同时改善了优化效果。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>
