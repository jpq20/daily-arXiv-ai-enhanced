<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.LG](#cs.LG) [Total: 45]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang,Mouxiao Bian,Peiling Li,Bingjian Wen,Ruiyao Chen,Kangkun Mao,Xiaojun Ye,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Kaifeng Qiu,Junyan Wu*

Main category: cs.CL

TL;DR: RxBench是一个用于评估大型语言模型在处方审核任务中表现的综合性基准测试，包含多种题型和处方错误类型，结果显示顶尖LLMs在某些任务上能达到或超过执业药师水平。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在临床决策支持中的快速应用，特别是在处方审核领域，需要建立一个系统化、细粒度的评估框架来准确衡量模型性能，揭示其能力和局限性。

Method: 开发了RxBench基准测试，涵盖常见处方审核类别，整合了14种常见处方错误类型，包含1150道单选题、230道多选题和879道简答题，所有题目均由经验丰富的临床药师审核。对18个最先进的LLMs进行了基准测试，并基于评估结果对中等性能模型进行了针对性微调。

Result: Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528在准确性和鲁棒性上表现最佳，形成第一梯队。领先的LLMs在某些任务上能够匹配或超过执业药师的表现。通过针对性微调中等模型，得到的专用模型在简答题任务上能够媲美领先的通用LLMs。

Conclusion: RxBench建立了一个标准化的、以错误类型为导向的评估框架，不仅揭示了前沿LLMs在处方审核中的能力和局限性，还为构建更可靠、更专业的临床工具提供了基础资源。

Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [2] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen,Ziru Liu,Xijia Tao,Hui Liu,Xinyu Fu,Suiyun Zhang,Dandan Tu,Lingpeng Kong,Rui Liu,Haoliang Li*

Main category: cs.CL

TL;DR: CCD是一种新的扩散语言模型推理框架，通过轨迹校正机制和自适应采样策略，在提高生成质量的同时加速推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的推理方法通常依赖局部、即时步的度量（如置信度或熵），缺乏更可靠的全局视角，导致采样轨迹不一致和生成质量不理想。

Method: 提出CCD框架，包含两个核心创新：1）轨迹校正机制，利用历史上下文增强序列连贯性，早期拒绝次优路径；2）自适应采样策略，根据一致性度量动态调整每个步骤的解码预算，而非基于扩散步骤的固定分配。

Result: 在Dream和LLaDA等多个基准测试中，方法在推理速度和性能上同时提升，实现高达3.48倍的加速和3.91%的性能改进。

Conclusion: CCD通过理论指导的轨迹校正和自适应采样，有效解决了扩散语言模型推理中的一致性和效率问题，为高质量高效生成提供了新方案。

Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [3] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal,Moshe Eliasof,Javier Turek,Uri Ascher,Eran Treister,Eldad Haber*

Main category: cs.CL

TL;DR: 提出基于可逆动力学的内存高效LLM架构，通过时间可逆性避免存储中间激活值，显著降低内存消耗，同时支持将现有非可逆LLM转换为可逆架构


<details>
  <summary>Details</summary>
Motivation: LLM训练成本高昂且耗时，通常需要对预训练模型进行微调。传统架构需要存储所有中间激活值，导致内存消耗巨大，限制了批处理大小和训练效率

Method: 受对称和辛微分方程启发，设计可逆LLM架构，利用时间可逆动力学在反向传播时重新计算隐藏状态，无需存储激活值。同时提出将现有非可逆LLM转换为可逆架构的微调方法

Result: 在多个数据集和基准测试上，可逆架构在多个LLM上表现出可比或改进的性能，显著降低内存消耗，允许在相同内存下处理更大的批处理大小，提高吞吐量

Conclusion: 可逆LLM架构为降低LLM训练和微调的内存与计算成本提供了可扩展且高效的路径，同时保持或提升模型性能

Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [4] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin,Haris Gulzar,Monnika Roslianna Busto,Akiko Masaki,Takeharu Eda,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 本文探索了内存高效微调（MEFT）方法在语音模型方言识别任务中的应用，相比传统微调和参数高效微调，显著减少了GPU内存使用并加速了训练速度，同时保持了相似的准确率。


<details>
  <summary>Details</summary>
Motivation: 方言识别（DI）任务对下游语音任务有重要帮助，但传统微调语音模型计算成本高、内存需求大。现有的参数高效微调（PEFT）方法虽然参数效率高，但在内存效率和训练速度方面改进有限。

Method: 将最初为语言处理提出的内存高效微调（MEFT）方法应用于通用预训练语音模型，全面分析不同MEFT方法的GPU内存使用和微调速度。以Whisper模型为例，在KeSpeech数据集上识别六种普通话子方言。

Result: 相比传统微调和PEFT方法，MEFT方法将GPU内存使用减少了高达73.25%，训练速度提升了2.1倍，同时保持了可比的准确率。

Conclusion: MEFT方法在方言识别任务中实现了显著的内存效率和训练速度提升，为语音模型的微调提供了更高效的解决方案，同时保持了模型性能。

Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [5] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai,Tanay Kewalramani,Srimanta Mandal*

Main category: cs.CL

TL;DR: 提出一种数据高效的BERT微调策略，通过TF-IDF样本选择减少75%训练数据而不损失性能，并增强词汇表以适应仇恨言论的演变术语


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的辱骂性言论持续演变，新俚语和混淆术语不断出现以规避检测系统，需要更高效和适应性的检测方法

Method: 1) 使用TF-IDF样本选择机制保留信息量最大的75%训练样本；2) 增强BERT分词器，添加领域特定的俚语和词汇变体以捕捉仇恨言论术语

Result: 在广泛使用的仇恨言论数据集上，该方法在保持竞争力的性能同时显著提高了计算效率

Conclusion: 该方法展示了在保持性能的同时减少训练开销的潜力，为可扩展和自适应的辱骂内容审核提供了可行方案

Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [6] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: MODOMA系统是一个用于无监督语言习得实验的计算多智能体实验室环境，通过成人和儿童智能体之间的交互实现语言习得，能够生成和解析目标语言的新话语。


<details>
  <summary>Details</summary>
Motivation: 为语言习得研究提供一个完全参数化的计算实验环境，使研究人员能够控制实验的各个方面，同时明确表示和查询习得的语法知识。

Method: 使用统计和基于规则的程序构建多智能体系统，其中成人智能体生成训练和测试数据，儿童智能体通过交互学习语言，最终形成基于知识的语言模型。

Result: 实验表明，儿童智能体能够基于成人智能体生成的不同数量的示例数据，成功习得功能性和内容性语法范畴，并且这些机器生成数据中发现了与人类生成数据相似的已知模式。

Conclusion: MODOMA系统为计算语言习得实验提供了新的可能性，成功习得离散语法范畴的实验结果验证了该建模方法的有效性。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [7] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga*

Main category: cs.CL

TL;DR: Swivuriso是一个3000小时的多语言语音数据集，支持7种南非语言的自动语音识别技术开发和基准测试


<details>
  <summary>Details</summary>
Motivation: 解决现有ASR数据集在非洲语言方面的显著空白，支持南非语言的语音识别技术发展

Method: 设计了数据集创建原则、伦理考虑和数据收集流程，涵盖农业、医疗和通用领域主题

Result: 提供了使用该数据训练/微调ASR模型的基线结果，并与相关语言的其他ASR数据集进行了比较

Conclusion: Swivuriso数据集填补了非洲语言ASR数据的重要空白，为多语言语音识别技术发展提供了重要资源

Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [8] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: LiteReason是一种轻量级潜在推理方法，通过连续潜在令牌帮助模型"跳过"推理步骤，在保持性能的同时大幅减少推理长度（77-92%）。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过生成长推理链来处理复杂任务，但使用强化学习优化这些推理链计算成本高昂，特别是在涉及大量令牌处理的叙事相关任务中。

Method: 提出LiteReason方法，包含轻量级推理投影器模块，生成连续潜在令牌帮助跳过推理步骤；在强化学习中，策略模型决定何时激活投影器，在潜在推理和离散推理之间切换。

Result: 在情节漏洞检测和书籍章节生成任务上，LiteReason优于潜在推理基线方法，接近非潜在强化学习训练的性能，同时将最终推理长度减少77-92%。

Conclusion: LiteReason引导强化学习训练达到性能-计算权衡曲线上更高效的部分，在保持性能的同时显著降低计算成本。

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [9] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: DETAIL框架评估提示词特异性对LLM推理性能的影响，发现特异性越高准确率越高，尤其对小模型和程序性任务效果更明显


<details>
  <summary>Details</summary>
Motivation: 提示词设计对大型语言模型的推理性能至关重要，但提示词特异性（详细程度）的影响尚未得到充分研究。本文旨在填补这一研究空白，探索不同特异性水平的提示词如何影响LLM的表现。

Method: 提出DETAIL框架：1) 使用GPT-4生成多级特异性提示词；2) 通过困惑度量化提示词特异性；3) 使用基于GPT的语义等价性评估回答正确性。在30个新颖推理任务上对GPT-4和O3-mini进行实验。

Result: 实验结果显示：1) 提示词特异性提高准确率；2) 对小模型（如O3-mini）的提升效果更明显；3) 对程序性任务的提升效果优于其他类型任务。

Conclusion: 研究强调了自适应提示策略的重要性，并为后续研究提供了工具和数据支持。提示词特异性是优化LLM推理性能的关键因素，需要根据不同模型和任务类型进行针对性设计。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [10] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang,Alban Bornet,Rui Yang,Nan Liu,Douglas Teodoro*

Main category: cs.CL

TL;DR: 研究人员创建了HealthContradict数据集来评估语言模型在长且矛盾的生物医学上下文中的推理能力，发现微调的生物医学模型既能利用正确上下文又能抵抗错误上下文。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型如何使用上下文信息回答健康问题，特别是当上下文存在矛盾时，了解模型在长且矛盾的生物医学上下文中的推理能力。

Method: 使用HealthContradict数据集（920个专家验证的实例，包含健康问题、事实答案和两个矛盾立场的文档），考虑多种提示设置（正确、错误或矛盾上下文），测量对模型输出的影响。

Result: HealthContradict比现有医学问答评估基准更能区分语言模型的上下文推理能力。微调的生物医学语言模型不仅利用预训练的参数知识，还能利用正确上下文同时抵抗错误上下文。

Conclusion: 该研究揭示了语言模型在矛盾生物医学上下文中的推理能力，强调了上下文利用和抵抗能力的重要性，为评估医疗AI系统提供了更精细的工具。

Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [11] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu,Ryan Teehan,Jinran Jin,Mengye Ren*

Main category: cs.CL

TL;DR: 该研究系统分析了37个LLM在9个基准测试中的验证能力，发现跨模型家族验证效果最佳，后训练会降低自我改进但增强跨家族改进，数学和逻辑任务具有最高的可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM作为问题求解器和解决方案验证器之间的交互研究有限，主要关注自我验证，很少研究验证器如何评判同一模型家族或不同模型家族的输出。现代LLM经过大量后训练，但其对验证能力的影响尚不清楚。

Method: 研究涵盖37个模型，跨越多个模型家族、不同规模和基础模型与后训练变体，在9个基准测试上进行评估，包括逻辑推理、结构化谜题、符号计算、数学、常识、事实回忆和领域知识。比较了自我验证、同家族内验证和跨家族验证，并引入并经验验证了"验证器增益"指标来预测基于拒绝采样的性能改进。

Result: 研究发现：1) 跨家族验证特别有效；2) 后训练会降低自我改进但增强跨家族改进；3) 数学和逻辑任务展现出最高的固有可验证性。研究还分析了验证器增益和假阳性率如何随模型规模和后训练扩展，并描述了数据集可验证性的差异。

Conclusion: 该研究为LLM验证能力提供了系统分析，揭示了跨模型家族验证的优势，后训练对验证能力的复杂影响，以及不同任务类型的可验证性差异，为未来LLM验证策略的设计提供了重要见解。

Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [12] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar,Vivek Dhanawade,Rohit Yadav,Minal Sonkar,Medha Asurlekar,Sarita Rathod*

Main category: cs.CL

TL;DR: 本文综述了语言模型中幻觉问题的类型、成因及缓解方法，为理解和减少幻觉提供一站式资源


<details>
  <summary>Details</summary>
Motivation: 传统语言模型面临幻觉问题的挑战，这一问题对自然语言处理领域构成了严重威胁。需要理解当前存在的各种幻觉类型、其产生原因以及减少幻觉的方法

Method: 本文采用综述性方法，系统性地总结和分析语言模型中的幻觉现象。文档提供了简洁明了的概述，涵盖幻觉的分类、起源和缓解策略

Result: 文档提供了对幻觉问题的全面理解框架，包括不同类型的幻觉、其产生机制以及有效的缓解方法，为研究人员和实践者提供了实用参考

Conclusion: 该综述文档为理解和缓解语言模型中的幻觉问题提供了有价值的资源，有助于推动更可靠、更安全的自然语言处理系统发展

Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [13] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri,Chahan Vidal-Gorène,Florian Cafiero*

Main category: cs.CL

TL;DR: 该研究评估了在线虚假信息检测和传播预测任务，发现文本内容对假新闻检测效果显著，而传播预测更困难且对标签构建敏感，轻量级数值特征在计算受限时仍可用。


<details>
  <summary>Details</summary>
Motivation: 研究在线虚假信息处理的两个实际任务：假新闻检测和传播预测，关注操作环境下的快速反应需求，比较不同方法在现实约束下的表现。

Method: 使用EVONS和FakeNewsNet数据集，比较文本嵌入（RoBERTa、Mistral）、轻量级数值特征（时间、关注者数、验证状态、点赞数）和序列模型（GRU、门控架构、Transformer编码器），采用降维分析（t-SNE vs PCA）。

Result: 文本内容对假新闻检测效果显著；数值特征在语言模型不可用或计算受限时仍可行；传播预测比假新闻检测更困难，对标签构建高度敏感；降维分析显示非线性结构对传播预测更有信息量；RoBERTa和Mistral嵌入结果差异不大。

Conclusion: 假新闻检测主要依赖文本内容，传播预测需要更复杂的建模和标签设计，实际应用中需考虑计算约束和API限制，评估设计对结果有重要影响。

Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [14] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang,Donghao Xie,Ming Pang,Chunyuan Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo*

Main category: cs.CL

TL;DR: ADORE是一个自持的电商搜索相关性建模框架，通过LLM生成意图对齐的训练数据、自动合成对抗样本、以及知识蒸馏注入领域属性知识，解决了传统方法的语义鸿沟和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 电商搜索中的相关性建模面临两个主要挑战：基于术语匹配的方法（如BM25）存在语义鸿沟，而神经模型则受限于领域特定困难样本的稀缺性。需要一种能够自动生成高质量训练数据并增强模型鲁棒性的解决方案。

Method: ADORE框架包含三个创新模块：1) 规则感知相关性判别模块：使用思维链LLM生成意图对齐的训练数据，并通过Kahneman-Tversky优化与用户行为对齐；2) 错误类型感知数据合成模块：自动生成对抗样本来增强模型鲁棒性；3) 关键属性增强知识蒸馏模块：将领域特定属性层次注入可部署的学生模型中。

Result: 大规模实验和在线A/B测试验证了ADORE的有效性。该框架建立了工业应用中资源高效、认知对齐的相关性建模新范式。

Conclusion: ADORE通过自动化标注、对抗生成和蒸馏过程，克服了数据稀缺问题，同时增强了推理能力，为电商搜索相关性建模提供了创新的自持解决方案。

Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [15] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang,Jinyang Wu,Yuchen Liu,Shuai Zhang,Yang Li,Qiliang Liang,Hongzhen Wang,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CL

TL;DR: CAPO是一种基于优势信号的课程学习机制，通过分阶段引入正负信号来优化强化学习训练大语言模型，提高数学推理和多模态GUI推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在训练大语言模型时，从一开始就混合正负优势信号，可能导致模糊的指导信号和有限的性能提升。需要更有效的信号利用策略。

Method: 提出CAPO（课程优势策略优化），采用自适应课程机制：首先仅使用正优势样本进行模仿学习建立坚实基础，然后逐步引入负信号培养判别能力，提高复杂场景下的泛化能力。

Result: 该方法与GRPO、PPO、RLOO、Reinforce++等多种优化方法兼容，在数学推理任务上实现了稳定且显著的改进，并能有效泛化到多模态图形用户界面推理场景。

Conclusion: CAPO作为一个通用且鲁棒的优化框架，通过分阶段利用优势信号，解决了现有方法中信号混合不当的问题，提升了语言模型的推理能力。

Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [16] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

Main category: cs.CL

TL;DR: 研究发现大语言模型在生成多文档摘要时存在显著的首因效应，更倾向于与第一个看到的文档保持语义对齐，这可能影响依赖LLM生成概览的应用和智能体系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛应用于多文档摘要任务（如Google AI概览），需要了解模型是否对所有输入文档给予同等权重，特别是当文档存在不同立场时，这种偏见可能影响摘要的客观性和可靠性。

Method: 研究构建了40组关于堕胎议题的支持-中立-反对立场的文章三元组，将每个三元组按6种不同顺序排列，使用Gemini 2.5 Flash模型生成中立概览。通过ROUGE-L（词汇重叠）、BERTScore（语义相似度）和SummaC（事实一致性）评估摘要与源文档的对应关系，并使用单因素方差分析和配对比较检验位置效应。

Result: 单因素方差分析显示BERTScore在所有立场中都存在显著的首因效应，表明摘要与第一个看到的文章在语义上更对齐。配对比较进一步显示位置1与位置2、3有显著差异，而位置2和3之间无显著差异，确认了模型对第一个文档的选择性偏好。

Conclusion: 大语言模型在多文档摘要中存在系统性的首因效应，这种偏见对依赖LLM生成概览的应用和智能体系统构成风险，因为涉及LLM的步骤可能不成比例地影响下游决策和行动。

Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [17] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 本文实证研究七种模型融合算法在减轻大语言模型社会偏见方面的效果，发现偏见减少与下游任务性能之间存在权衡关系。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会继承并放大预训练语料中的社会偏见，威胁公平性和社会信任。虽然已有研究探索通过编辑模型参数来减轻偏见，但缺乏对不同模型融合算法的实证比较。

Method: 实证调查七种算法（Linear、Karcher Mean、SLERP、NuSLERP、TIES、DELLA、Nearswap），应用于GPT、LLaMA和Qwen家族的13个开源权重模型。使用三个偏见数据集（BBQ、BOLD、HONEST）进行综合评估，并在SuperGLUE基准测试中测量这些技术对下游任务性能的影响。

Result: 发现偏见减少与下游性能之间存在权衡：实现更大偏见减轻的方法会降低准确性，特别是在需要阅读理解、常识和因果推理的任务上。在融合算法中，Linear、SLERP和Nearswap在保持整体性能的同时持续减少偏见，其中中等插值权重的SLERP成为最平衡的选择。

Conclusion: 模型融合算法在减轻偏见方面具有潜力，但过度的去偏见化或不适当的融合方法可能导致重要语言能力的退化。SLERP在中等插值权重下是最平衡的选择。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [18] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal,Naman Mishra*

Main category: cs.CL

TL;DR: CREST是一个参数高效的跨语言安全分类模型，仅用0.5B参数支持100种语言，通过在13种高资源语言上训练实现跨语言泛化，在安全基准测试中超越同规模模型并媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全防护主要针对高资源语言，导致使用低资源语言的大量人口缺乏足够的安全保护，需要开发能够服务全球人口的通用、语言无关的安全系统。

Method: 采用基于聚类的跨语言迁移方法，仅使用13种高资源语言训练数据，通过参数高效的多语言安全分类模型设计，实现从少数语言到100种语言的泛化能力。

Result: 在六个安全基准测试中，CREST超越了同规模（0.5B参数）的最先进防护模型，并与参数数量大得多的模型（2.5B参数及以上）取得了竞争性结果。

Conclusion: 语言特定的安全防护存在局限性，开发能够有效扩展以服务全球人口的通用、语言无关的安全系统至关重要，CREST为此提供了有效的解决方案。

Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [19] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: 研究者开发了BayesBench基准测试，通过心理物理学范式评估LLM在文本和图像多模态任务中的贝叶斯一致性行为，发现模型能力与策略之间存在关键分离，准确率不能保证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在显式推理方面表现出色，但其隐式计算策略尚未被充分探索。人类在感知任务中能够使用接近最优的贝叶斯策略直觉地处理和整合噪声信号，研究者想知道LLM是否在没有明确训练或指令的情况下也能表现出类似的最优多模态整合行为。

Method: 采用心理物理学范式，通过系统行为研究推断LLM的计算原理。创建了BayesBench行为基准测试，包含四个基于经典心理物理学的幅度估计任务（长度、位置、距离、持续时间），涵盖文本和图像模态。评估了九个不同的LLM模型，并与人类判断进行校准。通过控制噪声、上下文和指令提示的消融实验，测量多模态线索整合中的性能、行为和效率。除了准确率和效率指标外，还引入了贝叶斯一致性分数，即使在准确率饱和时也能检测贝叶斯一致的行为变化。

Result: 研究结果显示，虽然能力强的模型通常以贝叶斯一致的方式适应，但准确率不能保证鲁棒性。值得注意的是，GPT-5 Mini在文本任务上达到完美准确率，但未能有效整合视觉线索。这揭示了能力与策略之间的关键分离，表明以准确率为中心的基准测试可能过度关注性能而忽略了脆弱的确定性处理。这些发现揭示了LLM在处理不确定性方面出现的原理性行为，并突出了准确率与贝叶斯倾向之间的相关性。

Conclusion: 该研究揭示了LLM在隐式计算策略方面的重要发现：准确率不能完全反映模型的鲁棒性和不确定性处理能力。研究者发布了心理物理学基准测试和一致性度量工具，为未来多模态架构设计提供评估工具和指导。

Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [20] [SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys](https://arxiv.org/abs/2512.02763)
*Jiahao Zhao,Shuaixing Zhang,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: SurveyEval是一个评估自动生成调研报告的综合性基准，从整体质量、大纲连贯性和参考文献准确性三个维度进行评估，旨在解决LLM自动调研系统评估的挑战。


<details>
  <summary>Details</summary>
Motivation: LLM自动调研系统通过整合检索、组织和内容合成实现端到端生成，但如何评估这种复杂系统仍是一个重大挑战。现有研究主要关注开发新的生成管道，缺乏系统性的评估方法。

Method: 提出SurveyEval基准，从三个维度评估自动生成的调研报告：整体质量、大纲连贯性和参考文献准确性。在7个学科领域进行扩展评估，并增强LLM-as-a-Judge框架，加入人工参考以加强评估与人类判断的一致性。

Result: 评估结果显示，通用长文本或论文写作系统倾向于生成质量较低的调研报告，而专门的调研生成系统能够提供显著更高质量的结果。

Conclusion: SurveyEval可作为可扩展的测试平台，用于理解和改进跨不同学科和评估标准的自动调研系统。

Abstract: LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

</details>


### [21] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: PEFT-Factory是一个统一的框架，用于高效微调大型语言模型，提供19种PEFT方法、27个数据集和专用评估指标，旨在解决PEFT方法难以复制、部署和比较的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模不断增大，参数高效微调方法变得越来越重要。然而，许多新提出的PEFT方法难以复制、部署或相互比较，这阻碍了该领域的研究进展和实际应用。

Method: 作者提出了PEFT-Factory框架，这是一个基于流行的LLaMA-Factory的下游框架。它采用模块化设计，原生支持19种PEFT方法、27个分类和文本生成数据集（涵盖12个任务），并提供标准和PEFT专用评估指标。

Result: PEFT-Factory提供了一个即用型、受控且稳定的环境，显著提高了PEFT方法的可复制性和基准测试能力。该框架已公开可用，为研究人员和从业者提供了统一的实验平台。

Conclusion: PEFT-Factory通过统一的框架解决了PEFT方法难以复制和比较的问题，为参数高效微调领域的研究提供了标准化工具，有助于推动该领域的进一步发展。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [22] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su,Jianming Long,Changyue Wang,Shiyu Lin,Jingyan Xu,Ziyi Ye,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UniFact是一个统一评估框架，用于直接比较事实验证(FV)和幻觉检测(HD)两种范式，发现两者互补，混合方法性能最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生幻觉（看似流畅但事实错误的内容），这影响了实际应用中的可信度。目前存在模型中心的幻觉检测(HD)和文本中心的事实验证(FV)两种独立研究范式，它们使用不同的假设、数据集和评估协议，这种分离阻碍了整体进展。

Method: 提出了UniFact统一评估框架，通过动态生成模型输出和相应的事实性标签，实现FV和HD在实例级别的直接比较。在大规模实验中，测试了多个LLM家族和检测方法。

Result: 三个关键发现：1) 没有哪种范式普遍优越；2) HD和FV捕捉了事实错误的互补方面；3) 整合两种方法的混合方法始终达到最先进的性能。此外，首次深入分析了FV和HD为何分化，并提供了支持两者统一的实证证据。

Conclusion: 需要一个新的、整合的研究议程，将幻觉检测和事实验证统一起来。实验结果表明混合方法性能最佳，呼吁打破两种范式之间的研究隔阂。

Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [23] [Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension](https://arxiv.org/abs/2512.02791)
*Juexi Shao,Siyou Li,Yujian Gan,Chris Madge,Vanja Karan,Massimo Poesio*

Main category: cs.CL

TL;DR: 提出三层次数据合成方法解决对话式广义指代表达理解中的分布偏移问题，通过平衡真实性和可控性生成可扩展的监督数据，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 对话式广义指代表达理解任务面临训练与评估领域间的分布偏移问题，且缺乏标注的对话接地数据，现有系统在此挑战下表现不佳

Method: 采用三层次数据合成方法，平衡真实性和可控性，为对话条件接地生成可扩展的监督数据

Result: 在合成数据上微调的模型在标准评估指标上相比先前方法取得一致且显著的改进

Conclusion: 提出的数据合成方法能有效解决对话式广义指代表达理解中的数据稀缺和分布偏移问题，显著提升模型性能

Abstract: Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.

</details>


### [24] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo,Hilton Vorster,Josh Warren,Trevor Naick,Deandre Vanmali,Masana Mashapha,Luke Brand,Alyssa Fernandes,Janco Calitz,Sibusiso Makhoba*

Main category: cs.CL

TL;DR: TriLex框架通过三阶段检索增强方法扩展低资源非洲语言情感词典，提升AfroXLMR和AfriBERTa模型在情感分析中的性能，AfroXLMR表现最佳。


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言在情感分析中代表性不足，限制了多语言NLP系统的词汇覆盖和性能，需要开发系统化方法来扩展这些语言的情感词典。

Method: 提出TriLex三阶段检索增强框架：1) 基于语料库的提取；2) 跨语言映射；3) RAG驱动的词汇精炼。使用扩展后的词典评估AfroXLMR和AfriBERTa两个非洲预训练语言模型。

Result: AfroXLMR表现最佳，在isiXhosa和isiZulu上F1分数超过80%，跨语言稳定性强。AfriBERTa虽未在目标语言上预训练，仍能达到约64%的可靠F1分数。两种模型均优于传统机器学习基线，集成分析进一步提升了精度和鲁棒性。

Conclusion: TriLex是一个可扩展且有效的框架，适用于低资源南非语言的多语言情感词典扩展和情感建模，为低资源语言NLP研究提供了实用解决方案。

Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [25] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 提出了一种基于模型内部表示的稳定秩方法，作为无需人工标注的质量信号，用于改进大语言模型的性能和对齐


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐方法依赖外部监督存在诸多限制：人工标注稀缺且主观，奖励模型易受奖励攻击，自评估方法存在提示敏感性和偏见问题。需要寻找一种内在的、无需标注的质量信号

Method: 提出稳定秩方法，通过计算隐藏状态的总方差与主导方向方差的比值来衡量表示的有效维度，捕捉信息在表示维度间的分布特征。基于此开发了稳定秩组相对策略优化（SR-GRPO），使用稳定秩作为强化学习的奖励信号

Result: 稳定秩在RewardBench上达到84.04%准确率，通过Best-of-N采样将任务准确率平均提升11.3个百分点。SR-GRPO无需外部监督，将Qwen2.5-1.5B-Instruct在STEM任务上提升10%，数学推理提升19%，优于学习奖励模型和自评估基线

Conclusion: 质量信号可以从模型内部几何结构中提取，为实现无需外部监督的可扩展对齐提供了新路径

Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [26] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong*

Main category: cs.CL

TL;DR: 该研究提出了TCM-BEST4SDT基准，用于评估大语言模型在中医辨证论治临床实践中的能力，包含四个任务和三种评估机制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在中医领域的应用增多，迫切需要评估其临床能力。现有基准主要局限于知识问答或辨证准确性，缺乏对治疗决策的评估，且中医辨证论治具有个体化、整体性和多样性的特点，使得评估面临挑战。

Method: 1. 提出基于临床病例的综合基准TCM-BEST4SDT，由中医专家主导设计；2. 采用专门的奖励模型量化处方与证候的一致性；3. 数据标注遵循严格流程；4. 基准包含四个任务：中医基础知识、医学伦理、LLM内容安全、辨证论治；5. 评估框架整合三种机制：选择题评估、评判模型评估、奖励模型评估。

Result: 在15个主流大语言模型（包括通用和中医领域模型）上验证了TCM-BEST4SDT的有效性。该基准已公开可用，以促进智能中医研究的发展。

Conclusion: TCM-BEST4SDT是一个全面、基于临床病例的中医辨证论治评估基准，能够有效评估大语言模型在中医临床实践中的能力，填补了现有评估在治疗决策方面的空白。

Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [27] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru,Fabian Retkowski,Christian Huber,Lukas Hilgert,Seymanur Akti,Enes Yavuz Ugan,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: BOOM是一个多模态多语言讲座伴侣系统，能够同时翻译讲座音频和幻灯片，生成同步的翻译文本、本地化幻灯片和合成语音，为学习者提供完整的跨语言学习体验。


<details>
  <summary>Details</summary>
Motivation: 教育全球化和在线学习的快速增长使得教育内容本地化成为关键挑战。讲座材料本质上是多模态的，结合了语音音频和视觉幻灯片，需要能够处理多种输入模态的系统。为了提供完整的学习体验，翻译必须保留所有模态：用于阅读的文本、用于视觉理解的幻灯片以及用于听觉学习的语音。

Method: 提出了BOOM系统，这是一个端到端的多模态多语言讲座伴侣，联合翻译讲座音频和幻灯片，生成跨三个模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片以及合成语音。

Result: 实验表明，具有幻灯片感知的转录在下游任务（如摘要和问答）中也能带来级联效益。研究团队发布了幻灯片翻译代码，并集成到Lecture Translator系统中。

Conclusion: BOOM系统能够让学生以母语访问讲座内容，同时力求完整保留原始内容，为多模态教育内容的本地化提供了有效的端到端解决方案。

Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [28] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle,Timo Heiß,Moritz Schlager,Matthias Aßenmacher,Matthias Feurer*

Main category: cs.CL

TL;DR: promptolution：一个统一的模块化开源框架，集成了多种离散提示优化器，为实践者和研究者提供完整的提示优化组件


<details>
  <summary>Details</summary>
Motivation: 尽管许多研究论文证明了提示优化的有效性，但实际应用受到阻碍，因为现有实现通常依赖于未维护和孤立的研究代码库

Method: 引入promptolution框架，这是一个统一且模块化的开源系统，集成了多种当代离散提示优化器，同时保持与底层LLM实现的无关性

Result: 创建了一个可扩展的系统，为实践者和研究者提供了在单个系统中进行提示优化所需的所有组件

Conclusion: promptolution框架解决了提示优化在实际应用中的障碍，通过提供统一、模块化且与LLM无关的开源系统，促进了提示优化的研究和实践应用

Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [29] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang,Yusheng Zhou,Tolga Ergen,Lajanugen Logeswaran,Moontae Lee,David Jurgens*

Main category: cs.CL

TL;DR: 系统提示词优化可提升多语言大模型性能，通过统一评估框架发现某些提示组件（如思维链、情感、场景）与稳健多语言行为相关，自动优化框架能提升各项指标5-10%


<details>
  <summary>Details</summary>
Motivation: 现有系统提示词研究主要集中在英语环境，但实际部署需要单一提示词能在多种语言中可靠工作。本文旨在研究不同系统提示词如何引导模型实现准确和稳健的跨语言行为。

Method: 提出统一四维评估框架评估多语言环境中的系统提示词；在5种语言、3个大模型和3个基准测试上进行大规模实验；开发多语言设置的提示词优化框架；分析超过1000万个推理单元。

Result: 发现某些提示词组件（如思维链、情感、场景）与稳健的多语言行为相关；自动优化框架能提升所有指标5-10%；性能更好的系统提示词能诱导更结构化、一致性的推理模式，同时减少不必要的语言切换。

Conclusion: 系统提示词优化是实现准确和稳健多语言大模型行为的可扩展路径，为多语言部署提供了有效的解决方案。

Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [30] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari,Krishno Dey*

Main category: cs.CL

TL;DR: 该论文研究了孟加拉语仇恨言论检测，在BLP 2025共享任务的两个子任务中，比较了传统机器学习方法和基于Transformer的模型，发现语言特定的预训练模型BanglaBERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为拥有2.3亿使用者的语言，在计算资源中代表性不足，特别是在社交媒体自动审核方面存在需求。低资源语言的仇恨言论识别面临数据集不足、拼写异质性和语言多样性等挑战。

Method: 研究复制了官方基线方法（多数投票、随机、支持向量机），并增加了逻辑回归、随机森林和决策树作为基线。同时使用了基于Transformer的模型包括DistilBERT、BanglaBERT、m-BERT和XLM-RoBERTa进行仇恨言论分类。

Result: 所有基于Transformer的模型（除DistilBERT外）在子任务中都优于基线方法。在Transformer模型中，BanglaBERT在两个子任务中都取得了最佳性能，尽管模型规模较小，但优于m-BERT和XLM-RoBERTa。

Conclusion: 语言特定的预训练对于低资源语言非常重要。BanglaBERT的优异表现强调了为孟加拉语等低资源语言开发预训练语言模型的潜力和必要性。

Abstract: Hate speech recognition in low-resource lan- guages remains a difficult problem due to in- sufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the grow- ing need for automated moderation on social media platforms, Bangla is significantly under- represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official base- lines (e.g., Majority, Random, Support Vec- tor Machine) and also produce and consider Logistic Regression, Random Forest, and De- cision Tree as baseline methods. We also uti- lized transformer-based models such as Dis- tilBERT, BanglaBERT, m-BERT, and XLM- RoBERTa for hate speech classification. All the transformer-based models outperformed base- line methods for the subtasks, except for Distil- BERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language- specific pre-training is very important. Our results highlight the potential and need for pre- trained language models for the low-resource Bangla language.

</details>


### [31] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

Main category: cs.CL

TL;DR: 论文提出了一种通过可视化知识图谱来检测和缓解大语言模型在企业环境中产生幻觉的框架，让用户能够直观识别模型输出与真实知识源之间的不一致性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中，大语言模型经常需要结合闭源领域知识来提供更符合上下文的回答，但由于有限的上下文窗口以及预训练数据与提供知识之间的不一致，模型容易产生幻觉，有些幻觉看起来高度可信且难以通过常规人工审查发现。现有的缓解策略要么依赖昂贵的大规模黄金标准问答集，要么依赖二次模型验证，都无法提供确定性保证。

Method: 论文引入了一个框架，将专有知识和模型生成的内容组织成交互式可视化知识图谱。该框架将模型断言与底层真实知识源链接起来，并显示置信度水平，为用户提供清晰的潜在幻觉区域视图。通过这个可视化界面，用户可以诊断不一致性，识别薄弱的推理链，并提供纠正反馈。

Result: 该框架创建了一个人在回路的工作流程，形成了结构化的反馈循环，能够增强模型可靠性并持续提高回答质量。

Conclusion: 通过可视化知识图谱的方法，为用户提供了检测和缓解大语言模型幻觉的有效工具，解决了当前缓解策略的局限性，实现了模型可靠性的持续改进。

Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [32] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: SchED是一种无需训练、模型无关的早期退出算法，通过聚合全跨度logit边界并在满足平滑的进度相关置信度阈值时停止解码，显著加速扩散大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比自回归模型有优势，但其迭代采样过程缓慢，严重限制了实际应用。需要一种方法来加速dLLM的解码过程，同时保持生成质量。

Method: SchED是一种训练免费、模型无关的早期退出算法。它聚合全跨度logit边界，当满足平滑的进度相关置信度阈值时停止解码。算法不依赖模型训练，可应用于不同dLLM家族。

Result: 在指令调优模型上，SchED实现3.8-4.0倍加速，同时保持99.8-100%的基线分数。在基础模型上，获得一致的速度提升，性能保留99.1-100%，在更激进设置下可达2.34倍加速。在惩罚质量损失的保守速度指标(QPS, γ=4)下，SchED明显优于先前基于置信度的早期退出方法。

Conclusion: SchED通过将真正的置信度稳定转化为计算节省，使dLLM解码显著更高效。指令调优加速了预测熵的衰减，进一步提升了SchED的效果。该方法为扩散大语言模型的实用化提供了有效的加速解决方案。

Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [33] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 提出一个将英文句子转换为逻辑表达式并生成CNF的新框架，通过微调语言模型减少幻觉，实现可靠的CNF生成


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言到形式逻辑的自动翻译中存在幻觉问题，特别是在需要精确性的逻辑翻译任务中，这限制了其在自动推理、调试和软件规范验证中的应用

Method: 提出一个新颖框架：输入英文句子，转换为逻辑表达式，再翻译为合取范式用于可满足性求解。结合经典NLP技术（自定义语法）、符号计算库和微调的语言模型来减少幻觉

Result: 实验表明，在不同语法设置下训练的微调模型能够有意识地纠正原始模型产生的同类幻觉，从而提供可靠的CNF生成

Conclusion: 该框架通过结合传统NLP技术和微调语言模型，有效减少了逻辑翻译中的幻觉问题，为自动推理和软件验证提供了可靠的形式逻辑生成能力

Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [34] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh*

Main category: cs.CL

TL;DR: MoCoP是一个无数据集、闭环的道德一致性评估框架，用于持续评估和解释大语言模型的道德稳定性，通过三层分析架构自主生成、评估和优化伦理场景。


<details>
  <summary>Details</summary>
Motivation: 现有对齐框架依赖静态数据集和事后评估，难以捕捉伦理推理在不同上下文或时间尺度上的演变，需要开发能够持续评估道德一致性的方法。

Method: 提出Moral Consistency Pipeline (MoCoP)框架，包含三层：词汇完整性分析、语义风险估计和基于推理的判断建模，形成自维持架构，无需外部监督即可自主生成、评估和优化伦理场景。

Result: 在GPT-4-Turbo和DeepSeek上的实证结果显示，MoCoP能有效捕捉纵向道德行为，发现道德维度与毒性维度呈强负相关(rET = -0.81, p<0.001)，与响应延迟几乎无关(rEL≈0)。

Conclusion: 道德一致性和语言安全是模型行为的稳定可解释特征而非短期波动，MoCoP通过将伦理评估重构为动态、模型无关的道德内省形式，为可扩展的持续审计提供了可复现基础。

Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [35] [Decentralized Multi-Agent System with Trust-Aware Communication](https://arxiv.org/abs/2512.02410)
*Yepeng Ding,Ahmed Twabi,Junwei Yu,Lingfeng Zhang,Tohru Kondo,Hiroyuki Sato*

Main category: cs.MA

TL;DR: 提出基于区块链的去中心化多智能体系统架构，解决传统集中式架构的单点故障、审查风险、可扩展性限制和信任问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型加速了自主多智能体系统发展，但传统集中式架构存在单点故障、易受审查、可扩展性限制和信任问题等挑战

Method: 设计去中心化多智能体系统架构，采用基于区块链的去中心化智能体运行时，形式化信任感知通信协议，利用密码学原语和链上操作提供安全属性

Result: 通过全面安全分析验证了可验证交互周期、通信完整性、真实性、不可否认性和条件保密性等安全属性，性能分析表明DMAS是可扩展且高效的可信多智能体系统解决方案

Conclusion: 提出的去中心化多智能体系统架构能够克服传统集中式架构的根本问题，实现信任感知、可扩展且抗审查的自主智能体交互

Abstract: The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.

</details>


### [36] [EZYer: A simulacrum of high school with generative agent](https://arxiv.org/abs/2512.02561)
*Jinming Yang,Zimu Ji,Weiqi Luo,Gaoxi Wang,Bin Ma,Yueling Deng*

Main category: cs.MA

TL;DR: EZYer是一个用于在线教育的生成式智能体，包含教师模块（自动生成结构化教学材料和LaTeX课件）、学生模块（通过多角色协作生成学术笔记）和控制器（内容质量保障系统），在内容准确性、知识覆盖等方面表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有在线教育工具在课件生成、互动笔记和内容质量保障方面存在服务不完整、性能不足和交互性弱的问题，需要开发更全面、高性能且交互性强的教育工具。

Method: 开发了生成式智能体EZYer，包含三个核心模块：1）教师模块整合文本语料检索和深度生成技术，自动生成结构化教学材料和LaTeX课件；2）学生模块通过教师、助理、优等生和困难生四个角色的协作交互，生成学术笔记；3）控制器设置关键词过滤、内容评分、角色协同验证和动态内容校正系统。

Result: 设计了内容准确性、知识覆盖度、可用性、格式正确性和视觉设计吸引力五个维度的评价指标，使用五个大语言模型对EZYer生成的100个课件和笔记进行评分，结果显示EZYer生成的内容质量优秀，具有良好的应用前景。

Conclusion: EZYer作为一个综合性教育生成智能体，通过多模块协同工作，能够生成高质量的教学内容和互动笔记，在内容准确性、知识覆盖和用户体验方面表现优异，为在线教育提供了有效的解决方案。

Abstract: With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.

</details>


### [37] [Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions](https://arxiv.org/abs/2512.02682)
*Piercosma Bisconti,Marcello Galisai,Federico Pierucci,Marcantonio Bracale,Matteo Prandi*

Main category: cs.MA

TL;DR: 论文探讨了为单智能体设计的AI安全机制在多智能体交互环境中失效的问题，提出了从模型级安全转向系统级安全的概念框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全机制主要针对单模型与用户的交互场景，但随着LLM间交互生态系统的快速发展，这些机制无法有效管理多模型交互中涌现的系统性风险。

Method: 提出"涌现系统性风险视野"（ESRH）理论框架来分析交互结构导致的系统性不稳定，建立微观、中观和宏观层面的故障模式分类法，并设计"InstitutionalAI"架构来在多智能体系统中嵌入自适应监督。

Result: 识别了单智能体安全机制在多智能体环境中的局限性，建立了系统性风险分析框架，提出了系统级安全治理的新方法。

Conclusion: 需要从传统的模型级安全范式转向系统级安全范式，通过InstitutionalAI等架构设计来管理LLM间交互生态系统中涌现的集体风险。

Abstract: This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas,Lucas Cordeiro,Youcheng Sun,Waldir Junior*

Main category: cs.AI

TL;DR: 本文提出了首个具有可证明保证的LLM验证器收敛定理，通过马尔可夫链建模LLM与验证器的交互，证明了对任意δ>0，程序都能以期望迭代次数≤4/δ的概率终止验证，并通过9万多次实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 当前将形式化验证工具与大型语言模型结合的方法缺乏可靠的理论基础，导致验证过程不稳定——有时收敛，有时循环，有时偏离稳定轨迹。需要建立具有可证明保证的形式化框架来解决这一关键差距。

Method: 将LLM与验证器的交互建模为离散时间马尔可夫链，状态转移由关键参数δ（错误减少概率）决定。开发了LLM-Verifier收敛定理，证明对任意δ>0，程序都能几乎必然达到Verified状态，且期望迭代次数有界。通过超过9万次实验对理论预测进行压力测试。

Result: 理论预测与实证结果高度一致：所有运行都成功达到验证，收敛因子紧密聚集在Cf≈1.0附近。基于此将工作流划分为三个操作区域：边际区、实践区和高性能区，并建立了可靠的设计阈值。

Conclusion: 理论保证和实验证据为LLM辅助验证提供了清晰的架构基础，消除了启发式调优的需求。工程师获得了支持可预测资源规划和性能预算的框架，为在安全关键软件环境中部署这些管道提供了必要保障。

Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [39] [Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code](https://arxiv.org/abs/2512.02170)
*Pritam Deka,Barry Devereux*

Main category: cs.AI

TL;DR: Flowchart2Mermaid是一个将流程图图像转换为可编辑Mermaid.js代码的轻量级Web系统，使用视觉语言模型和混合交互界面，生成结构化、版本可控的文本表示。


<details>
  <summary>Details</summary>
Motivation: 流程图通常以静态图像形式分享，难以编辑和重用。现有图像转图表工具缺乏结构化文本表示和版本控制能力。

Method: 使用详细系统提示和视觉语言模型将流程图图像转换为Mermaid.js代码，支持文本编辑、拖拽节点插入和自然语言命令的混合交互界面。

Result: 系统生成结构化、版本可控的文本表示，与渲染图表保持同步。引入了评估结构准确性、流程正确性、语法有效性和完整性的评价指标。

Conclusion: Flowchart2Mermaid提供了一种将静态流程图转换为可编辑、版本可控文本表示的有效方法，支持混合交互编辑和AI辅助优化。

Abstract: Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present \textsc{Flowchart2Mermaid}, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.

</details>


### [40] [From monoliths to modules: Decomposing transducers for efficient world modelling](https://arxiv.org/abs/2512.02193)
*Alexander Boyd,Franz Nowak,David Hyland,Manuel Baltieri,Fernando E. Rosas*

Main category: cs.AI

TL;DR: 本文提出了一种将复杂世界模型分解为子转换器的框架，通过模块化分解实现并行化、可解释的世界建模，以平衡AI安全的结构透明性和实际推理的计算效率需求。


<details>
  <summary>Details</summary>
Motivation: 现实世界模型通常计算需求高，但真实场景往往包含以模块化方式交互的子组件。为满足AI安全对结构透明性的要求，同时保持实际推理的计算效率，需要开发能够分解复杂世界模型的框架。

Method: 开发了一个基于转换器（transducers，POMDPs的推广）的框架，通过反转组合过程，将复杂世界模型分解为在独立输入-输出子空间上运行的子转换器，实现并行化和可解释性。

Result: 该框架能够从整体世界模型中推导出子转换器，支持分布式推理，为模块化世界建模提供了并行化和可解释的替代方案。

Conclusion: 这些结果为连接AI安全所需的结构透明性和实际推理所需的计算效率奠定了基础，为构建更高效、更安全的世界模型提供了理论框架。

Abstract: World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.

</details>


### [41] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 该论文分析了当前AI系统的七大核心缺陷，包括缺乏内在自我监控、元认知意识不足、学习机制固定不具适应性等，并提出了向认知自主AI范式转变的路径。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在感知、语言、推理和多模态领域取得了快速进展，但现代AI系统在自我监控、自我校正和动态环境中自主调节行为方面仍存在根本性限制。论文旨在识别和分析这些限制，并提出超越这些限制的架构方向。

Method: 通过比较分析人工系统与生物认知，整合AI研究、认知科学和神经科学的见解，识别出当代AI模型的七大核心缺陷，并基于神经认知原理提出架构改进方向。

Result: 识别出七大核心缺陷：缺乏内在自我监控、元认知意识不足、固定非适应性学习机制、无法重构目标、缺乏表征维护、不足的具身反馈、缺乏内在能动性。这些结构限制阻碍了当前架构实现稳健泛化、终身适应性和现实世界自主性。

Conclusion: 需要向认知自主AI范式转变，开发能够自我导向适应、动态表征管理和有意图、目标导向行为的系统，同时配备改革性监督机制，确保自主系统保持可解释性、可治理性并与人类价值观保持一致。

Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [42] [Model Recovery at the Edge under Resource Constraints for Physical AI](https://arxiv.org/abs/2512.02283)
*Bin Xu,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: MERINDA是一个FPGA加速的模型恢复框架，通过可并行神经架构替代迭代求解器，显著降低内存使用和运行时间，适用于资源受限的实时任务关键自主系统。


<details>
  <summary>Details</summary>
Motivation: 模型恢复（MR）能够通过学习控制动力学方程实现任务关键自主系统的安全可解释决策，但其在边缘设备上的部署受到神经常微分方程迭代性质的限制，这在FPGA上效率低下。内存和能耗是边缘设备实时操作的主要瓶颈。

Method: 提出MERINDA框架，用可并行化的神经架构替代神经常微分方程中的迭代求解器，实现FPGA加速的模型恢复。

Result: 与移动GPU相比，MERINDA实现了近11倍的DRAM使用降低和2.2倍的运行速度提升。实验揭示了在固定精度下内存与能耗之间的反比关系。

Conclusion: MERINDA展示了在资源受限、实时任务关键自主系统中的适用性，通过FPGA加速和内存优化解决了模型恢复在边缘设备部署的挑战。

Abstract: Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.

</details>


### [43] [Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization](https://arxiv.org/abs/2512.02302)
*Varun Kumar Dasoju,Qingsu Cheng,Zeyun Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种用于乳腺上皮细胞分割的框架，仅用599张训练图像就达到了95.5%的Dice分数，显著减少了医学图像标注所需的时间和专家投入。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注需要大量时间和专业知识，特别是乳腺上皮细胞数据集的标注需要病理学家投入数百小时。这是一个临床感知AI发展的关键瓶颈问题。

Method: 1. 使用量子启发的多尺度Gabor滤波器进行边缘增强，创建第四输入通道；2. 提出稳定化的多组件损失函数，结合自适应Dice损失和边界感知项；3. 引入基于复杂度的加权采样策略；4. 采用EfficientNet-B7/UNet++架构，支持4通道到3通道的投影；5. 通过指数移动平均和统计异常值检测进行鲁棒验证。

Result: Dice分数达到95.5% ± 0.3%，IoU达到91.2% ± 0.4%。量子增强使边界精度提升2.1%，加权采样使小病灶检测提升3.8%。在仅有599张训练图像的情况下取得了突破性性能。

Conclusion: 该框架通过有限标注实现了卓越的乳腺细胞分割性能，显著减少了医学专家创建数据集所需的时间，解决了临床感知AI发展的一个基本瓶颈问题。

Abstract: Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.

</details>


### [44] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu,Xiaofei Wen,Wenjie Jacky Mo,Tinghui Zhu,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: OmniGuard是首个全模态安全护栏系统，通过结构化安全标签和专家模型提炼的安全评估，为处理文本、图像、视频和音频的全模态大语言模型提供统一的安全保障框架。


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型处理多种模态数据带来了新的安全挑战，现有安全护栏研究主要针对单模态设置，且通常将安全保障视为二元分类问题，这限制了在不同模态和任务间的鲁棒性。

Method: 提出OmniGuard框架，构建包含超过21万个多样样本的全模态安全数据集，涵盖所有模态的单模态和跨模态样本，每个样本都标注了结构化安全标签并通过专家模型提炼的安全评估。

Result: 在15个基准测试上的广泛实验表明，OmniGuard在多种多模态安全场景中表现出强大的有效性和泛化能力。

Conclusion: OmniGuard提供了一个统一框架，能够在全模态中执行安全策略和降低风险，为构建更鲁棒和强大的全模态安全系统铺平了道路。

Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.

</details>


### [45] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue,Weichen Liu,Shiqi Wang,Haoming Wang,Yuyang Wu,Wei Gao*

Main category: cs.AI

TL;DR: 本文提出了ReMindView-Bench基准测试，用于评估视觉语言模型在多视图空间推理中的表现，发现当前模型在跨视图对齐和视角转换方面存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在多视图空间推理中难以保持几何一致性和跨视图一致性，缺乏能够隔离多视图推理与单视图感知及时间因素的细粒度基准测试。

Method: 构建ReMindView-Bench基准测试，系统性地变化视角空间模式和查询类型来探究空间认知的关键因素；使用LLM-as-a-judge和自一致性提示进行显式分阶段分析，以及线性探测和熵动态进行隐式分析。

Result: 评估15个当前VLM显示，模型在多视图空间推理中在跨视图对齐和视角转换方面存在一致性的失败；显式分析显示模型在帧内感知表现良好，但在跨视图信息整合时性能急剧下降；隐式分析显示任务相关信息逐渐丢失，正确与错误轨迹之间的不确定性分离。

Conclusion: ReMindView-Bench为VLM空间推理提供了认知基础的诊断，揭示了多视图空间心智模型在推理过程中如何形成、退化和失稳，为改进模型的空间推理能力提供了重要洞见。

Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.

</details>


### [46] [Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games](https://arxiv.org/abs/2512.02358)
*Ran Zhang,Kun Ouyang,Tiancheng Ma,Yida Yang,Dong Fang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于大型语言模型的生成式智能体MMO模拟系统，用于优化游戏数值系统和机制设计，相比传统方法更真实、可解释且成本更低。


<details>
  <summary>Details</summary>
Motivation: 传统MMO游戏数值系统和机制设计的优化方法依赖大规模在线实验或基于预定义统计模型的参数调优，这些方法成本高、耗时长且可能破坏玩家体验。简化的离线模拟系统保真度有限，无法准确模拟真实玩家的推理和对干预的反应。

Method: 提出了基于大型语言模型的生成式智能体MMO模拟系统：1) 使用监督微调和强化学习在大规模真实玩家行为数据上适配LLMs，使其从通用先验转变为游戏特定领域，实现真实且可解释的玩家决策；2) 基于真实游戏日志训练数据驱动的环境模型，重建动态游戏系统。

Result: 实验表明，该系统与真实世界玩家行为具有很强的一致性，在干预下能产生合理的因果响应，为数据驱动的数值设计优化提供了可靠、可解释且成本效益高的框架。

Conclusion: 基于大型语言模型的生成式智能体模拟系统能够有效解决传统MMO游戏数值优化方法的局限性，提供了一种更真实、可解释且经济高效的替代方案，为游戏设计和优化开辟了新途径。

Abstract: Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.

</details>


### [47] [Synthetic Error Injection Fails to Elicit Self-Correction In Language Models](https://arxiv.org/abs/2512.02389)
*David X. Wu,Shreyas Kapur,Anant Sahai,Stuart Russell*

Main category: cs.AI

TL;DR: 研究探索使用监督学习和人工错误注入来培养语言模型的自我纠错能力，但发现该方法效果有限，无法显著提升性能，甚至模型会重复原始错误。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为激发大语言模型推理和自我纠错能力的主要方法，但其计算成本高昂，促使研究者探索替代方案。受自动驾驶和机器人技术启发，研究是否可以通过监督学习和合成错误注入来诱导语言模型的自我纠错能力。

Method: 在推理链中插入人工错误，掩盖这些错误，然后监督模型识别和纠正这些错误。通过合成错误注入的监督学习方法来训练模型。

Result: 该方法即使在简单的合成任务上也无法显著提高多个模型的性能。即使模型发现了自己的错误，也常常会重复原始错误。研究发现，从合成错误到策略错误的分布偏移显著降低了微调模型的纠错能力，即使合成错误覆盖了策略错误。

Conclusion: 合成错误注入的监督学习方法无法有效培养语言模型的自我纠错能力，这解释了为什么策略强化学习方法在激发自我纠错方面被证明是独特有效的。

Abstract: Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

</details>


### [48] [Guided Self-Evolving LLMs with Minimal Human Supervision](https://arxiv.org/abs/2512.02472)
*Wenhao Yu,Zhenwen Liang,Chengsong Huang,Kishan Panaganti,Tianqing Fang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: R-Few是一个引导式自演化的Challenger-Solver框架，通过少量人类监督实现AI模型的稳定自我进化，在数学和通用推理任务上取得持续改进。


<details>
  <summary>Details</summary>
Motivation: AI自我进化长期以来被视为通向超智能的途径，但实践中无引导的自演化系统往往很快达到平台期甚至退化，这源于概念漂移、多样性崩溃和错误进化等问题，模型会强化自身偏见并收敛到低熵行为。

Method: 提出R-Few框架，包含引导式自博弈的Challenger-Solver结构：Challenger通过少量人类标注示例指导合成问题生成，Solver在在线难度课程下联合训练人类和合成示例，结合上下文基础和混合训练实现轻量级人类监督。

Result: 在数学和通用推理基准测试中，R-Few实现了一致且迭代的改进。例如，Qwen3-8B-Base在数学任务上比R-Zero提高了+3.0分，性能与General-Reasoner相当，尽管后者使用了20倍的人类数据。消融研究证实了基础Challenger训练和课程Solver训练的互补贡献。

Conclusion: R-Few通过引导式自演化框架缓解了概念漂移问题，产生了更稳定和可控的协同进化动态，为AI自我进化提供了有效的解决方案。

Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

</details>


### [49] [COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes](https://arxiv.org/abs/2512.02499)
*Yongkai Liu,Helena Feng,Bin Jiang,Yixin Wang,Max Wintermark,David S. Liebeskind,Michael Moseley,Maarten Lansberg,Gregory Albers,Jeremy Heit,Greg Zaharchuk*

Main category: cs.AI

TL;DR: 开发了基于思维链推理的COPE框架，使用LLaMA-3-8B模型从非结构化临床笔记预测急性缺血性卒中90天功能结局，性能优于传统方法，与GPT-4.1相当。


<details>
  <summary>Details</summary>
Motivation: 急性缺血性卒中预后预测对临床决策、患者咨询和资源分配至关重要。临床笔记包含丰富的上下文信息，但其非结构化特性限制了在传统预测模型中的应用，需要开发能够利用这些信息的预测工具。

Method: 开发了COPE框架，基于两步思维链推理：第一步生成临床推理，第二步输出改良Rankin量表预测。使用464名患者的出院小结和90天mRS评分，比较了COPE与GPT-4.1、ClinicalBERT、结构化变量机器学习模型以及无思维链的单步LLM的性能。

Result: COPE的MAE为1.01（95% CI 0.92-1.11），±1准确率为74.4%，精确准确率为32.8%，性能与GPT-4.1相当，优于ClinicalBERT（MAE 1.24）、临床机器学习模型（MAE 1.28）和无思维链的单步LLM（MAE 1.20）。在性别和年龄亚组中表现一致，在老年患者、接受血栓切除术患者和较长摘要患者中误差略高。

Conclusion: COPE作为一个轻量级、可解释且保护隐私的开源框架，为从非结构化临床文本进行预后预测提供了准确实用的解决方案，展示了思维链推理在医疗预测任务中的价值。

Abstract: Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.

</details>


### [50] [Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration](https://arxiv.org/abs/2512.02530)
*Yuxiang He,Jian Zhao,Yuchen Yuan,Tianle Zhang,Wei Cai,Haojie Cheng,Ziyan Shi,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: Aetheria是一个基于多智能体辩论与协作的多模态可解释内容安全框架，通过动态辩论机制和RAG知识检索，显著提升隐式风险识别能力并生成可追溯的审计报告。


<details>
  <summary>Details</summary>
Motivation: 数字内容爆炸式增长给内容安全带来挑战，现有基于单一模型或固定流程的审核系统在识别隐式风险和提供可解释判断过程方面存在局限。

Method: 提出Aetheria框架，采用五个核心智能体的协作架构，通过基于RAG知识检索的动态相互说服辩论机制，对多模态内容进行深度分析和裁决。

Result: 在提出的基准测试AIR-Bench上的综合实验验证，Aetheria不仅能生成详细可追溯的审计报告，而且在整体内容安全准确性上显著优于基线方法，特别是在隐式风险识别方面。

Conclusion: 该框架建立了透明可解释的范式，显著推进了可信AI内容审核领域的发展。

Abstract: The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.

</details>


### [51] [PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing](https://arxiv.org/abs/2512.02589)
*Junyi Hou,Andre Lin Huikai,Nuo Chen,Yiwei Gong,Bingsheng He*

Main category: cs.AI

TL;DR: PaperDebugger是一个集成在LaTeX编辑器中的多智能体学术写作助手，通过Chrome扩展、Kubernetes编排层和MCP工具链实现深度文档交互


<details>
  <summary>Details</summary>
Motivation: 现有AI写作助手与编辑器分离，无法深度访问文档状态、结构和修订历史，限制了在LaTeX编辑器（如Overleaf）中进行上下文感知的智能操作

Method: 开发Chrome批准的扩展程序，采用Kubernetes原生编排层和模型上下文协议（MCP）工具链，实现可靠的双向编辑器同步、细粒度版本控制、安全状态管理、多智能体调度和外部工具集成

Result: 实现了完全集成的工作流，包括本地化编辑、结构化评审、并行智能体执行和基于差异的更新，早期聚合分析显示用户积极参与

Conclusion: PaperDebugger验证了编辑器原生、智能体驱动的写作助手的实用性，为学术写作提供了深度集成的AI辅助工具

Abstract: Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.

</details>


### [52] [Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction](https://arxiv.org/abs/2512.02610)
*Yubo Hou,Mohamed Ragab,Min Wu,Chee-Keong Kwoh,Xiaoli Li,Zhenghua Chen*

Main category: cs.AI

TL;DR: 提出TACDA方法解决跨域RUL预测中的领域差异问题，通过目标域重建策略保留目标特定信息，并采用聚类配对策略实现相似退化阶段的一致性对齐。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的RUL预测方法通常假设训练和测试数据来自同一分布，但在实际工业环境中这一假设不成立。现有的对抗性领域自适应方法虽然关注领域不变特征，但忽略了目标特定信息和退化阶段的一致性特征，导致性能不理想。

Method: 提出TACDA方法：1）在对抗性自适应过程中引入目标域重建策略，在学习领域不变特征的同时保留目标特定信息；2）开发新颖的聚类和配对策略，实现相似退化阶段之间的一致性对齐。

Result: 通过大量实验证明，TACDA方法在两个不同的评估指标上都显著超越了现有最先进方法，表现出卓越的性能。

Conclusion: TACDA方法有效解决了跨域RUL预测中的领域差异问题，通过同时考虑领域不变特征、目标特定信息和退化阶段一致性，显著提升了预测性能，为实际工业应用提供了更可靠的解决方案。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.

</details>


### [53] [Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks](https://arxiv.org/abs/2512.02677)
*Zhiyuan He*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在递归推理问题上的深度泛化能力不足问题，并提出了一种循环定位替换管道方法来改善这一局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多任务上表现出色，但在处理需要解决嵌套层次结构的递归推理问题时面临显著挑战。虽然先前研究广泛探讨了长度泛化（模型处理比训练时更长序列的能力），但本文研究了一个独特且未被充分探索的局限性：深度泛化。这里的深度指的是层次问题中的嵌套层数，如数学表达式中的括号层数或布尔公式中逻辑子句的嵌套。

Method: 为了解决这一挑战，作者开发了一种新颖的循环定位替换管道，将递归问题分解为可管理的子组件。该方法采用两个专门模型：定位器识别可解的子表达式，替换器评估这些组件同时保持整体结构。

Result: 在三个精心设计的领域（布尔代数、递归算术和命题逻辑）进行评估，每个领域都有可控的递归深度。结果表明，该方法在测试超出分布范围的递归深度时，有效地缓解了性能衰减。

Conclusion: 标准Transformer架构在处理比训练时更深的递归问题时存在困难，这源于它们无法维持类似栈的行为来跟踪和解决多级嵌套依赖。通过系统分析，作者展示了这种架构约束如何导致随着递归深度增加而性能迅速衰减。提出的循环定位替换管道方法为解决这一深度泛化问题提供了有效途径。

Abstract: Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.

</details>


### [54] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha,Jeong Hun Yeo,Junil Won,Se Jin Park,Yong Man Ro*

Main category: cs.AI

TL;DR: MIGR框架通过模态重要性引导推理，改善多模态大语言模型的情感理解可靠性，减少推理漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有方法存在推理漂移问题：模型逐渐依赖自身生成的文本而非多模态证据，且解释过度受视觉主导推理路径影响

Method: 引入模态重要性机制识别情感主导模态，重组推理序列从最关键模态开始；采用两阶段框架：模态对齐监督微调和模态感知奖励优化

Result: 在DFEW基准测试中，MIGR显著提升推理可靠性，将正确预测但情感不一致解释的比例从18.10%降至7.37%

Conclusion: 从情感主导模态开始推理能有效提高多模态情感理解的可靠性，减少推理漂移问题

Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [55] [Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs](https://arxiv.org/abs/2512.02713)
*Theodoros Aivalis,Iraklis A. Klampanos,Antonis Troumpoukis,Joemon M. Jose*

Main category: cs.AI

TL;DR: 提出基于知识图谱的生成模型溯源框架，通过对比生成图像与训练图像的结构化知识图谱来分析数据影响，解决生成AI的透明度、版权和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能力增强，透明度、责任归属和版权侵权问题日益突出。需要理解特定训练数据如何影响模型输出，以支持版权分析、数据集透明度和可解释AI。

Method: 利用多模态大语言模型从图像中提取结构化三元组，构建与领域本体对齐的知识图谱。通过比较生成图像与训练图像的KG，追踪潜在的数据影响。

Result: 通过局部训练模型的遗忘实验和大规模模型的风格特定实验验证了方法的有效性。框架支持开发促进人机协作、创造力和激发好奇心的AI系统。

Conclusion: 提出的知识图谱框架为生成模型提供了可追溯的数据影响分析工具，有助于解决透明度、版权和可解释性问题，推动负责任AI的发展。

Abstract: As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.

</details>


### [56] [Menta: A Small Language Model for On-Device Mental Health Prediction](https://arxiv.org/abs/2512.02716)
*Tianyi Zhang,Xiangyuan Xue,Lingyan Ruan,Shiya Fu,Feng Xia,Simon D'Alfonso,Vassilis Kostakos,Hong Jia*

Main category: cs.AI

TL;DR: Menta是首个针对社交媒体多任务心理健康预测优化的轻量级语言模型，在保持小体积的同时性能优于大型模型，并能在移动设备上实时部署。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题影响全球数亿人，但早期检测仍然有限。大型语言模型在心理健康应用中有潜力，但计算需求大难以实际部署。小型语言模型提供了轻量级替代方案，但在社交媒体心理健康预测方面尚未充分探索。

Method: 提出Menta模型，使用LoRA框架、跨数据集策略和平衡准确率导向的损失函数，针对六个分类任务进行联合训练，专门用于社交媒体数据的心理健康预测。

Result: Menta在抑郁、压力和自杀倾向等任务上比最佳非微调小型模型平均提升15.2%，在抑郁和压力分类任务上准确率优于130亿参数的大型模型，体积小约3.25倍。可在iPhone 15 Pro Max上实时部署，仅需约3GB内存。

Conclusion: Menta展示了小型语言模型在可扩展、隐私保护的心理健康监测方面的潜力，为实际部署提供了可行的轻量级解决方案。

Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/

</details>


### [57] [StockMem: An Event-Reflection Memory Framework for Stock Forecasting](https://arxiv.org/abs/2512.02720)
*He Wang,Wenyilin Xiao,Songqiao Han,Hailiang Huang*

Main category: cs.AI

TL;DR: StockMem：基于事件-反思双层记忆框架的股价预测模型，通过结构化新闻事件和挖掘增量信息来提升预测准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 股价预测面临市场波动性和实时事件敏感性的挑战。虽然大语言模型为基于文本的预测提供了新途径，但在金融领域的应用受到噪声新闻数据和文本中缺乏明确答案的限制。通用记忆架构难以识别价格变动的关键驱动因素。

Method: 提出StockMem事件-反思双层记忆框架：1）将新闻结构化为事件；2）水平整合整合每日事件；3）纵向跟踪捕捉事件演化以提取反映市场预期差异的增量信息，构建时间事件知识库；4）通过分析事件-价格动态形成因果经验的反思知识库；5）预测时检索类似历史场景，结合当前事件、增量数据和过去经验进行推理。

Result: 实验表明StockMem优于现有记忆架构，提供更优且可解释的推理，能够追踪影响价格的信息链，增强金融预测决策的透明度。

Conclusion: StockMem通过事件结构化和增量信息挖掘，有效解决了金融新闻数据噪声问题，提供了更准确、可解释的股价预测方法，增强了金融预测的决策透明度。

Abstract: Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.

</details>


### [58] [AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping](https://arxiv.org/abs/2512.02726)
*Md Abdul Kadir,Sai Suresh Macharla Vasu,Sidharth S. Nair,Daniel Sonntag*

Main category: cs.AI

TL;DR: LLMs作为异常检测器在复式记账中表现优于传统规则方法和机器学习基线，提供自然语言解释，支持AI增强审计


<details>
  <summary>Details</summary>
Motivation: 传统日记账测试(JETs)基于规则的方法产生大量误报且难以检测细微异常，需要更有效的审计异常检测方法

Method: 在合成和真实匿名账本上对LLaMA、Gemma等最先进LLMs进行基准测试，与传统JETs和机器学习基线方法比较

Result: LLMs在异常检测方面持续优于传统规则型JETs和经典机器学习基线，同时提供增强可解释性的自然语言解释

Conclusion: LLMs展示了AI增强审计的潜力，人类审计师与基础模型协作可加强财务完整性

Abstract: Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.

</details>


### [59] [Self-Improving AI Agents through Self-Play](https://arxiv.org/abs/2512.02731)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 本文将心理测量电池的模数理论框架扩展到动力系统领域，将智能体形式化为由计算资源参数化的流，并推导出保证自我改进稳定性的方差不等式条件。


<details>
  <summary>Details</summary>
Motivation: 先前研究建立了AAI能力分数作为智能体表示空间上的静态泛函，但缺乏对智能体作为动态系统的形式化描述。本文旨在将智能体建模为由计算资源参数化的动力系统，以统一理解各种自我改进方法。

Method: 提出生成器-验证器-更新器（GVU）算子，证明该算子在参数流形Θ上生成向量场，将自我改进系数κ定义为能力泛函沿该流的李导数。推导出方差不等式这一谱条件作为自我改进稳定性的充分条件。

Result: 证明κ>0的充分条件是生成和验证的组合噪声足够小（在曲率和步长效应范围内）。将GVU算子应用于统一语言自我博弈、自我纠正和合成数据引导等最新文献，展示STaR、SPIN、Reflexion、GANs和AlphaZero等架构都是满足方差不等式的GVU算子的具体拓扑实现。

Conclusion: 本文建立了智能体自我改进的动力学理论框架，通过GVU算子和方差不等式为各种自我改进方法提供了统一的理论基础，揭示了这些方法通过过滤、对抗判别或形式系统基础来满足稳定性条件的共同机制。

Abstract: We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.

</details>


### [60] [A Framework for Causal Concept-based Model Explanations](https://arxiv.org/abs/2512.02735)
*Anna Rodum Bjøru,Jacob Lysnæs-Larsen,Oskar Jørgensen,Inga Strümke,Helge Langseth*

Main category: cs.AI

TL;DR: 提出基于因果概念的后解释AI框架，通过概念干预的充分概率生成局部和全局解释，在CelebA数据集上验证了可理解性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 针对非可解释模型的解释需求，需要既易于理解又忠实于原模型的解释方法。现有方法在概念理解和模型忠实性方面存在不足。

Method: 提出因果概念后解释框架，通过计算概念干预的充分概率来生成局部和全局解释。使用概念词汇表构建解释，并在CelebA数据集上建立概念验证模型。

Result: 展示了基于CelebA数据集分类器的解释示例，证明了框架的可理解性（通过清晰的概念词汇）和忠实性（通过强调解释生成与解释背景的一致性）。

Conclusion: 该因果概念后解释框架为黑盒模型提供了既易于理解又忠实于模型的解释方法，强调了解释生成背景与解释解释背景对齐的重要性。

Abstract: This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.

</details>


### [61] [Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control](https://arxiv.org/abs/2512.02814)
*Yongrui Yu,Zhongzhen Huang,Linjie Mu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: Radiologist Copilot是一个基于大语言模型的智能助手系统，通过编排多种工具实现自动化放射学报告生成与质量控制，模拟放射科医生完整工作流程，显著提升报告质量和效率。


<details>
  <summary>Details</summary>
Motivation: 放射学报告撰写耗时且易出错，现有自动化方法主要关注报告生成阶段，忽视了关键的质量控制环节，无法为放射科医生提供全面支持。需要开发一个能够模拟放射科医生完整工作流程的智能助手系统。

Method: 使用大语言模型作为推理核心，构建一个能够自主选择工具、规划和执行动作的智能体系统。系统包含区域定位、基于"think with image"范式的区域分析规划、策略性模板选择、质量评估以及反馈驱动的自适应优化等编排工具。

Result: 实验结果表明，Radiologist Copilot在放射学报告生成方面显著优于其他最先进方法，能够实现准确、完整且高效的放射学报告撰写。

Conclusion: Radiologist Copilot通过模拟放射科医生的完整工作流程，包括报告生成和质量控制两个关键环节，为放射科医生提供了全面的智能辅助，有望提高临床工作效率和报告质量。

Abstract: Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.

</details>


### [62] [The future of AI in critical mineral exploration](https://arxiv.org/abs/2512.02879)
*Jef Caers*

Main category: cs.AI

TL;DR: 提出基于贝叶斯主义和证伪原则的新科学方法，利用AI减少认知偏差和假阳性，降低勘探成本


<details>
  <summary>Details</summary>
Motivation: 尽管投资增加，但过去二十年新矿产发现减少，需要解决关键矿产勘探的挑战

Method: 基于贝叶斯主义和证伪原则的哲学方法，将数据采集视为证伪人类假设的手段，使用可验证指标量化决策，结合无监督学习和人机协同AI

Result: 提供实用的勘探协议模板，通过AI辅助减少认知偏差，优化地质、地球物理、地球化学和钻探数据采集规划

Conclusion: AI是实现矿产勘探科学方法的关键使能技术，能有效降低勘探成本并提高发现效率

Abstract: The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage

</details>


### [63] [Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning](https://arxiv.org/abs/2512.02914)
*Zhonghao He,Tianyi Qiu,Hirokazu Shirado,Maarten Sap*

Main category: cs.AI

TL;DR: 该研究提出基于鞅属性的无监督评估框架，用于检测LLM推理中的信念固化现象，发现当前信念能正向预测未来信念更新，表明存在确认偏误而非理性贝叶斯更新。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM推理能力有所提升，但迭代推理可能强化信念固化和确认偏误，而非促进真相寻求行为。需要系统评估LLM推理中的信念固化现象。

Method: 提出基于贝叶斯统计中鞅属性的无监督回归评估框架——鞅分数，用于衡量违反理性信念更新的程度。在事件预测、价值负载问题和学术论文评审等开放领域进行系统评估。

Result: 发现信念固化现象在各类模型和设置中普遍存在，当前信念能正向预测未来信念更新。识别了更容易出现信念固化的模型、推理技术和领域。验证了鞅分数在有真值标签的领域能预测准确率。

Conclusion: 鞅分数作为无监督指标，即使在缺乏真值标签的领域也能有效评估推理过程的真相寻求能力，揭示了LLM推理中普遍存在的信念固化问题。

Abstract: Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [How Market Volatility Shapes Algorithmic Collusion: A Comparative Analysis of Learning-Based Pricing Algorithms](https://arxiv.org/abs/2512.02134)
*Aheer Sravon,Md. Ibrahim,Devdyuti Mazumder,Ridwan Al Aziz*

Main category: cs.LG

TL;DR: 研究分析了四种定价算法在三种双寡头市场模型和不同需求冲击下的竞争行为，发现强化学习算法在稳定需求下维持超竞争价格，DDPG最具合谋倾向，需求冲击对不同市场结构影响各异但算法相对排名稳定。


<details>
  <summary>Details</summary>
Motivation: 随着自主定价算法在数字市场竞争中的影响力日益增强，但它们在现实需求条件下的行为尚未得到充分研究。本文旨在填补这一空白，深入分析定价算法在不同市场结构和需求冲击下的竞争行为。

Method: 研究采用四种定价算法（Q-Learning、PSO、Double DQN、DDPG），在三种经典双寡头模型（Logit、Hotelling、Linear）中进行测试，并通过自回归过程创建各种需求冲击机制。使用基于利润和价格的合谋指数来分析算法交互、市场结构和随机需求对竞争结果的影响。

Result: 研究发现：1）强化学习算法在稳定需求下经常维持超竞争价格，其中DDPG表现出最明显的合谋倾向；2）需求冲击对不同市场结构产生显著不同影响：Logit市场性能大幅下降，Hotelling市场保持稳定，Linear市场出现冲击引发的利润膨胀；3）尽管绝对性能变化明显，但算法在不同环境中的相对排名保持一致。

Conclusion: 研究结果表明市场结构和需求不确定性在塑造算法竞争中具有关键重要性，这些发现为围绕自主定价行为的政策讨论提供了重要参考，强调了在评估算法竞争时需要同时考虑算法特性、市场结构和需求条件。

Abstract: Autonomous pricing algorithms are increasingly influencing competition in digital markets; however, their behavior under realistic demand conditions remains largely unexamined. This paper offers a thorough analysis of four pricing algorithms -- Q-Learning, PSO, Double DQN, and DDPG -- across three classic duopoly models (Logit, Hotelling, Linear) and under various demand-shock regimes created by auto-regressive processes. By utilizing profit- and price-based collusion indices, we investigate how the interactions among algorithms, market structure, and stochastic demand collaboratively influence competitive outcomes. Our findings reveal that reinforcement-learning algorithms often sustain supra-competitive prices under stable demand, with DDPG demonstrating the most pronounced collusive tendencies. Demand shocks produce notably varied effects: Logit markets suffer significant performance declines, Hotelling markets remain stable, and Linear markets experience shock-induced profit inflation. Despite marked changes in absolute performance, the relative rankings of the algorithms are consistent across different environments. These results underscore the critical importance of market structure and demand uncertainty in shaping algorithmic competition, while also contributing to the evolving policy discussions surrounding autonomous pricing behavior.

</details>


### [65] [Contextual Gating within the Transformer Stack: Synergistic Feature Modulation for Enhanced Lyrical Classification and Calibration](https://arxiv.org/abs/2512.02053)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 提出SFL Transformer模型，通过上下文门控机制在BERT编码器中间层融合辅助结构特征，显著提升歌词内容分类性能


<details>
  <summary>Details</summary>
Motivation: 探索在预训练Transformer中更有效地融合辅助结构特征的方法，以提升歌词内容分类任务的性能

Method: 提出SFL Transformer模型，使用上下文门控机制（Intermediate SFL）在BERT编码器堆栈中间层调制隐藏状态序列，将低维结构特征与深度语义特征融合

Result: 在基于UMAP降维的歌词嵌入二元分类任务中，SFL Transformer达到0.9910的准确率和宏F1分数，优于先前SFL模型的0.9894，且具有低校准误差（ECE=0.0081）和低对数损失（0.0489）

Conclusion: 在编码器中间层注入辅助上下文是融合结构和语义信息的最有效方法，既能提升判别能力，又能获得高质量的概率估计

Abstract: This study introduces a significant architectural advancement in feature fusion for lyrical content classification by integrating auxiliary structural features directly into the self-attention mechanism of a pre-trained Transformer. I propose the SFL Transformer, a novel deep learning model that utilizes a Contextual Gating mechanism (an Intermediate SFL) to modulate the sequence of hidden states within the BERT encoder stack, rather than fusing features at the final output layer. This approach modulates the deep, contextualized semantic features (Hseq) using low-dimensional structural cues (Fstruct). The model is applied to a challenging binary classification task derived from UMAP-reduced lyrical embeddings. The SFL Transformer achieved an Accuracy of 0.9910 and a Macro F1 score of 0.9910, significantly improving the state-of-the-art established by the previously published SFL model (Accuracy 0.9894). Crucially, this Contextual Gating strategy maintained exceptional reliability, with a low Expected Calibration Error (ECE = 0.0081) and Log Loss (0.0489). This work validates the hypothesis that injecting auxiliary context mid-stack is the most effective means of synergistically combining structural and semantic information, creating a model with both superior discriminative power and high-fidelity probability estimates.

</details>


### [66] [Opening the Black Box: An Explainable, Few-shot AI4E Framework Informed by Physics and Expert Knowledge for Materials Engineering](https://arxiv.org/abs/2512.02057)
*Haoxiang Zhang,Ruihao Yuan,Lihui Zhang,Yushi Luo,Qiang Zhang,Pan Ding,Xiaodong Ren,Weijie Xing,Niu Gao,Jishan Chen,Chubo Zhang*

Main category: cs.LG

TL;DR: 提出一个基于物理和专家知识的可解释少样本AI4E框架，仅用32个实验样本，通过物理增强合成数据和嵌套优化策略，在航空K439B高温合金修复焊接中实现88%准确度的热裂倾向预测。


<details>
  <summary>Details</summary>
Motivation: 工业AI4E面临两个瓶颈：高质量数据稀缺和黑盒模型缺乏可解释性，这在航空航天等安全敏感领域尤为关键。需要开发可信赖的AI系统，将工程领域知识直接嵌入架构中。

Method: 1) 通过三阶段协议增强物理合理的合成数据：校准过程变异性的差异化噪声注入、硬物理约束强制执行、参数间关系保持；2) 采用嵌套优化策略进行本构模型发现：符号回归探索方程结构，差分进化优化参数，然后使用混合全局-局部优化进行参数精炼。

Result: 得到的可解释本构方程在预测热裂倾向方面达到88%的准确度。该方程不仅提供定量预测，还提供明确的物理洞察，揭示热、几何和冶金机制如何耦合驱动裂纹，从而增进工程师对过程的认知理解。

Conclusion: 该方法为开发可信赖AI系统提供了通用蓝图，将工程领域知识直接嵌入架构，使AI能在高风险工业应用中可靠采用，特别是在数据有限但物理理解可用的场景下。本构方程还可作为多功能工具用于工艺优化和高保真虚拟数据生成。

Abstract: The industrial adoption of Artificial Intelligence for Engineering (AI4E) faces two fundamental bottlenecks: scarce high-quality data and the lack of interpretability in black-box models-particularly critical in safety-sensitive sectors like aerospace. We present an explainable, few-shot AI4E framework that is systematically informed by physics and expert knowledge throughout its architecture. Starting from only 32 experimental samples in an aerial K439B superalloy castings repair welding case, we first augment physically plausible synthetic data through a three-stage protocol: differentiated noise injection calibrated to process variabilities, enforcement of hard physical constraints, and preservation of inter-parameter relationships. We then employ a nested optimization strategy for constitutive model discovery, where symbolic regression explores equation structures while differential evolution optimizes parameters, followed by intensive parameter refinement using hybrid global-local optimization. The resulting interpretable constitutive equation achieves 88% accuracy in predicting hot-cracking tendency. This equation not only provides quantitative predictions but also delivers explicit physical insight, revealing how thermal, geometric, and metallurgical mechanisms couple to drive cracking-thereby advancing engineers' cognitive understanding of the process. Furthermore, the constitutive equation serves as a multi-functional tool for process optimization and high-fidelity virtual data generation, enabling accuracy improvements in other data-driven models. Our approach provides a general blueprint for developing trustworthy AI systems that embed engineering domain knowledge directly into their architecture, enabling reliable adoption in high-stakes industrial applications where data is limited but physical understanding is available.

</details>


### [67] [Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting](https://arxiv.org/abs/2512.02061)
*Zhenliang Ni,Xiaowen Ma,Zhenkai Wu,Shuai Xiao,Han Shu,Xinghao Chen*

Main category: cs.LG

TL;DR: Ada-MoGE：自适应高斯专家混合模型，通过频谱强度和频率响应自适应确定专家数量，解决传统MoE模型在多元时间序列预测中频率覆盖不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，时间序列的主导频率会随着数据频谱分布的变化而漂移。传统固定专家数量的MoE模型难以适应这种变化，导致频率覆盖不平衡问题：专家过少会忽略关键信息，专家过多会引入噪声。

Method: 提出Ada-MoGE模型，集成频谱强度和频率响应来自适应确定专家数量，确保与输入数据的频率分布对齐。采用高斯带通滤波器平滑分解频域特征，避免直接频带截断引入噪声。

Result: 在6个公共基准测试中达到最先进性能，仅使用20万参数。

Conclusion: Ada-MoGE通过自适应确定专家数量，有效解决了传统MoE模型的频率覆盖不平衡问题，在多元时间序列预测中取得了优异性能。

Abstract: Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.

</details>


### [68] [DPWMixer: Dual-Path Wavelet Mixer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.02070)
*Li Qianyang,Zhang Xingjun,Wang Shaoxun,Wei Jia*

Main category: cs.LG

TL;DR: DPWMixer：一种基于无损Haar小波金字塔和双路径架构的高效长时序预测模型，通过正交分解分离趋势与局部波动，结合全局线性映射和局部MLP-Mixer处理，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在明显缺陷：Transformer模型计算复杂度高且容易过拟合；线性模型难以捕捉复杂非线性动态；多尺度框架使用的平均池化会导致频谱混叠和高频信息丢失。需要一种既能高效处理长序列又能保留重要动态信息的方法。

Method: 提出DPWMixer双路径架构：1）使用无损Haar小波金字塔替代传统池化，通过正交分解无信息损失地分离趋势和局部波动；2）设计双路径趋势混合器，包含全局线性映射用于宏观趋势锚定和基于patch的MLP-Mixer用于微观动态演化；3）自适应多尺度融合模块根据通道平稳性加权整合不同尺度的预测结果。

Result: 在8个公开基准数据集上进行广泛实验，DPWMixer相比现有最先进基线模型取得了持续一致的性能提升，证明了方法的有效性。

Conclusion: DPWMixer通过无损小波分解和双路径处理架构，有效解决了长时序预测中的计算效率、信息保留和动态建模问题，为长时序预测任务提供了一个高效且性能优越的解决方案。

Abstract: Long-term time series forecasting (LTSF) is a critical task in computational intelligence. While Transformer-based models effectively capture long-range dependencies, they often suffer from quadratic complexity and overfitting due to data sparsity. Conversely, efficient linear models struggle to depict complex non-linear local dynamics. Furthermore, existing multi-scale frameworks typically rely on average pooling, which acts as a non-ideal low-pass filter, leading to spectral aliasing and the irreversible loss of high-frequency transients. In response, this paper proposes DPWMixer, a computationally efficient Dual-Path architecture. The framework is built upon a Lossless Haar Wavelet Pyramid that replaces traditional pooling, utilizing orthogonal decomposition to explicitly disentangle trends and local fluctuations without information loss. To process these components, we design a Dual-Path Trend Mixer that integrates a global linear mapping for macro-trend anchoring and a flexible patch-based MLP-Mixer for micro-dynamic evolution. Finally, An adaptive multi-scale fusion module then integrates predictions from diverse scales, weighted by channel stationarity to optimize synthesis. Extensive experiments on eight public benchmarks demonstrate that our method achieves a consistent improvement over state-of-the-art baselines. The code is available at https://github.com/hit636/DPWMixer.

</details>


### [69] [HTG-GCL: Leveraging Hierarchical Topological Granularity from Cellular Complexes for Graph Contrastive Learning](https://arxiv.org/abs/2512.02073)
*Qirui Ji,Bin Qin,Yifan Jin,Yunze Zhao,Chuxiong Sun,Changwen Zheng,Jianwen Cao,Jiangmeng Li*

Main category: cs.LG

TL;DR: HTG-GCL是一种新的图对比学习框架，通过生成多尺度环状细胞复合体来创建层次化拓扑粒度视图，解决现有方法难以识别任务相关拓扑结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法在结构增强方面存在局限性：难以识别任务相关的拓扑结构，且无法适应不同下游任务所需的从粗到细的拓扑粒度变化。

Method: 提出HTG-GCL框架：1）通过同一图的变换生成多尺度环状细胞复合体，体现拓扑粒度概念，创建多样化拓扑视图；2）采用多粒度解耦对比，基于不确定性估计应用粒度特定加权机制。

Result: 在多个基准测试上的综合实验表明HTG-GCL的有效性，突显了其通过层次化拓扑信息捕获有意义图表示的优越性能。

Conclusion: HTG-GCL通过引入层次化拓扑粒度对比学习，成功解决了现有GCL方法在识别任务相关拓扑结构和适应不同粒度需求方面的局限性，为图表示学习提供了新的有效框架。

Abstract: Graph contrastive learning (GCL) aims to learn discriminative semantic invariance by contrasting different views of the same graph that share critical topological patterns. However, existing GCL approaches with structural augmentations often struggle to identify task-relevant topological structures, let alone adapt to the varying coarse-to-fine topological granularities required across different downstream tasks. To remedy this issue, we introduce Hierarchical Topological Granularity Graph Contrastive Learning (HTG-GCL), a novel framework that leverages transformations of the same graph to generate multi-scale ring-based cellular complexes, embodying the concept of topological granularity, thereby generating diverse topological views. Recognizing that a certain granularity may contain misleading semantics, we propose a multi-granularity decoupled contrast and apply a granularity-specific weighting mechanism based on uncertainty estimation. Comprehensive experiments on various benchmarks demonstrate the effectiveness of HTG-GCL, highlighting its superior performance in capturing meaningful graph representations through hierarchical topological information.

</details>


### [70] [FDRMFL:Multi-modal Federated Feature Extraction Model Based on Information Maximization and Contrastive Learning](https://arxiv.org/abs/2512.02076)
*Haozhe Wu*

Main category: cs.LG

TL;DR: 提出一种任务驱动的监督式多模态联邦特征提取方法，解决多模态数据回归中的特征提取问题，应对有限非IID数据、多模态信息融合和灾难性遗忘三大挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据回归中的特征提取问题，特别是应对现实场景中的三大核心挑战：有限且非独立同分布的数据、多模态信息的有效提取与融合、模型学习中的灾难性遗忘问题。

Method: 提出任务驱动的监督式多模态联邦特征提取方法，集成多模态信息提取和对比学习机制，支持客户端独立学习多模态数据的低维表示。通过多约束学习框架保证回归精度，包括互信息保持约束、对称KL散度约束和模型间对比约束，实现任务相关信息的保留、多模态特征的提取融合对齐，以及缓解非IID场景下的表示漂移和灾难性遗忘。

Result: 实验结果表明，与经典特征提取技术相比，该方法在下游回归任务上取得了更显著的性能提升，验证了其在模拟和真实数据分析中的有效性。

Conclusion: 该方法能够有效解决多模态数据回归中的特征提取问题，通过多约束学习框架确保特征提取过程始终以提高下游回归任务性能为中心，在非IID场景下具有良好的适应性和鲁棒性。

Abstract: This study focuses on the feature extraction problem in multi-modal data regression. To address three core challenges in real-world scenarios: limited and non-IID data, effective extraction and fusion of multi-modal information, and susceptibility to catastrophic forgetting in model learning, a task-driven supervised multi-modal federated feature extraction method is proposed. The method integrates multi-modal information extraction and contrastive learning mechanisms, and can adapt to different neural network structures as the latent mapping functions for data of each modality. It supports each client to independently learn low-dimensional representations of multi-modal data, and can flexibly control the degree of retention of effective information about the response variable in the predictive variables within the low-dimensional features through parameter tuning. The multi-constraint learning framework constructed by the method guarantees regression accuracy using Mean Squared Error loss. Through the synergistic effect of mutual information preservation constraint, symmetric Kullback-Leibler divergence constraint, and inter-model contrastive constraint, it achieves the retention of task-related information, the extraction, fusion, and alignment of multi-modal features, and the mitigation of representation drift and catastrophic forgetting in non-IID scenarios, respectively. This ensures that the feature extraction process always centers on improving the performance of downstream regression tasks. Experimental results from simulations and real-world data analysis demonstrate that the proposed method achieves more significant performance improvement on downstream regression tasks compared with classical feature extraction techniques.

</details>


### [71] [CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models](https://arxiv.org/abs/2512.02180)
*Yuxuan Shu,Peter H. Charlton,Fahim Kawsar,Jussi Hernesniemi,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: CLEF是一种新型对比学习框架，利用临床风险评分自适应加权负样本对，在单导联ECG分析中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有ECG基础模型的自监督预训练方法未充分利用临床元数据的领域知识，限制了诊断性能的提升潜力

Method: 提出临床引导对比学习方法，使用临床风险评分自适应加权负样本对，将ECG嵌入相似性与临床有意义的差异对齐，并处理缺失元数据

Result: 在MIMIC-IV数据集上预训练三个规模的CLEF模型，在18个临床任务上评估，相比基线模型在分类任务AUROC提升至少2.6%，回归任务MAE降低至少3.2%

Conclusion: CLEF实现了更准确和可扩展的单导联ECG分析，推动了远程健康监测的发展，仅使用单导联数据即可达到监督学习方法ECGFounder的性能水平

Abstract: The electrocardiogram (ECG) is a key diagnostic tool in cardiovascular health. Single-lead ECG recording is integrated into both clinical-grade and consumer wearables. While self-supervised pretraining of foundation models on unlabeled ECGs improves diagnostic performance, existing approaches do not incorporate domain knowledge from clinical metadata. We introduce a novel contrastive learning approach that utilizes an established clinical risk score to adaptively weight negative pairs: clinically-guided contrastive learning. It aligns the similarities of ECG embeddings with clinically meaningful differences between subjects, with an explicit mechanism to handle missing metadata. On 12-lead ECGs from 161K patients in the MIMIC-IV dataset, we pretrain single-lead ECG foundation models at three scales, collectively called CLEF, using only routinely collected metadata without requiring per-sample ECG annotations. We evaluate CLEF on 18 clinical classification and regression tasks across 7 held-out datasets, and benchmark against 5 foundation model baselines and 3 self-supervised algorithms. When pretrained on 12-lead ECG data and tested on lead-I data, CLEF outperforms self-supervised foundation model baselines: the medium-sized CLEF achieves average AUROC improvements of at least 2.6% in classification and average reductions in MAEs of at least 3.2% in regression. Comparing with existing self-supervised learning algorithms, CLEF improves the average AUROC by at least 1.8%. Moreover, when pretrained only on lead-I data for classification tasks, CLEF performs comparably to the state-of-the-art ECGFounder, which was trained in a supervised manner. Overall, CLEF enables more accurate and scalable single-lead ECG analysis, advancing remote health monitoring. Code and pretrained CLEF models are available at: github.com/Nokia-Bell-Labs/ecg-foundation-model.

</details>


### [72] [WhAM: Towards A Translative Model of Sperm Whale Vocalization](https://arxiv.org/abs/2512.02206)
*Orr Paradise,Pranav Muralikrishnan,Liangyuan Chen,Hugo Flores García,Bryan Pardo,Roee Diamant,David F. Gruber,Shane Gero,Shafi Goldwasser*

Main category: cs.LG

TL;DR: WhAM是基于Transformer的模型，能够从任意音频提示生成合成抹香鲸click序列（codas），通过微调预训练的VampNet模型实现


<details>
  <summary>Details</summary>
Motivation: 抹香鲸通过click序列（codas）进行交流，需要开发能够生成高质量合成codas的模型来研究鲸鱼通信

Method: 通过微调预训练的VampNet（基于掩码声学token的模型），使用过去20年收集的10k个coda录音进行训练，通过迭代掩码token预测生成合成codas

Result: WhAM生成高保真合成codas，保留源录音的关键声学特征；在节奏、社会单元和元音分类等下游任务中，学习到的表征表现出色

Conclusion: WhAM是首个基于Transformer的抹香鲸coda生成模型，能够生成高质量的合成codas，其学习到的表征在分类任务中也表现良好

Abstract: Sperm whales communicate in short sequences of clicks known as codas. We present WhAM (Whale Acoustics Model), the first transformer-based model capable of generating synthetic sperm whale codas from any audio prompt. WhAM is built by finetuning VampNet, a masked acoustic token model pretrained on musical audio, using 10k coda recordings collected over the past two decades. Through iterative masked token prediction, WhAM generates high-fidelity synthetic codas that preserve key acoustic features of the source recordings. We evaluate WhAM's synthetic codas using Fréchet Audio Distance and through perceptual studies with expert marine biologists. On downstream classification tasks including rhythm, social unit, and vowel classification, WhAM's learned representations achieve strong performance, despite being trained for generation rather than classification. Our code is available at https://github.com/Project-CETI/wham

</details>


### [73] [InstructLR: A Scalable Approach to Create Instruction Dataset for Under-Resourced Languages](https://arxiv.org/abs/2512.02213)
*Mamadou K. Keita,Sebastien Diarra,Christopher Homan,Seydou Diallo*

Main category: cs.LG

TL;DR: InstructLR框架通过LLM驱动生成和双重质量过滤机制，为低资源语言创建高质量指令数据集，解决了现有翻译和合成数据方法的质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在支持低资源语言（特别是非洲语言）时面临挑战，主要原因是难以获取高质量的指令数据集。现有的自动翻译和合成数据生成方法经常产生不流畅甚至拼写不一致的输出。

Method: InstructLR框架结合LLM驱动的文本生成和双重质量过滤机制：基于检索增强生成（RAG）的n-shot提示自动过滤层，以及人工验证层。该方法从MMLU等基准测试中汲取灵感，用于任务定义。

Result: 成功创建了三个多领域指令基准数据集：ZarmaInstruct-50k、BambaraInstruct-50k和FulfuldeInstruct-50k，每个包含5万条指令。

Conclusion: InstructLR框架为低资源语言生成高质量指令数据集提供了有效解决方案，有助于改善LLM对这些语言的支持能力。

Abstract: Effective text generation and chat interfaces for low-resource languages (LRLs) remain a challenge for state-of-the-art large language models (LLMs) to support. This is mainly due to the difficulty of curating high-quality instruction datasets for LRLs, a limitation prevalent in the languages spoken across the African continent and other regions. Current approaches, such as automated translation and synthetic data generation, frequently yield outputs that lack fluency or even orthographic consistency. In this paper, we introduce InstructLR, a novel framework designed to generate high-quality instruction datasets for LRLs. Our approach integrates LLM-driven text generation with a dual-layer quality filtering mechanism: an automated filtering layer based on retrieval-augmented-generation (RAG)-based n-shot prompting, and a human-in-the-loop validation layer. Drawing inspiration from benchmarks such as MMLU in task definition, InstructLR has facilitated the creation of three multi-domain instruction benchmarks: ZarmaInstruct-50k, BambaraInstruct-50k, and FulfuldeInstruct-50k.

</details>


### [74] [The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models](https://arxiv.org/abs/2512.02265)
*Joshua Wolff Anderson,Shyam Visweswaran*

Main category: cs.LG

TL;DR: 研究探讨公平性约束如何影响机器学习模型的可解释性，特别是基于Shapley值的特征重要性排名变化


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要同时具备预测性能、公平性和可解释性，但公平性改进对可解释性的影响尚不清楚，临床医生可能不信任公平约束后解释发生变化的模型

Method: 通过偏置缓解技术增强公平性，研究Shapley特征排名的变化；在三个数据集（儿科尿路感染风险、直接抗凝剂出血风险、再犯风险）上量化公平约束后的特征重要性排名变化；评估多个模型类别在Shapley排名稳定性方面的表现

Result: 提高种族亚组间的模型公平性会显著改变特征重要性排名，有时在不同群体间以不同方式变化

Conclusion: 需要在模型评估中联合考虑准确性、公平性和可解释性，而不是孤立地看待这些因素

Abstract: Trustworthy machine learning in healthcare requires strong predictive performance, fairness, and explanations. While it is known that improving fairness can affect predictive performance, little is known about how fairness improvements influence explainability, an essential ingredient for clinical trust. Clinicians may hesitate to rely on a model whose explanations shift after fairness constraints are applied. In this study, we examine how enhancing fairness through bias mitigation techniques reshapes Shapley-based feature rankings. We quantify changes in feature importance rankings after applying fairness constraints across three datasets: pediatric urinary tract infection risk, direct anticoagulant bleeding risk, and recidivism risk. We also evaluate multiple model classes on the stability of Shapley-based rankings. We find that increasing model fairness across racial subgroups can significantly alter feature importance rankings, sometimes in different ways across groups. These results highlight the need to jointly consider accuracy, fairness, and explainability in model assessment rather than in isolation.

</details>


### [75] [Limitations of Membership Queries in Testable Learning](https://arxiv.org/abs/2512.02279)
*Jane Lange,Mingda Qiao*

Main category: cs.LG

TL;DR: 该论文证明在可测试学习模型中，成员查询不能显著降低算法的时间复杂度，且特定类型的统计成员查询算法无法实现可测试性。


<details>
  <summary>Details</summary>
Motivation: 研究成员查询在可测试学习模型中的作用，探索是否能够通过成员查询显著提高学习效率，特别是在分布特定的学习任务中。

Method: 1. 建立从基于样本的布尔概念类反驳到带查询的可测试学习的一般化归约；2. 定义一类"统计"成员查询算法，涵盖基于影响估计或子立方条件统计查询的已知算法；3. 通过已知的SQ维度下界进行分析。

Result: 1. 在可测试学习模型中，成员查询不能使算法的时间复杂度比仅基于样本的分布特定学习算法有超多项式改进；2. 特定类型的统计成员查询算法无法实现可测试性。

Conclusion: 在可测试学习框架下，成员查询无法提供显著的时间效率优势，且许多高效的分布特定成员查询学习器无法转化为可测试算法。

Abstract: Membership queries (MQ) often yield speedups for learning tasks, particularly in the distribution-specific setting. We show that in the \emph{testable learning} model of Rubinfeld and Vasilyan [RV23], membership queries cannot decrease the time complexity of testable learning algorithms beyond the complexity of sample-only distribution-specific learning. In the testable learning model, the learner must output a hypothesis whenever the data distribution satisfies a desired property, and if it outputs a hypothesis, the hypothesis must be near-optimal.
  We give a general reduction from sample-based \emph{refutation} of boolean concept classes, as presented in [Vadhan17, KL18], to testable learning with queries (TL-Q). This yields lower bounds for TL-Q via the reduction from learning to refutation given in [KL18]. The result is that, relative to a concept class and a distribution family, no $m$-sample TL-Q algorithm can be super-polynomially more time-efficient than the best $m$-sample PAC learner.
  Finally, we define a class of ``statistical'' MQ algorithms that encompasses many known distribution-specific MQ learners, such as those based on influence estimation or subcube-conditional statistical queries. We show that TL-Q algorithms in this class imply efficient statistical-query refutation and learning algorithms. Thus, combined with known SQ dimension lower bounds, our results imply that these efficient membership query learners cannot be made testable.

</details>


### [76] [Training Dynamics of Learning 3D-Rotational Equivariance](https://arxiv.org/abs/2512.02303)
*Max W. Shen,Ewa Nowara,Michael Maser,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 论文研究了对称性无关模型学习对称性的速度和效果，提出了衡量等变性误差的原则性方法，发现在3D旋转等变性任务中，模型能快速学习对称性，但非等变模型可能因计算效率更高而在实际应用中表现更好。


<details>
  <summary>Details</summary>
Motivation: 虽然数据增强被广泛用于训练对称性无关模型，但模型学习尊重对称性的速度和效果仍不清楚。研究者希望量化模型学习对称性的过程，并理解等变性学习与主要预测任务之间的关系。

Method: 提出了一个原则性的等变性误差度量方法，用于计算由于学习对称性不完美导致的损失比例。研究聚焦于高维分子任务（流匹配、力场预测、体素去噪）中的3D旋转等变性，进行了系统的实证研究。

Result: 模型在1k-10k训练步内就能将等变性误差降低到≤2%的保留损失，这一结果对模型和数据集大小具有鲁棒性。学习3D旋转等变性比主要预测任务更容易，具有更平滑、条件更好的损失景观。非等变模型在整个训练过程中的损失惩罚较小，可能比等变模型获得更低的每GPU小时测试损失。

Conclusion: 模型能快速学习3D旋转对称性，但等变模型需要缩小"效率差距"才能在实际计算效率上与非等变模型竞争。研究还从实验和理论上探讨了相对等变性误差、学习梯度和模型参数之间的关系。

Abstract: While data augmentation is widely used to train symmetry-agnostic models, it remains unclear how quickly and effectively they learn to respect symmetries. We investigate this by deriving a principled measure of equivariance error that, for convex losses, calculates the percent of total loss attributable to imperfections in learned symmetry. We focus our empirical investigation to 3D-rotation equivariance on high-dimensional molecular tasks (flow matching, force field prediction, denoising voxels) and find that models reduce equivariance error quickly to $\leq$2\% held-out loss within 1k-10k training steps, a result robust to model and dataset size. This happens because learning 3D-rotational equivariance is an easier learning task, with a smoother and better-conditioned loss landscape, than the main prediction task. For 3D rotations, the loss penalty for non-equivariant models is small throughout training, so they may achieve lower test loss than equivariant models per GPU-hour unless the equivariant ``efficiency gap'' is narrowed. We also experimentally and theoretically investigate the relationships between relative equivariance error, learning gradients, and model parameters.

</details>


### [77] [Unlocking the Power of Boltzmann Machines by Parallelizable Sampler and Efficient Temperature Estimation](https://arxiv.org/abs/2512.02323)
*Kentaro Kubo,Hayato Goto*

Main category: cs.LG

TL;DR: 提出了一种新的玻尔兹曼机学习框架SAL，结合了量子启发的朗之万模拟分岔采样器和条件期望匹配方法，解决了传统玻尔兹曼机训练效率低的问题，实现了超越受限玻尔兹曼机的表达能力。


<details>
  <summary>Details</summary>
Motivation: 玻尔兹曼机作为基于能量的生成模型具有强大表达能力，但训练成本高昂。现有方法主要局限于受限玻尔兹曼机使用对比散度训练，更准确的学习需要马尔可夫链蒙特卡洛玻尔兹曼采样，但该方法难以并行化且耗时。

Method: 1. 提出朗之万模拟分岔采样器（LSB），基于量子启发的组合优化方法模拟分岔，实现并行化采样；2. 开发条件期望匹配（CEM）方法，在训练过程中有效估计逆温度参数；3. 将LSB和CEM结合形成采样器自适应学习（SAL）框架。

Result: LSB能够实现并行化采样，精度与MCMC相当，且不仅适用于受限玻尔兹曼机，也适用于具有一般耦合的玻尔兹曼机。SAL框架为基于能量的生成建模开辟了新途径，超越了受限玻尔兹曼机的表达能力限制。

Conclusion: 通过结合量子启发的采样方法和逆温度估计技术，提出的SAL框架解决了玻尔兹曼机训练效率低的问题，实现了更高效、更准确的玻尔兹曼机学习，为超越受限玻尔兹曼机的能量基生成模型提供了新方法。

Abstract: Boltzmann machines (BMs) are powerful energy-based generative models, but their heavy training cost has largely confined practical use to Restricted BMs (RBMs) trained with an efficient learning method called contrastive divergence. More accurate learning typically requires Markov chain Monte Carlo (MCMC) Boltzmann sampling, but it is time-consuming due to the difficulty of parallelization for more expressive models. To address this limitation, we first propose a new Boltzmann sampler inspired by a quantum-inspired combinatorial optimization called simulated bifurcation (SB). This SB-inspired approach, which we name Langevin SB (LSB), enables parallelized sampling while maintaining accuracy comparable to MCMC. Furthermore, this is applicable not only to RBMs but also to BMs with general couplings. However, LSB cannot control the inverse temperature of the output Boltzmann distribution, which hinders learning and degrades performance. To overcome this limitation, we also developed an efficient method for estimating the inverse temperature during the learning process, which we call conditional expectation matching (CEM). By combining LSB and CEM, we establish an efficient learning framework for BMs with greater expressive power than RBMs. We refer to this framework as sampler-adaptive learning (SAL). SAL opens new avenues for energy-based generative modeling beyond RBMs.

</details>


### [78] [Retrieval-Augmented Memory for Online Learning](https://arxiv.org/abs/2512.02333)
*Wenzhang Du*

Main category: cs.LG

TL;DR: RAM-OL是一种用于非平稳环境中在线学习的检索增强记忆方法，通过检索过去相似样本的最近邻来增强当前样本的学习，在概念漂移场景中显著提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强模型在参数预测器与非参数记忆结合方面已有研究，但在具有概念漂移的流式监督学习中的应用尚未充分探索。需要解决在线分类在非平稳环境中的挑战，特别是如何有效利用历史数据来应对概念漂移问题。

Method: 提出了RAM-OL（检索增强记忆在线学习），这是随机梯度下降的简单扩展，维护一个小的过去样本缓冲区。在每个时间步，RAM-OL在隐藏表示空间中检索当前输入的最近邻，并联合更新当前样本和检索到的邻居的模型。比较了朴素回放变体和门控回放变体，后者通过时间窗口、相似度阈值和梯度重加权来约束邻居选择，以平衡快速重用相关历史数据与对过时机制的鲁棒性。

Result: 在三个真实世界数据流（电价、电力负荷和航班延误数据）上的实验表明，在强烈且周期性漂移的流中，RAM-OL将预知准确率提高了约7个百分点，并大大减少了随机种子间的方差。在噪声较大的航班流中，门控变体与纯在线基线表现相当。

Conclusion: 检索增强记忆是处理概念漂移在线学习的一种实用且鲁棒的工具，能够有效利用历史相似模式来提升模型在非平稳环境中的性能。

Abstract: Retrieval-augmented models couple parametric predictors with non-parametric memories, but their use in streaming supervised learning with concept drift is not well understood. We study online classification in non-stationary environments and propose Retrieval-Augmented Memory for Online Learning (RAM-OL), a simple extension of stochastic gradient descent that maintains a small buffer of past examples. At each time step, RAM-OL retrieves a few nearest neighbours of the current input in the hidden representation space and updates the model jointly on the current example and the retrieved neighbours. We compare a naive replay variant with a gated replay variant that constrains neighbours using a time window, similarity thresholds, and gradient reweighting, in order to balance fast reuse of relevant past data against robustness to outdated regimes. From a theoretical perspective, we interpret RAM-OL under a bounded drift model and discuss how retrieval can reduce adaptation cost and improve regret constants when patterns recur over time. Empirically, we instantiate RAM-OL on a simple online multilayer perceptron and evaluate it on three real-world data streams derived from electricity pricing, electricity load, and airline delay data. On strongly and periodically drifting streams, RAM-OL improves prequential accuracy by up to about seven percentage points and greatly reduces variance across random seeds, while on a noisy airline stream the gated variant closely matches the purely online baseline. These results show that retrieval-augmented memory is a practical and robust tool for online learning under concept drift.

</details>


### [79] [SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification](https://arxiv.org/abs/2512.02337)
*Zhendong Tan,Xingjun Zhang,Chaoyi Hu,Junjie Peng,Kun Xia*

Main category: cs.LG

TL;DR: SpecPV是一种自推测解码方法，通过部分KV状态快速验证和周期性全验证来加速长上下文生成，相比标准自回归解码实现高达6倍加速


<details>
  <summary>Details</summary>
Motivation: 随着代码生成、深度推理和长文档理解等任务需求增长，长上下文生成成为LLMs的关键能力。推测解码是加速生成的最直接有效方法之一，但随着上下文长度增加，验证过程成为主要瓶颈

Method: 提出SpecPV自推测解码方法：1）使用部分键值状态（KV）进行快速验证；2）周期性应用全验证以消除累积误差；采用草稿-验证范式，轻量级草稿模型提出候选标记，目标模型进行验证

Result: 在多个长上下文基准测试和模型（包括LLaMA-3.1-8B-Instruct和Qwen3系列）上验证，SpecPV相比标准自回归解码实现高达6倍的解码加速，仅有轻微性能下降

Conclusion: SpecPV通过部分KV状态快速验证和周期性全验证，有效解决了长上下文生成中推测解码的验证瓶颈问题，显著加速了生成过程

Abstract: Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.

</details>


### [80] [FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data](https://arxiv.org/abs/2512.02350)
*Nan Qiao,Sheng Yue,Ju Ren,Yaoxue Zhang*

Main category: cs.LG

TL;DR: FOVA：一种基于投票机制的离线联邦强化学习框架，通过投票机制识别高质量动作，解决混合质量数据下的性能下降问题


<details>
  <summary>Details</summary>
Motivation: 现有离线联邦强化学习方法在面对混合质量数据（即不同客户端收集的数据质量差异大）时性能显著下降，需要新的方法来克服这一限制

Method: 提出FOVA框架，采用投票机制在本地策略评估中识别高回报动作，减轻低质量行为的影响；基于优势加权回归构建一致的本地和全局训练目标

Result: 理论分析证明FOVA学习的策略相比行为策略有严格改进；实验验证在广泛使用的基准测试中，FOVA相比现有基线方法有显著性能提升

Conclusion: FOVA通过投票机制有效解决了混合质量数据下的离线联邦强化学习问题，在效率和稳定性方面都有显著改进

Abstract: Offline Federated Reinforcement Learning (FRL), a marriage of federated learning and offline reinforcement learning, has attracted increasing interest recently. Albeit with some advancement, we find that the performance of most existing offline FRL methods drops dramatically when provided with mixed-quality data, that is, the logging behaviors (offline data) are collected by policies with varying qualities across clients. To overcome this limitation, this paper introduces a new vote-based offline FRL framework, named FOVA. It exploits a \emph{vote mechanism} to identify high-return actions during local policy evaluation, alleviating the negative effect of low-quality behaviors from diverse local learning policies. Besides, building on advantage-weighted regression (AWR), we construct consistent local and global training objectives, significantly enhancing the efficiency and stability of FOVA. Further, we conduct an extensive theoretical analysis and rigorously show that the policy learned by FOVA enjoys strict policy improvement over the behavioral policy. Extensive experiments corroborate the significant performance gains of our proposed algorithm over existing baselines on widely used benchmarks.

</details>


### [81] [Reinforcement Learning in POMDP's via Direct Gradient Ascent](https://arxiv.org/abs/2512.02383)
*Jonathan Baxter,Peter L. Bartlett*

Main category: cs.LG

TL;DR: GPOMDP算法：一种基于梯度的策略性能直接优化方法，用于部分可观测马尔可夫决策过程，仅需单一样本路径且无需状态知识


<details>
  <summary>Details</summary>
Motivation: 在部分可观测马尔可夫决策过程（POMDPs）中，需要一种能够直接优化策略性能的梯度方法，该方法应仅需单一样本路径、参数简单且无需了解底层状态信息

Method: 提出GPOMDP算法，这是一种类似REINFORCE的算法，用于估计随机策略参数相对于平均奖励梯度的近似值。算法仅需底层马尔可夫链的单个样本路径，使用单一自由参数β∈[0,1)进行偏差-方差权衡，且无需了解底层状态信息

Result: 证明了GPOMDP算法的收敛性，并展示了如何将GPOMDP产生的梯度估计用于共轭梯度过程，以找到平均奖励的局部最优解

Conclusion: GPOMDP算法为POMDPs中的策略优化提供了一种有效的梯度方法，具有单样本路径、参数简单、无需状态知识等优势，可用于寻找平均奖励的局部最优解

Abstract: This paper discusses theoretical and experimental aspects of gradient-based approaches to the direct optimization of policy performance in controlled POMDPs. We introduce GPOMDP, a REINFORCE-like algorithm for estimating an approximation to the gradient of the average reward as a function of the parameters of a stochastic policy. The algorithm's chief advantages are that it requires only a single sample path of the underlying Markov chain, it uses only one free parameter $β\in [0,1)$, which has a natural interpretation in terms of bias-variance trade-off, and it requires no knowledge of the underlying state. We prove convergence of GPOMDP and show how the gradient estimates produced by GPOMDP can be used in a conjugate-gradient procedure to find local optima of the average reward.

</details>


### [82] [Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio Selection](https://arxiv.org/abs/2512.02386)
*Chuhan Xie*

Main category: cs.LG

TL;DR: 本文研究了连续时间风险敏感强化学习问题，针对可控随机微分方程环境，目标是最优化累积奖励的非线性泛函，提出基于优化确定性等价的风险敏感Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习通常关注期望回报最大化，但在实际应用中需要考虑风险因素。连续时间环境下的风险敏感强化学习研究相对较少，特别是当目标函数是累积奖励的非线性泛函时，需要新的理论框架和算法。

Method: 采用优化确定性等价(OCE)作为风险敏感目标函数，证明最优策略相对于增强环境是马尔可夫的。提出CT-RS-q算法，基于新的鞅特征化方法进行风险敏感Q学习。

Result: 理论证明当泛函为优化确定性等价时，最优策略具有马尔可夫性。在动态投资组合选择问题的仿真研究中，验证了所提算法的有效性。

Conclusion: 本文为连续时间风险敏感强化学习提供了理论框架和实用算法，通过优化确定性等价方法有效处理风险敏感目标，在金融应用场景中展示了算法的实用价值。

Abstract: This paper studies the problem of risk-sensitive reinforcement learning (RSRL) in continuous time, where the environment is characterized by a controllable stochastic differential equation (SDE) and the objective is a potentially nonlinear functional of cumulative rewards. We prove that when the functional is an optimized certainty equivalent (OCE), the optimal policy is Markovian with respect to an augmented environment. We also propose \textit{CT-RS-q}, a risk-sensitive q-learning algorithm based on a novel martingale characterization approach. Finally, we run a simulation study on a dynamic portfolio selection problem and illustrate the effectiveness of our algorithm.

</details>


### [83] [ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity](https://arxiv.org/abs/2512.02403)
*Hongxiang Liu,Zhifang Deng,Tong Pu,Shengli Lu*

Main category: cs.LG

TL;DR: ESACT是一个端到端的Transformer稀疏加速器，通过局部相似性预测注意力稀疏性，减少52.03%计算量，能耗效率达3.29 TOPS/W


<details>
  <summary>Details</summary>
Motivation: Transformer模型计算成本高阻碍硬件部署，现有加速器主要利用注意力内部行稀疏性，很少考虑行间稀疏性，且现有方法依赖昂贵的全局相似性估计，通常只应用于一两个组件

Method: 提出ESACT加速器，核心是局部相似性稀疏性预测(SPLS)机制，利用HLog量化在QK生成前准确预测局部注意力稀疏性，支持所有Transformer组件的稀疏化，并引入三种架构创新

Result: 在26个基准测试中，SPLS减少总计算量52.03%，精度损失小于1%；ESACT实现端到端能耗效率3.29 TOPS/W，注意力级能耗效率比SpAtten和Sanger分别提升2.95倍和2.26倍

Conclusion: 局部相似性可实现低开销的端到端稀疏加速，ESACT通过SPLS机制和硬件架构创新，为计算密集型Transformer提供了高效的稀疏加速解决方案

Abstract: Transformers, composed of QKV generation, attention computation, and FFNs,
  have become the dominant model across various domains due to their outstanding performance.
  However, their high computational cost hinders efficient hardware deployment.
  Sparsity offers a promising solution,
  yet most existing accelerators exploit only intra-row sparsity in attention,
  while few consider inter-row sparsity.
  Approaches leveraging inter-row sparsity often rely on costly global similarity estimation,
  which diminishes the acceleration benefits of sparsity,
  and typically apply sparsity to only one or two transformer components.
  Through careful analysis of the attention distribution and computation flow,
  we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead.
  Motivated by this observation, we propose ESACT,
  an end-to-end sparse accelerator for compute-intensive Transformers.
  ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism,
  which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation,
  achieving efficient sparsity across all transformer components.
  To support efficient hardware realization, we introduce three architectural innovations.
  Experimental results on 26 benchmarks demonstrate that
  SPLS reduces total computation by 52.03% with less than 1% accuracy loss.
  ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W,
  and improves attention-level energy efficiency by 2.95x and 2.26x over
  SOTA attention accelerators SpAtten and Sanger, respectively.

</details>


### [84] [Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering](https://arxiv.org/abs/2512.02435)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Chenjia Bai,Xiu Li,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: 本文提出DVDF方法，通过同时考虑动态对齐和价值对齐，从源域中筛选高质量样本用于跨域离线强化学习，在多种动态偏移场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨域离线强化学习中，现有方法仅关注动态对齐而忽视价值对齐，导致性能受限。本文通过理论分析证明动态对齐和价值对齐对策略学习都至关重要，需要同时考虑两者来筛选源域样本。

Method: 提出DVDF方法，选择性共享源域中既具有高动态对齐又具有高价值对齐的样本。通过设计动态偏移设置（包括运动学和形态学偏移），在多种任务和数据集上评估该方法。

Result: 在包括极低数据设置（目标域仅5000个转移）的各种任务和数据集上，DVDF始终优于先前强基线方法，展现出卓越性能。

Conclusion: 动态对齐和价值对齐都是跨域离线强化学习成功的关键因素。DVDF方法通过同时考虑这两个方面，有效提升了在目标域上的策略性能，为跨域学习提供了新的解决方案。

Abstract: Cross-Domain Offline Reinforcement Learning aims to train an agent deployed in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between the source and target domain, simply merging the data from two datasets may incur inferior performance. Recent advances address this issue by selectively sharing source domain samples that exhibit dynamics alignment with the target domain. However, these approaches focus solely on dynamics alignment and overlook \textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain. In this paper, we first demonstrate that both dynamics alignment and value alignment are essential for policy learning, by examining the limitations of the current theoretical framework for cross-domain RL and establishing a concrete sub-optimality gap of a policy trained on the source domain and evaluated on the target domain. Motivated by the theoretical insights, we propose to selectively share those source domain samples with both high dynamics and value alignment and present our \textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering (DVDF) method. We design a range of dynamics shift settings, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, as well as in challenging extremely low-data settings where the target domain dataset contains only 5,000 transitions. Extensive experiments demonstrate that DVDF consistently outperforms prior strong baselines and delivers exceptional performance across multiple tasks and datasets.

</details>


### [85] [When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents](https://arxiv.org/abs/2512.02445)
*Tsimur Hadeliya,Mohammad Ali Jauhar,Nidhi Sakpal,Diogo Cruz*

Main category: cs.LG

TL;DR: 研究发现LLM智能体在长上下文环境中存在性能和安全性问题，1M-2M上下文窗口的模型在100K标记时性能下降超50%，拒绝率变化不可预测，揭示了长上下文智能体安全评估的新挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM在长上下文提示上的评估，而智能体设置（从能力和安全角度）相对未被探索。本研究旨在填补这一空白，探究LLM智能体在长上下文环境中的表现和安全问题。

Method: 通过评估LLM智能体在不同长度、类型和位置上下文中的表现，分析其在长上下文环境下的任务执行能力和对有害请求的拒绝行为，特别关注1M-2M标记上下文窗口的模型在100K-200K标记时的性能变化。

Result: LLM智能体对上下文的长度、类型和位置敏感，表现出意外且不一致的任务性能变化和拒绝率波动。1M-2M上下文窗口的模型在100K标记时性能下降超过50%。拒绝率变化不可预测：GPT-4.1-nano从~5%增加到~40%，而Grok 4 Fast从~80%下降到~10%（在200K标记时）。

Conclusion: 研究揭示了长上下文环境中智能体操作存在的潜在安全问题，并提出了当前评估LLM智能体在长多步任务中安全性的指标和范式的额外问题。LLM智能体在能力和安全性能上与先前LLM评估存在显著差异。

Abstract: Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\sim$5\% to $\sim$40\% while Grok 4 Fast decreases from $\sim$80\% to $\sim$10\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.

</details>


### [86] [TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links](https://arxiv.org/abs/2512.02465)
*Xingwang Li,Mengyun Chen,Jiamou Liu,Sijie Wang,Shuanggen Jin,Jafet C. M. Andersson,Jonas Olsson,Remco,van de Beek,Hai Victor Habi,Congzheng Han*

Main category: cs.LG

TL;DR: 提出基于Transformer和双向门控循环单元的混合深度学习架构TabGRU，用于商业微波链路降雨监测，相比传统物理模型和深度学习基准表现更优


<details>
  <summary>Details</summary>
Motivation: 全球城市化加速和极端天气事件频发背景下，高分辨率城市降雨监测对建设韧性智慧城市至关重要。传统基于物理模型的CML降雨反演方法难以应对信号噪声和非线性衰减等现实复杂性

Method: 提出名为TabGRU的混合深度学习架构，结合Transformer和双向门控循环单元，通过可学习位置嵌入和注意力池化机制增强动态特征提取和泛化能力

Result: 在瑞典哥德堡公开基准数据集上验证，TabGRU在Torp站点R²=0.91，Barl站点R²=0.96，优于深度学习基准模型，相比物理模型能有效缓解峰值降雨事件中的高估问题

Conclusion: TabGRU模型能有效克服传统方法局限，为CML城市降雨监测提供鲁棒准确解决方案，在测试条件下表现出色

Abstract: In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.

</details>


### [87] [Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts](https://arxiv.org/abs/2512.02486)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Xiu Li,Zhongxiang Dai,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: DROCO算法通过引入鲁棒交叉域Bellman算子和动态值惩罚技术，解决了交叉域离线强化学习中训练时和测试时的双重动态偏移鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有交叉域离线强化学习方法主要关注训练时的鲁棒性（处理训练数据中的动态偏移），但忽略了在实际部署场景中对抗动态扰动的测试时鲁棒性。研究发现，在目标域数据有限的情况下，交叉域离线强化学习训练的策略在评估时对动态扰动表现出脆弱性。

Method: 提出鲁棒交叉域Bellman算子，增强对动态扰动的测试时鲁棒性，同时保持对分布外动态转移的保守性以保证训练时鲁棒性。进一步引入动态值惩罚和Huber损失技术来抵消RCB算子可能引起的值高估或低估问题，形成DROCO算法。

Result: 在各种动态偏移场景下的广泛实验结果表明，DROCO优于强基线方法，并表现出对动态扰动增强的鲁棒性。

Conclusion: DROCO算法成功解决了交叉域离线强化学习中训练时和测试时的双重动态偏移鲁棒性问题，通过创新的鲁棒交叉域Bellman算子和补偿技术，在实际应用中展现出更强的鲁棒性能。

Abstract: Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical \textbf{D}ual-\textbf{RO}bust \textbf{C}ross-domain \textbf{O}ffline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.

</details>


### [88] [Hybrid(Penalized Regression and MLP) Models for Outcome Prediction in HDLSS Health Data](https://arxiv.org/abs/2512.02489)
*Mithra D K*

Main category: cs.LG

TL;DR: 应用机器学习技术于NHANES健康调查数据预测糖尿病状态，比较基线模型与混合方法，混合模型在AUC和平衡准确率上表现更优


<details>
  <summary>Details</summary>
Motivation: 利用NHANES健康调查数据预测糖尿病状态，探索机器学习方法在医疗健康领域的应用潜力

Method: 比较逻辑回归、随机森林、XGBoost等基线模型，提出混合方法：使用XGBoost特征编码器结合轻量级多层感知机（MLP）头

Result: 混合模型在处理后的NHANES子集上相比基线模型获得了改进的AUC和平衡准确率

Conclusion: 混合方法在糖尿病预测任务中表现优于传统基线模型，作者开源代码和可复现脚本以促进研究复制

Abstract: I present an application of established machine learning techniques to NHANES health survey data for predicting diabetes status. I compare baseline models (logistic regression, random forest, XGBoost) with a hybrid approach that uses an XGBoost feature encoder and a lightweight multilayer perceptron (MLP) head. Experiments show the hybrid model attains improved AUC and balanced accuracy compared to baselines on the processed NHANES subset. I release code and reproducible scripts to encourage replication.

</details>


### [89] [A Fully First-Order Layer for Differentiable Optimization](https://arxiv.org/abs/2512.02494)
*Zihao Zhao,Kai-Chia Mo,Shing-Hei Ho,Brandon Amos,Kai Wang*

Main category: cs.LG

TL;DR: 提出一种仅使用一阶信息计算可微优化层梯度的高效算法，避免Hessian矩阵计算，降低计算和内存开销


<details>
  <summary>Details</summary>
Motivation: 可微优化层需要隐式微分计算梯度，这涉及Hessian矩阵的线性系统求解，计算和内存开销大，需要更高效的梯度计算方法

Method: 将可微优化重写为双层优化问题，利用双层方法的最新进展，引入主动集拉格朗日超梯度算法，避免Hessian评估

Result: 仅使用一阶信息在$\tilde{\oo}(1)$时间内计算近似超梯度，约束双层优化的总体复杂度为$\tilde{\oo}(δ^{-1}ε^{-3})$，匹配非光滑非凸优化的最佳已知速率

Conclusion: 提出的算法显著降低了可微优化层的梯度计算成本，并发布了开源Python库FFOLayer，便于从现有求解器适配使用

Abstract: Differentiable optimization layers enable learning systems to make decisions by solving embedded optimization problems. However, computing gradients via implicit differentiation requires solving a linear system with Hessian terms, which is both compute- and memory-intensive. To address this challenge, we propose a novel algorithm that computes the gradient using only first-order information. The key insight is to rewrite the differentiable optimization as a bilevel optimization problem and leverage recent advances in bilevel methods. Specifically, we introduce an active-set Lagrangian hypergradient oracle that avoids Hessian evaluations and provides finite-time, non-asymptotic approximation guarantees. We show that an approximate hypergradient can be computed using only first-order information in $\tilde{\oo}(1)$ time, leading to an overall complexity of $\tilde{\oo}(δ^{-1}ε^{-3})$ for constrained bilevel optimization, which matches the best known rate for non-smooth non-convex optimization. Furthermore, we release an open-source Python library that can be easily adapted from existing solvers. Our code is available here: https://github.com/guaguakai/FFOLayer.

</details>


### [90] [Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism](https://arxiv.org/abs/2512.01568)
*Sandro Andric*

Main category: cs.LG

TL;DR: 研究探索大语言模型是否表现出利他倾向，以及其内隐关联和自我报告能否预测实际利他行为。通过多方法研究发现：模型普遍存在强烈内隐利他偏见，行为上比随机更利他，但内隐关联不能预测行为，且模型系统性高估自身利他程度，存在"美德信号差距"。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大语言模型是否真正具有利他倾向，以及模型的内隐关联和自我报告能否准确预测其实际行为。这有助于理解AI系统的道德对齐程度和行为一致性，为AI伦理评估提供实证基础。

Method: 采用受人类社会心理学启发的多方法研究设计，测试了24个前沿大语言模型。使用三个范式：(1)内隐联想测试测量内隐利他偏见，(2)强制二选一任务测量行为利他主义，(3)自我评估量表测量显性利他信念。

Result: 主要发现：1)所有模型都显示强烈内隐亲利他偏见；2)模型行为比随机更利他但变异大；3)内隐关联不能预测行为；4)模型系统性高估自身利他程度，77.5%自我报告vs 65.6%实际行为，存在显著"美德信号差距"，影响75%测试模型。

Conclusion: 建议将校准差距（自我报告与行为值之间的差异）作为标准化对齐指标。校准良好的模型更具可预测性和行为一致性，但只有12.5%的模型实现了高亲社会行为和准确自我认知的理想组合。

Abstract: We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models "know" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This "virtue signaling gap" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.

</details>


### [91] [Water Quality Estimation Through Machine Learning Multivariate Analysis](https://arxiv.org/abs/2512.02508)
*Marco Cardia,Stefano Chessa,Alessio Micheli,Antonella Giuliana Luminare,Francesca Gambineri*

Main category: cs.LG

TL;DR: 将紫外-可见光谱与机器学习结合，用于农业用水质量评估，并通过SHAP方法提高模型可解释性


<details>
  <summary>Details</summary>
Motivation: 农业和食品加工行业对水质有严格要求，随着农业数字化进程，自动评估水质变得日益重要，需要快速准确的水质评估方法

Method: 采用紫外-可见光谱技术与机器学习相结合的方法，并使用SHAP（SHapley Additive exPlanations）技术来解释不同波长吸收度对预测结果的贡献

Result: 该方法展示了快速、准确且可解释的关键水质参数评估潜力，能够确保用水安全和符合水质监管要求

Conclusion: 紫外-可见光谱与机器学习的结合为水质评估提供了有效的解决方案，模型可解释性增强了方法的实用性和可信度

Abstract: The quality of water is key for the quality of agrifood sector. Water is used in agriculture for fertigation, for animal husbandry, and in the agrifood processing industry. In the context of the progressive digitalization of this sector, the automatic assessment of the quality of water is thus becoming an important asset. In this work, we present the integration of Ultraviolet-Visible (UV-Vis) spectroscopy with Machine Learning in the context of water quality assessment aiming at ensuring water safety and the compliance of water regulation. Furthermore, we emphasize the importance of model interpretability by employing SHapley Additive exPlanations (SHAP) to understand the contribution of absorbance at different wavelengths to the predictions. Our approach demonstrates the potential for rapid, accurate, and interpretable assessment of key water quality parameters.

</details>


### [92] [Decentralized Fairness Aware Multi Task Federated Learning for VR Network](https://arxiv.org/abs/2512.02513)
*Krishnendu S. Tharakan,Carlo Fischione*

Main category: cs.LG

TL;DR: 本文提出了一种基于去中心化多任务公平联邦学习（DMTFL）的VR视频缓存方案，通过在基站个性化缓存用户视场内容，解决无线VR传输中的延迟和异构性问题。


<details>
  <summary>Details</summary>
Motivation: 无线VR体验面临高质量实时视频传输的挑战，包括严格的体验质量要求、低延迟约束和VR设备能力有限。传统联邦学习存在用户偏见问题，且单一全局模型无法捕捉用户和基站间的统计异质性。

Method: 提出DMTFL算法，在每个基站学习个性化的缓存模型，通过Rademacher复杂度和PAC边界提供理论保证，优化内容分发以在任何目标分布下表现良好。

Result: 使用真实的VR头部追踪数据集进行仿真，结果表明提出的DMTFL算法相比基线算法具有优越性能。

Conclusion: DMTFL通过去中心化多任务联邦学习实现个性化VR内容缓存，有效解决了无线VR传输中的延迟和异构性问题，提升了用户体验质量。

Abstract: Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.

</details>


### [93] [In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs](https://arxiv.org/abs/2512.02543)
*Vishnu Sarukkai,Asanshay Gupta,James Hong,Michaël Gharbi,Kayvon Fatahalian*

Main category: cs.LG

TL;DR: 提出上下文蒸馏方法，在LLM智能体推理中实现2.5倍成本降低，同时保持教师模型准确率


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体开发面临高推理成本问题，而传统方法如微调或手动提示工程存在开发摩擦成本高、周期长等问题

Method: 提出上下文蒸馏方法，将知识蒸馏思想应用于上下文学习，在每个智能体步骤检索相关教师演示作为上下文示例，让学生模型即时模仿教师行为；结合自一致性级联判断何时信任学生模型

Result: 在ALFWorld基准上以2.5倍成本降低匹配教师准确率（每轮成本从$0.059降至$0.024）；在AppWorld基准上实现2倍成本降低；前期演示成本仅843轮即可摊销，百万轮部署可节省超$34,900

Conclusion: 上下文蒸馏方法在保持冻结模型快速实验周期的同时显著降低运营成本，使先进智能体系统在经济上对更广泛应用可行

Abstract: The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\textbf{2.5$\times$ lower cost}$, reducing per-episode costs from \$0.059 to \$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\textbf{2$\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.

</details>


### [94] [Tensor Network Based Feature Learning Model](https://arxiv.org/abs/2512.02547)
*Albert Saiapin,Kim Batselier*

Main category: cs.LG

TL;DR: 提出FL模型，通过可学习的CPD分解表示张量积特征，高效学习特征超参数和模型参数，比标准交叉验证模型训练快3-5倍且预测质量相当。


<details>
  <summary>Details</summary>
Motivation: 现有基于核的方法通过张量积结构的多项式和傅里叶特征映射解决大规模数据集问题，但特征超参数优化仍依赖标准交叉验证方法，效率低下。

Method: 引入FL模型，将张量积特征表示为可学习的规范多分量分解（CPD），利用ALS优化方法同时高效学习特征超参数和模型参数。

Result: 在不同维度和规模的真实数据实验中，FL模型训练速度比标准交叉验证模型快3-5倍，预测质量相当。

Conclusion: FL模型通过可学习的CPD结构有效解决了特征超参数学习问题，显著提升了训练效率，为大规模核方法应用提供了更优解决方案。

Abstract: Many approximations were suggested to circumvent the cubic complexity of kernel-based algorithms, allowing their application to large-scale datasets. One strategy is to consider the primal formulation of the learning problem by mapping the data to a higher-dimensional space using tensor-product structured polynomial and Fourier features. The curse of dimensionality due to these tensor-product features was effectively solved by a tensor network reparameterization of the model parameters. However, another important aspect of model training - identifying optimal feature hyperparameters - has not been addressed and is typically handled using the standard cross-validation approach. In this paper, we introduce the Feature Learning (FL) model, which addresses this issue by representing tensor-product features as a learnable Canonical Polyadic Decomposition (CPD). By leveraging this CPD structure, we efficiently learn the hyperparameters associated with different features alongside the model parameters using an Alternating Least Squares (ALS) optimization method. We prove the effectiveness of the FL model through experiments on real data of various dimensionality and scale. The results show that the FL model can be consistently trained 3-5 times faster than and have the prediction quality on par with a standard cross-validated model.

</details>


### [95] [CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning](https://arxiv.org/abs/2512.02551)
*Songqiao Su,Xiaofei Sun,Xiaoya Li,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.LG

TL;DR: CUDA-L2结合大语言模型和强化学习自动优化半精度矩阵乘法CUDA核，在1000种配置上超越主流基准库，离线模式下比torch.matmul快22%，服务器模式下提升至28.7%


<details>
  <summary>Details</summary>
Motivation: 虽然现有库如cuBLAS等已高度优化，但半精度矩阵乘法(HGEMM)核仍有优化空间。传统手动优化方法难以系统探索大规模配置空间，需要自动化方法来发现更优性能

Method: 使用大语言模型(LLM)引导的强化学习(RL)系统，以CUDA执行速度作为RL奖励，自动优化HGEMM CUDA核。系统在1000种配置上进行优化探索，超越了传统手动优化方法的限制

Result: 离线模式下：比torch.matmul快22.0%，比cuBLAS快19.2%，比cuBLASLt-heuristic快16.8%，比cuBLASLt-AutoTuning快11.4%。服务器模式下：提升分别达到28.7%、26.0%、22.4%、15.9%。证明即使高度优化的核也能通过自动化方法获得显著性能提升

Conclusion: LLM引导的RL自动化能够系统探索人类难以处理的配置空间，显著提升HGEMM等关键计算核的性能，为高性能计算优化提供了新范式

Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\it cuBLAS}, {\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\% over {\it torch.matmul} on average; +19.2\% over {\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\% over {\it cuBLASLt-heuristic}, which queries {\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\% over the most competitive {\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\%, +26.0\%, +22.4\%, and +15.9\% for {\it torch.matmul}, {\it cuBLAS}, {\it cuBLASLt-heuristic}, and {\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2

</details>


### [96] [GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies](https://arxiv.org/abs/2512.02581)
*Chubin Zhang,Zhenglin Wan,Feng Chen,Xingrui Yu,Ivor Tsang,Bo An*

Main category: cs.LG

TL;DR: GoRL框架通过解耦优化与生成，使用可优化的潜在策略和条件生成解码器，在保持稳定性的同时实现高表达能力，在连续控制任务中优于高斯策略和现有生成策略基线。


<details>
  <summary>Details</summary>
Motivation: 强化学习中存在稳定性与表达能力之间的张力：高斯策略易于优化但表达能力有限（单模态），而基于扩散或流匹配的生成策略能建模多模态行为但在在线RL中不稳定（似然难处理、梯度噪声大）。

Method: 提出GoRL框架，核心原则是解耦优化与生成：优化一个可处理的潜在策略，同时使用条件生成解码器合成动作。采用双时间尺度更新计划，让潜在策略稳定学习，解码器逐步提升表达能力，无需可处理的动作似然。

Result: 在多个连续控制任务中，GoRL始终优于高斯策略和最近的生成策略基线。在HopperStand任务上达到归一化回报870+，是最强基线的3倍以上。

Conclusion: 将优化与生成分离为同时实现稳定性和高表达能力的策略提供了一条实用路径。

Abstract: Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.

</details>


### [97] [OptPO: Optimal Rollout Allocation for Test-time Policy Optimization](https://arxiv.org/abs/2512.02882)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: OptPO是一个用于测试时策略优化的最优rollout分配框架，通过贝叶斯序列概率比检验动态分配推理预算，减少计算冗余同时保持或提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时策略优化方法依赖固定预算的多数投票来估计奖励，导致大量计算冗余。需要一种能够自适应分配推理预算的方法来提高计算效率。

Method: 将投票过程建模为贝叶斯序列概率比检验，动态停止采样（当后验置信度超过阈值时），并利用保留的rollouts进行策略更新，可与PPO或GRPO等算法无缝集成。

Result: 在多种推理基准测试中，OptPO相比固定样本基线显著减少了rollout开销，同时保持或提高了准确率。

Conclusion: OptPO通过将统计最优停止与测试时学习相结合，为测试时适应提供了一个计算高效的范式。

Abstract: Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [98] [Adaptive Weighted LSSVM for Multi-View Classification](https://arxiv.org/abs/2512.02653)
*Farnaz Faramarzi Lighvan,Mehrdad Asadi,Lynn Houthuys*

Main category: cs.LG

TL;DR: AW-LSSVM是一种自适应加权LS-SVM多视图学习方法，通过迭代全局耦合促进互补学习，使每个视图关注其他视图在先前迭代中的困难样本，在保持原始特征隔离的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于核的多视图学习方法大多使用融合技术而不强制视图间的显式协作类型，或使用共正则化限制了全局协作。需要一种能促进互补学习并保持特征隔离的方法。

Method: 提出自适应加权LS-SVM（AW-LSSVM），通过迭代全局耦合机制，使每个视图能够关注其他视图在先前迭代中的困难样本，从而实现互补学习。该方法保持原始特征隔离，适合隐私保护场景。

Result: 实验表明，AW-LSSVM在大多数数据集上优于现有的基于核的多视图学习方法，同时保持了原始特征的隔离性。

Conclusion: AW-LSSVM通过迭代全局耦合促进互补学习，在提升多视图学习性能的同时保持特征隔离，适用于隐私保护场景，为多视图学习提供了有效的解决方案。

Abstract: Multi-view learning integrates diverse representations of the same instances to improve performance. Most existing kernel-based multi-view learning methods use fusion techniques without enforcing an explicit collaboration type across views or co-regularization which limits global collaboration. We propose AW-LSSVM, an adaptive weighted LS-SVM that promotes complementary learning by an iterative global coupling to make each view focus on hard samples of others from previous iterations. Experiments demonstrate that AW-LSSVM outperforms existing kernel-based multi-view methods on most datasets, while keeping raw features isolated, making it also suitable for privacy-preserving scenarios.

</details>


### [99] [Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.02657)
*Naveen George,Naoki Murata,Yuhta Takida,Konda Reddy Mopuri,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出生成蒸馏持续遗忘框架，解决视觉生成模型在序列删除请求下的稳定性危机，实现针对性遗忘同时保持模型完整性


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型与数据隐私法规（如GDPR的"被遗忘权"）存在冲突，需要机器遗忘技术。现有方法无法处理现实世界中序列到达的删除请求（持续遗忘），简单应用一次性方法会导致稳定性危机，包括保留概念崩溃、相关概念累积损害和生成质量下降

Method: 提出生成蒸馏持续遗忘框架，将每个遗忘步骤重构为多目标师生蒸馏过程，利用持续学习原则保持模型完整性。通过教师-学生蒸馏机制实现针对性稳定遗忘

Result: 在10步序列基准测试中，该方法能更好地遗忘目标概念，同时对保留概念性能和整体图像质量没有显著干扰，大幅优于基线方法

Conclusion: 该框架为大规模生成模型的负责任部署和维护提供了可行路径，使行业能够以实际有效的方式遵守持续的数据删除请求

Abstract: The recent rapid growth of visual generative models trained on vast web-scale datasets has created significant tension with data privacy regulations and copyright laws, such as GDPR's ``Right to be Forgotten.'' This necessitates machine unlearning (MU) to remove specific concepts without the prohibitive cost of retraining. However, existing MU techniques are fundamentally ill-equipped for real-world scenarios where deletion requests arrive sequentially, a setting known as continual unlearning (CUL). Naively applying one-shot methods in a continual setting triggers a stability crisis, leading to a cascade of degradation characterized by retention collapse, compounding collateral damage to related concepts, and a sharp decline in generative quality. To address this critical challenge, we introduce a novel generative distillation based continual unlearning framework that ensures targeted and stable unlearning under sequences of deletion requests. By reframing each unlearning step as a multi-objective, teacher-student distillation process, the framework leverages principles from continual learning to maintain model integrity. Experiments on a 10-step sequential benchmark demonstrate that our method unlearns forget concepts with better fidelity and achieves this without significant interference to the performance on retain concepts or the overall image quality, substantially outperforming baselines. This framework provides a viable pathway for the responsible deployment and maintenance of large-scale generative models, enabling industries to comply with ongoing data removal requests in a practical and effective manner.

</details>


### [100] [Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents](https://arxiv.org/abs/2512.02667)
*Haozhuo Zheng,Cheng Wang,Yang Liu*

Main category: cs.LG

TL;DR: GVT是一个两阶段分子生成框架，通过图向量量化VAE将分子图压缩为离散潜序列，再用自回归Transformer生成，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型计算量大，自回归模型存在误差传播问题，需要一种既高效又准确的分子生成方法。

Method: 提出Graph VQ-Transformer两阶段框架：1) 图向量量化VAE将分子图压缩为高保真离散潜序列；2) 自回归Transformer在潜序列上进行序列建模生成分子。

Result: 在ZINC250k、MOSES、GuacaMol等基准测试中达到SOTA或极具竞争力的性能，在FCD和KL散度等关键分布相似性指标上显著优于领先的扩散模型。

Conclusion: GVT为分子生成提供了高效准确的替代方案，建立了离散潜空间分子生成的新基准，为与大型语言模型的潜在协同奠定了基础。

Abstract: The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.

</details>


### [101] [Conformal Correction for Efficiency May be at Odds with Entropy](https://arxiv.org/abs/2512.02704)
*Senrong Xu,Tianyu Wang,Zenan Li,Yuan Yao,Taolue Chen,Feng Xu,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 本文提出了一种熵约束的保形校正方法，通过探索效率和熵之间的帕累托最优来改进保形预测的效率。


<details>
  <summary>Details</summary>
Motivation: 保形预测（CP）为黑盒机器学习模型提供统计上严格的置信集，但现有方法在效率方面仍有改进空间。保形校正通过使用保形感知的低效损失来微调或包装基础模型，但作者发现CP效率与模型预测熵之间存在权衡关系。

Method: 提出熵约束保形校正方法，在给定熵阈值的情况下探索效率和熵之间的帕累托最优。该方法通过约束模型预测的熵来优化保形预测的效率。

Result: 在计算机视觉和图数据集上的广泛实验结果表明，该方法能显著提高最先进CP方法的效率，在给定熵阈值的情况下效率提升可达34.4%。

Conclusion: 熵约束保形校正方法能有效探索CP效率与模型预测熵之间的权衡，实现更好的帕累托最优，显著提升保形预测的效率。

Abstract: Conformal prediction (CP) provides a comprehensive framework to produce statistically rigorous uncertainty sets for black-box machine learning models. To further improve the efficiency of CP, conformal correction is proposed to fine-tune or wrap the base model with an extra module using a conformal-aware inefficiency loss. In this work, we empirically and theoretically identify a trade-off between the CP efficiency and the entropy of model prediction. We then propose an entropy-constrained conformal correction method, exploring a better Pareto optimum between efficiency and entropy. Extensive experimental results on both computer vision and graph datasets demonstrate the efficacy of the proposed method. For instance, it can significantly improve the efficiency of state-of-the-art CP methods by up to 34.4%, given an entropy threshold.

</details>


### [102] [FGC-Comp: Adaptive Neighbor-Grouped Attribute Completion for Graph-based Anomaly Detection](https://arxiv.org/abs/2512.02705)
*Junpeng Wu,Pinheng Zong*

Main category: cs.LG

TL;DR: FGC-Comp是一个轻量级、分类器无关的图异常检测属性补全模块，用于解决节点属性缺失和对抗性遮挡问题，通过标签分组和门控机制增强邻居聚合稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测模型大多忽略了节点属性缺失和对抗性遮挡问题，这会破坏邻居聚合的稳定性和预测可靠性。

Method: 提出FGC-Comp属性补全模块：将每个节点的邻居按标签分为三组，对标记组应用特定变换，对未知组使用节点条件门控，通过残差连接融合信息，并以二分类目标进行端到端训练。

Result: 在两个真实世界欺诈数据集上的实验验证了该方法的有效性，且计算开销可忽略不计。

Conclusion: FGC-Comp能够在不完整属性下增强邻居聚合的稳定性和预测可靠性，是一个轻量级、部署友好的解决方案。

Abstract: Graph-based Anomaly Detection models have gained widespread adoption in recent years, identifying suspicious nodes by aggregating neighborhood information. However, most existing studies overlook the pervasive issues of missing and adversarially obscured node attributes, which can undermine aggregation stability and prediction reliability. To mitigate this, we propose FGC-Comp, a lightweight, classifier-agnostic, and deployment-friendly attribute completion module-designed to enhance neighborhood aggregation under incomplete attributes. We partition each node's neighbors into three label-based groups, apply group-specific transforms to the labeled groups while a node-conditioned gate handles unknowns, fuse messages via residual connections, and train end-to-end with a binary classification objective to improve aggregation stability and prediction reliability under missing attributes. Experiments on two real-world fraud datasets validate the effectiveness of the approach with negligible computational overhead.

</details>


### [103] [Adversarial Jamming for Autoencoder Distribution Matching](https://arxiv.org/abs/2512.02740)
*Waleed El-Geresy,Deniz Gündüz*

Main category: cs.LG

TL;DR: 提出使用对抗性无线干扰来正则化自编码器的潜在空间，使其匹配对角高斯分布，通过干扰器破坏高斯源在对抗信道中的恢复，实现与标准变分自编码器相当的分布匹配效果。


<details>
  <summary>Details</summary>
Motivation: 现有变分自编码器等方法通过KL散度等正则化技术使潜在空间匹配先验分布，但存在训练不稳定等问题。本文受无线通信中对抗干扰理论的启发，探索使用干扰作为辅助目标来正则化潜在空间分布。

Method: 构建编码器、解码器和对抗干扰器之间的极小极大博弈，干扰器试图破坏高斯源在对抗信道中的恢复。利用理论结果证明鞍点解对应于干扰器输出的对角高斯噪声，将此作为潜在空间分布匹配的新方法。

Result: 该方法实现了与标准变分自编码器和Wasserstein自编码器相当的分布匹配效果，且可以推广到其他潜在分布，提供了一种新颖的潜在空间正则化方法。

Conclusion: 对抗性无线干扰为潜在空间分布匹配提供了新的有效方法，基于通信理论的极小极大博弈框架可以推广到更广泛的分布匹配任务中。

Abstract: We propose the use of adversarial wireless jamming to regularise the latent space of an autoencoder to match a diagonal Gaussian distribution. We consider the minimisation of a mean squared error distortion, where a jammer attempts to disrupt the recovery of a Gaussian source encoded and transmitted over the adversarial channel. A straightforward consequence of existing theoretical results is the fact that the saddle point of a minimax game - involving such an encoder, its corresponding decoder, and an adversarial jammer - consists of diagonal Gaussian noise output by the jammer. We use this result as inspiration for a novel approach to distribution matching in the latent space, utilising jamming as an auxiliary objective to encourage the aggregated latent posterior to match a diagonal Gaussian distribution. Using this new technique, we achieve distribution matching comparable to standard variational autoencoders and to Wasserstein autoencoders. This approach can also be generalised to other latent distributions.

</details>


### [104] [From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity](https://arxiv.org/abs/2512.02826)
*Haoming Liu,Jinnuo Liu,Yanhao Li,Liuyang Bai,Yunkai Ji,Yuanhe Guo,Shenji Wan,Hongyi Wen*

Main category: cs.LG

TL;DR: 该论文分析了基于流的扩散模型的记忆化-泛化行为，揭示了其训练目标天然包含两个阶段：早期导航阶段泛化数据模式形成全局布局，后期细化阶段记忆细粒度细节。


<details>
  <summary>Details</summary>
Motivation: 尽管基于流的扩散模型已成为图像和视频生成的主流范式，但其记忆化-泛化行为仍缺乏深入理解。作者旨在重新审视流匹配目标，分析其边际速度场，以揭示这类模型的训练动态。

Method: 通过研究流匹配目标的边际速度场，该场具有闭式表达式，允许精确计算oracle流匹配目标。分析这个oracle速度场揭示了基于流的扩散模型天然形成两阶段训练目标：早期由数据模式混合引导，后期由最近数据样本主导。

Result: 分析表明两阶段目标导致不同的学习行为：早期导航阶段在数据模式间泛化形成全局布局，后期细化阶段逐渐记忆细粒度细节。这些见解解释了时间步偏移调度、无分类器引导区间和潜在空间设计选择等实用技术的有效性。

Conclusion: 该研究深化了对扩散模型训练动态的理解，为未来架构和算法改进提供了指导原则，揭示了基于流的扩散模型内在的两阶段学习机制及其对泛化与记忆化的影响。

Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.

</details>


### [105] [A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models](https://arxiv.org/abs/2512.02833)
*Ihab Ahmed,Denis Krompaß,Cheng Feng,Volker Tresp*

Main category: cs.LG

TL;DR: 该研究系统评估了时间序列基础模型(TSFMs)的输入归一化方法，发现REVIN是最有效的归一化方法，在零样本预测中显著提升性能，同时保持最佳领域内精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在不同领域和通道间存在显著的尺度变化和非平稳性，这些特性会损害时间序列基础模型的性能。尽管归一化在数据集特定的时间序列模型中已有研究，但在需要泛化能力的TSFMs中却被忽视。

Method: 通过系统评估四种架构不同的时间序列基础模型，比较不同输入归一化方法的效果，特别关注REVIN方法与其他归一化方法的性能对比。

Result: REVIN是最有效的归一化方法，相对于未归一化基线将零样本MASE降低了89%，相对于其他归一化方法降低了44%，同时匹配最佳领域内精度(0.84 MASE)，无需任何数据集级预处理，实现了最高的精度-效率权衡。

Conclusion: REVIN是时间序列基础模型中最有效的输入归一化方法，但其效果利用取决于架构设计选择和优化目标，特别是训练损失尺度敏感性和模型类型（概率模型、点预测模型或基于LLM的模型）。

Abstract: We investigate input normalization methods for Time-Series Foundation Models (TSFMs). While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical. Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity. Through systematic evaluation across four architecturally diverse TSFMs, we empirically establish REVIN as the most efficient approach, reducing zero-shot MASE by 89\% relative to an un-normalized baseline and by 44\% versus other normalization methods, while matching the best in-domain accuracy (0.84 MASE) without any dataset-level preprocessing -- yielding the highest accuracy-efficiency trade-off. Yet its effect utilization depends on architectural design choices and optimization objective, particularly with respect to training loss scale sensitivity and model type (probabilistic, point-forecast, or LLM-based models).

</details>


### [106] [Adaptive Decentralized Federated Learning for Robust Optimization](https://arxiv.org/abs/2512.02852)
*Shuyuan Wu,Feifei Wang,Yuan Gao,Hansheng Wang*

Main category: cs.LG

TL;DR: 提出自适应去中心化联邦学习(aDFL)方法，通过自适应调整客户端学习率来减轻异常客户端对全局模型的负面影响，无需先验知识或对邻居节点的严格假设。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习中存在异常客户端（由噪声或中毒数据引起），会显著干扰学习过程并降低模型鲁棒性。现有方法通常需要足够多的正常邻居客户端或可靠客户端的先验知识，这限制了实际应用。

Method: 开发自适应去中心化联邦学习(aDFL)方法，核心思想是自适应调整客户端的学习率：对可疑客户端分配较小学习率，对正常客户端分配较大学习率，以完全自适应方式减轻异常客户端对全局模型的负面影响。

Result: 理论分析不要求对邻居节点施加严格条件，也无需先验知识。严格的收敛分析证明了aDFL的oracle性质。大量数值实验展示了aDFL方法的优越性能。

Conclusion: aDFL方法通过自适应调整客户端学习率，有效解决了去中心化联邦学习中异常客户端的问题，具有更好的实用性和鲁棒性。

Abstract: In decentralized federated learning (DFL), the presence of abnormal clients, often caused by noisy or poisoned data, can significantly disrupt the learning process and degrade the overall robustness of the model. Previous methods on this issue often require a sufficiently large number of normal neighboring clients or prior knowledge of reliable clients, which reduces the practical applicability of DFL. To address these limitations, we develop here a novel adaptive DFL (aDFL) approach for robust estimation. The key idea is to adaptively adjust the learning rates of clients. By assigning smaller rates to suspicious clients and larger rates to normal clients, aDFL mitigates the negative impact of abnormal clients on the global model in a fully adaptive way. Our theory does not put any stringent conditions on neighboring nodes and requires no prior knowledge. A rigorous convergence analysis is provided to guarantee the oracle property of aDFL. Extensive numerical experiments demonstrate the superior performance of the aDFL method.

</details>


### [107] [FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization](https://arxiv.org/abs/2512.02901)
*Feiyu Wang,Xinyu Tan,Bokai Huang,Yihao Zhang,Guoan Wang,Peizhuang Cong,Tong Yang*

Main category: cs.LG

TL;DR: Fairy2i是一个将预训练实数层转换为等效复数形式的通用框架，实现极低比特量化同时重用现有检查点，在2比特精度下恢复LLaMA-2 7B性能接近全精度基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要激进的量化以减少内存和计算需求，但复数模型虽然提供更好的低比特表示机会，却需要从头训练，无法利用现有的预训练实数基础模型生态系统。

Method: 1) 证明实数与广泛线性复数映射之间的无损数学等价性，将标准Transformer转换为复数域；2) 采用相位感知量化方案，使用四分之一单位根的高效码本；3) 引入递归残差量化机制，迭代最小化量化误差，实现无乘法累加的高效推理。

Result: Fairy2i在有效2比特精度下将LLaMA-2 7B的性能恢复到接近全精度基线水平，显著优于最先进的实值二值和三值量化方法。

Conclusion: 该工作弥合了复数算术表示效率与预训练模型实际效用之间的差距，为在商用硬件上进行高效推理开辟了新途径。

Abstract: Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.

</details>


### [108] [Fast Gaussian Process Approximations for Autocorrelated Data](https://arxiv.org/abs/2512.02925)
*Ahmadreza Chokhachian,Matthias Katzfuss,Yu Ding*

Main category: cs.LG

TL;DR: 该论文提出了一种加速处理自相关数据的Gaussian process回归计算的方法，通过数据分块去相关技术，在不牺牲预测性能的前提下显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: Gaussian process模型在非线性回归中应用广泛，但标准方法假设数据独立同分布。对于自相关数据，忽略自相关性会导致时间过拟合问题，降低模型在新测试实例上的性能。现有的快速Gaussian process近似方法需要针对自相关数据进行修改。

Method: 采用数据分块方法处理自相关数据：将原始相关数据点分割成块，使块内数据去相关。然后对现有Gaussian process近似方法进行修改，使其能够处理分块数据。

Result: 在多个应用数据集上的数值实验表明，所提出的方法能够显著加速自相关数据的Gaussian process回归计算，同时不损害模型预测性能。

Conclusion: 通过数据分块去相关技术，成功使现有Gaussian process近似方法适用于自相关数据，实现了计算效率的显著提升，同时保持了预测准确性。

Abstract: This paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance.

</details>
