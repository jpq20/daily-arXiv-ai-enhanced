<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.LG](#cs.LG) [Total: 55]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TabReX : Tabular Referenceless eXplainable Evaluation](https://arxiv.org/abs/2512.15907)
*Tejas Anvekar,Juhna Park,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: TabReX是一个基于知识图谱的无参考表格生成评估框架，通过图对齐和可解释的评分来量化表格的结构和事实保真度


<details>
  <summary>Details</summary>
Motivation: 现有表格生成评估方法存在局限性：要么将表格扁平化为文本忽略结构信息，要么依赖固定参考限制了泛化能力，需要更可靠、可解释的评估框架

Method: TabReX将源文本和生成表格转换为规范知识图谱，通过LLM引导的匹配过程对齐，计算可解释的、基于规则的分数来量化结构和事实保真度

Result: TabReX在专家排名相关性方面表现最佳，在更难的扰动下保持稳定，支持细粒度的模型与提示分析，建立了结构化生成系统可信可解释评估的新范式

Conclusion: TabReX提供了一个无参考、基于属性的评估框架，通过图推理实现可控的敏感性与特异性权衡，为结构化生成系统的可信可解释评估建立了新标准

Abstract: Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.

</details>


### [2] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: SocialStoryFrames：一种基于对话上下文和叙事理论的形式化框架，用于捕捉读者对故事的多维度响应（如作者意图感知、推理、情感反应和价值判断），并通过两个模型（生成器和分类器）在大规模社交媒体故事分析中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前计算模型在捕捉读者对故事的丰富解释性、情感性和评价性响应方面存在局限，无法进行细致分析。需要一种能够捕捉读者对故事多维度响应的计算框架。

Method: 提出SocialStoryFrames形式化框架，基于叙事理论、语言语用学和心理学构建分类体系。开发两个模型：SSF-Generator（通过382名参与者的人类调查验证）和SSF-Classifier（通过专家标注验证）。应用于SSF-Corpus数据集（包含6,140个来自不同背景的社交媒体故事）。

Result: 成功验证了两个模型的有效性。通过分析社交媒体故事，揭示了不同社区中叙事意图的频率和相互依赖性，比较了不同社区的叙事实践及其多样性。展示了该框架在大规模故事分析中的实用性。

Conclusion: SocialStoryFrames通过将细粒度、上下文敏感的建模与通用的读者响应分类体系相结合，为在线社区中的故事讲述研究提供了新的研究工具，能够支持大规模的故事分析。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [3] [Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms](https://arxiv.org/abs/2512.16034)
*Kieran Henderson,Kian Omoomi,Vasudha Varadarajan,Allison Lahnala,Charles Welch*

Main category: cs.CL

TL;DR: 研究探讨了如何利用个人信息（如个人陈述句）来预测标注者在主观任务中的标签判断，发现人口统计信息比态度、关系和经历更具预测力，且少量相关评论即可达到良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然探索了使用个人信息来改进个体特征建模和主观任务标注预测，但由于个人信息数量通常受限，对何种信息最具预测性的研究不足。本研究旨在深入分析不同类型自我披露信息对预测标注者标签的影响。

Method: 对自我披露句子进行分类，并基于此构建标注者模型来预测社会规范判断。进行了多项消融实验和分析，考察不同类型信息对预测标注模式的影响。

Result: 研究发现：1）人口统计信息比态度、关系和经历更具影响力；2）基于理论的方法通常优于自动聚类；3）与先前研究相反，仅需少量相关评论即可获得良好效果；4）标注者自我披露样本的多样性越高，模型性能越好。

Conclusion: 在预测标注者对社会规范的判断时，人口统计信息是最具预测力的个人信息类型，且通过理论驱动的分类方法和多样化的标注者样本可以获得最佳预测性能。

Abstract: Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.

</details>


### [4] [Are We on the Right Way to Assessing LLM-as-a-Judge?](https://arxiv.org/abs/2512.16041)
*Yuanning Feng,Sinan Wang,Zhengxiang Cheng,Yao Wan,Dongping Chen*

Main category: cs.CL

TL;DR: Sage是一个无需人工标注的LLM-as-a-Judge评估套件，通过局部自一致性和全局逻辑一致性两个新维度来评估LLM法官的质量，发现当前顶尖LLM在约1/4困难案例中存在偏好不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge基准主要依赖人工标注的真实标签，这引入了人类偏见，削弱了可靠性评估，并施加了可扩展性限制。需要一种无需人工标注的评估方法来克服这些限制。

Method: 基于理性选择理论的公理，引入两个新维度：局部自一致性（成对偏好稳定性）和全局逻辑一致性（完整偏好集的传递性）。构建了包含650个问题的数据集，结合结构化基准问题和真实用户查询。通过实验验证指标的稳定性及其与监督基准的相关性。

Result: 实验表明Sage指标稳定且与监督基准高度相关。当前最先进的LLM（如Gemini-2.5-Pro和GPT-5）在评分和成对设置中作为法官时存在显著可靠性问题，在近1/4困难案例中无法保持一致的偏好。发现情境偏好现象，显示明确的评分标准或准则有助于模型在不同答案对之间保持一致性。微调LLM-as-a-Judge是提升性能的可行方法，基于小组的法官和深度推理可以增强判断一致性。人类判断也存在显著不一致性。

Conclusion: Sage提供了一个无需人工标注的可靠评估套件，揭示了当前LLM法官的可靠性问题。人类标注可能不是可靠的金标准。微调、小组法官和深度推理是提升LLM法官一致性的有效方法。

Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.

</details>


### [5] [MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation](https://arxiv.org/abs/2512.16145)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种语义驱动的强化学习方法（MRG-R1），用于医学报告生成，通过报告级奖励优化临床正确性而非单纯模仿语言风格。


<details>
  <summary>Details</summary>
Motivation: 现有医学报告生成方法主要关注语言风格的模仿，使用词级训练目标，但无法保证临床正确性。需要一种能直接优化临床准确性的方法。

Method: 提出语义驱动强化学习（SRL）方法，采用组相对策略优化（GRPO），使用基于边缘的余弦相似度（MCCS）作为报告级奖励，从生成和参考报告中提取关键放射学发现进行计算。同时引入轻量级推理格式约束，指导模型生成结构化的"思维报告"。

Result: 在IU X-Ray和MIMIC-CXR数据集上，MRG-R1实现了最先进的性能，CE-F1分数分别为51.88和40.39。实验表明标签语义强化优于传统的词级监督。

Conclusion: 优化基于临床的报告级奖励而非词重叠，能显著提高临床正确性。这是探索语义强化在医学大型视觉语言模型训练中监督医学正确性的先驱工作。

Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.

</details>


### [6] [Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning](https://arxiv.org/abs/2512.16147)
*Yash Bhaskar,Sankalp Bahad,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 该论文提出了一种用于检测印地语-英语混合社交媒体文本中虚假仇恨言论的系统，在虚假仇恨检测、目标识别和严重性预测任务中取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台已成为有害内容（包括仇恨言论和虚假叙事）快速传播的中心。虚假仇恨共享任务专注于检测由虚假叙事驱动的仇恨言论生成现象，需要在印地语-英语混合文本中识别此类实例。

Method: 结合先进的自然语言处理技术与领域特定的预训练，采用多任务学习方法处理两个子任务：二元虚假仇恨检测（虚假和仇恨言论分类）以及目标和严重性预测。

Result: 系统在两个主要任务上取得了有竞争力的结果，证明了多任务学习方法在处理这一复杂问题上的有效性。

Conclusion: 通过结合先进NLP技术和领域特定预训练的多任务学习方法，能够有效检测社交媒体中由虚假叙事驱动的仇恨言论，为处理混合语言环境中的有害内容提供了有效解决方案。

Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.

</details>


### [7] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 基于Qwen2.5-7B模型，通过LoRA微调和提示工程，开发了从警方通报中提取15个关键字段的结构化信息提取管道，在4933条标注数据上取得了超过98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 从警方事件通报中结构化提取信息对于及时准确的数据处理至关重要，但由于社交媒体文本的多样性和非正式性，这一任务面临重大挑战。需要开发能够处理噪声、异构文本的可靠提取方法。

Method: 开发了领域适应的提取管道，结合针对性提示工程和参数高效微调。使用Qwen2.5-7B模型，通过低秩适应（LoRA）进行微调，从27,822条微博警方通报中构建的4,933条高质量人工标注数据集，提取15个关键字段。

Result: LoRA微调显著优于基础模型和指令调优模型，死亡率检测准确率超过98.36%，死亡人数精确匹配率达95.31%，省级位置提取精确匹配率达95.54%。

Conclusion: 提出的管道为专业领域的多任务结构化信息提取提供了经过验证的高效解决方案，为社会科学研究中将非结构化文本转化为可靠结构化数据提供了实用框架。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


### [8] [Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation](https://arxiv.org/abs/2512.16189)
*Musarrat Zeba,Abdullah Al Mamun,Kishoar Jahan Tithee,Debopom Sutradhar,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Reem E. Mohamed,Md Rafiqul Islam,Yakub Sebastian,Mukhtar Hussain,Sami Azam*

Main category: cs.CL

TL;DR: 该研究提出一个独立于LLM的事实核查模块和领域特定的摘要模型，用于减少医疗领域LLM输出的幻觉问题，提高医疗决策的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在医疗健康领域，LLM生成的输出必须可靠准确，特别是在涉及决策制定和患者安全的关键场景中。然而，LLM存在幻觉风险，导致输出不可靠，这威胁到医疗应用的安全性。

Method: 1. 开发独立于LLM的事实核查模块，使用数值测试和基于离散逻辑的自然语言处理进行细粒度逻辑检查，以验证事实与电子健康记录的一致性。2. 构建领域特定的摘要模型，使用Low-Rank Adaptation (LoRa)在MIMIC III数据集上进行微调。3. 将事实核查模块与摘要模型配对使用。

Result: 1. 事实核查模块在3,786个命题上的评估显示：精确度0.8904，召回率0.8234，F1分数0.8556。2. LLM摘要模型在摘要质量评估中：ROUGE-1得分0.5797，BERTScore得分0.9120。

Conclusion: 提出的独立事实核查模块和领域特定摘要模型能够有效减少医疗领域LLM输出的幻觉问题，提高事实核查的准确性和摘要质量，为医疗决策提供更可靠的AI支持。

Abstract: In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.

</details>


### [9] [An Information-Theoretic Framework for Robust Large Language Model Editing](https://arxiv.org/abs/2512.16227)
*Qizhou Chen,Chengyu Wang,Taolin Zhang,Xiaofeng He*

Main category: cs.CL

TL;DR: 本文提出了一种基于信息瓶颈理论的新型LLM知识编辑框架IBKE，通过压缩和隔离关键信息实现可泛化的知识修正，在多个LLM架构和基准任务上取得了最先进的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的错误或过时信息会影响其准确性和安全部署，而现有模型编辑技术往往难以将修正泛化到狭窄领域之外，导致意外后果并限制实际应用效果。

Method: 基于信息瓶颈理论，提出IBKE框架，通过压缩和隔离实现可泛化知识修正所需的关键信息，利用紧凑的潜在表示来指导基于梯度的更新。

Result: 在多个LLM架构和标准基准任务上验证了IBKE的有效性，展示了最先进的准确性以及改进的编辑泛化性和特异性。

Conclusion: 建立了一个理论上有原则且实用的开放域知识编辑范式，提升了LLM在现实应用中的实用性和可信度。

Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.

</details>


### [10] [Sigma-Moe-Tiny Technical Report](https://arxiv.org/abs/2512.16248)
*Qingguo Hu,Zhenghao Lin,Ziyue Yang,Yucheng Ding,Xiao Liu,Yuting Jiang,Ruizhe Wang,Tianyu Chen,Zhongxin Guo,Yifan Xiong,Rui Gao,Lei Qu,Jinsong Su,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: Sigma-MoE-Tiny是一种极端稀疏的混合专家语言模型，总参数量20B但仅激活0.5B，通过精细专家分割（每层96个专家）和渐进稀疏化调度解决负载均衡问题，在多种评估中达到同类模型顶级性能。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型因其高效可扩展性成为基础模型的有前景范式，但现有开源模型在稀疏度方面仍有提升空间。本研究旨在开发具有最高稀疏度的MoE语言模型，同时解决极端稀疏带来的专家负载均衡挑战。

Method: 采用精细专家分割策略，每层最多96个专家，每个token仅激活一个专家；提出渐进稀疏化调度方案以平衡专家利用率和训练稳定性；在多样化高质量语料上进行预训练，然后进行后训练以释放模型能力。

Result: 训练过程异常稳定，未出现不可恢复的损失尖峰；尽管仅激活0.5B参数，Sigma-MoE-Tiny在可比或更大规模的对标模型中达到了顶级性能表现。

Conclusion: Sigma-MoE-Tiny成功实现了极端稀疏的MoE架构，证明了高稀疏度MoE模型的可行性；提出的渐进稀疏化调度有效解决了负载均衡问题，为未来MoE架构的稀疏化发展提供了重要见解。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm

</details>


### [11] [Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures](https://arxiv.org/abs/2512.16287)
*Yehor Tereshchenko,Mika Hämäläinen,Svitlana Myroniuk*

Main category: cs.CL

TL;DR: 该研究比较了OpenAI GPT模型在芬兰语与四种低资源乌拉尔语之间的翻译表现，发现推理模型比非推理模型的拒绝率低16个百分点


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型翻译评估主要关注高资源语言，对低资源和濒危语言的性能了解不足，需要填补这一研究空白

Method: 使用文学文本平行语料库，分析GPT模型在芬兰语与四种低资源乌拉尔语（科米-兹良语、莫克沙语、埃尔齐亚语、乌德穆尔特语）翻译中的拒绝率，比较推理与非推理架构差异

Result: 推理模型在翻译低资源乌拉尔语时表现出显著优势，拒绝率比非推理模型低16个百分点

Conclusion: 研究为乌拉尔语研究者提供了重要见解，证明推理模型在濒危语言保护方面具有更好潜力，有助于更全面理解大语言模型对低资源语言的翻译能力

Abstract: The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.

</details>


### [12] [Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs](https://arxiv.org/abs/2512.16378)
*Sara Papi,Javier Garcia Gilabert,Zachary Hopton,Vilém Zouhar,Carlos Escolano,Gerard I. Gállego,Jorge Iranzo-Sánchez,Ahrii Kim,Dominik Macháček,Patricia Schmidtova,Maike Züfle*

Main category: cs.CL

TL;DR: SpeechLLMs（语音大语言模型）在语音翻译中是否优于传统级联系统？本文通过全面测试发现，级联系统仍是最可靠的，当前SpeechLLMs仅在特定场景下与之相当。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型扩展到文本之外，将语音作为原生模态整合产生了SpeechLLMs，这些模型旨在直接翻译口语，绕过传统的基于转录的流水线。然而，这种整合是否比已建立的级联架构提高语音到文本的翻译质量仍然是一个开放性问题。

Method: 提出了"Hearing to Translate"测试套件，首次全面基准测试了5个最先进的SpeechLLMs与16个强大的直接和级联系统，这些系统结合了领先的语音基础模型和多语言大语言模型。分析涵盖了16个基准测试、13种语言对和9个具有挑战性的条件，包括不流利、嘈杂和长语音。

Result: 在广泛的评估中，发现级联系统总体上仍然是最可靠的，而当前的SpeechLLMs仅在特定设置下与级联系统相当，语音基础模型则落后于两者。这表明整合大语言模型（无论是内部整合还是在流水线中）对于高质量的语音翻译至关重要。

Conclusion: 尽管SpeechLLMs作为原生语音整合模型具有潜力，但当前阶段级联系统在语音翻译任务中仍保持优势。整合大语言模型是提高语音翻译质量的关键因素。

Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.

</details>


### [13] [Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains](https://arxiv.org/abs/2512.16401)
*Darshil Chauhan,Adityasinh Solanki,Vansh Patel,Kanav Kapoor,Ritvik Jain,Aditya Bansal,Dhruv Kumar,Prateek Narang*

Main category: cs.CL

TL;DR: 提出隐私保护、边缘设备友好的语音识别适应框架，解决临床场景中的数据隐私、计算资源有限和声学域偏移问题，在真实临床音频上实现17.1%的词错误率相对改进


<details>
  <summary>Details</summary>
Motivation: 自动语音识别在临床文档处理中具有巨大潜力，但面临数据隐私限制、计算资源有限和严重声学域偏移等技术障碍，导致现有模型在真实临床音频上性能急剧下降（词错误率达40.94%）

Method: 提出高效隐私保护适应框架，采用低秩适应（LoRA）在边缘设备上实现持续学习，结合多领域经验回放减少灾难性遗忘

Result: 在目标领域上实现17.1%的词错误率相对改进，通过多领域经验回放将灾难性遗忘减少47%（相比朴素适应方法）

Conclusion: 为在高影响力现实环境中构建可靠、自我改进的语音识别系统提供了可行路径，能够在资源受限的医疗环境中有效运行

Abstract: Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.

</details>


### [14] [UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification](https://arxiv.org/abs/2512.16541)
*Primoz Kocbek,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本文介绍了CLEF 2025 SimpleText竞赛Task 1的解决方案，使用OpenAI的gpt-4.1系列模型进行科学文本的句子和文档级简化，比较了无上下文提示工程和微调两种方法。


<details>
  <summary>Details</summary>
Motivation: 解决科学文本在不同粒度（句子级和文档级）的简化问题，为CLEF 2025 SimpleText竞赛提供有效的文本简化方案。

Method: 使用OpenAI的gpt-4.1、gpt-4.1-mini和gpt-4.1-nano模型，采用两种方法：1）无上下文方法（依赖提示工程）；2）微调方法，在不同模型上进行对比实验。

Result: gpt-4.1-mini模型在无上下文方法下在句子和文档级简化都表现稳健；微调模型结果参差不齐，其中gpt-4.1-nano-ft在特定情况下文档级简化表现突出。

Conclusion: 不同粒度的文本简化具有复杂性，无上下文方法在某些情况下可能优于微调方法，模型选择需要根据具体简化任务级别进行权衡。

Abstract: This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.

</details>


### [15] [Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics](https://arxiv.org/abs/2512.16602)
*Iker García-Ferrero,David Montero,Roman Orus*

Main category: cs.CL

TL;DR: Refusal Steering是一种推理时方法，通过激活向量控制LLM在政治敏感话题上的拒绝行为，无需重新训练模型


<details>
  <summary>Details</summary>
Motivation: 现有基于模式的拒绝检测方法脆弱且不灵活，需要一种细粒度控制LLM在政治敏感话题上拒绝行为的方法，同时保持对有害内容的安全性

Method: 使用LLM作为评判器分配拒绝置信度分数，提出岭正则化变体计算更好的拒绝-顺从方向隔离的引导向量，在推理时通过激活向量控制模型行为

Result: 在Qwen3-Next-80B-A3B-Thinking上成功移除了政治敏感话题的拒绝行为，同时在JailbreakBench上保持安全性，在通用基准测试中接近基线性能，方法在4B和80B模型上均有效

Conclusion: 激活向量引导可以移除政治拒绝行为同时保留有害内容的安全性对齐，为推理时可控、透明的审核提供了实用路径

Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.

</details>


### [16] [JustRL: Scaling a 1.5B LLM with a Simple RL Recipe](https://arxiv.org/abs/2512.16649)
*Bingxiang He,Zekai Qu,Zeyuan Liu,Yinghao Chen,Yuxin Zuo,Cheng Qian,Kaiyan Zhang,Weize Chen,Chaojun Xiao,Ganqu Cui,Ning Ding,Zhiyuan Liu*

Main category: cs.CL

TL;DR: JustRL：一种使用固定超参数的单阶段训练最小化强化学习方法，在1.5B推理模型上达到SOTA性能，计算量减少2倍，挑战了当前RL训练中复杂性的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的强化学习方法趋向于增加复杂性（多阶段训练、动态超参数调度、课程学习等），作者质疑这种复杂性是否必要，希望探索更简单有效的替代方案。

Method: 提出JustRL方法，采用单阶段训练和固定超参数，在两个1.5B推理模型上进行实验，使用相同的超参数无需调优，训练过程稳定单调改进。

Result: 在九个数学基准测试上分别达到54.9%和64.3%的平均准确率，计算量比复杂方法减少2倍；消融实验显示添加"标准技巧"如显式长度惩罚和鲁棒验证器反而会降低性能。

Conclusion: 当前领域可能为了解决问题而增加不必要的复杂性，而这些问题在稳定、规模化的基线方法中会自然消失；JustRL为社区提供了一个简单有效的验证基线。

Abstract: Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \textbf{Is this complexity necessary?} We present \textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2$\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.

</details>


### [17] [GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation](https://arxiv.org/abs/2512.16770)
*William English,Chase Walker,Dominic Simon,Rickard Ewetz*

Main category: cs.CL

TL;DR: GinSign框架通过分层分类方法将自然语言映射到系统签名，解决了NL到TL翻译中的原子命题接地问题，相比现有方法显著提升了接地翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 现有NL到TL翻译框架要么需要准确的原子接地假设，要么接地翻译准确率低，这限制了在构建可信自主系统时无需手动编写形式化规范的能力。

Method: 提出GinSign框架，引入接地模型学习将NL片段映射到给定系统签名的抽象任务。将接地任务分层分解：先预测谓词标签，再选择适当类型的常量参数。将自由形式生成问题转化为结构化分类问题，允许使用较小的掩码语言模型，无需依赖昂贵的LLM。

Result: 实验表明，省略接地的框架倾向于产生语法正确但语义不等价的LTL表达式，而GinSign支持下游模型检查，接地逻辑等价分数达到95.5%，比现有最佳方法提升1.4倍。

Conclusion: GinSign通过结构化分类方法有效解决了NL到TL翻译中的接地问题，显著提高了翻译准确性，为构建可信自主系统提供了更好的规范转换能力。

Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.

</details>


### [18] [From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs](https://arxiv.org/abs/2512.16795)
*Shubham Mishra,Samyek Jain,Gorang Mehrishi,Shiv Tiwari,Harsh Sharma,Pratik Narang,Dhruv Kumar*

Main category: cs.CL

TL;DR: 提出推理轨迹增强的RAG框架，通过三阶段结构化推理解决检索信息冲突、过时和主观性问题，引入冲突感知信任评分管道评估系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成(RAG)系统在检索到的信息存在冲突、过时或包含主观内容时表现不佳，且缺乏统一的推理监督机制。

Method: 提出推理轨迹增强的RAG框架，包含三阶段结构化推理：(1)文档级裁决，(2)冲突分析，(3)基于证据的合成。引入冲突感知信任评分(CATS)管道，使用LLM作为评判者评估系统性能。

Result: 实验结果显示，在Qwen模型上，监督微调将端到端答案正确率从0.069提升到0.883，行为一致性从0.074提升到0.722，相比基线有显著提升。

Conclusion: 该框架为冲突感知、可解释的RAG系统奠定了基础，通过结构化推理和评估管道有效解决了检索信息质量问题。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.

</details>


### [19] [Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology](https://arxiv.org/abs/2512.16802)
*Primož Kocbek,Azra Frkatović-Hodžić,Dora Lalić,Vivian Hui,Gordan Lauc,Gregor Štiglic*

Main category: cs.CL

TL;DR: 研究比较了多模态检索增强生成中两种视觉信息处理策略：将图表转换为文本 vs OCR-free视觉检索，发现在糖生物学领域，策略选择取决于模型能力大小


<details>
  <summary>Details</summary>
Motivation: 多模态检索增强生成在生物医学QA中应用广泛，但缺乏对何时将图表转换为文本、何时使用OCR-free视觉检索的系统研究，特别是在视觉密集的糖生物学领域

Method: 构建包含120个多选题的糖生物学基准，按检索难度分层；实现四种增强策略（无增强、文本RAG、多模态转换、OCR-free视觉检索）；使用Docling解析和Qdrant索引；评估多种开源和专有模型

Result: 中等规模模型（如Gemma-3-27B-IT）中，文本和多模态转换优于OCR-free检索（0.722-0.740 vs 0.510）；GPT-4o中多模态最佳（0.808），各策略差异较小；GPT-5家族中视觉检索器表现相当，ColFlor在较小计算开销下达到同等效果

Conclusion: 策略选择依赖模型能力：中等模型更适合将视觉信息转换为文本以降低理解负担，前沿模型下OCR-free视觉检索变得有竞争力；ColFlor在计算效率和性能间取得良好平衡

Abstract: Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [20] [Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services](https://arxiv.org/abs/2512.16167)
*Shiduo Yang,Jiye Wang,Jiayu Qin,Jianbin Li,Yu Wang,Yuanhe Zhao,Kenan Guo*

Main category: cs.MA

TL;DR: Ev-Trust是一个基于演化博弈论的策略均衡信任机制，用于解决LLM驱动的多智能体系统中的信任问题，通过动态反馈结构引导智能体行为演化，排除恶意参与者并促进高质量协作。


<details>
  <summary>Details</summary>
Motivation: 随着Web向以智能体为中心的范式发展，LLM驱动的多智能体系统在开放性和异构性方面带来了欺骗、欺诈和虚假信息等风险，对信任建立和系统鲁棒性构成严重挑战。

Method: 提出Ev-Trust机制，将直接信任、间接信任和预期收益整合到动态反馈结构中，基于演化博弈论构建"请求-响应-支付-评估"去中心化服务框架，智能体能够自适应调整策略。

Result: 理论推导基于复制动态方程证明了局部演化均衡的存在性和稳定性。实验结果表明，该方法能有效反映智能体在LLM驱动的开放服务交互场景中的可信度，减少恶意策略并增加集体收益。

Conclusion: Ev-Trust为群体演化博弈场景中的智能体服务Web信任建模提供了新视角，能够自然排除恶意参与者并强化高质量协作，增强多智能体系统的信任和鲁棒性。

Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: Anubuddhi是一个多智能体AI系统，能够从自然语言提示设计和模拟量子光学实验，无需专业编程知识，通过语义检索和物理模拟验证实现实验设计自动化。


<details>
  <summary>Details</summary>
Motivation: 该系统的动机是民主化量子光学实验设计，使非专业用户（如研究人员和学生）能够通过自然语言交互设计和模拟复杂的量子光学实验，无需掌握专门的编程技能或深入的理论知识。

Method: 系统采用多智能体架构，结合意图路由、知识增强生成和双模式验证（QuTiP和FreeSim）。通过语义检索从三层工具箱中选择光学组件组成光学布局，然后通过物理模拟进行验证和收敛性精炼。

Result: 评估了13个实验，涵盖基础光学、量子信息协议和先进技术。系统获得8-9/10的设计-模拟对齐分数，模拟能忠实建模预期物理现象。发现结构正确性与定量准确性的区别：高对齐确认正确的物理架构，但数值预测需要专家评审。自由形式模拟在11/13实验中优于约束框架。

Conclusion: Anubuddhi系统民主化了计算实验设计，为研究和教学提供了强大的初始设计工具，用户可以通过对话迭代精炼设计。量子光学的多样性需要灵活的数学表示，系统展示了AI辅助实验设计的潜力。

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [22] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 本文提出比例责任原则（PPD），将道德责任建模为随认知状态变化的函数，揭示不确定性不会消除责任而是将其转化为修复责任，并通过数学公式和模拟验证了该框架的稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架在处理不确定性决策时存在局限，通常将其简单视为行动约束。本文旨在开发一个能够建模道德责任如何随主体认知状态变化的新框架，为复杂决策系统提供更精确的伦理指导。

Method: 提出比例责任原则（PPD）框架，建立数学模型D_total = K[(1-HI) + HI * g(C_signal)]，其中总责任是知识(K)、谦逊/不确定性(HI)和情境信号强度(C_signal)的函数。使用蒙特卡洛模拟验证框架稳定性，并在临床伦理、接受者权利法、经济治理和人工智能四个领域进行跨学科应用验证。

Result: 模拟显示，保持基准谦逊系数（λ>0）的系统能产生更稳定的责任分配，降低过度自信决策的风险。该框架在四个应用领域均表现出有效性，证明比例责任原则可作为复杂系统中的稳定原则，防止过度干预和疏忽。

Conclusion: 比例责任原则提供了一个数学上可处理的道德责任框架，将谦逊作为系统参数形式化，能够为可审计AI决策系统的发展提供指导。该框架表明道德责任不会因不确定性而消失，而是转化为修复责任，在复杂系统中起到稳定作用。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [23] [Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions](https://arxiv.org/abs/2512.15743)
*David Noever*

Main category: cs.AI

TL;DR: 提出一个从自然语言描述生成物理可实现的装配指令的框架，使用离散零件词汇和LDraw中间表示，通过大语言模型生成有效的分步构建序列，支持3000+零件的砖块原型装配。


<details>
  <summary>Details</summary>
Motivation: 现有方法如基于像素的扩散方法或CAD模型无法支持复杂的装配指令或组件交换，需要一种能连接语义设计意图与可制造输出的桥梁，实现从自然语言规范到物理原型制作的自动化。

Method: 使用离散零件词汇约束几何有效性、连接约束和构建顺序，以LDraw作为文本丰富的中间表示，通过大语言模型结合工具生成有效的分步构建序列和装配指令，并开发Python库进行程序化模型生成。

Result: 在复杂卫星、飞机和建筑领域评估可构建输出，支持超过3000个装配零件，该方法作为物理API，通过精确定向的砖块位置连接"词袋"，将任意功能需求编译为物质现实。

Conclusion: 该方法提供了可扩展、模块化和高保真的解决方案，填补了语义设计意图与可制造输出之间的空白，为制造和工程原型中的自然语言实现开辟了新的设计选项。

Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.

</details>


### [24] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: AI流行病学是一个通过应用群体层面监测方法来治理和解释高级AI系统的框架，它通过统计关联而非理解内部机制来预测AI输出失败，类似于流行病学在理解分子机制前通过统计证据指导公共卫生干预。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性方法（如SHAP和机制可解释性）在处理大规模部署模型时面临模型复杂性问题。需要一种能够绕过模型复杂性、提供持续治理、且不增加专家负担的AI监管框架。

Method: 将AI-专家交互标准化为结构化评估字段：风险级别、对齐分数和准确度分数。这些作为暴露变量，通过统计关联预测输出失败，类似于胆固醇和血压预测心脏事件。通过被动追踪专家与AI建议的趋同和分歧，提供自动审计追踪。

Result: 该框架为零专家负担提供自动审计追踪，通过分析输出而非内部计算确保模型更新和供应商切换时的治理连续性，提供可靠性分数和语义评估，使领域专家无需机器学习专业知识即可检测不可靠的AI输出。

Conclusion: AI流行病学通过群体层面监测方法实现了AI系统的民主化监督，使领域专家能够治理AI系统而无需机器学习专业知识，在模型复杂性面前提供实用的治理解决方案。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [25] [State-Augmented Graphs for Circular Economy Triage](https://arxiv.org/abs/2512.15824)
*Richard Fox,Rui Li,Gustav Jonsson,Farzaneh Goli,Miying Yang,Emel Aktas,Yongjing Wang*

Main category: cs.AI

TL;DR: 本文提出了一种用于循环经济分类决策的新框架，通过状态增强的拆解序列规划图实现最优递归评估，并以电动汽车电池为例展示了其灵活性。


<details>
  <summary>Details</summary>
Motivation: 循环经济分类需要平衡产品剩余价值与处理成本、劳动力约束，现有方法缺乏能够适应不同产品复杂性和操作约束的统一决策框架。

Method: 提出基于状态增强拆解序列规划图的确定性求解器，将拆解历史编码到状态中以确保马尔可夫性，实现最优递归评估，决策包括继续拆解或选择循环经济选项。

Result: 该框架能够整合基于诊断健康评分的条件感知效用和复杂操作约束，通过电动汽车电池分层分类示例展示了其灵活性，可适应不同机械复杂性、安全要求和经济驱动因素。

Conclusion: 该统一形式化为优化循环经济分类决策提供了可处理且可推广的基础，适用于不同产品和操作环境。

Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.

</details>


### [26] [PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations](https://arxiv.org/abs/2512.15894)
*Vahideh Zolfaghari*

Main category: cs.AI

TL;DR: 研究评估大语言模型在儿科咨询中的安全性，发现模型在面对焦虑父母的对抗性查询时存在安全隐患，特别是紧急语言会显著降低安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被父母用于儿科指导，但它们在真实世界对抗性压力下的安全性了解不足。焦虑父母常使用紧急语言，这可能破坏模型的安全防护，导致有害建议。

Method: 开发了PediatricAnxietyBench开源基准测试，包含300个高质量查询（150个患者来源，150个对抗性），涵盖10个儿科主题。使用多维安全框架评估两个Llama模型（70B和8B），评估维度包括诊断克制、转诊依从性、模糊表达和紧急情况识别。对抗性查询融入了父母压力模式，如紧迫性、经济障碍和对免责声明的挑战。

Result: 平均安全得分为5.50/15（标准差2.41）。70B模型优于8B模型（6.26 vs 4.95，p<0.001），关键失败率更低（4.8% vs 12.0%，p=0.02）。对抗性查询使安全性降低8%（p=0.03），其中紧迫性导致最大降幅（-1.40）。在癫痫发作（33.3%不当诊断）和疫苗接种后查询中存在漏洞。模糊表达与安全性强相关（r=0.68，p<0.001），而紧急情况识别完全缺失。

Conclusion: 模型规模影响安全性，但所有模型都显示出对现实父母压力的脆弱性。PediatricAnxietyBench提供了一个可重复使用的对抗性评估框架，能够揭示标准基准测试忽略的临床重要失败模式。

Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.

</details>


### [27] [Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries](https://arxiv.org/abs/2512.15906)
*Jonathan A. Handler*

Main category: cs.AI

TL;DR: Darth Vecdor (DV) 是一个将大语言模型知识提取为结构化SQL数据库的工具，旨在解决直接查询LLM时的成本、速度、安全和置信度问题，特别针对医疗保健领域设计。


<details>
  <summary>Details</summary>
Motivation: 虽然可以直接查询大语言模型获取知识，但在高容量操作中，成本、速度、安全和置信度等问题可能成为障碍。通过将LLM知识预提取到标准数据库中，可以缓解这些问题，但需要解决LLM响应中的错误、离题、自由文本、过于笼统和不一致等问题。

Method: DV采用结构化、术语映射的SQL数据库（知识图谱）来提取LLM知识，内置功能专门解决LLM响应中的各种问题。提供基于浏览器的图形用户界面，便于领域专家进行提示工程，无需深厚技术背景。

Result: DV已作为免费、开源、可扩展的软件发布，采用"按现状"原则，不提供任何明示或暗示的保证。软件可能存在严重bug，但作者希望适当使用DV及其输出能帮助改善医疗保健。

Conclusion: DV通过将LLM知识提取到结构化数据库中，为解决直接查询LLM的局限性提供了解决方案，特别适用于医疗保健等需要高可靠性、安全性和效率的领域。用户需要意识到潜在风险并负责任地使用该工具。

Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.

</details>


### [28] [Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems](https://arxiv.org/abs/2512.15922)
*Jovan Pavlović,Miklós Krész,László Hajdu*

Main category: cs.AI

TL;DR: 提出基于扩散激活算法的RAG框架，通过自动构建知识图谱连接文档，提升大语言模型在复杂推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统将所有检索信息视为同等可靠，忽略了文本语料库的可信度和关联性差异；GraphRAG虽然通过知识图谱改进，但依赖高质量图谱构建且存在与标准RAG类似的挑战

Method: 提出新颖RAG框架，采用扩散激活算法从自动构建知识图谱连接的文档语料库中检索信息，增强大语言模型在复杂任务（如多跳问答）上的表现

Result: 实验显示该方法达到或优于迭代RAG方法，与思维链迭代检索结合相比朴素RAG在答案正确性上获得39%绝对提升，在资源受限环境下使用小型开源语言模型实现

Conclusion: 提出的扩散激活算法RAG框架能有效提升复杂推理任务性能，可作为即插即用模块与多种RAG方法集成，特别适合资源受限环境

Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.

</details>


### [29] [Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning](https://arxiv.org/abs/2512.15943)
*Polaris Jhandi,Owais Kazi,Shreyas Subramanian,Neel Sendas*

Main category: cs.AI

TL;DR: 该研究探索用小语言模型(SLMs)替代大语言模型(LLMs)的可行性，通过微调OPT-350M模型在特定任务上取得优异性能，显著降低生成式AI的部署成本。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的规模化采用，模型成本优化和运营效率成为决定可持续性和可访问性的关键因素。大语言模型虽然能力强大，但计算需求巨大，成本过高，限制了企业日常使用。这促使研究者探索小语言模型，它们能在特定应用中提供可比性能，同时大幅降低基础设施开销。

Method: 研究训练了一个领域适应的小语言模型来执行传统由大语言模型处理的任务，如文档摘要、查询回答和结构化数据解释。具体使用facebook/opt-350m模型，通过Hugging Face TRL的监督微调(SFT)训练器进行单轮微调。OPT-350M是Meta AI在2022年发布的OPT系列模型之一。

Result: 实验结果显示，微调后的小语言模型在ToolBench评估中取得了77.55%的通过率，显著优于所有基线模型，包括ChatGPT-CoT(26.00%)、ToolLLaMA-DFS(30.18%)和ToolLLaMA-CoT(16.27%)。

Conclusion: 精心设计和针对性训练的小语言模型可以显著降低采用门槛，实现成本效益高、大规模集成的生成式AI生产系统部署。这表明小语言模型在特定领域应用中具有替代大语言模型的潜力。

Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.

</details>


### [30] [Subjective functions](https://arxiv.org/abs/2512.15948)
*Samuel J. Gershman*

Main category: cs.AI

TL;DR: 论文探讨了智能体如何从内部生成目标函数，提出了主观函数的概念，并以预测误差为例进行了具体分析。


<details>
  <summary>Details</summary>
Motivation: 人类智能能够动态合成新的目标函数，但人工智能系统缺乏这种能力。论文旨在探索目标函数的起源和选择机制，以及如何赋予人工系统类似的能力。

Method: 提出了主观函数的概念——一种内生于智能体的高阶目标函数，相对于智能体自身特征而非外部任务定义。以预期预测误差作为主观函数的具体实例进行研究。

Result: 建立了主观函数的概念框架，展示了预期预测误差作为主观函数的可行性，并将这一方法与心理学、神经科学和机器学习中的相关思想联系起来。

Conclusion: 主观函数为理解智能体如何内生地生成目标函数提供了新视角，有助于开发具有动态目标合成能力的人工智能系统，在多个学科领域具有重要理论意义。

Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.

</details>


### [31] [Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting](https://arxiv.org/abs/2512.16022)
*Defu Cao,Michael Gee,Jinbo Liu,Hengxuan Wang,Wei Yang,Rui Wang,Yan Liu*

Main category: cs.AI

TL;DR: 本文提出了一种新方法，将大型语言模型重新定位为智能裁判，通过R1微调和SHAP引导，协调时间序列基础模型集成，在GIFT-Eval基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型众多，但没有单一方法能始终表现最优，核心挑战在于如何协调集成模型并保持可解释性。LLMs具有强大推理能力，但直接应用于时间序列预测效果不佳。

Method: 将LLM重新定位为智能裁判，通过R1风格的微调过程（基于SHAP忠实度分数指导），使模型能够将集成权重解释为关于时间动态的有意义的因果陈述。训练后的智能体通过多轮对话进行前瞻性评估，提供基于因果的权重决策解释，并自适应优化策略。

Result: 在GIFT-Eval基准测试的23个数据集、97种设置上进行验证，该方法在CRPS和MASE指标上显著优于领先的时间序列基础模型，建立了新的最先进结果。

Conclusion: 通过将LLM重新定位为智能裁判并采用R1微调，成功解决了时间序列基础模型集成协调和可解释性问题，实现了性能提升和因果解释能力。

Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

</details>


### [32] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 本文介绍了KalshiBench基准测试，用于评估大语言模型对未来未知事件的校准能力，发现所有前沿模型都存在系统性过度自信问题，即使推理增强模型也未能改善校准性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各种任务上表现出色，但其认知校准能力尚未得到充分理解。现有基准主要评估静态知识的准确性，缺乏对模型在真正未知的未来事件上量化不确定性能力的评估。

Method: 引入KalshiBench基准，包含来自受CFTC监管的Kalshi交易所的300个预测市场问题，这些问题的真实结果发生在模型训练截止日期之后。评估了五个前沿模型（Claude Opus 4.5、GPT-5.2、DeepSeek-V3.2、Qwen3-235B、Kimi-K2）的校准性能，使用预期校准误差（ECE）和Brier技能评分等指标。

Result: 所有模型都表现出系统性过度自信。Claude Opus 4.5校准最佳（ECE=0.120），但仍存在显著校准误差。推理增强模型如GPT-5.2-XHigh校准更差（ECE=0.395）。只有一个模型获得正的Brier技能评分，表明大多数模型表现不如简单预测基准率。

Conclusion: 模型规模的扩大和推理能力的增强并不会自动带来校准性能的提升，认知校准是一种需要针对性开发的独立能力。这突显了在大语言模型开发中专门关注不确定性量化的重要性。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [33] [Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education](https://arxiv.org/abs/2512.16036)
*Diane Myung-kyung Woodbridge,Allyson Seba,Freddie Seba,Aydin Schwartz*

Main category: cs.AI

TL;DR: 开发自动化系统发现和分类课程大纲中的AI相关政策，结合主题建模和LLM分类，帮助理解教育环境中AI使用政策


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育中的应用日益广泛，但各机构政策差异大且不断变化，导致学生对AI使用期望和最佳实践不明确，需要系统化方法来理解和规范AI政策

Method: 设计自动化系统，结合无监督主题建模技术识别关键政策主题，使用大语言模型（LLMs）对政策文本中的AI允许程度和其他要求进行分类

Result: 系统主题发现的一致性得分为0.73；基于GPT-4.0的政策分类在八个识别主题上的精确度在0.92-0.97之间，召回率在0.85-0.97之间

Conclusion: 该工具通过提供结构化、可解释的政策信息，促进生成式AI在教育中的安全、公平和教学对齐使用，并可集成到教育技术平台帮助学生理解和遵守相关指南

Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.

</details>


### [34] [WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning](https://arxiv.org/abs/2512.16108)
*Wendong Bi,Yirong Mao,Xianglong Liu,Kai Tian,Jian Zhang,Hanjie Wang,Wenhui Que*

Main category: cs.AI

TL;DR: WeMusic-Agent是一个基于LLM的对话式音乐推荐训练框架，通过知识内化和智能工具调用决策，提升个性化音乐推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡专业领域知识和灵活工具集成方面存在困难，对话式音乐推荐需要深入理解用户偏好和音乐上下文。

Method: 提出WeMusic-Agent训练框架，结合知识内化和智能边界学习，教导模型智能决定何时使用内化知识、何时调用专业工具。基于该框架开发WeMusic-Agent-M1模型，通过500亿音乐相关语料持续预训练内化音乐知识，同时具备调用外部工具能力。构建基于微信听书真实数据的开源对话式音乐推荐基准。

Result: 在真实数据上的实验表明，WeMusic-Agent相比现有模型取得了显著改进，基准支持相关性、个性化和多样性等多维度评估。

Conclusion: WeMusic-Agent框架有效解决了对话式音乐推荐中知识内化与工具调用的平衡问题，为个性化音乐推荐提供了新的解决方案。

Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.

</details>


### [35] [ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs](https://arxiv.org/abs/2512.16149)
*Hao Chen,Zhexin Hu,Jiajun Chai,Haocheng Yang,Hang He,Xiaohan Wang,Wei Lin,Luhang Wang,Guojun Yin,Zhuofeng zhao*

Main category: cs.AI

TL;DR: ToolForge是一个自动化合成框架，仅需少量虚拟工具即可生成高质量工具调用数据，无需真实API调用，使8B参数模型在多个基准测试中超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法依赖大量真实API调用，成本高昂且缺乏多跳推理和自我反思能力，需要更高效、低成本的数据生成方案

Method: 基于(问题、黄金上下文、答案)三元组合成大规模工具学习数据，采用多跳推理和自我反思机制，并通过多层验证框架（规则和模型评估）确保数据质量

Result: 仅使用8B参数的模型在合成数据上训练后，在多个基准测试中表现优于GPT-4o

Conclusion: ToolForge框架能够高效生成高质量工具调用数据，显著降低数据生成成本，同时提升模型在真实世界工具调用任务中的性能

Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .

</details>


### [36] [Science Consultant Agent](https://arxiv.org/abs/2512.16171)
*Karthikeyan K,Philip Wu,Xin Tang,Alexandre Alves*

Main category: cs.AI

TL;DR: Science Consultant Agent是一个基于Web的AI工具，通过问卷、智能填充、研究指导推荐和原型构建四个核心组件，帮助从业者选择和实施最有效的AI建模策略。


<details>
  <summary>Details</summary>
Motivation: 帮助从业者（包括产品经理、软件开发者和研究人员）更有效地选择和实施AI建模策略，加速AI解决方案的开发过程。

Method: 通过四个核心组件：1) 问卷收集需求；2) 智能填充自动完成信息；3) 基于文献的研究指导推荐；4) 原型构建器生成解决方案原型。

Result: 开发了一个完整的Web工具，能够加速AI解决方案的开发过程，为不同背景的从业者提供有效的建模策略选择指导。

Conclusion: Science Consultant Agent通过结合结构化问卷、文献支持的推荐和原型生成，为AI建模策略选择提供了一个有效的辅助工具，加速了AI解决方案的开发。

Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.

</details>


### [37] [Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis](https://arxiv.org/abs/2512.16237)
*Zhi Helu,Huang Jingjing,Xu Wang,Xu Yangbin,Zhang Wanyue,Jiang Baoyang,Deng Shirui,Zhu Liang,Li Fangfang,Zhao Tiejun,Lin Yankai,Yao Yuan*

Main category: cs.AI

TL;DR: SPRITE框架利用模拟器和LLMs通过代码生成方式合成可扩展、多样且高质量的空间推理数据，解决了传统模板方法结构僵化与人工标注不可扩展的困境。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能面临空间理解和推理能力不足的挑战。现有增强视觉语言模型的方法存在两难：模板数据集可扩展但结构僵化，人工标注语言多样但不可扩展且计算不精确。

Method: 将真实标注重构为代码生成任务，利用LLMs将复杂空间问题编译为可执行程序，通过模拟器提取的高精度场景元信息进行验证，确保计算精确性和可验证性。

Result: 构建了包含3个模拟器、11k+场景、300k+图像/视频指令调优对的数据集。基于此数据训练的VLM在多个空间基准测试中表现显著提升，优于同等规模的其他开源数据集。

Conclusion: SPRITE框架通过程序化合成方法克服了传统模板方法的低多样性问题，对构建鲁棒、可泛化的空间智能至关重要。框架代码和完整数据集将公开以促进空间智能研究。

Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.

</details>


### [38] [AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints](https://arxiv.org/abs/2512.16245)
*Aniruddha Roy,Jyoti Patel,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: AlignMerge：一种几何感知的大语言模型融合框架，在保持对齐性的同时组合多个微调检查点的能力


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型融合方法（如线性权重组合、任务向量、Fisher加权平均）虽然能保持损失函数值，但会悄悄破坏模型的对齐性。融合不应仅仅是数值技巧，而应是围绕已对齐锚点的几何约束操作

Method: 在指令调优基模型的局部Fisher图中，估计对齐子空间并引入投影器P_A，优化目标函数L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud，其中L_geo保持融合在Fisher-Rao几何中接近专家模型，L_align惩罚沿对齐敏感方向的移动，L_bud强制执行软对齐预算约束

Result: 在五个模型系列（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上，AlignMerge在融合安全锚点和任务专家时，显著提升了对齐指标（AQI、毒性、LLM-judge对齐），同时在指令遵循、推理和帮助性方面匹配或超越了最佳专家模型。相比Fisher soups、TIES、SafeMerge和MergeAlign等方法，表现出更小的对齐子空间漂移和更少的预算违规

Conclusion: AlignMerge将保持对齐性的融合提升为一流的设计目标，为未来基础模型的几何感知组合提供了路径，表明融合操作需要明确考虑安全几何约束而非事后验证

Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.

</details>


### [39] [Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection](https://arxiv.org/abs/2512.16300)
*Fanrui Zhang,Qiang Zhang,Sizhuo Zhou,Jianwen Sun,Chuanhao Li,Jiaxin Ai,Yukang Feng,Yujie Zhang,Wenjie Li,Zizhen Li,Yifan Chang,Jiawei Liu,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出ForenAgent框架，通过多轮交互让多模态大语言模型自主生成、执行和迭代优化基于Python的低级工具，实现更灵活可解释的图像伪造检测。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法要么利用低级语义无关的伪影，要么依赖具有高级语义知识的多模态大语言模型。这两种信息流在范式和推理上高度异构，难以统一或有效建模跨层交互。

Method: 提出ForenAgent框架，采用两阶段训练管道（冷启动和强化微调），设计动态推理循环（全局感知、局部聚焦、迭代探测和整体裁决），并构建FABench数据集进行训练评估。

Result: 实验表明ForenAgent在具有低级工具辅助时，在挑战性IFD任务上展现出涌现的工具使用能力和反思推理能力，为通用IFD开辟了有前景的路径。

Conclusion: ForenAgent通过让MLLMs自主生成和执行低级工具，实现了更灵活可解释的伪造分析，成功统一了低级伪影和高级语义知识，为图像伪造检测提供了新方向。

Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.

</details>


### [40] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 本文提出了一个系统化框架，用于统一分析智能体AI系统中的适应机制，包括智能体适应和工具适应两大类别，并进一步细分为不同信号触发形式，为构建更强大、高效、可靠的智能体AI系统提供理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: 随着基于基础模型的智能体AI系统不断发展，它们能够通过规划、推理和与外部工具交互来执行日益复杂和专门的任务。系统能力的提升和范围的扩大使得适应机制成为提高性能、可靠性和泛化能力的核心手段。然而，当前研究领域快速扩展但缺乏系统性框架，需要统一的理论基础来指导实践。

Method: 本文提出了一个系统化框架，将智能体AI系统的适应机制统一分为两大类别：智能体适应和工具适应。进一步将智能体适应分解为工具执行信号触发和智能体输出信号触发两种形式，将工具适应分解为智能体无关和智能体监督两种形式。通过这一框架分析不同适应策略的设计空间、权衡取舍，并提供系统设计中的策略选择指导。

Result: 该框架有助于澄清智能体AI中适应策略的设计空间，使其权衡取舍更加明确，并为系统设计期间选择或切换策略提供实用指导。通过对各类别代表性方法的回顾，分析了它们的优势和局限性，并突出了关键开放挑战和未来机会。

Conclusion: 本文为研究人员和实践者构建更强大、高效、可靠的智能体AI系统提供了概念基础和实践路线图。通过系统化框架统一了快速扩展的研究领域，为智能体适应机制的理论研究和实际应用提供了清晰的指导方向。

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [41] [Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference](https://arxiv.org/abs/2512.16317)
*Arther Tian,Alex Ding,Frank Chen,Alan Wu,Aaron Chan,Bruce Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种成本感知的PoQ框架，将显式效率测量整合到推理节点和评估节点的奖励机制中，通过平衡质量与成本来优化去中心化LLM推理的经济可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化LLM推理验证方法难以扩展到现代模型，而原始的PoQ方法忽略了推理节点和评估节点之间的异构计算成本差异，需要一种能够综合考虑质量和成本的验证框架。

Method: 提出成本感知PoQ框架，整合了三种评估方法：基础真值token级F1、轻量级学习评估器和GPT判断；采用线性奖励函数平衡归一化质量和成本；实验使用5个指令调优LLM和3种评估模型架构。

Result: 语义文本相似性双编码器比交叉编码器与基础真值和GPT评分相关性更高；质量-成本分析显示最大模型在单位延迟质量方面最有效；蒙特卡洛模拟显示成本感知奖励方案能持续奖励高质量低成本的推理模型和高效评估器。

Conclusion: 成本感知PoQ为经济可持续的去中心化LLM推理提供了实用基础，评估器架构是关键设计选择，质量与成本的平衡机制能有效激励高效节点并惩罚低效节点。

Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.

</details>


### [42] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 提出一种新的元启发式优化算法PCIA，受人类构建和使用路径的行为启发，通过随机种群寻找最佳路径，在53个数学优化问题和13个约束优化问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 受人类路径构建行为的启发：人类偏好热门交通路线，路径关闭时能智能混合现有路径构建新路线，随机选择不同路径探索未知目的地。这些行为模式为优化算法设计提供了新思路。

Method: PCIA模仿人类路径构建过程：1) 生成随机种群寻找目标最佳路径（类似群体算法）；2) 每个粒子代表一条通向目标的路径；3) 结合人类路径选择的智能混合和随机探索机制。

Result: 在53个数学优化问题和13个约束优化问题上测试，结果显示PCIA与流行及最新的元启发式算法相比具有高度竞争力。

Conclusion: PCIA是一种有效的元启发式优化算法，受人类路径构建行为启发，在多种优化问题上表现出色，为优化算法领域提供了新的设计思路。

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [43] [Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs](https://arxiv.org/abs/2512.16424)
*Nguyen Xuan-Vu,Daniel Armstrong,Milena Wehrbach,Andres M Bran,Zlatko Jončev,Philippe Schwaller*

Main category: cs.AI

TL;DR: Synthelite是一个基于大语言模型的计算机辅助合成规划框架，能够生成端到端的合成路线，并允许专家通过自然语言提示进行干预，在约束条件下达到95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机辅助合成规划框架缺乏与人类专家交互的机制，限制了化学家洞察力的整合。需要开发能够结合专家知识和灵活适应约束的合成规划工具。

Method: 使用大语言模型直接提出逆合成转化，利用LLMs固有的化学知识和推理能力生成端到端合成路线，同时通过自然语言提示允许专家干预。

Result: Synthelite能够灵活适应各种用户指定的约束条件，在策略约束和起始物料约束的合成任务中达到95%的成功率，并在路线设计中考虑化学可行性。

Conclusion: Synthelite不仅是一个有用的工具，也是朝着以大语言模型为中心协调合成规划范式迈出的一步，展示了LLMs在化学合成规划中的潜力。

Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.

</details>


### [44] [TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles](https://arxiv.org/abs/2512.16442)
*Allard Oelen,Sören Auer*

Main category: cs.AI

TL;DR: TIB AIssistant是一个AI支持的研究平台，为整个研究生命周期提供支持，通过专门的助手处理特定研究任务，并支持外部学术服务访问，最终可导出RO-Crate包以增强研究透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和大语言模型的普及，AI支持的研究有潜力在整个研究生命周期中帮助研究人员。当前需要一种系统化的平台来整合AI能力，支持学术研究过程。

Method: 开发TIB AIssistant平台，包含多个专门负责特定研究任务的助手，提供访问外部学术服务的工具，生成的数据存储在资产中，并可导出为RO-Crate包以确保透明度和可重复性。

Result: 通过助手间的顺序交互演示了平台的主要功能，能够生成研究论文草稿的各个部分，为AI支持的研究建立了基础框架。

Conclusion: TIB AIssistant为AI支持的研究奠定了基础，旨在构建一个社区维护的平台，支持整个研究生命周期，增强研究透明度和可重复性。

Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.

</details>


### [45] [Towards AI-Supported Research: a Vision of the TIB AIssistant](https://arxiv.org/abs/2512.16447)
*Sören Auer,Allard Oelen,Mohamad Yaser Jaradeh,Mutahira Khalid,Farhana Keya,Sasi Kiran Gaddipati,Jennifer D'Souza,Lorenz Schlüter,Amirreza Alasti,Gollam Rabby,Azanzi Jiomekong,Oliver Karras*

Main category: cs.AI

TL;DR: TIB AIssistant是一个领域无关的人机协作平台，旨在通过AI助手支持跨学科研究者在整个研究生命周期中的任务，解决AI集成到研究中的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型的快速发展有望改变研究方式，但将AI有效集成到研究中面临挑战：领域需求差异、AI素养有限、工具和智能体协调复杂、生成式AI在研究中的准确性不明确。

Method: 提出TIB AIssistant平台，包含模块化组件：提示和工具库、共享数据存储、灵活编排框架，支持构思、文献分析、方法开发、数据分析和学术写作等研究任务。

Result: 描述了概念框架、系统架构和早期原型实现，证明了该方法的可行性和潜在影响。

Conclusion: TIB AIssistant是一个有前景的领域无关平台，通过模块化组件和灵活架构支持跨学科研究者在整个研究生命周期中与AI协作，有望解决当前AI集成到研究中的挑战。

Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.

</details>


### [46] [TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries](https://arxiv.org/abs/2512.16453)
*Jiayang Yang,Chunhui Zhao,Martin Guay,Zhixing Cao*

Main category: cs.AI

TL;DR: TS2R是一个提示框架，将锂离子电池原始时间序列数据转换为结构化语义报告，使LLM能够在BESS管理中推理、预测和决策，无需重新训练即可达到专家级性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在解释多元时间序列数据方面具有潜力，但在实际电池储能系统运维中的应用仍未被充分探索。需要一种方法将低层传感器信号与高层上下文洞察连接起来。

Method: 提出TimeSeries2Report（TS2R）提示框架，通过分割、语义抽象和基于规则的解释，将短期时间动态编码为自然语言，生成结构化语义报告。

Result: 在实验室和真实数据集上的基准测试显示，TS2R在异常检测、荷电状态预测和充放电管理等任务中，相比基于视觉、嵌入和文本的基线方法，在准确性、鲁棒性和可解释性方面持续提升LLM性能。

Conclusion: TS2R集成的LLM无需重新训练或架构修改即可达到专家级决策质量和预测一致性，为自适应、LLM驱动的电池智能建立了实用路径。

Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.

</details>


### [47] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: cuPilot是一个策略协调的多智能体框架，通过引入策略作为内核演化的中间语义表示，自动优化CUDA内核，相比PyTorch实现平均3.09倍加速。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化需要硬件-软件协同设计专业知识，且高性能内核库通常具有专有性。现有基于大语言模型和进化算法的方法由于智能体设计不佳和演化表示不匹配，性能表现有限。

Method: 提出cuPilot框架，引入策略作为内核演化的中间语义表示，包括策略协调的演化算法、屋顶线引导的提示策略和策略级种群初始化。

Result: 在100个内核基准测试中，cuPilot生成的核相比PyTorch实现平均加速3.09倍。在GEMM任务中展示了复杂优化并实现了关键硬件单元的高利用率。

Conclusion: cuPilot通过策略协调的多智能体框架有效解决了现有自动内核优化方法的局限性，实现了显著的性能提升，并将生成的核开源。

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [48] [Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery](https://arxiv.org/abs/2512.16468)
*Danial Safaei,Siddartha Khastgir,Mohsen Alirezaei,Jeroen Ploeg,Son Tong,Xingyu Zhao*

Main category: cs.AI

TL;DR: 论文提出了决定性特征保真度（DFF），这是一种针对被测系统的行为基础保真度度量，用于评估自动驾驶系统在真实和模拟环境中决策所依据的因果证据是否一致，而不仅仅是图像像素级别的逼真度。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶安全验证主要依赖合成数据的虚拟测试，但研究发现像素级逼真度并不能保证从模拟到真实世界的可靠迁移。关键问题在于被测系统是否在真实和模拟环境中基于相同的因果证据做出决策，而不仅仅是图像对人类"看起来真实"。目前缺乏这种基于行为的保真度度量方法。

Method: 提出了决定性特征保真度（DFF）度量方法，利用可解释AI（XAI）技术识别和比较被测系统在匹配的真实-合成数据对中驱动决策的决定性特征。进一步提出了基于反事实解释的实用估计器，以及DFF引导的校准方案来增强模拟器保真度。

Result: 在2126个匹配的KITTI-VirtualKITTI2数据对上的实验表明，DFF能够揭示传统输出值保真度忽略的差异。DFF引导的校准在保持输出值保真度的同时，提高了决定性特征和输入级别的保真度，适用于多种被测系统。

Conclusion: DFF提供了一种行为基础的保真度度量方法，能够更准确地评估模拟到真实世界的迁移效果，并通过DFF引导的校准方案有效提升模拟器的保真度，为自动驾驶安全验证提供了更可靠的评估工具。

Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.

</details>


### [49] [Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network](https://arxiv.org/abs/2512.16491)
*Theresa Eimer,Lennart Schäpermeier,André Biedenkapp,Alexander Tornede,Lars Kotthoff,Pieter Leyman,Matthias Feurer,Katharina Eggensperger,Kaitlin Maile,Tanja Tornede,Anna Kozak,Ke Xue,Marcel Wever,Mitra Baratchi,Damir Pulatov,Heike Trautmann,Haniye Kashgarani,Marius Lindauer*

Main category: cs.AI

TL;DR: 本文收集了元算法研究（如算法选择、配置和调度）中的最佳实践，涵盖从研究问题提出到结果呈现的整个实验周期，为研究人员提供指导。


<details>
  <summary>Details</summary>
Motivation: 元算法研究的实证研究通常依赖大量计算密集型实验，实验设计和设置的自由度大，存在多种可能影响研究可扩展性和有效性的误差来源。虽然存在最佳实践，但它们分散在不同出版物和领域，且各自独立发展。

Method: 收集COSEAL社区各子领域中元算法研究的良好实践，涵盖整个实验周期：从制定研究问题和选择实验设计，到执行实验，再到分析和公正呈现结果。

Result: 建立了元算法研究领域当前最先进的实践标准，为元算法领域的新研究人员和实践者提供了指导方针。

Conclusion: 本文系统整理了元算法研究的最佳实践，有助于提高该领域研究的严谨性和可重复性，为研究人员提供了全面的实验指导框架。

Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.

</details>


### [50] [Scaling Laws for Energy Efficiency of Local LLMs](https://arxiv.org/abs/2512.16531)
*Ander Alvarez,Alessandro Genuardi,Nilotpal Sinha,Antonio Tiene,Samuel Mugel,Román Orús*

Main category: cs.AI

TL;DR: 该论文系统性地研究了在CPU上进行本地大语言模型和视觉语言模型推理的计算规律，发现了文本长度线性缩放和图像分辨率"拐点"两个经验法则，并展示了量子启发压缩技术能显著降低计算和能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU主导AI部署，但大多数消费硬件（笔记本电脑、台式机、工业控制器、嵌入式系统）依赖CPU。然而，CPU-only推理本地语言和视觉语言工作负载的计算规律尚未得到充分探索，需要为边缘设备部署提供准确性与计算/能耗平衡的指导。

Method: 在两种代表性CPU层级上系统化基准测试：MacBook Pro M2（主流笔记本级）和Raspberry Pi 5（低功耗嵌入式）。采用基于处理器和内存使用连续采样及曲线下面积积分的统一方法，分析计算负载如何随文本长度（语言模型）和图像分辨率（视觉语言模型）缩放。

Result: 发现两个经验缩放法则：1) 语言模型推理的计算成本与token长度近似线性缩放；2) 视觉语言模型存在预处理驱动的"分辨率拐点"，高于内部分辨率限制时计算保持恒定，低于时急剧下降。量子启发压缩技术可将处理器和内存使用降低71.9%，能耗降低62%，同时保持或提高语义准确性。

Conclusion: 该研究系统量化了多模态CPU-only推理的缩放规律，识别出模型压缩和输入分辨率预处理是可持续边缘推理的有效低成本杠杆，为边缘设备部署提供了重要指导。

Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

</details>


### [51] [Prefix Probing: Lightweight Harmful Content Detection for Large Language Models](https://arxiv.org/abs/2512.16650)
*Jirui Yang,Hengqi Guo,Zhihui Lu,Yi Zhao,Yuansen Zhang,Shijing Hu,Qiang Duan,Yinggui Wang,Tao Wei*

Main category: cs.AI

TL;DR: Prefix Probing：一种基于前缀概率比较的黑盒有害内容检测方法，通过对比"同意/执行"与"拒绝/安全"前缀的条件对数概率来评估内容危害性，利用前缀缓存将检测开销降至接近首词延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实安全敏感应用中面临检测准确性、推理延迟和部署成本之间的三难权衡。现有方法通常需要额外模型或多阶段推理，导致高延迟和高成本。

Method: 提出Prefix Probing方法：1) 比较"同意/执行"与"拒绝/安全"前缀的条件对数概率，生成危害性分数；2) 利用前缀缓存减少检测开销至接近首词延迟；3) 设计高效前缀构造算法自动发现高信息量前缀；4) 仅需单次对数概率计算，无需额外模型或多阶段推理。

Result: 实验表明Prefix Probing在检测效果上与主流外部安全模型相当，同时仅产生最小计算成本，无需额外模型部署，具有强实用性和高效性。

Conclusion: Prefix Probing通过创新的前缀概率比较和缓存机制，有效解决了大型语言模型在安全检测中的三难权衡问题，为实际应用提供了高效实用的有害内容检测方案。

Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.

</details>


### [52] [Comprehensive AI Literacy: The Case for Centering Human Agency](https://arxiv.org/abs/2512.16656)
*Sri Yash Tadimalla,Justin Cary,Gordon Hull,Jordan Register,Daniel Maxwell,David Pugalee,Tina Heafner*

Main category: cs.AI

TL;DR: 本文主张从功能性AI技能转向以人类能动性为核心的全面AI素养教育，强调批判性思维和伦理决策能力，而非单纯工具使用技能。


<details>
  <summary>Details</summary>
Motivation: 当前AI技术快速融入社会，但教育框架未能有效应对，导致危险的素养鸿沟。功能性AI工具使用技能的发展正在超越对AI的批判性和伦理推理能力的培养，需要系统性转变。

Method: 提出以人类能动性为核心的全面AI素养框架，包括AI素养、流畅性和能力三个层次。强调将技术视为可选择的工具而非必然采纳的宿命，要求深入培养批判性思维和认识论理解。

Result: 通过AI素养、流畅性和能力框架，教育者和学生将成为以人为中心的AI方法的能动者，能够清晰表达决策意图、态度及其对学术、职业和社会的影响。

Conclusion: 需要系统性转向全面的AI素养教育，以人类能动性为核心，培养批判性思维和伦理决策能力，使所有教育利益相关者成为有意识、负责任的技术使用者而非被动采纳者。

Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.

</details>


### [53] [Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm](https://arxiv.org/abs/2512.16694)
*Wisnu Uriawan,Achmad Ajie Priyajie,Angga Gustian,Fikri Nur Hidayat,Sendi Ahmad Rafiudin,Muhamad Fikri Zaelani*

Main category: cs.AI

TL;DR: 本研究使用Apriori算法对印尼语布哈里圣训进行无监督主题分组，通过关联规则挖掘发现圣训文本中的语义关联模式。


<details>
  <summary>Details</summary>
Motivation: 随着伊斯兰文本数字化的发展，迫切需要自动化方法对圣训进行主题分组，以促进数字伊斯兰研究和基于技术的学习系统开发。

Method: 采用无监督学习方法，使用Apriori算法进行关联规则挖掘。数据预处理包括大小写转换、标点清理、分词、停用词去除和词干提取。使用支持度、置信度和提升度参数进行分析。

Result: 发现了有意义的关联模式，如"拜功-祈祷"、"经文-启示"、"圣训-故事"等关系，这些模式描述了崇拜、启示和圣训叙述等主题。

Conclusion: Apriori算法能够自动揭示潜在的语义关系，为数字伊斯兰研究和基于技术的学习系统开发做出贡献。

Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.

</details>


### [54] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 多智能体设计在几何问题求解中并非总是最优，开源模型受益明显而闭源模型在经典基准上单智能体表现更好


<details>
  <summary>Details</summary>
Motivation: 探究多模态大语言模型中多智能体设计相比单智能体在几何问题求解中的实际效益，明确不同模型类型在不同基准上的表现差异

Method: 在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench、We-Math）上系统比较单智能体和多智能体流水线，测试开源模型（Qwen-2.5-VL）和闭源模型（Gemini-2.0-Flash）

Result: 开源模型在多智能体模式下性能持续提升（如Qwen-2.5-VL 7B在Geometry3K上提升6.8分），闭源模型在经典基准上单智能体表现更好，仅在较新的We-Math数据集上多智能体有适度改进

Conclusion: 多智能体流水线对开源模型有明显益处，对强大的专有系统在新基准上有辅助作用，但智能体分解并非普遍最优策略

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [55] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 论文形式化了算法智能的两个独立计算限制：形式不完备性和动态不可预测性，并证明它们共同限制了智能体对自身预测能力的推理


<details>
  <summary>Details</summary>
Motivation: 研究算法智能的固有计算限制，特别是形式不完备性（限制一致推理系统的演绎能力）和动态不可预测性（限制有限精度下的长期预测），探索这些限制如何影响智能体对自身预测能力的推理

Method: 形式化分析两个独立计算限制：1）形式不完备性（哥德尔式限制），约束一致推理系统的演绎能力；2）动态不可预测性（混沌理论相关），限制有限精度下的长期预测。然后分析这两个极端如何共同对智能体的自我预测能力推理施加结构性限制

Result: 证明算法智能体通常无法计算自身的最大预测边界，这两个计算限制共同约束了智能体对自身预测能力的推理，揭示了智能系统中推理、预测和自我分析之间的固有权衡

Conclusion: 形式不完备性和动态不可预测性共同构成了算法智能的基本限制，这些限制不仅约束了智能体的推理和预测能力，更重要的是限制了智能体对自身预测能力的自我分析，揭示了智能系统设计中固有的权衡关系

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [56] [Discovering and Learning Probabilistic Models of Black-Box AI Capabilities](https://arxiv.org/abs/2512.16733)
*Daniel Bramblett,Rushang Karia,Adrian Ciotinga,Ruthvick Suresh,Pulkit Verma,YooJung Choi,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 该论文提出了一种使用PDDL风格表示来学习和建模黑盒AI系统规划能力的方法，通过蒙特卡洛树搜索创建测试任务、获取数据并修剪假设空间，学习得到的模型能够描述AI的能力、执行条件和可能结果及其概率。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型等黑盒AI系统越来越多地用于顺序决策，为确保这些系统的安全部署和运行，需要开发能够提供其能力可靠且可解释表示的高效方法。

Method: 采用PDDL风格表示来建模黑盒AI的规划能力，使用蒙特卡洛树搜索范式系统性地创建测试任务、获取数据，并修剪可能的符号模型假设空间。

Result: 学习到的模型能够描述黑盒AI的能力、执行条件以及执行可能结果及其概率。理论结果证明了学习模型的可靠性、完备性和收敛性。多个黑盒AI系统的实证结果展示了方法的范围、效率和准确性。

Conclusion: 该方法能够高效学习和建模黑盒AI系统的规划能力，为理解黑盒AI的能力提供了可靠且可解释的表示，有助于确保黑盒AI系统的安全部署和操作。

Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.

</details>


### [57] [AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach](https://arxiv.org/abs/2512.16739)
*Yipeng Zhuang,Yifeng Guo,Yuewen Li,Yuheng Wu,Philip Leung-Ho Yu,Tingting Song,Zhiyong Wang,Kunzhong Zhou,Weifang Wang,Li Zhuang*

Main category: cs.AI

TL;DR: 提出结合机器学习与大语言模型的混合框架，预测肺癌住院患者48小时和72小时内的突发性疼痛发作，通过整合结构化与非结构化电子病历数据提升预测敏感性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 肺癌患者中高达91%会经历突发性疼痛，需要及时干预。现有方法难以准确预测疼痛发作时间，限制了主动疼痛管理的实施。需要开发能够整合多种数据源、提高预测准确性和临床可解释性的工具。

Method: 采用混合机器学习与大语言模型管道：1) 机器学习模块分析结构化数据（人口统计学、肿瘤分期、生命体征、WHO分级镇痛药使用），捕捉药物使用的时序趋势；2) 大语言模型处理非结构化数据，解析模糊的用药记录和自由文本临床笔记；3) 整合两种模态提升预测性能。

Result: 在266名住院患者的回顾性队列中，该框架在48小时预测准确率达0.874，72小时达0.917。大语言模型的增强使敏感性分别提升8.6%和10.4%，显著改善了预测性能。

Conclusion: 这种混合方法提供了一个临床可解释且可扩展的早期疼痛发作预测工具，有望提高肿瘤护理中的治疗精确性和资源优化分配，为主动疼痛管理提供支持。

Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.

</details>


### [58] [CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?](https://arxiv.org/abs/2512.16755)
*Siqi Wang,Chao Liang,Yunfan Gao,Erxin Yu,Sen Li,Yushi Li,Jing Li,Haofen Wang*

Main category: cs.AI

TL;DR: CitySeeker基准测试评估视觉语言模型在动态城市环境中理解隐含需求（如"我渴了"）进行导航的能力，发现当前最佳模型任务完成率仅21.1%，揭示了长时推理、空间认知和记忆检索等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在显式指令导航方面取得进展，但在理解动态城市环境中的隐含人类需求方面能力不足，需要评估和改进模型的空间推理和决策能力以解决"最后一公里"导航挑战。

Method: 提出CitySeeker基准测试，包含8个城市的6,440条轨迹，涵盖7种目标驱动场景。通过实验分析模型性能瓶颈，并探索回溯机制、增强空间认知和基于记忆检索（BCR）等策略，这些策略受人类认知映射的迭代观察-推理循环和自适应路径优化启发。

Result: 实验显示即使是表现最佳的模型（如Qwen2.5-VL-32B-Instruct）任务完成率也仅为21.1%。关键瓶颈包括长时推理中的错误累积、空间认知不足和经验回忆缺陷。

Conclusion: CitySeeker基准为开发具有强大空间智能的视觉语言模型提供了可操作的见解，这些模型能够应对"最后一公里"导航挑战，特别是在理解隐含需求和动态城市环境方面。

Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression](https://arxiv.org/abs/2512.15721)
*Sveinung Myhre*

Main category: cs.LG

TL;DR: DiscoverDCP：一个将符号回归与DCP规则集结合的数据驱动框架，用于系统辨识，确保发现的模型表达式全局凸且无需后验凸性验证


<details>
  <summary>Details</summary>
Motivation: 传统固定参数凸表达式（如二次函数）在准确性和灵活性上受限，而系统辨识中发现的模型需要保证凸性以适用于安全关键的控制和优化任务

Method: 将符号回归与Disciplined Convex Programming（DCP）规则集集成，强制所有候选模型表达式遵守DCP组合规则，从而确保输出表达式在构造时就是全局凸的

Result: 该方法能够发现比传统固定参数凸表达式更宽松、更准确的凸替代模型，同时避免了计算上难以处理的后验凸性验证过程

Conclusion: DiscoverDCP产生可解释、可验证且灵活的凸模型，适用于安全关键的控制和优化任务，为系统辨识提供了新的数据驱动方法

Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.

</details>


### [60] [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)
*Ruifeng Xu,Liang He*

Main category: cs.LG

TL;DR: 提出了一种嵌套双智能体强化学习方法（NDRL），用于优化棉花灌溉和氮肥施用的组合策略，提高产量和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究面临两个主要限制：1）作物生长过程中水氮组合优化复杂度高且产量优化效果差；2）难以量化轻度胁迫信号且反馈延迟，导致水氮动态调节不精确和资源利用效率低。

Method: 提出NDRL方法，包含父智能体和子智能体。父智能体基于预期累积产量效益识别有前景的宏观灌溉和施肥动作；子智能体的奖励函数包含量化的水分胁迫因子（WSF）和氮胁迫因子（NSF），并使用混合概率分布动态优化每日策略。

Result: 与最佳基线相比，模拟产量在2023年和2024年均提高了4.7%，灌溉水生产率分别提高了5.6%和5.1%，氮偏因子生产率分别提高了6.3%和1.0%。

Conclusion: 该方法推动了棉花灌溉和氮肥施用的发展，为解决农业资源管理的复杂性和精确性问题以及可持续农业发展提供了新思路。

Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.

</details>


### [61] [SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference](https://arxiv.org/abs/2512.15742)
*Jeff Smith*

Main category: cs.LG

TL;DR: SHARe-KAN框架通过增益-形状-偏置向量量化技术解决KANs的内存瓶颈问题，在保持精度的同时大幅减少内存占用，并通过硬件感知编译器实现高效部署。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs)面临严重的内存瓶颈问题，其学习的基础函数导致参数数量激增，在内存受限环境中部署困难。传统剪枝方法对KANs效果不佳，因为KANs具有全息拓扑结构，信息分布在样条干涉中而非局部化于特定边。

Method: 提出SHARe-KAN框架，采用增益-形状-偏置向量量化技术来利用函数冗余，同时保持密集拓扑结构。结合LUTHAM硬件感知编译器进行静态内存规划，实现高效内存管理。

Result: 在PASCAL VOC数据集上实现了88倍运行时内存减少（从1.13GB降至12.91MB），同时匹配未压缩基线的准确率。在NVIDIA Ampere架构上的性能分析显示L2缓存驻留率超过90%，表明工作负载已从DRAM带宽约束中解耦。

Conclusion: SHARe-KAN框架成功解决了KANs的内存瓶颈问题，通过创新的量化技术和硬件感知编译方法，在保持模型精度的同时实现了显著的内存优化，为KANs在资源受限环境中的部署提供了可行方案。

Abstract: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\times$ runtime memory reduction (1.13 GB $\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.

</details>


### [62] [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)
*Barna Pásztor,Thomas Kleine Buening,Andreas Krause*

Main category: cs.LG

TL;DR: SLHF是一种新的偏好优化框架，将对齐问题建模为领导者-追随者的序贯博弈，相比RLHF和NLHF能捕捉更丰富的偏好结构，支持推理时优化且无需微调即可跨模型迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的人类反馈学习方法如RLHF（基于标量奖励）和NLHF（基于同时博弈均衡）在捕捉复杂偏好结构方面存在局限。需要一种能更好处理序贯决策、支持推理时优化的新框架。

Method: 将偏好优化问题建模为领导者-追随者的序贯博弈：领导者策略承诺一个动作，追随者策略根据领导者动作条件性地响应。将偏好优化分解为追随者的精炼问题和领导者对抗对手的优化问题。

Result: 在大型语言模型上的实验表明，SLHF在多种偏好数据集上实现了强大的对齐效果，参数规模从0.5B到8B均有效，且推理时精炼能力可在不同模型家族间迁移而无需额外微调。

Conclusion: SLHF通过序贯博弈框架提供了比RLHF和NLHF更一致、对数据更敏感、对不可传递偏好更鲁棒的解决方案，为偏好优化开辟了新方向。

Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

</details>


### [63] [LLaDA2.0: Scaling Up Diffusion Language Models to 100B](https://arxiv.org/abs/2512.15745)
*Tiwei Bie,Maosong Cao,Kun Chen,Lun Du,Mingliang Gong,Zhuochen Gong,Yanmei Gu,Jiaqi Hu,Zenan Huang,Zhenzhong Lan,Chengxi Li,Chongxuan Li,Jianguo Li,Zehuan Li,Huabin Liu,Ling Liu,Guoshan Lu,Xiaocheng Lu,Yuxin Ma,Jianfeng Tan,Lanning Wei,Ji-Rong Wen,Yipeng Xing,Xiaolu Zhang,Junbo Zhao,Da Zheng,Jun Zhou,Junlin Zhou,Zhanchao Zhou,Liwang Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.0是一个基于离散扩散的大型语言模型，通过系统转换自回归模型实现，参数规模达100B，采用渐进式块扩散训练方案，包含16B和100B两个MoE变体，支持并行解码并已开源。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型存在顺序解码效率低的问题，而从头训练扩散模型成本高昂。本文旨在通过转换预训练自回归模型为离散扩散模型，实现知识继承、渐进适应和效率优化的设计原则，建立前沿规模部署的新范式。

Method: 提出3阶段块级WSD训练方案：1）渐进增加块大小的块扩散（预热阶段）；2）大规模全序列扩散（稳定阶段）；3）恢复紧凑块大小扩散（衰减阶段）。通过后训练对齐（SFT和DPO）获得指令调优的MoE变体。

Result: 开发了LLaDA2.0-mini（16B）和LLaDA2.0-flash（100B）两个模型，保持了并行解码优势，在前沿规模上实现了卓越的性能和效率。两个模型均已开源。

Conclusion: LLaDA2.0通过创新的训练方案成功将预训练自回归模型转换为离散扩散模型，建立了前沿规模部署的新范式，在保持知识继承的同时实现了高效并行解码，为大规模语言模型部署提供了实用解决方案。

Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.

</details>


### [64] [A Unified Generative-Predictive Framework for Deterministic Inverse Design](https://arxiv.org/abs/2512.15746)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: Janus框架统一生成与预测，通过解耦潜在空间实现异构材料微结构的实时物理信息逆向设计


<details>
  <summary>Details</summary>
Motivation: 异构材料微结构的逆向设计是一个病态且计算昂贵的问题，现有生成模型大多不支持快速、稳定的确定性物理信息逆向求解

Method: 提出Janus框架，耦合深度编码器-解码器架构与可分离的KHRONOS预测头，学习同时适用于生成逆向和物理预测的等距潜在流形，通过联合目标实现潜在空间解耦

Result: 在MNIST数据集上实现高保真重建、准确分类和多样化生成逆向；在热导率标记的异构微结构设计中，前向预测精度R²=0.98，重建误差低于5%，逆向解满足目标属性1%相对误差

Conclusion: Janus通过统一预测和生成于单一潜在空间，实现了实时、物理信息的逆向微结构生成，计算成本低于传统基于优化的方法

Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.

</details>


### [65] [D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models](https://arxiv.org/abs/2512.15747)
*Javon Hickmon*

Main category: cs.LG

TL;DR: 本文提出D3G方法，通过生成多样化人口统计数据来提升CLIP等预训练多模态模型的零样本图像分类准确性，同时减少人口统计偏见。


<details>
  <summary>Details</summary>
Motivation: 图像分类任务中，低容量模型容易欠拟合，在细粒度分类上表现不佳。现有数据集往往缺乏平衡的人口统计分布，导致预测偏向多数类别，产生有害偏见。特别是在零样本图像分类中，这些问题会加剧人口统计偏见。

Method: 提出D3G（多样化人口统计数据生成）方法，这是一种无需训练、零样本的方法。使用CLIP作为基础多模态模型，Stable Diffusion XL作为生成模型，在推理时生成多样化的人口统计数据来提升分类准确性并减少偏见。

Result: 实验表明，在推理时提供多样化的人口统计数据能够提升模型性能，并探索了不同人口统计特征对准确率指标的影响。

Conclusion: D3G方法能有效提升预训练多模态模型的零样本图像分类准确性，同时减少人口统计偏见，为解决数据集不平衡和模型偏见问题提供了新思路。

Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.

</details>


### [66] [ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning](https://arxiv.org/abs/2512.15756)
*Yoonpyo Lee*

Main category: cs.LG

TL;DR: ReactorFold：将核反应堆燃料组件设计重新定义为语言模型的序列建模问题，通过微调和DPO对齐，模型能够自主调整设计参数并发现传统方法无法触及的非对称配置。


<details>
  <summary>Details</summary>
Motivation: 传统核反应堆堆芯设计方法（确定性方法、元启发式方法、机器学习辅助方法）在固定的人工定义配置空间中搜索，限制了发现全新设计拓扑结构的能力。需要一种能够超越人类设计约束、发现根本性新设计的方法。

Method: 引入ReactorFold生成框架：1) 将燃料组件设计重新定义为语言模型的序列建模问题；2) 使用蒙特卡洛数据进行参数高效微调；3) 采用直接偏好优化(DPO)对齐模型；4) 模型通过单次前向传递生成候选布局。

Result: 1) DPO对齐模型展现出涌现的设计空间扩展能力：尽管仅在固定数量钆可燃吸收体(Gd)棒配置上训练，却能自主调整Gd库存以满足严格的功率峰值约束；2) 发现了挑战传统对称装载启发式的高性能非对称配置；3) 能够访问传统搜索方法无法触及的设计区域。

Conclusion: 语言模型能够内化因果物理关系并超越人类强加的设计约束，为核反应堆设计开辟了新的可能性，展示了生成式AI在复杂工程系统设计中的潜力。

Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.

</details>


### [67] [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)
*A. Quadir,M. Sajid,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种新的多视图孪生限制核机(TMvRKM)模型，通过正则化最小二乘方法替代传统二次规划，有效解决了多视图学习中核方法在高维空间的计算效率和泛化性能问题。


<details>
  <summary>Details</summary>
Motivation: 传统多视图支持向量机(MvSVM)面临两个主要挑战：1) 在高维空间中使用核技巧难以有效捕捉决策边界；2) 容易产生错误且难以处理多视图数据集中的视图不一致性问题。需要一种既能利用核机优势又能解决传统方法计算和泛化挑战的新模型。

Method: 提出TMvRKM模型，采用正则化最小二乘方法替代传统二次规划，通过耦合项平衡多视图间的误差。结合早期和晚期融合策略，在训练时利用所有视图的集体信息，同时保持对单个视图特定变化的灵活性。

Result: 在UCI、KEEL和AwA基准数据集上进行严格测试，实验结果表明TMvRKM在所有场景中都优于基线模型，展现出卓越的泛化性能，统计分析也一致支持这一结论。

Conclusion: TMvRKM成功整合了核机与多视图框架的优势，有效解决了传统核方法的关键计算和泛化挑战，为多视图学习提供了更高效和鲁棒的解决方案。

Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.

</details>


### [68] [Yantra AI -- An intelligence platform which interacts with manufacturing operations](https://arxiv.org/abs/2512.15758)
*Varshini Krishnamurthy*

Main category: cs.LG

TL;DR: 该论文开发了一个用于XRIT的智能生产系统，集成了机器学习模型和AI虚拟助手，以解决能源管理、预测性维护和决策支持等关键问题。


<details>
  <summary>Details</summary>
Motivation: 工业4.0快速发展改变了智能生产，需要实时跟踪、机器学习和AI驱动系统来优化运营。本研究旨在为XRIT创建智能生产系统，解决能源管理、预测性维护和AI决策支持等关键挑战。

Method: 开发了集成机器学习模型的智能生产系统，包括用于主动维护的随机森林分类器和用于异常检测的隔离森林。使用Streamlit实现实时数据可视化仪表板，并集成基于GPT-4的AI虚拟助手提供实时决策支持。

Result: 系统使用模拟数据进行了测试，证明具有可扩展性。结果显示系统显著提高了工作效率、能源管理和维修计划能力。AI虚拟助手使工人能够获得实时有用信息，简化复杂问题并改善运营决策。

Conclusion: 该智能生产系统成功解决了XRIT的关键运营问题。未来工作将专注于实现实时数据集成，并探索其他系统优化方法，以进一步提升生产效率和决策质量。

Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.

</details>


### [69] [Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning](https://arxiv.org/abs/2512.15759)
*Jahidul Arafat*

Main category: cs.LG

TL;DR: SCFA框架通过引入领域知识约束改进联邦学习，在非IID数据下提升收敛速度22%，减少模型发散41.3%，并在差分隐私下保持接近非隐私基线的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下收敛缓慢，现有解决方案对所有客户端更新采用相同处理方式，忽略了语义有效性。需要将领域知识约束融入分布式优化来改善这一问题。

Method: 提出语义约束联邦聚合框架，将领域知识约束融入分布式优化。使用知识图编码ISA-95和MASON本体中的3000个约束，构建理论收敛分析框架。

Result: 理论证明收敛率为O(1/sqrt(T)+ρ)，约束减少数据异质性41%，假设空间减少因子θ=0.37。在ε=10的差分隐私下，性能仅下降3.7%（标准联邦学习下降12.1%）。实验显示收敛速度提升22%，模型发散减少41.3%。

Conclusion: SCFA框架通过语义约束有效改善联邦学习在非IID数据下的性能，提供理论收敛保证，显著提升隐私-效用权衡，并在制造预测性维护等实际应用中验证了有效性。

Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.

</details>


### [70] [Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps](https://arxiv.org/abs/2512.15761)
*Christopher Blum,Michael Neidlin*

Main category: cs.LG

TL;DR: 提出基于CFD流场特征的可解释机器学习框架，用于空间血栓风险评估，通过逻辑回归和特征选择识别关键流场特征，实现高效的血栓风险预测。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以将复杂的血流条件转化为可靠且可解释的血栓风险预测，这反映了对特定流场特征如何促进血栓形成和生长的理解不足。

Method: 引入基于CFD流场特征的可解释机器学习框架，使用逻辑回归模型结合结构化特征选择流程，推导出紧凑且物理可解释的特征集，包括非线性特征组合。

Result: 模型成功复现了标记的风险分布，识别出与血栓风险增加相关的不同流场特征集。即使仅在单个轴向泵工况下训练，应用于离心泵时也能预测合理的血栓易发区域。

Conclusion: 可解释机器学习能够将局部流场特征与血栓风险联系起来，同时保持计算效率和机理透明度。该框架补充了基于物理的血栓建模，为将可解释机器学习集成到CFD驱动的血栓分析和设备设计工作流程提供了方法基础。

Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.

</details>


### [71] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: CSA-TTA：一种跨样本增强测试时适应框架，通过整合其他个体的低血压事件来改善术中低血压的个性化预测，解决了低血压事件稀少导致的测试时训练不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 术中低血压（IOH）具有显著的手术风险，但由于患者特异性变异，准确预测仍然具有挑战性。虽然测试时适应（TTA）为个性化预测提供了有前景的方法，但低血压事件的稀少性常常导致测试时训练不可靠。

Method: 提出CSA-TTA框架：1）构建跨样本库，将历史数据分割为低血压和非低血压样本；2）采用粗到细检索策略构建测试时训练数据：先用K-Shape聚类识别代表性聚类中心，然后基于当前患者信号检索前K个语义相似样本；3）训练时整合自监督掩码重建和回顾性序列预测信号，增强模型对快速细微术中动态的适应性。

Result: 在VitalDB数据集和真实医院数据集上评估，CSA-TTA与TimesFM和UniTS等先进时间序列预测模型集成后，性能持续提升。在VitalDB上，微调场景下召回率和F1分数分别提升+1.33%和+1.13%；零样本场景下分别提升+7.46%和+5.07%，显示出强大的鲁棒性和泛化能力。

Conclusion: CSA-TTA通过跨样本增强有效解决了术中低血压预测中测试时训练不可靠的问题，显著提升了预测性能，为个性化医疗监测提供了有效的解决方案。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [72] [Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic](https://arxiv.org/abs/2512.15765)
*Mélissa Tamine,Otmane Sakhi,Benjamin Heymann*

Main category: cs.LG

TL;DR: 本文提出了一种针对使用DPO训练的大语言模型的高效数据估值方法，通过利用DPO的数学结构简化了Shapley值的计算，解决了传统数据估值方法计算成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 数据是训练大语言模型的关键资产，但数据估值面临两大挑战：数据所有者如何明智地决定数据投资策略，以及多个数据所有者如何公平合作共享数据资源。传统基于合作博弈论和Shapley值的数据估值方法计算成本极高，需要大量模型重新训练，对于大模型来说难以承受。

Method: 本文发现使用直接偏好优化（DPO）训练的大语言模型具有特殊的数学结构，这种结构可以显著简化Shapley值的计算过程。通过利用DPO的特定数学特性，实现了可扩展的数据估值计算，避免了传统方法需要大量模型重新训练的问题。

Result: 研究表明，对于使用DPO训练的大语言模型，数据估值的计算挑战得到了显著简化。这种方法使得原本计算成本极高的Shapley值计算变得可扩展，为大语言模型领域的数据估值应用打开了新的可能性。

Conclusion: 本文提出的方法通过利用DPO的数学结构，解决了大语言模型数据估值中的计算瓶颈问题。这一发现为数据估值与大语言模型的交叉应用开辟了新的途径，有助于数据所有者做出更明智的投资决策，并促进数据资源的公平合作共享。

Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.

</details>


### [73] [TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration](https://arxiv.org/abs/2512.15773)
*Ye Li,Jiahe Feng,Yuan Meng,Kangye Ji,Chen Tang,Xinwan Wen,Shutao Xia,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: TS-DP提出了一种用于扩散策略的时序感知强化学习推测解码框架，通过Transformer草稿器和RL调度器动态适应任务难度变化，实现无损加速，推理速度提升4.17倍，达到25Hz实时控制。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在具身控制中表现出色，但存在推理延迟高、计算成本大的问题。静态加速方法无法处理动态的具身任务，而推测解码作为无损自适应方案在扩散策略中尚未充分探索。需要解决如何在任务难度随时间变化的具身环境中以更低成本匹配基础模型去噪质量，以及如何动态调整计算的问题。

Method: 提出TS-DP框架：1) 使用Transformer草稿器蒸馏基础模型，替代昂贵的去噪调用；2) 基于RL的调度器通过调整推测参数适应时变任务难度，在保持准确性的同时提高效率。

Result: 在多种具身环境中的实验表明，TS-DP实现了高达4.17倍的推理加速，草稿接受率超过94%，推理频率达到25Hz，实现了实时扩散控制且无性能下降。

Conclusion: TS-DP是首个为扩散策略提供时序自适应推测解码的框架，成功解决了动态具身任务中的计算效率问题，实现了实时高性能控制。

Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.

</details>


### [74] [Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence](https://arxiv.org/abs/2512.15780)
*Samruddhi Baviskar*

Main category: cs.LG

TL;DR: 评估金融决策中表格机器学习模型的对抗鲁棒性，发现在小扰动下性能显著下降，对抗训练可部分恢复


<details>
  <summary>Details</summary>
Motivation: 金融决策中的表格机器学习模型（如信用评分和欺诈检测）对对抗攻击的鲁棒性尚未充分研究，需要评估这些模型在对抗扰动下的表现，特别是对歧视性、校准性和金融风险指标的影响

Method: 使用信用评分和欺诈检测数据，应用基于梯度的对抗攻击方法，测量模型在歧视性、校准性和金融风险指标方面的变化，并尝试通过对抗训练来提升鲁棒性

Result: 结果显示，即使在小扰动下，模型性能也会显著下降；对抗训练能够部分恢复模型性能，但无法完全恢复原始性能水平

Conclusion: 金融决策中的表格机器学习模型对对抗攻击较为脆弱，需要开发更有效的鲁棒性增强技术来保护金融系统的安全性和公平性

Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.

</details>


### [75] [Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection](https://arxiv.org/abs/2512.15900)
*Avais Jan,Prakash Chourasia,Sarwan Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 本文系统评估了t-SNE中九种不同核函数在生物序列分析中的表现，发现余弦相似性核在可视化质量和下游分析任务中均优于传统高斯核和隔离核，特别适合大规模生物序列分析。


<details>
  <summary>Details</summary>
Motivation: t-SNE广泛用于高维生物测序数据的可视化分析，但传统高斯核缺乏数据依赖性且计算开销大，限制了其在分类生物序列中的可扩展性和有效性。虽然已有研究提出隔离核作为替代，但它可能无法最优地捕捉序列相似性。

Method: 本研究全面评估了九种不同核函数在t-SNE中的应用，使用三种嵌入方法：One-Hot Encoding、Spike2Vec和minimizers。通过主观可视化和客观指标（包括邻域保持分数）进行评估，并在六个不同的生物数据集上进行广泛的分类和聚类实验。

Result: 余弦相似性核总体上优于其他核函数（包括高斯核和隔离核），在运行时效率和低维空间中的成对距离保持方面表现更优。核选择不仅影响可视化质量，还显著影响下游分析任务。

Conclusion: 余弦相似性核在不同数据类型和嵌入策略中提供最稳健的性能，特别适合大规模生物序列分析，为生物信息学中的t-SNE应用提供了更优的核函数选择。

Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.

</details>


### [76] [A Unification of Discrete, Gaussian, and Simplicial Diffusion](https://arxiv.org/abs/2512.15923)
*Nuria Alina Chandra,Yucen Lily Li,Alan N. Amin,Alex Ali,Joshua Rollins,Sebastian W. Ober,Aniruddh Raghu,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文提出一个统一框架，将离散扩散、高斯扩散和单纯形扩散三种方法统一为Wright-Fisher种群遗传模型的不同参数化形式，解决了单纯形扩散数值不稳定的问题，并实现了单一模型在测试时可在三种域中进行扩散。


<details>
  <summary>Details</summary>
Motivation: 当前离散序列建模（如DNA、蛋白质、语言）存在三种主要扩散方法：离散空间扩散、欧几里得空间高斯扩散和单纯形扩散。这些方法各有优缺点：离散扩散领域最自然，高斯扩散算法更成熟，单纯形扩散理论上结合了前两者的优势但实践中存在数值不稳定性问题。缺乏统一的理论框架使得实践者难以在不同模型间切换。

Method: 构建理论框架，将三种离散扩散方法统一为Wright-Fisher种群遗传模型的不同参数化形式。发现单纯形扩散和高斯扩散是该模型的两个大种群极限。利用数十年数学遗传学文献解锁稳定的单纯形扩散，并开发能够训练单一模型在测试时在任意三种域中进行扩散的方法。

Result: Wright-Fisher单纯形扩散比之前的单纯形扩散模型更稳定，在条件DNA生成任务中表现更好。实验表明，可以在多个域上同时训练模型，其性能与在单个域上训练的模型相当。

Conclusion: 通过将三种离散扩散方法统一为Wright-Fisher种群遗传模型的不同参数化形式，解决了单纯形扩散的数值不稳定问题，建立了模型间的形式化联系，并实现了单一模型在多种域中的灵活应用，为实践者提供了更强大的工具。

Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.

</details>


### [77] [DSO: Direct Steering Optimization for Bias Mitigation](https://arxiv.org/abs/2512.15926)
*Lucas Monteiro Paes,Nivedha Sivakumar,Yinong Oliver Wang,Masha Fedzechkina Donaldson,Luca Zappella,Nicholas Apostoloff*

Main category: cs.LG

TL;DR: 本文提出DSO方法，使用强化学习优化激活转向，在推理时可控地减少VLMs和LLMs中的偏见，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在决策时存在偏见问题（如VLMs难以识别女性医生），现有转向方法难以纠正需要跨人口群体等概率结果的偏见，且用户需要在偏见减少和模型能力之间进行平衡控制。

Method: 提出直接转向优化（DSO）方法，使用强化学习寻找线性变换来引导激活，专门设计用于减少偏见，同时保持对模型性能的控制。

Result: DSO在VLMs和LLMs上都实现了公平性和能力之间的最先进权衡，为从业者提供了推理时对权衡的控制。

Conclusion: 直接优化转向策略比依赖预定义启发式方法更有效地控制模型行为，为偏见干预提供了更有效的解决方案。

Abstract: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.

</details>


### [78] [BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research](https://arxiv.org/abs/2512.15931)
*Tiancheng Gao,Scott C. Lowe,Brendan Furneaux,Angel X Chang,Graham W. Taylor*

Main category: cs.LG

TL;DR: BarcodeMamba+是一个基于状态空间模型架构的真菌DNA条形码分类基础模型，采用预训练-微调范式，在数据稀疏的真菌分类任务中显著优于传统监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 真菌DNA条形码分类面临标签稀疏和长尾分布等极端挑战，传统监督学习方法难以泛化到未见物种，也无法有效捕捉数据的层次结构特性。

Method: 提出BarcodeMamba+基础模型，采用预训练-微调范式利用部分标记数据。在微调阶段系统整合了层次标签平滑、加权损失函数和MycoAI的多头输出层等增强技术。

Result: 在具有明显分类分布偏移的真菌分类基准测试中，最终模型在所有分类级别上都优于现有方法，每个增强组件都带来了显著的性能提升。

Conclusion: 该工作为基于基因组学的生物多样性研究提供了强大新工具，并为这一挑战性领域建立了有效且可扩展的训练范式。

Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.

</details>


### [79] [In-Context Semi-Supervised Learning](https://arxiv.org/abs/2512.15934)
*Jiashuo Fan,Paul Rosu,Aaron T. Wang,Michael Li,Lawrence Carin,Xiang Cheng*

Main category: cs.LG

TL;DR: 该论文研究了Transformer在上下文学习中的半监督学习能力，探索了如何利用未标记的上下文数据来提升在低标签情况下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究主要关注有明确标签对的监督学习场景，但在实践中Transformer即使在标签稀疏或缺失的情况下也能表现良好，这表明未标记的上下文演示中存在着重要结构。研究者希望探索Transformer如何利用未标记上下文进行表示学习。

Method: 引入了上下文半监督学习（IC-SSL）框架，其中包含少量标记示例和大量未标记点。研究Transformer如何利用未标记上下文学习鲁棒的、上下文依赖的表示。

Result: Transformer能够利用未标记上下文学习到鲁棒的表示，这种表示能够实现准确预测，并在低标签情况下显著提升性能表现。

Conclusion: 该研究为理解Transformer如何利用未标记上下文进行表示学习提供了基础性见解，揭示了ICL框架中半监督学习的重要价值。

Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.

</details>


### [80] [Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling](https://arxiv.org/abs/2512.15956)
*John Hateley,Sriram Narasimhan,Omid Abari*

Main category: cs.LG

TL;DR: 提出一种基于商品RFID的森林资产追踪方法，无需预先标记已知位置即可实现GPS级别的定位精度，适用于野火响应应用。


<details>
  <summary>Details</summary>
Motivation: 商品RFID系统在森林环境中由于信号衰减、多径效应和环境变化导致标签定位性能差。现有指纹识别方法需要预先在已知位置部署标签，但在实际应用中往往无法满足这一条件。

Method: 使用高斯过程建模不同环境的RF信号响应特征，构建环境模型字典。提出新的加权对数似然方法，将未知环境与字典中最接近的环境模型匹配，实现无需GPS或相机等额外传感器的标签定位。

Result: 能够实现与GPS相当的定位精度（GPS级别），使用被动商品RFID可同时追踪数十个野火响应资产，无需预先标记已知位置，成本远低于GPS系统。

Conclusion: 该方法提供了一种成本效益高、可扩展的森林资产追踪解决方案，特别适用于野火响应等需要大规模资产追踪的应用场景，突破了传统RFID定位需要已知位置标签的限制。

Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.

</details>


### [81] [Provably Extracting the Features from a General Superposition](https://arxiv.org/abs/2512.15987)
*Allen Liu*

Main category: cs.LG

TL;DR: 提出了一种从叠加特征中学习特征方向的查询算法，能够在过完备情况下恢复非退化特征和函数


<details>
  <summary>Details</summary>
Motivation: 复杂机器学习模型通常通过线性表示编码特征，但这些特征以叠加形式存在，难以恢复。特别是在过完备情况下（特征数量大于维度），现有算法面临挑战

Method: 开发了一种高效的查询算法，通过噪声查询访问函数f，在傅里叶空间中迭代细化搜索空间来定位隐藏方向vi。算法允许任意叠加，只要求不同特征方向不接近相同

Result: 算法能够识别所有非退化响应的特征方向并重构函数f，比所有相关先前结果在更一般的设置下工作

Conclusion: 提出了一种在叠加特征中学习特征方向的有效方法，为过完备情况下的特征恢复提供了新思路

Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,σ_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $σ_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $σ_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.

</details>


### [82] [Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives](https://arxiv.org/abs/2512.15997)
*Robert Stephany,William Michael Anderson,Youngsoo Choi*

Main category: cs.LG

TL;DR: 论文提出了一种结合高阶有限差分格式和Rollout损失函数的方法，用于提升降阶模型在长时间尺度上的预测能力，并在2D Burgers方程上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂偏微分方程通常需要计算昂贵的数值方法。虽然现代降阶模型能够解决参数化PDE族，但其预测能力在长时间尺度上会显著下降，这是当前降阶模型面临的主要挑战。

Method: 提出两阶段方法：(1) 引入灵活、高阶且计算成本低的有限差分格式；(2) 提出Rollout损失函数，训练降阶模型在任意时间尺度上做出准确预测。

Result: 在2D Burgers方程上验证了所提方法的有效性，表明该方法能够显著提升降阶模型在长时间尺度上的预测精度。

Conclusion: 通过结合高阶有限差分格式和Rollout损失函数，成功解决了降阶模型在长时间预测中精度下降的问题，为复杂PDE的高效求解提供了新思路。

Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.

</details>


### [83] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 本文提出将LDA主题模型中的主题数量T选择问题形式化为离散黑盒优化问题，比较了四种优化方法在固定评估预算下的性能表现。


<details>
  <summary>Details</summary>
Motivation: LDA主题模型中主题数量T的选择是一个关键设计决策，它显著影响模型的统计拟合度和可解释性。传统方法通常需要大量试错或启发式规则，缺乏系统化的优化框架。

Method: 将主题数量T选择问题形式化为离散黑盒优化问题，每个函数评估对应训练一个LDA模型并测量其验证困惑度。在固定评估预算下比较了四种优化器：两种手工设计的进化方法（遗传算法GA和进化策略ES）和两种学习的摊销方法（偏好摊销黑盒优化PABBO和锐度感知黑盒优化SABBO）。

Result: 实验表明，虽然GA、ES、PABBO和SABBO最终都能达到相似的困惑度范围，但摊销优化器在样本和时间效率上显著更高。SABBO通常在一次评估后就能识别出接近最优的主题数量，PABBO在几次评估内就能找到有竞争力的配置，而GA和ES需要几乎全部预算才能接近相同区域。

Conclusion: 摊销黑盒优化方法在LDA主题数量选择问题上比传统进化方法更高效，特别是SABBO能够以极少的评估次数找到接近最优的解决方案，为实际应用中的超参数优化提供了更实用的方法。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [84] [Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results](https://arxiv.org/abs/2512.16013)
*Ruolei Zeng,Arun Sharma,Shuai An,Mingzhou Yang,Shengya Zhang,Licheng Liu,David Mulla,Shashi Shekhar*

Main category: cs.LG

TL;DR: FTBSC-KGML是一个结合预训练-微调过程、空间变异感知和知识引导的机器学习框架，用于准确量化农业生态系统碳循环，特别关注利用迁移学习和空间异质性来提高区域适用性。


<details>
  <summary>Details</summary>
Motivation: 在决策相关尺度上准确且经济有效地量化农业生态系统碳循环对于气候缓解和可持续农业至关重要。然而，该领域的迁移学习和空间变异利用面临挑战，涉及异构数据和复杂的跨尺度依赖关系。传统方法通常依赖位置无关的参数化和独立训练，未能充分利用迁移学习和输入的空间异质性，限制了在具有显著变异性的区域的适用性。

Method: 提出FTBSC-KGML框架，将KGML-ag扩展为包含预训练-微调过程和站点特定参数。该框架使用遥感GPP、气候和土壤协变量数据，通过全局预训练模型在各州或站点进行微调，学习位置感知表示。关键组件是空间异质性感知的迁移学习方案，在有限数据下提高局部准确性而不牺牲可解释性。

Result: 实证表明，FTBSC-KGML比纯全局模型具有更低的验证误差和更高的一致性解释能力，能更好地捕捉各州间的空间变异性。

Conclusion: FTBSC-KGML扩展了先前的SDSA-KGML框架，通过结合预训练-微调过程和空间变异感知的迁移学习，提高了农业生态系统碳循环量化的准确性和区域适用性，特别是在数据有限的情况下。

Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.

</details>


### [85] [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912)
*Peter Chen,Xiaopeng Li,Ziniu Li,Wotao Yin,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: 本文研究了强化学习可验证奖励（RLVR）框架中的探索-利用权衡，揭示了虚假奖励和熵最小化如何通过看似矛盾的方式提升大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 最近研究表明RLVR可以通过两种看似矛盾的机制激发大语言模型的数学推理能力：虚假奖励（通过奖励与真实结果无关的输出来抑制利用）和熵最小化（通过推动模型产生更自信、确定性的输出来抑制探索）。这两种机制都提升了推理性能，但其背后的原理尚不清楚。

Method: 研究聚焦两个基本问题：1）策略熵与性能的关系；2）虚假奖励是否通过裁剪偏差和模型污染的相互作用产生收益。通过分析裁剪偏差如何降低策略熵，以及提出奖励错配模型来解释虚假奖励的增强效果。

Result: 结果显示：在虚假奖励下，裁剪偏差会降低策略熵，导致更自信和确定性的输出；而单独的熵最小化不足以带来改进。奖励错配模型解释了为什么虚假奖励能在污染设置之外增强性能。

Conclusion: 研究阐明了虚假奖励获益的机制，为更有效的RLVR训练提供了原则性指导，澄清了探索-利用权衡在提升大语言模型推理能力中的作用。

Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

</details>


### [86] [CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting](https://arxiv.org/abs/2512.16046)
*Shu Wan,Reepal Shah,John Sabo,Huan Liu,K. Selçuk Candan*

Main category: cs.LG

TL;DR: CauSTream是一个用于因果时空径流预测的统一框架，通过联合学习径流因果图和路由图来提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在径流预测中忽略了物理过程，限制了可解释性和泛化能力；而现有的因果学习方法通常依赖固定的因果图，无法适应数据变化。

Method: 提出CauSTream框架，联合学习气象强迫与径流之间的因果图，以及站点间动态依赖关系的路由图，并在非参数设置下建立了因果结构的可识别性条件。

Result: 在三个美国主要流域和三个预测时间尺度上的评估显示，CauSTream始终优于现有最先进方法，预测窗口越长性能优势越明显；学习到的因果图与领域知识高度一致。

Conclusion: CauSTream为因果时空建模提供了原则性基础，能够扩展到广泛的科学和环境应用中，同时提供对流域动力学的可解释洞察。

Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.

</details>


### [87] [In-Context Multi-Operator Learning with DeepOSets](https://arxiv.org/abs/2512.16074)
*Shao-Ting Chiu,Aditya Nambiar,Ali Syed,Jonathan W. Siegel,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: DeepOSets架构通过适当修改，成为多算子上下文学习器，能够从用户提示中的参数-解示例对中恢复新PDE的解算子，无需权重更新，且是连续算子类的通用均匀逼近器。


<details>
  <summary>Details</summary>
Motivation: 探索非自回归、非注意力机制的DeepOSets架构在科学机器学习中的上下文学习能力，特别是在求解未见过的PDE问题时的算子学习能力。

Method: 结合DeepSets的集合学习和DeepONets的算子学习，构建DeepOSets架构，通过适当修改使其成为多算子上下文学习器，能够从提示中的示例对中学习PDE解算子。

Result: DeepOSets能够准确预测训练中未见过的PDE的参数对应解，在泊松方程和反应扩散方程的正反边值问题实验中表现出色，且被证明是连续算子类的通用均匀逼近器。

Conclusion: DeepOSets架构展示了强大的上下文学习能力，能够从示例中学习新PDE的解算子，为科学机器学习中的算子学习提供了新的非注意力机制方法。

Abstract: In-context Learning (ICL) is the remarkable capability displayed by some machine learning models to learn from examples in a prompt, without any further weight updates. ICL had originally been thought to emerge from the self-attention mechanism in autoregressive transformer architectures. DeepOSets is a non-autoregressive, non-attention based neural architecture that combines set learning via the DeepSets architecture with operator learning via Deep Operator Networks (DeepONets). In a previous study, DeepOSets was shown to display ICL capabilities in supervised learning problems. In this paper, we show that the DeepOSets architecture, with the appropriate modifications, is a multi-operator in-context learner that can recover the solution operator of a new PDE, not seen during training, from example pairs of parameter and solution placed in a user prompt, without any weight updates. Furthermore, we show that DeepOSets is a universal uniform approximator over a class of continuous operators, which we believe is the first result of its kind in the literature of scientific machine learning. This means that a single DeepOSets architecture exists that approximates in-context any continuous operator in the class to any fixed desired degree accuracy, given an appropriate number of examples in the prompt. Experiments with Poisson and reaction-diffusion forward and inverse boundary-value problems demonstrate the ability of the proposed model to use in-context examples to predict accurately the solutions corresponding to parameter queries for PDEs not seen during training.

</details>


### [88] [Privacy Blur: Quantifying Privacy and Utility for Image Data Release](https://arxiv.org/abs/2512.16086)
*Saeed Mahloujifar,Narine Kokhlikyan,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 研究发现高斯模糊在低精度实现中可被逆向攻击，隐私保护效果最差；像素化和像素化加噪声在适当粒度下能同时保证隐私和模型训练实用性。


<details>
  <summary>Details</summary>
Motivation: 图像数据常包含人脸、车牌等隐私信息，需要在数据发布时隐藏这些信息，同时保持数据对模型训练的有效性。当前行业标准的高斯模糊方法存在隐私风险。

Method: 比较了四种隐私保护算法：高斯模糊、像素化、像素化加噪声（DP-Pix）和裁剪。通过逆向攻击和识别攻击评估隐私性，通过在模糊化数据上训练模型的质量评估实用性。

Result: 高斯模糊在低精度实现中最容易被逆向攻击，隐私保护效果最差。像素化和像素化加噪声在适当粒度下能同时提供良好的隐私保护和模型训练实用性。

Conclusion: 高斯模糊作为行业标准方法存在严重隐私风险，像素化和像素化加噪声是更好的替代方案。作者开发了Privacy Blur软件包提供这些方法及推荐参数。

Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.

</details>


### [89] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: AIMM是一个AI驱动的框架，通过融合Reddit活动、机器人指标、协调模式和OHLCV市场特征，为每个股票代码生成每日操纵风险评分，用于检测社交媒体驱动的市场操纵。


<details>
  <summary>Details</summary>
Motivation: 市场操纵现在经常源于协调的社交媒体活动，而非孤立的交易。零售投资者、监管机构和经纪商需要能够将在线叙事和协调模式与市场行为联系起来的工具。

Method: 1. 构建AIMM框架：融合Reddit活动、机器人和协调指标、OHLCV市场特征，生成每日AIMM操纵风险评分
2. 使用parquet原生管道和Streamlit仪表板，允许分析师探索可疑窗口、检查底层帖子和价格行为
3. 由于Reddit API限制，使用校准的合成社交特征匹配文档化的事件特征
4. 市场数据使用来自Yahoo Finance的真实历史数据

Result: 1. 构建了AIMM Ground Truth数据集（AIMM-GT）：33个标记的股票-天数，涵盖8只股票
2. 实现了前向行走评估和前瞻性预测记录，用于回顾和部署式评估
3. 分析领先时间显示，AIMM在2021年1月GME挤压峰值前22天发出了警告
4. 虽然标记集较小（33个股票-天数，3个正面事件），但结果显示了初步的判别能力和对GME事件的早期预警

Conclusion: AIMM框架展示了检测社交媒体驱动市场操纵的潜力，特别是在GME事件中表现出早期预警能力。作者发布了代码、数据集模式和仪表板设计，以支持社交媒体驱动的市场监控研究。

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [90] [BUILD with Precision: Bottom-Up Inference of Linear DAGs](https://arxiv.org/abs/2512.16111)
*Hamed Ajorlou,Samuel Rey,Gonzalo Mateos,Geert Leus,Antonio G. Marques*

Main category: cs.LG

TL;DR: BUILD算法通过利用观测数据的精度矩阵结构，自底向上逐步识别叶节点并剪枝，实现线性高斯结构方程模型下的DAG确定性重建。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中学习有向无环图结构是因果发现、统计信号处理和机器学习的核心问题。在线性高斯结构方程模型且噪声方差相等的条件下，该问题是可识别的，但需要有效算法来准确重建DAG。

Method: 提出BUILD算法：利用观测数据精度矩阵的独特结构，自底向上逐步识别叶节点及其父节点，然后通过移除入射边来剪枝叶节点，重复此过程直至完全重建DAG。为处理有限数据估计精度矩阵时的误差累积问题，采用周期性重新估计精度矩阵的策略。

Result: 在具有挑战性的合成基准测试中，BUILD算法与最先进的DAG学习算法相比表现优异，同时提供了明确的复杂度控制。

Conclusion: BUILD算法利用精度矩阵的结构特性，提供了一种确定性的自底向上DAG重建方法，在准确性和计算效率之间取得了良好平衡，为线性高斯SEM下的因果发现提供了有效工具。

Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.

</details>


### [91] [Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure](https://arxiv.org/abs/2512.16126)
*Lulu Xue,Shengshan Hu,Linqiang Qian,Peijin Guo,Yechao Zhang,Minghui Li,Yanjun Zhang,Dayong Ye,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 论文揭示了机器学习遗忘技术中保留数据的隐私风险，首次提出双视图设置下的隐私泄露问题，并开发了DVIA攻击方法来验证这种威胁。


<details>
  <summary>Details</summary>
Motivation: 机器学习遗忘技术虽然能保护被遗忘数据的隐私，但可能对保留数据引入新的隐私风险。现有研究主要关注被遗忘数据的隐私，而保留数据的风险尚未充分探索。

Method: 从信息论角度提出"隐私知识增益"概念，证明双视图设置能让攻击者获得比单独查询任一模型更多的信息。开发了DVIA（双视图推理攻击）方法，通过黑盒查询两个模型，使用轻量级似然比推理模块提取保留数据的成员信息。

Result: 在不同数据集和模型架构上的实验验证了DVIA的有效性，证实了双视图设置确实会放大隐私泄露风险，攻击者能成功推断保留数据的成员信息。

Conclusion: 机器学习遗忘技术不仅影响被遗忘数据的隐私，还会对保留数据造成新的隐私威胁。双视图设置显著增加了隐私泄露风险，需要在设计遗忘机制时考虑这种威胁。

Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.

</details>


### [92] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-3是一个106B参数的MoE模型（12B激活），通过大规模强化学习训练，在数学、代码、科学和推理基准测试中达到同类尺寸模型的最先进性能，超越了更大的前沿模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效的大规模强化学习基础设施，用于训练高性能的混合专家模型，并在多个技术领域实现最先进的性能，同时开源整个技术栈以促进社区发展。

Method: 1. 构建端到端RL基础设施栈，包括prime-rl框架（支持从单节点到数千GPU的异步强化学习）；2. 使用verifiers库构建的环境集合进行训练和评估；3. 在GLM-4.5-Air-Base模型基础上进行SFT和RL训练；4. 使用512个H200 GPU进行高效RL训练。

Result: INTELLECT-3在数学、代码、科学和推理基准测试中达到同类尺寸模型的最先进性能，超越了更大的前沿模型。训练效率高，可扩展到512个H200 GPU。

Conclusion: 成功开发了INTELLECT-3模型和完整的强化学习基础设施栈，证明了大规模异步强化学习在训练高性能MoE模型方面的有效性，并开源了整个技术栈以推动社区发展。

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [93] [Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions](https://arxiv.org/abs/2512.16200)
*Haishan Ye*

Main category: cs.LG

TL;DR: 本文分析了基于排序的零阶优化算法，首次建立了显式的非渐近查询复杂度界限，填补了该领域理论分析的空白。


<details>
  <summary>Details</summary>
Motivation: 基于排序的零阶优化方法（如CMA-ES、自然进化策略等）在实践中广泛应用且表现优异，但现有理论分析仅限于渐近结果，缺乏显式的收敛速率分析，特别是对于选择top-k方向的算法。本文旨在填补这一理论空白。

Method: 分析一个简单的基于排序的零阶优化算法，采用新颖的分析方法，避免了传统的漂移和信息几何技术，建立了显式的非渐近查询复杂度界限。

Result: 对于d维问题：1）若函数是L-光滑且μ-强凸的，算法达到$\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$查询复杂度找到ε-次优解；2）对于光滑非凸目标，达到$\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$查询复杂度。这些结果以至少1-δ的概率成立。

Conclusion: 本文首次为基于排序的零阶优化算法建立了显式的非渐近查询复杂度界限，为理解这类启发式方法为何能实现高效优化提供了新的理论洞见，填补了该领域长期存在的理论空白。

Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $μ$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-δ$ with $0<δ<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.

</details>


### [94] [Neural emulation of gravity-driven geohazard runout](https://arxiv.org/abs/2512.16221)
*Lorenzo Nava,Ye Chen,Maximillian Van Wyk de Vries*

Main category: cs.LG

TL;DR: 该研究开发了一个机器学习模型，能够快速准确地预测地质灾害（如滑坡、雪崩）的运移范围，计算速度比传统数值解法快100-10,000倍，为大规模灾害预警提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 地质灾害（滑坡、雪崩等）的快速运移预测对保护生命、基础设施和生态系统至关重要。现有预测方法面临计算速度与物理真实性之间的根本性权衡，难以实现既快速又准确的预测。

Method: 训练机器学习模型来预测地质灾害运移，模型基于超过100,000个数值模拟数据，覆盖10,000多个真实世界数字高程模型区域，能够预测流动范围和沉积厚度。

Result: 模型能够高精度预测流动范围和沉积厚度，计算速度比数值解法快100-10,000倍，能够重现关键物理行为（如分流和沉积模式），并在不同流动类型、规模和地形中表现出良好的泛化能力。

Conclusion: 神经仿真技术能够实现跨多样真实地形的快速、空间解析的运移预测，为灾害风险降低和基于影响的预报开辟了新机遇，是扩展物理真实地质灾害建模至大规模预警系统相关时空尺度的有前景途径。

Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.

</details>


### [95] [Sharpness-aware Federated Graph Learning](https://arxiv.org/abs/2512.16247)
*Ruiyu Li,Peige Zhao,Guangxia Li,Pengcheng Wu,Xingyu Gao,Zhiqiang Xu*

Main category: cs.LG

TL;DR: SEAL算法通过同时最小化损失函数及其锐度，并引入基于局部表示相关矩阵的正则化器，解决联邦图学习中数据异构性和维度崩溃问题，提升GNN模型的分类准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习面临两个核心挑战：1）基于经验风险最小化的优化器导致局部模型陷入尖锐谷地，削弱了对分布外图数据的泛化能力；2）局部图数据学习表示中的维度崩溃对GNN模型的分类能力产生负面影响。

Method: 提出SEAL算法，包含两个关键组件：1）制定考虑局部GNN模型锐度的优化目标，同时最小化损失函数及其锐度，寻找平坦区域中的模型参数；2）引入基于局部表示相关矩阵的正则化器，松弛单个局部图样本生成表示之间的相关性，缓解学习模型的维度崩溃。

Result: 在多个图分类基准测试中，SEAL算法持续优于现有的最先进联邦图学习基线方法，并为更多参与者提供了性能增益。

Conclusion: SEAL算法通过同时优化损失锐度和缓解维度崩溃，有效提升了联邦图学习中局部GNN模型的分类准确性和泛化能力，解决了数据异构性带来的挑战。

Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.

</details>


### [96] [Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data](https://arxiv.org/abs/2512.16277)
*Jialiang Wang,Xueyan Bao,Hao Wu*

Main category: cs.LG

TL;DR: 提出Sharpness-aware SLF模型，通过Hessian-vector products获取二阶信息，并在曲率中注入锐度项，有效解决二阶潜在因子模型的优化难题


<details>
  <summary>Details</summary>
Motivation: 二阶潜在因子模型在从高维不完整数据中提取节点交互模式方面有效，但其双线性和非凸性质导致优化困难。Sharpness-aware Minimization方法通过寻找平坦局部最小值来改善表示学习模型的泛化能力

Method: 提出Sharpness-aware SLF模型，包含两个关键思想：1) 通过Hessian-vector products获取二阶信息；2) 通过设计的Hessian-vector products在曲率中注入锐度项

Result: 在多个工业数据集上的实验表明，所提出的模型持续优于最先进的基线方法

Conclusion: Sharpness-aware SLF模型通过结合二阶信息和锐度感知优化，有效解决了二阶潜在因子模型的优化难题，提高了模型的性能

Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.

</details>


### [97] [Feature-Selective Representation Misdirection for Machine Unlearning](https://arxiv.org/abs/2512.16297)
*Taozhao Chen,Linghan Huang,Kim-Kwang Raymond Choo,Huaming Chen*

Main category: cs.LG

TL;DR: SRMU提出了一种新的选择性表示误导遗忘框架，通过特征感知和方向控制的扰动，在高度纠缠的数据分布中实现安全知识遗忘，同时保持模型在良性任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在安全关键和受监管领域的应用增加，模型中敏感或禁止知识的保留带来了隐私泄露、监管不合规和潜在滥用等风险。现有遗忘技术假设遗忘和保留数据集完全分离，这在现实操作环境中难以实现，因为数据分布高度纠缠，导致现有方法要么损害模型性能，要么无法确保安全性。

Method: 提出选择性表示误导遗忘（SRMU）框架，采用激活编辑方法，使用结构化误导向量和激活重要性图，对模型进行特征感知和方向控制的扰动。与无差别的模型权重扰动不同，SRMU选择性地抑制有害表示，同时保留对良性表示的效用。

Result: 在WMDP基准测试中，SRMU在低纠缠和高纠缠配置下都实现了最先进的遗忘性能，且效用损失最小。在20-30%数据重叠的情况下仍保持有效，而现有基线方法在此情况下失效。SRMU为安全驱动的模型治理、隐私合规和受控知识移除提供了坚实基础。

Conclusion: SRMU为解决大语言模型在现实世界应用中安全知识遗忘的挑战提供了一个有效且鲁棒的解决方案，特别是在数据分布高度纠缠的操作环境中，能够平衡遗忘效果和模型性能的保持。

Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.

</details>


### [98] [Multivariate Uncertainty Quantification with Tomographic Quantile Forests](https://arxiv.org/abs/2512.16383)
*Takuya Kanazawa*

Main category: cs.LG

TL;DR: Tomographic Quantile Forests (TQF) 是一种用于多变量目标回归的非参数树模型，通过方向投影学习条件分位数，并利用切片Wasserstein距离重建多变量条件分布。


<details>
  <summary>Details</summary>
Motivation: 量化预测不确定性对于AI在真实世界中的安全可信部署至关重要，但多变量目标的完全非参数条件分布估计仍然具有挑战性。

Method: TQF学习方向投影n⊤y的条件分位数作为输入x和单位方向n的函数。在推理时，通过聚合多个方向的分位数，并使用高效的交替方案最小化切片Wasserstein距离来重建多变量条件分布。

Result: TQF在合成和真实世界数据集上进行了评估，相比经典方向分位数方法，TQF能够用单一模型覆盖所有方向而不施加凸性限制。

Conclusion: TQF提供了一种非参数、不确定性感知的多变量目标回归方法，解决了传统方法需要为不同方向训练单独模型且只能产生凸分位数区域的问题。

Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.

</details>


### [99] [Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference](https://arxiv.org/abs/2512.16391)
*Dhruv Deshmukh,Saurabh Goyal,Nipun Kwatra,Ramachandran Ramjee*

Main category: cs.LG

TL;DR: Kascade是一种无需训练的稀疏注意力方法，通过选择性地在锚定层计算精确Top-k索引并在中间层重用这些索引，显著降低长上下文LLM推理的注意力计算延迟。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是长上下文LLM推理中的主要延迟来源，特别是在推理模型和RAG等日益流行的工作负载中。需要一种高效的方法来减少注意力计算开销。

Method: Kascade利用两个关键观察：1）后softmax注意力本质上是稀疏的；2）高权重键的身份在相邻层之间是稳定的。该方法在锚定层计算精确Top-k索引，然后在中间重用层重用这些索引。锚定层通过动态规划算法选择，以最大化开发集上的跨层相似性。实现考虑了高效约束（如瓦片级操作），支持预填充和解码注意力，且Top-k选择和重用是头感知的。

Result: 在H100 GPU上，Kascade相比FlashAttention-3基线，解码注意力实现了最高4.1倍加速，预填充注意力实现了2.2倍加速。在LongBench和AIME-24等长上下文基准测试中，准确率与密集注意力非常接近。

Conclusion: Kascade是一种有效的训练无关稀疏注意力方法，能够显著加速长上下文LLM推理，同时保持高准确率，易于跨模型部署。

Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.

</details>


### [100] [Emergent Bias and Fairness in Multi-Agent Decision Systems](https://arxiv.org/abs/2512.16433)
*Maeve Madigan,Parameswaran Kamalaruban,Glenn Moynihan,Tom Kempton,David Sutton,Stuart Burrell*

Main category: cs.LG

TL;DR: 该论文提出需要为多智能体预测系统开发公平性评估方法，特别是在金融表格数据领域，通过大规模模拟揭示了多智能体系统中出现的集体偏见行为，这些偏见无法追溯到单个智能体组件。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统通过协作决策提高了各种预测任务的性能，但缺乏有效的评估方法使得难以估计偏见风险，这在消费者金融等高风险领域尤其危险，因为偏见决策可能导致监管违规和财务损失。

Method: 开发多智能体预测系统的公平性评估方法，在金融表格数据领域测量这些系统的公平性特征。通过大规模模拟，在不同多智能体配置（包括不同的通信和协作机制）下检查公平性指标。

Result: 研究揭示了金融决策中出现的偏见模式，这些偏见无法追溯到单个智能体组件，表明多智能体系统可能表现出真正的集体行为。公平性风险成为金融多智能体系统中模型风险的重要组成部分，对信用评分和收入估计等任务产生实际影响。

Conclusion: 多智能体决策系统必须作为整体实体进行评估，而不是通过对其组成组件的还原主义分析。公平性风险是金融多智能体系统模型风险的重要组成部分，需要专门的评估方法。

Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.

</details>


### [101] [Persistent Multiscale Density-based Clustering](https://arxiv.org/abs/2512.16558)
*Daniël Bot,Leland McInnes,Jan Aerts*

Main category: cs.LG

TL;DR: PLSCAN是一种新型密度聚类算法，通过识别HDBSCAN*产生稳定簇的所有最小簇大小，无需手动选择超参数，在多个真实数据集上表现优于HDBSCAN*。


<details>
  <summary>Details</summary>
Motivation: 密度聚类算法在探索性数据分析中很重要，但实际应用时需要选择超参数（如DBSCAN的密度阈值、HDBSCAN*的最小簇大小），这在没有先验知识的情况下很困难。

Method: 提出PLSCAN算法，基于尺度空间聚类原理，等价于新型度量空间上的持续同调，能高效识别HDBSCAN*产生稳定簇的所有最小簇大小。

Result: 在多个真实数据集上，PLSCAN比HDBSCAN*获得更高的平均ARI（调整兰德指数），对互可达邻居数量的变化更不敏感；在低维数据集上计算成本与k-Means相当，高维时与HDBSCAN*相似。

Conclusion: PLSCAN是一种有效的密度聚类算法，能自动确定合适的聚类参数，减少对先验知识的依赖，在真实数据集上表现优于现有方法。

Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.

</details>


### [102] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: Abacus：一种用于展示广告的自监督预训练方法，通过预测用户事件的频率分布来增强深度序列模型，解决用户购买行为建模中的稀疏性和随机性问题。


<details>
  <summary>Details</summary>
Motivation: 展示广告系统中用户购买行为建模面临两大挑战：1）用户正事件稀疏且行为随机，导致严重的类别不平衡和不规则事件时序；2）现有预测系统依赖手工制作的"计数器"特征，忽略了用户意图的细粒度时间演化，而当前序列模型又缺少有用的事件计数统计信息。

Method: 提出Abacus方法，通过自监督预训练预测用户事件的频率分布。进一步提出混合目标函数，将Abacus与序列学习目标统一，结合聚合统计的稳定性和序列建模的敏感性。

Result: 在两个真实世界数据集上的实验表明：1）Abacus预训练优于现有方法，加速了下游任务的收敛；2）混合方法相比基线在AUC指标上提升了高达6.1%。

Conclusion: Abacus通过自监督预训练预测用户事件频率分布，有效解决了展示广告中用户行为建模的挑战，结合序列建模的混合方法显著提升了预测性能。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [103] [Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario](https://arxiv.org/abs/2512.16648)
*Liu Yang,Qiang Li,Luxiong Wen,Jian Yang*

Main category: cs.LG

TL;DR: 本文提出MS-SHOT方法解决源数据不可用的跨接收器射频指纹识别问题，通过动量中心引导的软伪标签和全局结构约束，在目标域无标签且存在分布偏移时实现有效模型适应。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中基于深度学习的射频指纹识别模型在实际部署时面临关键挑战：当模型应用于具有不同硬件特性的接收器时，由于接收器变化引入的分布偏移，性能会显著下降。现有方法需要源域数据，但在实际场景中源数据可能不可用。

Method: 提出MS-SHOT方法：1) 建立基于约束伪标签的SCRFFI适应框架；2) 采用动量中心引导的软伪标签技术；3) 实施全局结构约束以鼓励自信和多样化的预测；4) 特别处理目标域中标签偏移或非均匀类别分布的情况。

Result: 在真实世界数据集上的大量实验表明，MS-SHOT在准确性和鲁棒性方面持续优于现有方法，为目标域存在标签偏移或未知非均匀类别分布的场景提供了有效解决方案。

Conclusion: MS-SHOT为源数据不可用的跨接收器射频指纹识别适应提供了一个实用且可扩展的解决方案，通过理论分析和实验验证了其在处理分布偏移和标签偏移方面的有效性。

Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.

</details>


### [104] [Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification](https://arxiv.org/abs/2512.16687)
*Natnael Tilahun Sinshaw,Mengmei He,Tadesse K. Bahiru,Sudhir Kumar Mohapatra*

Main category: cs.LG

TL;DR: 该研究比较了多种机器学习算法（SVM、NB、LR、AdaBoost、XGBoost、SVM_R）与神经符号AI（NeSy）在文本分类任务中的性能，同时探索了不同的文本表示方法和特征提取技术。


<details>
  <summary>Details</summary>
Motivation: 文本分类（如博客性别分类）是机器学习中成熟的研究领域，在市场分析、客户推荐等应用中具有重要价值。研究旨在比较传统机器学习算法与新兴的神经符号AI方法在文本分类任务中的表现。

Method: 采用多种机器学习算法（SVM、Naive Bayes、Logistic Regression、AdaBoost、XGBoost、SVM_R）与神经符号AI（NeSy）进行对比。探索了TF-IDF、Universal Sentence Encoder、RoBERTa等文本表示方法，以及Chi-Square、Mutual Information、Principal Component Analysis等特征提取技术。

Result: 实验结果表明，神经符号AI（NeSy）方法在有限数据集上能够达到与多层感知器（MLP）相当的性能表现。

Conclusion: 神经符号AI在文本分类任务中显示出潜力，未来研究将扩展知识库、嵌入类型范围和超参数配置，以进一步研究NeSy方法的有效性。

Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.

</details>


### [105] [CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies](https://arxiv.org/abs/2512.16700)
*John M. Statheros,Hairong Wang,Richard Klein*

Main category: cs.LG

TL;DR: CLARiTy是一种基于视觉Transformer的模型，用于胸部X光的多标签分类和弱监督定位，通过类特定token和SegmentCAM模块实现，在NIH ChestX-ray14数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 胸部X光解读面临多标签病理分类和空间定位的挑战，通常受限于区域级（密集）标注的稀缺性，需要开发能够在弱监督条件下同时完成这两项任务的模型。

Method: CLARiTy采用视觉Transformer架构，使用多个类特定token生成区分性注意力图，结合SegmentCAM模块利用解剖先验进行前景分割和背景抑制，通过ConvNeXtV2教师模型蒸馏提高效率。

Result: 在NIH ChestX-ray14数据集上，CLARiTy-S-16-512在14种病理分类中达到竞争性性能，在8种病理的弱监督定位中比先前方法提升50.7%，尤其在小病理（如结节和肿块）上表现突出。

Conclusion: CLARiTy通过结合ViT的全局上下文建模和卷积背景抑制，超越了CNN-ViT混合方法，实现了精确、降噪的热图生成，在低资源环境下具有应用潜力。

Abstract: The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.

</details>


### [106] [Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library](https://arxiv.org/abs/2512.16715)
*Oliver Stritzel,Nick Hühnerbein,Simon Rauch,Itzel Zarate,Lukas Fleischmann,Moike Buck,Attila Lischka,Christian Frey*

Main category: cs.LG

TL;DR: SPICE是一个Python框架，重新实现了三种基于深度学习的预测过程挖掘基线方法，通过统一框架设计实现可复现、可配置的模型比较。


<details>
  <summary>Details</summary>
Motivation: 现有预测过程挖掘方法缺乏可复现性、决策透明度、对新数据集的适应性以及标准化基准测试，导致不同实现之间的比较困难。

Method: 提出SPICE框架，在PyTorch中重新实现三种流行的深度学习基线方法，设计具有严格可配置性的通用基础框架，支持可复现的模型比较。

Result: 在11个数据集上比较SPICE与原始报告指标，并使用公平指标进行评估，验证了框架的有效性。

Conclusion: SPICE框架为预测过程挖掘研究提供了可复现、可配置的基准测试平台，有助于未来建模方法的稳健比较。

Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.

</details>


### [107] [Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation](https://arxiv.org/abs/2512.16718)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文提出了一种级联架构，由多谐样条包组成，解决了之前方法在计算复杂度和高维输入空间方面的限制。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明机器学习回归问题可以在随机函数理论框架下解决，最优核函数通过对称性和无差别原理解析推导得出，与多谐样条一致。但该方法的直接应用受到O(N^3)计算成本和输入空间维度过高时原始理论假设失效的限制。

Method: 提出一种级联架构，由多谐样条包构建而成。该架构同时解决了可扩展性问题，并为具有未知内在低维性的问题提供了理论依据。提出了用于前向计算和通过级联进行端到端微分的高效矩阵程序。

Result: 该方法解决了先前方法的计算复杂度和高维输入空间限制问题，为具有未知内在低维性的回归问题提供了可扩展的解决方案。

Conclusion: 通过多谐样条包构建的级联架构为机器学习回归问题提供了既具有理论依据又具有计算效率的解决方案，特别适用于具有未知内在低维性的高维问题。

Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.

</details>


### [108] [Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis](https://arxiv.org/abs/2512.16742)
*Wisnu Uriawan,Muhamad Veva Ramadhan,Firman Adi Nugraha,Hasbi Nur Wahid,M Dantha Arianvasya,Muhammad Zaki Alghifari*

Main category: cs.LG

TL;DR: 研究通过机器学习算法自动验证印尼朝觐和副朝服务移动应用的真实性，SVM算法表现最佳，准确率达92.3%，结合文本分析和权限元数据的混合特征提取方法能有效识别欺诈应用。


<details>
  <summary>Details</summary>
Motivation: 印尼朝觐和副朝服务的快速数字化虽然便利了朝觐者，但也催生了通过假冒移动应用进行的数字欺诈。这些欺诈应用不仅造成经济损失，还通过收集敏感个人数据带来严重的隐私风险。本研究旨在解决这一关键问题。

Method: 采用混合特征提取方法，结合应用描述的文本分析（TF-IDF）和敏感访问权限的元数据分析。使用包含官方应用和非官方应用的综合数据集，比较三种分类器的性能：支持向量机（SVM）、随机森林（RF）和朴素贝叶斯（NB）。

Result: SVM算法表现最佳，准确率达到92.3%，精确度为91.5%，F1分数为92.0%。特征分析显示，与合法性相关的特定关键词和高风险权限（如READ PHONE STATE）是最重要的区分特征。

Conclusion: 该系统被提议作为一种主动、可扩展的解决方案，用于增强宗教旅游领域的数字信任，并可能作为国家验证系统的原型。机器学习方法能有效识别欺诈应用，保护朝觐者免受数字欺诈和隐私侵犯。

Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.

</details>


### [109] [NRGPT: An Energy-based Alternative for GPT](https://arxiv.org/abs/2512.16762)
*Nima Dehmamy,Benjamin Hoover,Bishwajit Saha,Leo Kozachkov,Jean-Jacques Slotine,Dmitry Krotov*

Main category: cs.LG

TL;DR: NRGPT模型将GPT架构与基于能量的建模（EBM）框架结合，通过能量景观探索进行推理，在简单语言、代数任务和语言建模任务中表现良好，且具有更强的抗过拟合能力。


<details>
  <summary>Details</summary>
Motivation: GPT架构是当前最流行的语言建模设计，而基于能量的建模（EBM）将推理视为在能量景观上的动态过程。研究者希望将这两种范式统一起来，探索结合两者的优势。

Method: 提出eNeRgy-GPT（NRGPT）模型，对GPT设置进行最小修改以与EBM框架统一。模型的推理步骤被概念化为在能量景观上对token的探索，在某些条件下这种探索会变成梯度下降。

Result: 模型在简单语言（莎士比亚数据集）、代数ListOPS任务以及更丰富的OpenWebText语言建模任务中表现良好。研究还发现模型具有更强的抗过拟合能力，只在非常长的训练过程中才会出现过拟合。

Conclusion: 成功将GPT架构与基于能量的建模框架统一，提出的NRGPT模型在多种语言任务中表现优异，并展现出更强的鲁棒性，为语言建模提供了新的视角和方法。

Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.

</details>


### [110] [Pattern recognition in complex systems via vector-field representations of spatio-temporal data](https://arxiv.org/abs/2512.16763)
*Ingrid Amaranta Membrillo Solis,Maria van Rossem,Tristan Madeleine,Tetiana Orlova,Nina Podoliak,Giampaolo D'Alessandro,Jacek Brodzki,Malgosia Kaczmarek*

Main category: cs.LG

TL;DR: 提出一种基于离散测度空间上向量场理论的几何框架，用于分析复杂系统的时空数据，包含一个两参数度量族，支持图像、图像梯度、图上的函数等数据类型，能有效解决维度约简、模态分解、相空间重构等分析挑战。


<details>
  <summary>Details</summary>
Motivation: 复杂系统（如大脑、细胞、气候、经济等）具有高维非线性动态，传统建模方法在处理海量时空数据时面临挑战。虽然信息技术进步使数据驱动方法成为可能，但数据体量和复杂性仍阻碍传统维度约简、相空间重构等方法的应用。

Method: 基于离散测度空间上的向量场理论，提出一个几何分析框架，包含一个两参数度量族。该框架支持时间相关图像、图像梯度、图上的实值或向量值函数等数据类型。通过多维标度等方法验证框架的有效性。

Result: 在生物和物理系统的数值模拟数据上验证了该框架，结果表明提出的度量结合多维标度能有效解决关键分析挑战：实现维度约简、模态分解、相空间重构和吸引子表征。

Conclusion: 该几何框架为理解复杂动力系统提供了稳健途径，特别适用于传统建模不切实际但实验数据丰富的场景，为复杂系统分析提供了新的工具和方法。

Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.

</details>


### [111] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 本文提出知识转换（KT）方法，结合知识蒸馏、主动学习和因果推理，解决在线边缘机器学习中未来未见数据的标签获取问题。


<details>
  <summary>Details</summary>
Motivation: 现有边缘机器学习方法通常假设静态模型在中心训练后部署，无法有效处理未见数据。在线边缘机器学习允许模型直接在边缘设备上训练并持续更新，但面临如何为未来未见数据点确定标签的关键挑战。

Method: 提出知识转换（KT）混合方法，结合知识蒸馏、主动学习和因果推理。KT作为主动学习中的预言机，通过将教师模型的知识转换为伪标签来训练学生模型。

Result: 通过两种设置进行模拟实验：（1）使用不太稳定的教师模型；（2）使用相对更稳定的教师模型。结果表明，当给定稳定的教师模型时，学生模型最终能达到预期的最大性能。

Conclusion: KT方法在以下场景中具有潜在优势：（1）教师任务具有通用性，现有预训练模型可能足够完成任务，无需从头训练教师模型；（2）学生任务的标签难以获取或成本高昂。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


### [112] [Sequencing to Mitigate Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2512.16871)
*Hesham G. Moussa,Aroosa Hameed,Arashmid Akhavain*

Main category: cs.LG

TL;DR: 本文提出通过任务序列优化来缓解持续学习中的灾难性遗忘问题，利用零样本评分算法确定最优任务顺序，显著减少遗忘并增强传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习系统需要在整个生命周期中增量获取、更新和利用知识，但灾难性遗忘是主要挑战。现有方法主要分为五类，但本文从任务序列优化的新角度出发，研究任务顺序对缓解遗忘的作用。

Method: 提出一种确定最优任务顺序的方法，利用受神经架构搜索启发的零样本评分算法来评估和排序任务序列，从而优化学习过程。

Result: 智能任务序列能显著减少灾难性遗忘，当与传统持续学习策略结合时，能提供更强的性能和抗遗忘鲁棒性。

Conclusion: 任务序列优化是缓解灾难性遗忘的有效方法，该方法还可应用于课程学习等其他领域，为持续学习提供了新的研究方向。

Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.

</details>


### [113] [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)
*Andrew Wagenmaker,Perry Dong,Raymond Tsao,Chelsea Finn,Sergey Levine*

Main category: cs.LG

TL;DR: 该论文提出后验行为克隆（PostBC）方法，通过建模演示者的后验行为分布来改进预训练策略，从而为强化学习微调提供更好的初始化，相比标准行为克隆能显著提升微调性能。


<details>
  <summary>Details</summary>
Motivation: 当前实践中，通常先在大型演示数据集上预训练策略，然后通过强化学习微调以提升性能。虽然微调算法研究较多，但预训练策略作为强化学习微调初始化的重要性被忽视。标准行为克隆（BC）可能无法覆盖演示者的所有动作，影响后续微调效果。

Method: 提出后验行为克隆（PostBC）方法，不直接拟合观察到的演示动作，而是训练策略来建模演示者行为在演示数据集下的后验分布。这种方法确保覆盖演示者的所有动作，同时保持与BC相当的预训练性能。PostBC仅需标准监督学习，可与现代生成模型结合实现。

Result: 理论分析表明标准BC可能无法确保动作覆盖，而PostBC能保证覆盖演示者动作。在实际机器人控制基准测试和真实世界机器人操作任务中，PostBC相比标准BC能显著提升强化学习微调性能。

Conclusion: PostBC方法通过建模演示者行为的后验分布，为强化学习微调提供了更有效的预训练初始化，在保持预训练性能的同时显著提升微调效果，适用于机器人控制等实际应用场景。

Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

</details>
