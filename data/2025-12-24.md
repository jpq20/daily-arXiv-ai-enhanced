<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 52]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta,Arijeet Pramanik,Jerrin John Thomas,Regina Schwind,Lauren Wiener,Avi Raju,Jeremy Kornbluth,Yanshan Wang,Zhaohui Su,Hrituraj Singh*

Main category: cs.CL

TL;DR: 提出基于LLM的智能体框架，从电子健康记录非结构化笔记中提取结构化肿瘤学数据，在40万份临床笔记上实现平均F1分数0.93，显著降低标注成本


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化笔记包含丰富的临床信息，但可靠提取结构化肿瘤学数据面临挑战：变异性大、术语专业、格式不一致。手动提取准确但成本高昂，现有自动化方法通常只处理狭窄场景，无法充分处理患者级别的信息合成

Method: 提出智能体框架，将复杂肿瘤学数据提取分解为模块化、自适应任务。使用大型语言模型作为推理智能体，配备上下文敏感检索和迭代合成能力，从真实世界肿瘤学笔记中全面提取结构化临床变量

Result: 在包含2250名癌症患者的40多万份非结构化临床笔记和扫描PDF报告的大规模数据集上评估，平均F1分数达0.93，103个肿瘤学特定临床变量中有100个超过0.85，关键变量（如生物标志物和药物）超过0.95。集成到数据整理工作流中获得0.94的直接人工批准率

Conclusion: 这是首个基于LLM智能体的大规模端到端结构化肿瘤学数据提取应用，能够有效处理真实世界临床文档的复杂性，显著提高数据提取效率和准确性

Abstract: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale

</details>


### [2] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: LLMs在真实课堂转录中分类教学行为的能力评估：零样本表现中等，少样本提示显著提升性能，但存在可靠性限制


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在教育技术中广泛应用，了解其在未经大量定制的情况下解释真实教育场景的能力变得日益重要，这有助于设定预期和建立基准

Method: 比较六个LLM在真实课堂转录中分类教学行为的基础性能，评估零样本、单样本和少样本提示方法

Result: 零样本表现中等，提供全面示例（少样本提示）显著提升了最先进模型的性能，最强配置达到Cohen's Kappa = 0.58；但性能因教学行为而异，高召回率常伴随假阳性增加

Conclusion: 基础模型在解释教学话语方面表现出有意义但有限的能力，提示设计有助于展现能力但不能消除基本的可靠性限制

Abstract: Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.

</details>


### [3] [Counterfactual LLM-based Framework for Measuring Rhetorical Style](https://arxiv.org/abs/2512.19908)
*Jingyi Qiu,Hong Chen,Zongyi Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的反事实框架来量化机器学习论文中的修辞风格，发现愿景式修辞能显著预测下游关注度，且2023年后修辞强度急剧上升主要受LLM写作辅助工具驱动。


<details>
  <summary>Details</summary>
Motivation: AI领域的快速发展引发了人们对机器学习论文中"炒作"现象的担忧，但由于大胆语言可能源于实证结果或修辞风格，难以区分两者。需要一种独立于实质内容量化修辞风格的方法。

Method: 提出反事实的LLM框架：多个LLM修辞角色从相同实质内容生成反事实写作，LLM评委通过成对比较评估，使用Bradley-Terry模型聚合结果。应用于2017-2025年8,485篇ICLR投稿，生成超过25万篇反事实写作。

Result: 愿景式修辞能显著预测下游关注度（引用和媒体关注），即使控制同行评审评价后依然有效。发现2023年后修辞强度急剧上升，实证证据表明这主要由LLM写作辅助工具的采用驱动。框架可靠性得到验证（对角色选择的鲁棒性，LLM判断与人工标注高度相关）。

Conclusion: LLM可作为工具来测量和改进科学评估，为量化论文修辞风格提供了可靠方法，揭示了修辞风格对学术影响力的重要作用以及LLM技术对科学写作风格的影响。

Abstract: The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.

</details>


### [4] [Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems](https://arxiv.org/abs/2512.19950)
*Heet Bodara,Md Masum Mushfiq,Isma Farah Siddiqui*

Main category: cs.CL

TL;DR: 该研究探索大型语言模型在对话系统中的语调偏见问题，通过合成对话数据集和分类模型发现，即使在中性提示下生成的对话也存在系统性语调偏差，这对设计公平可信的对话AI具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话系统中广泛应用，但其回应常常带有微妙的语调偏见（如过度礼貌、欢快或谨慎），这些偏见会影响用户对信任、同理心和公平性的感知。研究旨在探索语调偏见作为大型语言模型的隐藏行为特征。

Method: 1. 创建两个合成对话数据集：一个由中性提示生成，另一个明确引导产生积极或消极语调；2. 使用预训练的DistilBERT模型进行弱监督标注；3. 训练多个分类器检测语调模式；4. 采用集成模型提高检测性能。

Result: 1. 即使中性数据集也显示出一致的语调偏斜，表明偏见可能源于模型的基础对话风格；2. 集成模型实现了高达0.92的宏观F1分数，证明语调偏见是系统性的、可测量的；3. 语调偏见与设计公平可信的对话AI密切相关。

Conclusion: 语调偏见是大型语言模型的一个系统性特征，可以通过合成对话和分类模型有效检测。这一发现对设计公平、可信赖的对话AI系统具有重要意义，需要关注模型训练数据和方法以减少这类偏见。

Abstract: Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.

</details>


### [5] [Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/abs/2512.19995)
*Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou*

Main category: cs.CL

TL;DR: 论文提出ThinkARM框架，使用Schoenfeld的Episode理论将LLM推理轨迹抽象为功能步骤，揭示数学问题解决中的思维动态和结构差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然展示推理轨迹，但其底层认知结构和步骤难以识别分析，超出表面统计层面。需要更系统的框架来理解推理的结构和过程。

Method: 采用Schoenfeld的Episode理论作为归纳性中间尺度视角，提出ThinkARM框架，将推理轨迹显式抽象为功能推理步骤（如分析、探索、实施、验证等）。应用于不同模型的数学问题解决，进行诊断性案例研究。

Result: 抽象揭示了可复现的思维动态和推理与非推理模型之间的结构差异，这些在token级视图中不明显。探索步骤是关键分支点与正确性相关，效率导向方法选择性地抑制评估反馈步骤而非均匀缩短响应。

Conclusion: Episode级表征使推理步骤显式化，能够系统分析现代语言模型中推理的结构、稳定性和变化方式。

Abstract: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>


### [6] [ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language](https://arxiv.org/abs/2512.20111)
*Aly Lidayan,Jakob Bjorner,Satvik Golechha,Kartik Goyal,Alane Suhr*

Main category: cs.CL

TL;DR: 提出ABBEL框架，通过语言表达的信念瓶颈来压缩长序列决策任务的历史，使用强化学习训练LLM生成和基于信念行动，在保持近恒定内存使用的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 随着序列决策任务长度增加，在上下文中保留完整交互历史变得计算上不切实际，需要一种方法来维护简洁的上下文

Method: 引入ABBEL框架，用信念状态（任务相关未知信息的自然语言摘要）替代长多步交互历史；每步先更新先验信念形成后验信念，然后仅使用后验信念选择行动；通过强化学习训练LLM生成和基于信念行动，包括信念评分和长度惩罚

Result: ABBEL支持生成可解释的信念，同时保持近恒定的内存使用；瓶颈方法容易产生错误传播，导致性能低于完整上下文设置；通过RL训练后，ABBEL性能超越完整上下文设置，同时比同期方法使用更少内存

Conclusion: ABBEL框架为LLM智能体提供了一种通过语言表达的信念瓶颈来压缩长序列决策历史的方法，结合RL训练可以超越完整上下文设置的性能，同时保持内存效率

Abstract: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.

</details>


### [7] [M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2512.20136)
*Hyeongcheol Park,Jiyoung Seo,Jaewon Mun,Hogun Park,Wonmin Byeon,Sung June Kim,Hyeonsoo Im,JeungSub Lee,Sangpil Kim*

Main category: cs.CL

TL;DR: M³KG-RAG：一种多跳多模态知识图谱增强的检索增强生成方法，通过构建上下文丰富的多模态实体三元组和引入GRASP机制，解决音频-视觉领域多模态RAG的模态覆盖不足和检索不精确问题，显著提升多模态大语言模型的推理深度和答案忠实度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索增强生成（RAG）在音频-视觉领域面临两大挑战：1）现有多模态知识图谱（MMKGs）的模态覆盖有限且缺乏多跳连接；2）基于共享多模态嵌入空间的相似性检索无法过滤无关或冗余知识，导致检索结果不精确。

Method: 提出M³KG-RAG框架：1）设计轻量级多智能体流水线构建多跳多模态知识图谱（M³KG），包含上下文丰富的多模态实体三元组，支持基于输入查询的模态感知检索；2）引入GRASP（Grounded Retrieval And Selective Pruning）机制，确保实体精确对齐查询、评估答案支持相关性，并剪枝冗余上下文，仅保留生成响应所需的关键知识。

Result: 在多个多模态基准测试上的广泛实验表明，M³KG-RAG相比现有方法显著提升了多模态大语言模型的多模态推理和实体对齐能力。

Conclusion: M³KG-RAG通过构建多跳多模态知识图谱和引入GRASP机制，有效解决了音频-视觉领域多模态RAG的局限性，为多模态大语言模型提供了更精确、更相关的知识检索能力，从而增强了模型的推理深度和答案忠实度。

Abstract: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.

</details>


### [8] [Multi-hop Reasoning via Early Knowledge Alignment](https://arxiv.org/abs/2512.20144)
*Yuxin Wang,Shicheng Fang,Bo Wang,Qi Luo,Xuanjing Huang,Yining Zheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了Early Knowledge Alignment (EKA)模块，通过让LLM在规划前与检索集对齐，显著提升迭代式RAG系统在复杂多跳问题上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有迭代式RAG系统在规划问题分解时通常不考虑可用的检索语料库信息，导致检索效率低下和推理链级联错误，最终影响性能。

Method: 提出Early Knowledge Alignment (EKA)模块，在迭代式RAG系统的规划阶段之前，让LLM与检索到的上下文相关知识对齐，建立更强的推理基础。

Result: 在六个标准RAG数据集上的实验表明，EKA显著提高了检索精度，减少了级联错误，提升了性能和效率。从熵的角度分析显示，早期知识整合减少了推理过程中的不必要探索。

Conclusion: EKA作为一种无需训练、可扩展的推理策略，推进了迭代式RAG系统的最新技术，揭示了强化学习增强框架中结构化推理与高效探索之间的关键相互作用。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.

</details>


### [9] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen,Yixin Ou,Quan Feng,Lei Li,Piji Li,Haibo Ye,Sheng-Jun Huang,Shuofei Qiao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: RetroPrompt是一种新的提示学习方法，通过检索机制从知识库中获取上下文信息，减少对死记硬背的依赖，提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的提示学习方法仍遵循参数化学习范式，在记忆和死记硬背的泛化稳定性方面存在不足，难以充分利用非典型实例，在有限数据下容易过拟合到浅层模式。

Method: RetroPrompt通过解耦知识和记忆，利用训练数据生成的公开知识库，在输入、训练和推理阶段都加入检索机制，使模型能够主动从语料库中检索相关上下文信息。

Result: 在自然语言处理和计算机视觉任务的各种数据集上进行实验，证明RetroPrompt在零样本和少样本场景下都表现出优越性能，有效减少对死记硬背的依赖，增强泛化能力。

Conclusion: RetroPrompt通过检索增强的提示学习方法，在记忆和泛化之间取得了更好的平衡，为预训练基础模型的提示学习提供了新的有效范式。

Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

</details>


### [10] [Fun-Audio-Chat Technical Report](https://arxiv.org/abs/2512.20156)
*Qian Chen,Luyao Cheng,Chong Deng,Xiangang Li,Jiaqing Liu,Chao-Hong Tan,Wen Wang,Junhao Xu,Jieping Ye,Qinglin Zhang,Qiquan Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: Fun-Audio-Chat是一个大型音频语言模型，通过双分辨率语音表示和核心鸡尾酒训练解决了语音-文本模型中的时间分辨率不匹配和灾难性遗忘问题，在多种音频任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有联合语音-文本模型面临三个关键挑战：1) 语音标记(25Hz)和文本标记(~3Hz)之间的时间分辨率不匹配会稀释语义信息；2) 高计算成本；3) 导致文本LLM知识的灾难性遗忘。

Method: 采用两种创新方法：1) 双分辨率语音表示(DRSR)：共享LLM以高效的5Hz处理音频（通过标记分组），而语音精炼头生成高质量的25Hz标记；2) 核心鸡尾酒训练：两阶段微调与中间合并，减轻灾难性遗忘。然后应用多任务DPO训练增强鲁棒性、音频理解、指令跟随和语音共情。

Result: Fun-Audio-Chat 8B和MoE 30B-A3B在语音转文本和语音转语音任务上表现优异，在口语问答基准测试中在相似规模模型中排名前列。在音频理解、语音功能调用、指令跟随和语音共情方面也达到竞争性甚至优越性能。还开发了全双工变体Fun-Audio-Chat-Duplex。

Conclusion: Fun-Audio-Chat通过创新的双分辨率架构和训练策略，有效解决了现有语音-文本模型的局限性，在保留文本LLM知识的同时获得了强大的音频理解、推理和生成能力，无需大规模音频-文本预训练。

Abstract: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.

</details>


### [11] [AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications](https://arxiv.org/abs/2512.20164)
*Honglin Mu,Jinghao Liu,Kaiyang Wan,Rui Xing,Xiuying Chen,Timothy Baldwin,Wanxiang Che*

Main category: cs.CL

TL;DR: LLMs在简历筛选中存在对抗指令攻击漏洞，攻击成功率超过80%。论文提出FIDS防御方法，结合LoRA适配实现15.4%攻击降低，综合防御可达26.3%攻击降低。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码审查、内容审核等任务中表现出色，但研究发现它们容易受到隐藏在输入数据（如简历、代码）中的"对抗指令"操纵，导致偏离预期任务。在简历筛选等常见应用中缺乏有效防御措施。

Method: 1) 建立评估简历筛选漏洞的基准测试；2) 评估两种防御机制：基于提示的防御和提出的FIDS（通过分离检测外来指令）方法，使用LoRA适配；3) 结合两种防御方法。

Result: 攻击成功率超过80%；基于提示的防御实现10.1%攻击降低但增加12.5%误拒率；FIDS实现15.4%攻击降低且仅增加10.4%误拒率；综合防御实现26.3%攻击降低。

Conclusion: 训练时防御（如FIDS）在安全性和实用性保护方面优于推理时缓解措施，为LLMs在简历筛选等应用中的安全部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by "adversarial instructions" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.

</details>


### [12] [FaithLens: Detecting and Explaining Faithfulness Hallucination](https://arxiv.org/abs/2512.20182)
*Shuzheng Si,Qingyi Wang,Haozhe Zhao,Yuzhuo Bai,Guanqiao Chen,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: FaithLens是一个用于检测大语言模型输出中忠实性幻觉的8B参数模型，能同时提供二元预测和解释，在12个任务上超越GPT-4.1和o3等先进模型。


<details>
  <summary>Details</summary>
Motivation: 识别大语言模型输出中的忠实性幻觉对于检索增强生成和摘要等实际应用至关重要，需要一种既高效又可信的检测方法。

Method: 1) 使用先进LLM合成带解释的训练数据，并通过数据过滤策略确保标签正确性、解释质量和数据多样性；2) 在精心策划的数据上进行微调作为冷启动；3) 使用基于规则的强化学习进一步优化，奖励预测正确性和解释质量。

Result: 在12个多样化任务上，8B参数的FaithLens超越了GPT-4.1和o3等先进模型，并能产生高质量的解释，在可信性、效率和效果之间取得了独特平衡。

Conclusion: FaithLens提供了一个成本效益高且有效的忠实性幻觉检测解决方案，能够同时提供可靠的预测和解释，增强了LLM输出的可信度。

Abstract: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.

</details>


### [13] [Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings](https://arxiv.org/abs/2512.20204)
*Marko Čechovič,Natália Komorníková,Dominik Macháček,Ondřej Bojar*

Main category: cs.CL

TL;DR: 该研究创建了一个用于评估跨语言会议自动翻译系统的语料库，包含12种语言的5小时对话录音，并提出了自动检测误解的方法，测试了大型语言模型在此任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 需要评估跨语言会议中自动语音翻译系统的性能，但缺乏真实、多语言的评估语料库。同时，跨语言交流中误解的自动检测也是一个重要但尚未充分研究的问题。

Method: 创建了包含12种语言、5小时对话录音的跨语言会议语料库，包含ASR转录、人工校对转录、自动翻译和人工修正翻译。为研究跨语言摘要，还包含了会议纪要。同时，人工标注了对话中的误解，并测试了Gemini等大型语言模型自动检测误解的能力。

Result: 构建了一个多语言、真实的跨语言会议语料库。在误解检测任务上，Gemini模型能够以77%的召回率和47%的精确度识别包含误解的文本片段。

Conclusion: 该研究提供了一个有价值的跨语言会议评估资源，并展示了大型语言模型在自动检测跨语言交流误解方面的潜力，尽管精确度仍有提升空间。

Abstract: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.
  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.

</details>


### [14] [Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives](https://arxiv.org/abs/2512.20298)
*Karolina Drożdż,Kacper Dudzic,Anna Sterna,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: LLMs在精神病学自我评估中表现出色，在波兰语自传叙述中诊断边缘型人格障碍的准确率超过人类专家21.91个百分点，但在自恋型人格障碍诊断中存在严重偏见和漏诊问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在精神病学自我评估中的使用日益增多，需要评估它们解释定性患者叙述的能力，特别是与心理健康专业人员的诊断准确性进行比较。

Method: 使用波兰语第一人称自传叙述，对最先进的LLMs和心理健康专业人员在诊断边缘型人格障碍和自恋型人格障碍方面进行首次直接比较。

Result: Gemini Pro模型在总体诊断准确率上超过人类专业人员21.91个百分点（65.48% vs 43.57%）。模型在识别边缘型人格障碍方面表现出色（F1=83.4 vs 人类80.0），但在自恋型人格障碍诊断中严重漏诊（F1=6.7 vs 人类50.0），显示出对"自恋"这一价值负载术语的回避倾向。

Conclusion: 虽然LLMs在解释复杂的第一人称临床数据方面能力很强，但仍存在关键的可信度和偏见问题，特别是在处理价值负载的诊断标签时表现出系统性偏见。

Abstract: Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term "narcissism." Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.

</details>


### [15] [SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision](https://arxiv.org/abs/2512.20308)
*Maxime Poli,Mahi Luthra,Youssef Benchekroun,Yosuke Higuchi,Martin Gleize,Jiayi Shen,Robin Algayres,Yu-An Chung,Mido Assran,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: SpidR是一种自监督语音表示模型，通过掩码预测、自蒸馏和在线聚类相结合的方法，从原始波形中学习包含丰富语音信息的表示，特别适合无文本语音语言建模，训练时间大幅缩短。


<details>
  <summary>Details</summary>
Motivation: 随着语言建模和语音表示学习的并行发展，研究者希望直接从语音中学习语言而无需文本中介，这需要从语音中直接提取语义表示。

Method: SpidR使用掩码预测目标结合自蒸馏和在线聚类进行训练。学生模型的中间层学习预测来自教师模型中间层的分配，这种学习目标稳定了在线聚类过程，产生更高质量的码本。

Result: SpidR在语言建模基准测试（sWUGGY、sBLIMP、tSC）上优于wav2vec 2.0、HuBERT、WavLM和DinoSR。同时验证了语音单元质量指标（ABX、PNMI）与语言建模性能的相关性。训练时间从HuBERT的一周缩短到仅需一天。

Conclusion: SpidR是一种高效的自监督语音表示模型，能够学习包含丰富语音信息的表示，特别适合无文本语音语言建模，训练时间大幅减少，为快速迭代和实验提供了便利。

Abstract: The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.

</details>


### [16] [Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://arxiv.org/abs/2512.20324)
*Nurul Labib Sayeedi,Md. Faiyaz Abdullah Sayeedi,Khushnur Binte Jahangir,Swakkhar Shatabda,Sarah Masud Preum*

Main category: cs.CL

TL;DR: 论文介绍了BanglaRiddleEval基准测试，包含1,244个孟加拉语谜语，评估LLMs在低资源比喻推理任务上的表现，结果显示当前模型远未达到人类水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在标准NLP基准上表现优异，但在比喻性、文化相关和低资源环境下的推理能力尚未充分探索，特别是在孟加拉语这样的低资源语言中。

Method: 使用基于LLM的流水线生成思维链解释、语义连贯的干扰项和细粒度歧义标注，评估开源和闭源模型在不同提示策略下的表现。

Result: 模型在生成式QA中达到中等语义重叠但正确率低，多项选择题准确率最高仅约56%（人类基线83%），歧义解决率在26%-68%之间，高质量解释仅限于最强模型。

Conclusion: 当前LLMs能捕捉孟加拉语谜语推理所需的部分线索，但远未达到人类水平，BanglaRiddleEval成为低资源比喻推理的挑战性新基准。

Abstract: Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.

</details>


### [17] [Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining](https://arxiv.org/abs/2512.20404)
*Junyi Liu,Stanley Kok*

Main category: cs.CL

TL;DR: 提出一个情感感知的文本摘要框架，将情感信号嵌入到抽取式和生成式摘要方法中，以更好地处理社交媒体等非结构化文本的情感信息。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体、评论和论坛等非结构化数据的快速增长，文本挖掘在信息系统(IS)中变得至关重要。现有摘要方法主要针对结构化新闻优化，在处理嘈杂、非正式内容时表现不佳，而情感线索对于品牌监控和市场分析等IS任务至关重要，但很少有研究将情感建模整合到短用户生成文本的摘要中。

Method: 提出一个情感感知框架，扩展了抽取式(TextRank)和生成式(UniLM)方法，通过将情感信号嵌入到排序和生成过程中。这种双重设计能够更好地捕捉情感细微差别和主题相关性。

Result: 该框架能够生成简洁、情感丰富的摘要，增强了对动态在线环境中及时干预和战略决策的支持。

Conclusion: 提出的情感感知摘要框架能够有效处理非结构化社交媒体文本，通过整合情感建模提升摘要质量，为信息系统中的品牌监控和市场分析等任务提供更好的支持。

Abstract: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.

</details>


### [18] [Step-DeepResearch Technical Report](https://arxiv.org/abs/2512.20491)
*Chen Hu,Haikuo Du,Heng Wang,Lin Lin,Mingrui Chen,Peng Liu,Ruihang Miao,Tianchi Yue,Wang You,Wei Ji,Wei Yuan,Wenjin Deng,Xiaojian Yuan,Xiaoyun Zhang,Xiangyu Liu,Xikai Liu,Yanming Xu,Yicheng Cao,Yifei Zhang,Yongyao Wang,Yubo Shu,Yurong Zhang,Yuxiang Zhang,Zheng Gong,Zhichao Chang,Binyan Li,Dan Ma,Furong Jia,Hongyuan Wang,Jiayu Liu,Jing Bai,Junlan Liu,Manjiao Liu,Na Wang,Qiuping Wu,Qinxin Du,Shiwei Li,Wen Sun,Yifeng Gong,Yonglin Chen,Yuling Zhao,Yuxuan Lin,Ziqi Ren,Zixuan Wang,Aihu Zhang,Brian Li,Buyun Ma,Kang An,Li Xie,Mingliang Li,Pan Li,Shidong Yang,Xi Chen,Xiaojia Liu,Yuchu Luo,Yuan Song,YuanHao Ding,Yuanwei Liang,Zexi Li,Zhaoning Zhang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: Step-DeepResearch是一个成本效益高的端到端深度研究智能体，通过原子能力数据合成策略和渐进式训练路径，在中文领域ADR-Bench上表现优异，证明中等规模模型也能达到专家级能力。


<details>
  <summary>Details</summary>
Motivation: 现有学术基准如BrowseComp无法满足现实世界对开放式深度研究的需求，需要更强的意图识别、长视野决策和跨源验证能力。同时中文领域缺乏深度研究评估基准。

Method: 1. 提出基于原子能力的数据合成策略来增强规划和报告写作能力；2. 采用从智能体中期训练到SFT和RL的渐进式训练路径；3. 引入清单式评判器提高鲁棒性；4. 建立中文领域ADR-Bench评估基准。

Result: Step-DeepResearch（32B）在Scale AI Research Rubrics上获得61.4%得分。在ADR-Bench上显著优于同类模型，并能与OpenAI和Gemini DeepResearch等闭源SOTA模型竞争。

Conclusion: 精细化训练使中等规模模型能够以行业领先的成本效益实现专家级深度研究能力，为自主智能体发展提供了有效路径。

Abstract: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.

</details>


### [19] [Distilling to Hybrid Attention Models via KL-Guided Layer Selection](https://arxiv.org/abs/2512.20569)
*Yanhong Li,Songlin Yang,Shawn Tan,Mayank Mishra,Rameswar Panda,Jiawei Zhou,Yoon Kim*

Main category: cs.CL

TL;DR: 提出了一种简单高效的层选择方法，用于将预训练的softmax注意力Transformer蒸馏为混合架构，通过少量通用文本数据训练得到层重要性分数来选择要转换为线性注意力的层。


<details>
  <summary>Details</summary>
Motivation: 将预训练的softmax注意力Transformer蒸馏为混合架构（交替使用softmax和线性注意力层）是提高LLM推理效率的可行方法，但关键挑战在于如何选择要转换的层。

Method: 使用少量通用文本数据训练得到层重要性分数来选择要转换为线性注意力的层，然后采用RADLADS蒸馏流程：注意力权重转移、隐藏状态对齐、KL分布匹配，最后进行少量微调。

Result: 该方法比现有层选择方法更有效，包括基于固定比例均匀交替线性注意力的启发式方法，以及依赖专门诊断数据集的复杂方法。

Conclusion: 提出的层选择方法简单高效，结合RADLADS蒸馏流程，能够有效地将预训练Transformer转换为混合架构，提高推理效率而不需要昂贵的从头预训练。

Abstract: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.

</details>


### [20] [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://arxiv.org/abs/2512.20578)
*Amirhosein Ghasemabadi,Di Niu*

Main category: cs.CL

TL;DR: Gnosis是一种轻量级自感知机制，使冻结的LLM能够通过解码隐藏状态和注意力模式的信号进行内在自我验证，以预测自身错误，仅增加约500万参数且与序列长度无关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能生成流畅复杂的输出，但常常无法识别自己的错误和幻觉。现有方法通常依赖外部评判器、多样本一致性或基于文本的自我批评，这些方法要么增加额外计算成本，要么与真实正确性相关性较弱。作者探索是否可以通过检查推理过程中的内部状态来预测LLM的失败。

Method: 引入Gnosis机制，被动观察内部痕迹（隐藏状态和注意力模式），将其压缩为固定预算的描述符，然后预测正确性。该方法仅增加约500万参数，推理成本可忽略，且独立于序列长度。

Result: 在数学推理、开放域问答和学术知识基准测试中，针对1.7B到20B参数的冻结骨干模型，Gnosis在准确性和校准方面始终优于强大的内部基线和大型外部评判器。此外，它能够零样本泛化到部分生成，实现失败轨迹的早期检测和计算感知控制。

Conclusion: 可靠的正确性线索内在于生成过程中，可以在没有外部监督的情况下高效提取。这表明LLM具备通过内部状态进行自我验证的潜力，为轻量级自我监控提供了新方向。

Abstract: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.

</details>


### [21] [Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs](https://arxiv.org/abs/2512.20595)
*Dhruv Anand,Ehsan Shareghi*

Main category: cs.CL

TL;DR: Cube Bench是一个基于魔方的基准测试，用于评估多模态大语言模型的空间和序列推理能力，包含五个技能维度，测试结果显示模型性能随魔方复杂度增加而急剧下降，闭源模型优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前需要评估多模态大语言模型在空间和序列推理方面的能力，魔方问题提供了一个紧凑、可复现的测试平台，能够系统性地分解和评估模型的不同认知技能。

Method: 创建Cube Bench基准测试，包含五个技能维度：从图像和文本重建魔方面、选择最优下一步、预测候选移动结果、执行多步计划并从中恢复、检测和修正自身错误。使用共享的魔方状态、相同提示和解析器，以及单一的距离解决度量标准，比较不同模型在不同复杂度下的表现。

Result: 七个MLLM的准确率随魔方复杂度增加而急剧下降；一旦轨迹停滞或偏离，模型很少能恢复；高面重建准确率不能保证良好的动作选择或多步执行能力。闭源模型在单步感知任务和多步控制任务上都领先，开源模型在最困难设置下接近随机水平；即使最好的MLLM在更高复杂度下也会退化。简单的自我修正（反思思维）带来适度改进，但也可能引入过度思考问题。

Conclusion: Cube Bench提供了一个紧凑、可复现的测试工具，用于评估MLLM的序列空间推理能力。当前MLLM在处理复杂空间序列问题时仍存在显著局限性，闭源模型表现优于开源模型，但所有模型在更高复杂度下都会退化，表明需要进一步改进模型的推理能力。

Abstract: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.

</details>


### [22] [MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts](https://arxiv.org/abs/2512.20604)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: MoE-DiffuSeq是一个基于专家混合的框架，用于增强扩散模型在生成长文档时的性能，通过稀疏注意力和专家混合架构解决现有扩散模型在长序列生成中的计算成本和内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本生成模型（如DiffuSeq）在处理长序列时存在高计算成本和内存开销的问题，限制了其在长文档生成场景中的实际应用。

Method: 1. 集成稀疏注意力与专家混合架构，实现高效可扩展的长序列建模；2. 引入定制的稀疏注意力机制，降低计算复杂度同时保持文本质量和连贯性；3. 在扩散过程中加入软吸收状态，加速序列重建并提高生成精度。

Result: MoE-DiffuSeq相比现有扩散模型显著提高了训练效率和采样速度，特别是在科学文章生成、代码库建模和长对话生成等长文档场景中表现优异，在效率、速度、准确性和表达能力方面均有提升。

Conclusion: MoE-DiffuSeq通过专家混合架构和稀疏注意力机制，有效解决了扩散模型在长文本生成中的计算瓶颈，推动了扩散模型在高品质长文本生成中的实际应用。

Abstract: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 该研究提出了一种双目标优化方法，用于灾后救援物资分配中的车辆路径规划，同时考虑效率（最小化总行程时间）和公平性（最小化基于基尼系数的不公平度量），通过分支定价算法有效解决该问题。


<details>
  <summary>Details</summary>
Motivation: 重大灾害中，预置的救援物资往往无法满足所有需求，需要在有限的物资供应下，规划从配送中心到避难所的车辆路线，同时平衡效率（及时送达）和公平性（公平分配）。

Method: 建立混合整数规划模型，使用ε约束法处理双目标优化问题；推导最优解的数学性质，引入有效不等式，设计给定可行车辆路线的最优配送分配算法；开发分支定价算法高效求解。

Result: 在土耳其Van地震和伊斯坦布尔Kartal地区预测数据的实际数据集上测试，分支定价算法显著优于商业MIP求解器；双目标方法在不影响效率的情况下将援助分配不公平性降低34%。

Conclusion: 当时间约束非常宽松或严格时，优先考虑需求覆盖的词典序优化是有效的；对于中等限制的时间约束，平衡方法对于避免不公平结果至关重要。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [24] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 该论文提出了一种基于类别原型和分割Vision Transformer的方法，用于解决PlantClef 2025挑战中的细粒度多标签物种识别问题，在私有排行榜上获得第五名。


<details>
  <summary>Details</summary>
Motivation: 解决PlantClef 2025挑战中的细粒度多标签物种识别问题，特别是在高分辨率植被图像中从多类别识别适应到多标签分类的领域适应需求。

Method: 1. 从训练数据集中提取特征并应用K-Means聚类创建类别原型；2. 构建定制化的窄Vision Transformer，用冻结的DinoV2替换patch embedding层；3. 训练分割模型从测试数据集图像重建训练数据集的类别原型；4. 使用注意力分数识别和定位感兴趣区域以指导分类过程。

Result: 在PlantCLEF 2025挑战的私有排行榜上获得第五名，F1分数为0.33331，与最佳提交结果仅差0.03，表现出竞争性性能。

Conclusion: 提出的基于类别原型和分割ViT的方法能够有效地实现从多类别识别到多标签分类的领域适应，在高分辨率植被图像物种识别任务中取得了有竞争力的结果。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [25] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 该论文提出了一种通过类内聚类生成伪标签进行分层分类的方法，以缓解细粒度视觉分类任务中的类内变异问题，特别是在样本不足的情况下。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类任务中，类内变异（同一类别内图像的差异程度）会阻碍深度学习模型的学习过程，特别是当这些类别样本不足时。需要一种方法来缓解类内变异问题，提高分类性能。

Method: 提出了一种新颖的方法：对每个类别单独进行聚类，发现编码图像间相似度的伪标签，然后将这些标签用于分层分类过程，从而学习更细粒度的视觉特征。

Result: 在PlantNet300k数据集上的初步实验揭示了几个关键点，虽然方法的一些组件尚未完全优化，但仍在该数据集上达到了最先进的性能。

Conclusion: 该方法通过类内聚类和分层分类有效缓解了细粒度视觉分类中的类内变异问题，未来工作需要进一步优化组件以获得更确凿的证据。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [26] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 提出LieFlow方法，通过李群上的流匹配直接从数据中学习对称性，能够发现更广泛的群类型且需要更少假设


<details>
  <summary>Details</summary>
Motivation: 对称性对理解物理系统和提升机器学习性能都很重要，但需要了解数据中的底层对称性。现有方法在发现对称性方面存在限制

Method: 将对称性发现定义为在更大假设群上学习分布，使学习到的分布与数据中观察到的对称性匹配。使用李群上的流匹配方法，并提出新的插值方案解决"最后一刻收敛"问题

Result: 在2D和3D点云上成功发现了离散群，包括通过复数域上的流匹配发现反射对称性。新方法比先前工作更灵活且需要更少假设

Conclusion: LieFlow方法能够直接从数据中学习对称性，解决了对称性发现中的关键挑战，为理解物理系统和提升机器学习性能提供了有效工具

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [27] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: SOF框架从无动作视频中学习基于光流的潜在技能表示，实现视频技能到机器人动作的转换和高层规划


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以转换为低级动作，而潜在动作模型缺乏高层规划能力，需要弥合视频学习与机器人动作执行之间的鸿沟

Method: 通过光流作为中间表示学习潜在技能空间，从大量无动作视频中提取技能，实现视频技能到动作的转换和高层规划

Result: 在多任务和长时域设置中性能持续提升，能够直接从原始视觉数据中获取和组合技能

Conclusion: SOF框架成功连接了视频学习与机器人动作执行，通过光流表示实现了技能抽象和高层规划

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [28] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 提出ProbGLC方法，结合概率性和确定性地理定位模型，通过不确定性量化增强模型可解释性，实现跨视图灾害地理定位，支持快速灾害响应。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化加剧，灾害事件日益频繁和严重，快速准确的灾害位置识别对于气候韧性和可持续性至关重要。现有方法在可解释性和定位性能方面存在局限。

Method: 提出ProbGLC（概率跨视图地理定位）方法，将概率性和确定性地理定位模型统一到一个框架中，通过不确定性量化增强可解释性，同时提供概率分布和可定位性评分等独特特征。

Result: 在两个跨视图灾害数据集（MultiIAN和SAGAINDisaster）上的实验显示，ProbGLC在1公里范围内达到0.86的准确率，25公里范围内达到0.97的准确率，同时通过概率分布和可定位性评分提供模型可解释性。

Conclusion: ProbGLC方法在灾害地理定位方面表现出优越性能，结合了高精度和可解释性，展示了生成式跨视图方法在增强位置感知、支持更快更好灾害响应方面的巨大潜力。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [29] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 本文系统研究了强化学习在内容审核任务中的规模化应用，发现RL在政策推理任务上表现优异，数据效率比监督微调高100倍，且呈现S型扩展行为。


<details>
  <summary>Details</summary>
Motivation: 大规模内容审核是数字生态系统中最紧迫的挑战之一，需要持续评估数十亿用户和AI生成内容是否违反政策。尽管大语言模型在政策基础审核方面显示出潜力，但在实际场景中训练这些系统达到专家级准确性的挑战仍未充分探索，特别是在标签稀疏、政策定义不断演变、需要超越浅层模式匹配的细致推理等情况下。

Method: 采用全面的实证研究方法，系统评估多种RL训练方案和奖励塑造策略，包括可验证奖励和LLM-as-judge框架，将通用语言模型转化为专门的政策对齐分类器，在三个真实世界内容审核任务上进行测试。

Result: RL表现出S型扩展行为，性能随着训练数据、rollouts和优化步骤的增加而平滑提升后逐渐饱和。RL在需要复杂政策推理的任务上显著提升性能，数据效率比监督微调高100倍，特别适用于专家标注稀缺或成本高的领域。

Conclusion: 强化学习为工业级内容审核系统提供了可行的解决方案，特别是在数据稀缺和政策推理复杂的场景中，RL方法展现出卓越的数据效率和性能扩展特性。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [30] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: Reason2Decide是一个两阶段训练框架，用于解决临床决策支持系统中预测准确性与解释对齐的挑战，通过分阶段训练减少暴露偏差，在小型模型上实现高性能的解释性决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在临床决策支持系统中面临关键挑战：需要同时实现高预测准确性和生成与预测一致的解释。现有方法存在暴露偏差问题，导致解释与预测不匹配。

Method: 提出Reason2Decide两阶段训练框架：第一阶段训练模型生成解释（rationale generation）；第二阶段联合训练标签预测和解释生成，采用计划采样技术逐步从基于黄金标签转向基于模型预测。

Result: 在三个医疗数据集（包括专有分诊数据集和公共生物医学QA数据集）上，Reason2Decide在预测准确率（F1）和解释保真度（BERTScore、BLEU、LLM-as-a-Judge）方面优于其他微调基准和部分零样本LLM。特别在分诊任务中，对LLM生成、护士撰写和护士后处理三种解释来源都表现出鲁棒性。

Conclusion: Reason2Decide框架显著降低了临床推理对大规模基础模型的依赖，使用比当代基础模型小40倍的模型实现了高性能的解释性决策支持，使资源受限环境下的临床决策支持更加可行。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [31] [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)
*Zhuo Yang,Yeyun chen,Jiaqing Xie,Ben Gao,Shuaike Shen,Wanhao Liu,Liujia Yang,Beilun Wang,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TL;DR: MolAct是一个基于智能体强化学习的分子设计框架，通过两阶段训练实现分子编辑和优化，将分子设计形式化为多步骤的工具增强过程。


<details>
  <summary>Details</summary>
Motivation: 分子编辑和优化是多步骤问题，需要迭代改进性质同时保持化学有效性和结构相似性。现有方法缺乏将分子设计形式化为智能体强化学习问题的框架。

Method: 提出MolAct框架，采用两阶段训练范式：首先建立编辑能力，然后重用学习到的编辑行为进行性质优化。框架允许智能体多轮交互，调用化学工具进行有效性检查、性质评估和相似性控制。

Result: MolEditAgent-7B在分子编辑任务中实现100、95、98的有效添加、删除和替换编辑，优于DeepSeek-R1等基线；MolOptAgent-7B在LogP优化上超越Claude 3.7，在溶解度上保持竞争力。

Conclusion: 将分子设计视为多步骤、工具增强的过程是实现可靠和可解释改进的关键，MolAct框架为分子设计提供了新的智能体强化学习范式。

Abstract: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

</details>


### [32] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种简单有效的方法：在时间序列数据token化前注入噪声，以提升冻结大语言模型在零样本时间序列预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究在使用冻结大语言模型进行时间序列预测时，性能对输入数据的文本表示非常敏感，因为模型参数无法适应分布变化。需要一种非侵入式的方法来增强模型的鲁棒性。

Method: 在原始时间序列数据token化前注入噪声，作为一种推理时数据增强策略，迫使冻结的大语言模型基于鲁棒的时间模式而非表面数值特征进行外推。

Result: 该方法在多个基准测试中表现出有效性。特别地，在专门构建的两个新时间序列数据集上（确保不在大语言模型预训练范围内），该方法持续提升了预测性能。

Conclusion: 噪声注入是一种简单而有效的策略，能够增强冻结大语言模型在时间序列预测中的鲁棒性，为直接利用现成大语言模型进行时间序列预测提供了进一步的支持。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [33] [A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers](https://arxiv.org/abs/2512.20161)
*Dhivya Dharshini Kannan,Anupam Trivedi,Dipti Srinivasan*

Main category: cs.AI

TL;DR: 本文开发了基于双向门控循环单元(BiGRU)的数据中心能耗效率(PUE)预测模型，并与GRU模型进行性能比较，使用特征选择算法优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗巨大且碳足迹显著，随着边缘计算和AI发展，数据中心存储容量持续增长。提高能源效率是应对气候变化、降低能源成本、提升商业竞争力和促进IT与环境可持续发展的有效途径。优化数据中心能源管理对全球可持续发展至关重要。

Method: 1. 开发基于双向门控循环单元(BiGRU)的PUE预测模型；2. 使用EnergyPlus模拟新加坡数据中心，获得包含52,560个样本和117个特征的数据集；3. 采用递归特征消除与交叉验证(RFECV)算法选择最相关特征集；4. 使用不同参数设置寻找最优超参数配置；5. 训练BiGRU模型并与GRU模型比较性能。

Result: 通过均方误差(MSE)、平均绝对误差(MAE)和R平方指标比较优化后的BiGRU模型与GRU模型的性能。BiGRU模型在PUE预测方面表现出更好的性能。

Conclusion: 基于BiGRU的PUE预测模型能够有效预测数据中心能耗效率，通过特征选择和超参数优化，该模型性能优于传统GRU模型，为数据中心能源管理优化提供了有效工具。

Abstract: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.

</details>


### [34] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 比较人类与大型语言模型在数字游戏中的概念推理能力，发现人类更灵活地结合规则与相似性推理，而LLMs更依赖数学规则，且人类具有更强的少样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类与大型语言模型在概念推理任务中的泛化能力差异，探索两者在归纳偏置和推理策略上的根本区别。

Method: 使用数字游戏作为概念推理任务，以贝叶斯模型为分析框架，对比分析人类和LLMs的归纳偏置和推理策略。

Result: 贝叶斯模型能更好地捕捉人类行为：人类灵活推断基于规则和相似性的概念，而LLMs更依赖数学规则；人类能从单个示例进行少样本泛化，而LLMs需要更多样本。

Conclusion: 人类与LLMs在数学概念推理和泛化方面存在根本差异，人类表现出更强的灵活性和少样本学习能力。

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [35] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种新的离线偏好强化学习框架PreSa，通过直接学习策略而非间接学习奖励和成本模型，结合偏好学习和安全对齐，在连续控制任务中实现了高效的安全策略学习。


<details>
  <summary>Details</summary>
Motivation: 传统的基于人类反馈的安全强化学习方法先学习奖励和成本模型，然后使用约束RL优化安全策略。但在长时域连续控制任务中，奖励和成本误差会累积，导致性能下降。需要一种能够直接学习安全策略的方法，避免误差累积问题。

Method: 提出PreSa方法：1) 直接基于奖励相关的成对偏好和轨迹段安全性的二元标签学习策略；2) 将偏好学习模块与安全对齐结合到约束优化问题中；3) 在拉格朗日框架内求解，直接学习奖励最大化且安全的策略，无需显式学习奖励和成本模型，也无需约束RL。

Result: 在连续控制任务上评估，使用合成和真实人类反馈。实验表明，该方法成功学习了高奖励的安全策略，优于现有最优基线方法，甚至优于使用真实奖励和成本的离线安全RL方法。

Conclusion: PreSa框架通过直接学习策略而非间接学习奖励和成本模型，有效解决了长时域连续控制任务中误差累积的问题，实现了更好的安全策略学习性能。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [36] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun,Kunlun Wu,Chuanjian Fu,Zeming Song,Langyong Shi,Zihe Xue,Bohan Jing,Ying Yang,Xiaomeng Gao,Aijia Li,Tianyu Guo,Huiying Li,Xueyuan Yang,Rongkai Liu,Xinyi He,Yuxi Wang,Yue Li,Mingyuan Liu,Yujie Lu,Hongzhao Xie,Shiyun Zhao,Bo Dai,Wei Wang,Tao Yuan,Song-Chun Zhu,Yujia Peng,Zhenliang Zhang*

Main category: cs.AI

TL;DR: TongSIM是一个高保真、通用型平台，用于训练和评估具身智能体，提供100多个多样化室内场景和开放式户外城镇模拟，支持从低级导航到高级复合活动的广泛研究需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI特别是多模态大语言模型的发展，研究重点从单模态文本处理转向更复杂的多模态和具身AI领域。现有模拟平台大多针对特定任务设计，缺乏能够支持从低级具身导航到高级复合活动（如多智能体社交模拟和人机协作）的通用训练环境。

Method: 开发TongSIM平台，提供100多个多样化多房间室内场景和开放式交互丰富的户外城镇模拟。平台具有定制化场景、任务自适应保真度、多样化智能体类型和动态环境模拟等特性，并包含全面的评估框架和基准测试。

Result: TongSIM为研究人员提供了灵活可扩展的统一平台，能够精确评估智能体的感知、认知、决策、人机协作以及空间和社会推理等能力，加速通用具身智能的发展。

Conclusion: TongSIM填补了通用具身智能训练环境的空白，作为一个高保真、通用型平台，能够支持广泛的研究需求，促进具身智能的训练、评估和进步。

Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

</details>


### [37] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai,Hang Gu,Teng Wang,Qianyu Cheng,Yifei Zheng,Zhiyong Qiu,Lei Gong,Wenqi Lou,Xuehai Zhou*

Main category: cs.AI

TL;DR: ActionFlow是一个针对边缘设备的VLA模型推理框架，通过跨请求流水线调度和内存优化，将推理速度提升2.55倍，实现实时动态操作


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在边缘设备上推理延迟高（3-5Hz），无法满足机器人交互所需的20-30Hz实时控制需求，现有优化方法需要大量重训练或会降低模型精度

Method: 提出ActionFlow系统级推理框架：1）跨请求流水线策略，将VLA推理重新定义为微请求的宏流水线，智能批处理内存绑定的解码阶段和计算绑定的预填充阶段；2）跨请求状态打包前向操作符；3）统一KV环形缓冲区，将碎片化内存操作融合为高效密集计算

Result: 在OpenVLA-7B模型上实现2.55倍的FPS提升，无需重训练即可在边缘硬件上实现实时动态操作

Conclusion: ActionFlow通过系统级优化解决了VLA模型在边缘设备上的高延迟问题，实现了实时机器人控制，为动态真实世界环境中的VLA部署提供了有效解决方案

Abstract: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

</details>


### [38] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Xuhua Duan,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Lei Chen,Wenjuan Tang,Biqiang Zhu,Xinggang Wang,Tao Sun,Wei Zhou,Dacheng Tao,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.AI

TL;DR: Janus-Pro-CXR是一个基于DeepSeek Janus-Pro的胸部X光解读系统，通过多中心前瞻性临床试验验证，在报告生成质量和临床关键发现检测方面优于现有模型，显著提高放射科医生工作效率。


<details>
  <summary>Details</summary>
Motivation: 全球放射科医生短缺，特别是基层医疗中胸部X光工作量巨大。现有多模态大语言模型评估主要依赖自动化指标或回顾性分析，缺乏严格的前瞻性临床验证。

Method: 开发了基于DeepSeek Janus-Pro模型的Janus-Pro-CXR系统，通过多中心前瞻性临床试验(NCT07117266)进行严格验证。采用轻量级架构和领域特定优化。

Result: 系统在自动报告生成方面优于最先进的X光报告生成模型，包括参数更大的ChatGPT 4o；可靠检测六种临床关键放射学发现；前瞻性临床部署中，AI辅助显著提高报告质量评分，减少18.3%的解读时间，54.3%的病例中专家更偏好AI辅助结果。

Conclusion: Janus-Pro-CXR通过轻量级架构和领域特定优化，提高了诊断可靠性和工作流程效率，特别是在资源有限的环境中。模型架构和实施框架将开源，以促进AI辅助放射学解决方案的临床转化。

Abstract: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [39] [Benchmarking LLMs for Predictive Applications in the Intensive Care Units](https://arxiv.org/abs/2512.20520)
*Chehak Malhotra,Mehak Gopal,Akshaya Devadiga,Pradeep Singh,Ridam Pal,Ritwik Kashyap,Tavpritesh Sethi*

Main category: cs.AI

TL;DR: 该研究比较了大型语言模型（LLMs）和特定领域语言模型（SLMs）在预测危重患者休克方面的表现，发现尽管GatorTron-Base在加权召回率上表现最佳，但整体性能相似，表明LLMs在预测临床事件方面并不天然优于SLMs。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，它们在自然语言处理领域的各种任务中展现出强大能力，但在预测性任务中的应用研究相对较少。及时预测休克能够实现早期干预，从而改善患者预后，因此需要评估LLMs在临床预测任务中的实际效果。

Method: 研究比较了GatorTron-Base（基于临床数据训练）、Llama 8B和Mistral 7B等LLMs与BioBERT、DocBERT、BioClinicalBERT、Word2Vec和Doc2Vec等SLMs在预测危重患者休克方面的表现。使用MIMIC III数据库中17,294例ICU住院患者的文本数据，筛选出住院时间>24小时且休克指数>0.7的患者（正常组355例，异常组87例）。在微调过程中使用焦点损失和交叉熵损失来处理类别不平衡问题。

Result: GatorTron-Base获得了最高的加权召回率（80.5%），但LLMs和SLMs之间的整体性能指标相当。这表明尽管LLMs在文本任务上表现出色，但在预测未来临床事件方面并不天然优于SLMs。

Conclusion: 为了实现有意义的临床结果，未来训练LLMs的努力应优先开发能够预测临床轨迹的模型，而不是专注于命名实体识别或表型分析等较简单的任务。LLMs在临床预测任务中并未显示出明显优势，需要针对预测性任务进行专门优化。

Abstract: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.

</details>


### [40] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan,Xiangren Wang,Hongyu Yuan,Qianli Xing*

Main category: cs.AI

TL;DR: 该研究构建了首个大规模教师多模态情感分析数据集T-MED，并提出基于非对称注意力的多模态教师情感分析模型AAM-TSA，显著提升了教师情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 教师的情感状态在教育场景中至关重要，但现有研究往往因教师的表演性质而无法准确捕捉其情感，且忽视了教学信息对情感表达的关键影响。

Method: 1. 构建T-MED数据集：采用人机协同标注流程，包含14,938个教师情感实例，涵盖11个学科从K-12到高等教育的250个真实课堂，整合多模态文本、音频、视频和教学信息。2. 提出AAM-TSA模型：引入非对称注意力机制和分层门控单元，实现差异化的跨模态特征融合和精确的情感分类。

Result: 实验结果表明，AAM-TSA模型在T-MED数据集上的准确性和可解释性方面显著优于现有的最先进方法。

Conclusion: 该研究通过构建大规模多模态数据集和创新模型，为教师情感分析提供了有效的解决方案，有助于提升教学效果和学生参与度。

Abstract: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [41] [When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)](https://arxiv.org/abs/2512.20457)
*Marco Aruta,Francesco Improta,Vadim Malvone,Aniello Murano*

Main category: cs.MA

TL;DR: HumanATLF：一种结合模糊语义和资源约束动作的逻辑，用于多智能体系统的形式化战略推理，考虑了人类决策的现实限制。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统战略推理假设智能体使用任意复杂策略、零成本执行动作、在完全清晰游戏结构上操作，这与人类在现实世界中的决策形成鲜明对比。现有自然策略框架虽然限制了策略复杂度，但仍忽略了动作成本和人类对事实感知的不确定性。

Method: 提出HumanATLF逻辑，在自然策略基础上结合模糊语义和资源约束动作：每个动作具有从不可补充预算中提取的实际成本，原子条件和目标在[0,1]范围内具有度。提供形式化语法和语义，分析模型检查的计算复杂度。

Result: 证明当策略复杂度k和资源预算b固定时，模型检查在P类中；仅允许一个布尔目标战略算子时为NP完全；当k和b变化时为Δ^P_2完全。基于记忆的策略可在PSPACE中判定。在开源工具VITAMIN中实现算法，并在对抗性资源感知无人机救援场景中验证。

Conclusion: HumanATLF逻辑成功地将模糊语义和资源约束整合到多智能体系统战略推理中，更贴近人类决策的现实特征，为实际应用提供了更实用的形式化框架。

Abstract: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Large Language Models for EDA Cloud Job Resource and Lifetime Prediction](https://arxiv.org/abs/2512.19701)
*Yuxuan Yin,Shengke Zhou,Yunjie Zhang,Ajay Mohindra,Boxun Xu,Peng Li*

Main category: cs.LG

TL;DR: 提出基于大语言模型微调的文本到文本回归框架，用于预测EDA云计算的资源和作业生命周期，通过科学记数法和前缀填充提升输出格式可靠性，并在真实数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: EDA行业云计算的快速增长对资源和作业生命周期预测提出了关键需求，传统机器学习方法难以处理EDA工作负载的复杂性和异构性，需要大量特征工程和领域专业知识。

Method: 提出微调大语言模型的框架，采用文本到文本回归方法，引入科学记数法和前缀填充来约束LLM输出格式，并发现全注意力微调和推理能提升滑动窗口注意力LLM的预测精度。

Result: 在真实世界云数据集上验证了所提框架的有效性，为EDA领域的性能预测建立了新的基准。

Conclusion: 通过微调大语言模型并采用文本到文本回归方法，能够有效解决EDA云计算中的资源和作业生命周期预测问题，相比传统方法具有更好的性能。

Abstract: The rapid growth of cloud computing in the Electronic Design Automation (EDA) industry has created a critical need for resource and job lifetime prediction to achieve optimal scheduling. Traditional machine learning methods often struggle with the complexity and heterogeneity of EDA workloads, requiring extensive feature engineering and domain expertise. We propose a novel framework that fine-tunes Large Language Models (LLMs) to address this challenge through text-to-text regression. We introduce the scientific notation and prefix filling to constrain the LLM, significantly improving output format reliability. Moreover, we found that full-attention finetuning and inference improves the prediction accuracy of sliding-window-attention LLMs. We demonstrate the effectiveness of our proposed framework on real-world cloud datasets, setting a new baseline for performance prediction in the EDA domain.

</details>


### [43] [Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches](https://arxiv.org/abs/2512.19713)
*Taoran Sheng,Manfred Huber*

Main category: cs.LG

TL;DR: 本文全面研究了可穿戴设备人体活动识别中的监督学习谱系，提出了一种新颖的弱自监督学习框架，在仅需10%标注数据的情况下达到接近全监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器的人体活动识别面临标注数据获取成本高的问题。全监督方法需要大量标注数据，而无监督方法性能不足。需要探索在标注需求与性能之间取得平衡的新方法。

Method: 开发并比较了六种方法：1)传统全监督学习；2)基本无监督学习；3)带约束的弱监督学习；4)知识共享的多任务学习；5)基于领域知识的自监督学习；6)新颖的弱自监督学习框架（结合领域知识和少量标注数据）。

Result: 实验表明：1)弱监督方法性能接近全监督方法，同时显著减少监督需求；2)多任务框架通过任务间知识共享提升性能；3)弱自监督方法仅需10%标注数据就表现出卓越效率。

Conclusion: 不同学习范式具有互补优势，可根据标注数据可用性定制HAR解决方案。提出的弱自监督框架为标注数据有限的实用HAR应用提供了有前景的解决方案。

Abstract: Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.

</details>


### [44] [Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data](https://arxiv.org/abs/2512.19716)
*Behrooz Mamandipoor,Chun-Nan Hsu,Martin Krause,Ulrich H. Schmidt,Rodney A. Gabriel*

Main category: cs.LG

TL;DR: 开发了一个多模态深度学习模型，利用结构化和非结构化临床数据预测ICU患者入院24小时后的院内死亡风险，在多个外部数据集上验证了模型性能。


<details>
  <summary>Details</summary>
Motivation: 早期预测危重患者的院内死亡率可以帮助临床医生优化治疗方案，但现有方法可能未能充分利用多种临床数据源。

Method: 使用MIMIC-III、MIMIC-IV、eICU和HiRID数据集，开发了多模态深度学习模型，整合时间不变变量、时间变量、临床笔记和胸部X光图像，在入院后24小时内预测后续死亡风险。

Result: 模型在整合结构化数据时AUROC为0.92，AUPRC为0.53，Brier分数为0.19；在eICU的8个不同机构外部验证中AUROC为0.84-0.92；加入临床笔记和影像数据后，AUROC从0.87提升到0.89。

Conclusion: 研究强调了整合多种患者信息源进行死亡率预测的重要性，以及外部验证的必要性，多模态方法能提高预测性能。

Abstract: Early prediction of in-hospital mortality in critically ill patients can aid clinicians in optimizing treatment. The objective was to develop a multimodal deep learning model, using structured and unstructured clinical data, to predict in-hospital mortality risk among critically ill patients after their initial 24 hour intensive care unit (ICU) admission. We used data from MIMIC-III, MIMIC-IV, eICU, and HiRID. A multimodal model was developed on the MIMIC datasets, featuring time series components occurring within the first 24 hours of ICU admission and predicting risk of subsequent inpatient mortality. Inputs included time-invariant variables, time-variant variables, clinical notes, and chest X-ray images. External validation occurred in a temporally separated MIMIC population, HiRID, and eICU datasets. A total of 203,434 ICU admissions from more than 200 hospitals between 2001 to 2022 were included, in which mortality rate ranged from 5.2% to 7.9% across the four datasets. The model integrating structured data points had AUROC, AUPRC, and Brier scores of 0.92, 0.53, and 0.19, respectively. We externally validated the model on eight different institutions within the eICU dataset, demonstrating AUROCs ranging from 0.84-0.92. When including only patients with available clinical notes and imaging data, inclusion of notes and imaging into the model, the AUROC, AUPRC, and Brier score improved from 0.87 to 0.89, 0.43 to 0.48, and 0.37 to 0.17, respectively. Our findings highlight the importance of incorporating multiple sources of patient information for mortality prediction and the importance of external validation.

</details>


### [45] [Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference](https://arxiv.org/abs/2512.19717)
*Zhan Zhang*

Main category: cs.LG

TL;DR: ICFA是一种针对大规模搜索空间的倒置因果聚焦算法，通过目标条件重加权过程重用现有采样器和相似度函数，自适应控制聚焦强度避免退化，在语言生成和稀疏奖励导航中验证效果。


<details>
  <summary>Details</summary>
Motivation: 在语言生成、规划和强化学习等领域，从庞大的候选空间中寻找稀有但有用的解决方案是一个实际挑战，需要高效的方法来减少样本需求。

Method: 提出倒置因果聚焦算法(ICFA)，将搜索视为目标条件重加权过程，重用现有提案采样器和任务特定相似度函数形成聚焦采样分布，自适应控制聚焦强度避免退化，并提供稳定性诊断和理论分析。

Result: 在受限语言生成和稀疏奖励导航两个可复现实验中验证了ICFA的有效性，展示了结构化提示如何实例化ICFA的近似语言级形式，并描述了结合提示推理与算法重加权的混合架构。

Conclusion: ICFA为大规模搜索问题提供了一个实用的框架，能够有效减少样本需求，在语言生成和导航任务中表现出色，并可与现有提示技术结合形成混合解决方案。

Abstract: Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.

</details>


### [46] [Per-Axis Weight Deltas for Frequent Model Updates](https://arxiv.org/abs/2512.19720)
*Stefan Kuyumdzhiev,Radostin Cholakov*

Main category: cs.LG

TL;DR: 提出了一种1-bit权重delta压缩方法，通过存储权重差异的符号和轻量级每轴缩放因子来大幅减少微调模型的存储和冷启动延迟


<details>
  <summary>Details</summary>
Motivation: 为多个任务专门化的LLM变体提供服务时，面临微调检查点体积过大和冷启动延迟高的问题。由于微调权重与基础模型的差异相对较小且具有结构化特征，需要一种高效的压缩表示方法

Method: 提出简单的1-bit delta方案：只存储权重差异的符号，配合轻量级的每轴（行/列）FP16缩放因子。这些缩放因子通过小型校准集学习得到，在保持1-bit delta紧凑性的同时更准确地捕捉权重维度的变化

Result: 该方法相比标量替代方案显著提高了重建质量。从系统角度看，简化的加载器通过每个模块的单次操作传输打包的delta，减少了冷启动延迟和存储开销，生成的工件比完整FP16检查点小数倍

Conclusion: 该方法即插即用，需要最小化的校准数据，通过避免密集重建保持推理效率，为频繁模型更新提供了有效的解决方案

Abstract: Serving many task-specialized LLM variants is often limited by the large size of fine-tuned checkpoints and the resulting cold-start latency. Since fine-tuned weights differ from their base model by relatively small structured residuals, a natural approach is to represent them as compressed deltas. We propose a simple 1-bit delta scheme that stores only the sign of the weight difference together with lightweight per-axis (row/column) FP16 scaling factors, learned from a small calibration set. This design preserves the compactness of 1-bit deltas while more accurately capturing variation across weight dimensions, leading to improved reconstruction quality over scalar alternatives. From a systems perspective, a streamlined loader that transfers packed deltas in a single operation per module reduces cold-start latency and storage overhead, with artifacts several times smaller than a full FP16 checkpoint. The method is drop-in, requires minimal calibration data, and maintains inference efficiency by avoiding dense reconstruction. Our experimental setup and source code are available at https://github.com/kuiumdjiev/Per-Axis-Weight-Deltas-for-Frequent-Model-Updates.

</details>


### [47] [Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals](https://arxiv.org/abs/2512.19721)
*Vineet Yadav*

Main category: cs.LG

TL;DR: 提出了一种符号感知的多状态Jaccard/Tanimoto框架，将基于重叠的距离从非负向量扩展到任意实值和复值信号，同时保持有界度量结构和正半定核。


<details>
  <summary>Details</summary>
Motivation: 现有Jaccard/Tanimoto相似度方法主要针对非负向量，无法直接处理包含正负值的实值信号和复值信号。需要一种能够处理任意符号信号同时保持良好数学性质的统一框架。

Method: 将信号表示为带符号状态空间上的原子测度，通过正负分割（实信号）、笛卡尔和极坐标分解（复信号）将信号嵌入到非负多状态表示中，应用Tanimoto构造得到[0,1]范围内的距离度量。

Result: 构建的框架产生有界度量距离，满足三角不等式，定义正半定核可直接用于核方法和基于图的学习。通过Möbius反演实现联盟分析，将信号幅度分解为非负可加贡献。

Conclusion: 该框架提供了一个统一的符号感知距离度量，同时具备有界度量结构、正半定核、概率语义和透明预算核算，适用于科学和金融应用中的相关图、特征工程、相似性图等分析工具。

Abstract: We introduce a sign-aware, multistate Jaccard/Tanimoto framework that extends overlap-based distances from nonnegative vectors and measures to arbitrary real- and complex-valued signals while retaining bounded metric and positive-semidefinite kernel structure. Formally, the construction is a set- and measure-theoretic geometry: signals are represented as atomic measures on a signed state space, and similarity is given by a generalized Jaccard overlap of these measures. Each signal is embedded into a nonnegative multistate representation, using positive/negative splits for real signals, Cartesian and polar decompositions for complex signals, and user-defined state partitions for refined regime analysis. Applying the Tanimoto construction to these embeddings yields a family of $[0,1]$ distances that satisfy the triangle inequality and define positive-semidefinite kernels usable directly in kernel methods and graph-based learning. Beyond pairwise distances, we develop coalition analysis via Möbius inversion, which decomposes signal magnitude into nonnegative, additive contributions with exact budget closure across coalitions of signals. Normalizing the same embeddings produces probability measures on coordinate -- state configurations, so that the distance becomes a monotone transform of total variation and admits a regime -- intensity decomposition. The resulting construction yields a single, mechanistically interpretable distance that simultaneously provides bounded metric structure, positive-semidefinite kernels, probabilistic semantics, and transparent budget accounting within one sign-aware framework, supporting correlograms, feature engineering, similarity graphs, and other analytical tools in scientific and financial applications.

</details>


### [48] [Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism](https://arxiv.org/abs/2512.19722)
*Alessandro Casadei,Clemens Grupp,Sreyoshi Bhaduri,Lu Guo,Wilson Fung,Rohit Malshe,Raj Ratan,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: 提出基于节点特定成本函数不对称性的预测调整方法，通过动态整合成本不对称性到预测误差概率分布中，实现年度510万美元的节约


<details>
  <summary>Details</summary>
Motivation: 传统预测方法未考虑节点特定的成本函数不对称性，导致在实际应用中可能产生不必要的成本。需要一种能够根据成本不对称性动态调整预测的方法来优化决策结果。

Method: 提出一种将成本不对称性动态整合到预测误差概率分布中的方法，使预测偏向成本最低的情景。采用自我调节机制根据观察到的节约调整调整幅度，适应站点特定条件和未建模因素。

Result: 实证结果表明该方法能够实现510万美元的年度节约，证明了模型在实际应用中的有效性。

Conclusion: 该方法通过考虑成本不对称性显著提高了预测的经济效益，自我调节机制使其能够适应各种实际条件，具有重要的实际应用价值。

Abstract: This work introduces a methodology to adjust forecasts based on node-specific cost function asymmetry. The proposed model generates savings by dynamically incorporating the cost asymmetry into the forecasting error probability distribution to favor the least expensive scenario. Savings are calculated and a self-regulation mechanism modulates the adjustments magnitude based on the observed savings, enabling the model to adapt to station-specific conditions and unmodeled factors such as calibration errors or shifting macroeconomic dynamics. Finally, empirical results demonstrate the model's ability to achieve \$5.1M annual savings.

</details>


### [49] [End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment](https://arxiv.org/abs/2512.19723)
*Firas Bayram,Bestoun S. Ahmed,Erik Hallin*

Main category: cs.LG

TL;DR: 提出一个端到端框架，将数据质量评估与机器学习模型操作实时集成，在钢铁制造ESR真空泵过程中验证了12%性能提升和4倍延迟降低


<details>
  <summary>Details</summary>
Motivation: 现有方法将数据质量评估和ML系统作为独立流程处理，存在理论与实际应用的差距，需要解决动态工业环境中实时、质量驱动的ML决策问题

Method: 开发了一个轻量级端到端框架，整合动态漂移检测、自适应数据质量指标和MLOps，实现实时质量驱动的ML决策，计算开销最小

Result: 在钢铁制造公司ESR真空泵过程中验证，模型性能提升12%（R2=94%），预测延迟降低4倍，探索了数据质量可接受阈值对预测性能的影响

Conclusion: 该框架代表了MLOps的重要进展，为动态工业环境中时间敏感的数据驱动决策提供了稳健解决方案，平衡了数据质量标准与预测性能

Abstract: This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.

</details>


### [50] [Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking](https://arxiv.org/abs/2512.19725)
*Srishti Gupta,Riccardo Balia,Daniele Angioni,Fabio Brau,Maura Pintor,Ambra Demontis,Alessandro Sebastian,Salvatore Mario Carta,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 论文探讨了机器学习模型在现实世界部署中面临的挑战，特别是模型需要适应不断变化的数据分布和检测异常输入的问题，提出了联合解决持续学习和分布外检测的重要性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习模型面临数据分布随时间变化和遇到意外输入的问题，传统的i.i.d.假设不成立，重新训练模型成本高昂且不切实际，需要开发能够持续学习和检测异常的系统。

Method: 论文提出了联合解决持续学习和分布外检测的方法框架，使模型能够从不断演化的数据流中增量学习而不遗忘过去知识，同时识别和响应新颖或异常输入。

Result: 通过联合处理持续学习和分布外检测，可以开发出更鲁棒、高效和自适应的AI系统，能够应对现实世界中的动态变化和不确定性。

Conclusion: 联合解决持续学习和分布外检测对于构建能够在现实世界中可靠运行的机器学习系统至关重要，这是实现真正自适应AI的关键方向。

Abstract: Recent years have witnessed significant progress in the development of machine learning models across a wide range of fields, fueled by increased computational resources, large-scale datasets, and the rise of deep learning architectures. From malware detection to enabling autonomous navigation, modern machine learning systems have demonstrated remarkable capabilities. However, as these models are deployed in ever-changing real-world scenarios, their ability to remain reliable and adaptive over time becomes increasingly important. For example, in the real world, new malware families are continuously developed, whereas autonomous driving cars are employed in many different cities and weather conditions. Models trained in fixed settings can not respond effectively to novel conditions encountered post-deployment. In fact, most machine learning models are still developed under the assumption that training and test data are independent and identically distributed (i.i.d.), i.e., sampled from the same underlying (unknown) distribution. While this assumption simplifies model development and evaluation, it does not hold in many real-world applications, where data changes over time and unexpected inputs frequently occur. Retraining models from scratch whenever new data appears is computationally expensive, time-consuming, and impractical in resource-constrained environments. These limitations underscore the need for Continual Learning (CL), which enables models to incrementally learn from evolving data streams without forgetting past knowledge, and Out-of-Distribution (OOD) detection, which allows systems to identify and respond to novel or anomalous inputs. Jointly addressing both challenges is critical to developing robust, efficient, and adaptive AI systems.

</details>


### [51] [Hard Negative Sample-Augmented DPO Post-Training for Small Language Models](https://arxiv.org/abs/2512.19728)
*Haocheng Lu,Minjun Zhu,Henry Yu*

Main category: cs.LG

TL;DR: 提出一个轻量级后训练流程，使用小型数学验证器分析解题过程中的结构化错误，通过验证器信号挖掘困难负样本并定义重要性权重，结合加权DPO提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 当前LLM在数学推理上仍有困难，常见的后训练流程将解题结果简化为二元判断（正确/错误），这种视角在实践中有限，因为思维链推理中的失败通常是结构化的——看似合理的解法可能包含微妙的逻辑、代数或数值错误。同时，依赖大型奖励模型或LLM作为评判的RLHF变体通常昂贵、难以扩展且迭代不稳定。

Method: 1. 从MetaMathQA风格的思维链数据进行监督微调（SFT）开始；2. 引入紧凑的MathVerifier，将候选解法分解为六维错误特征，并聚合为可解释的错误性和荒谬性分数；3. 验证器信号用于两个目的：(i) 挖掘接近正确但存在结构性缺陷的困难负样本，(ii) 定义强调最具信息量的偏好对的每样本重要性权重；4. 通过验证器引导的加权公式将两者集成到离线直接偏好优化（DPO）目标中。

Result: 在1.5B参数的Qwen2.5模型上的实验表明，验证器引导的加权DPO比普通的SFT和未加权的DPO产生更有针对性的改进，特别是在解法数值上接近正确但逻辑不一致的问题上，同时避免了训练大型奖励模型或依赖外部评判的开销。

Conclusion: 提出的轻量级后训练流程能够有效针对数学推理中的结构化错误，通过小型验证器提供细粒度反馈，结合加权DPO实现更精准的改进，为实际计算预算下的数学推理能力提升提供了实用解决方案。

Abstract: Large language models (LLMs) continue to struggle with mathematical reasoning, and common post-training pipelines often reduce each generated solution to a binary outcome: correct or incorrect. This perspective is limiting in practice, as failures in chain-of-thought (CoT) reasoning are frequently structured; solutions may appear convincing while containing subtle logical, algebraic, or numerical flaws. Meanwhile, reinforcement learning from human feedback (RLHF) variants that rely on large reward models or LLM-as-a-judge signals are often expensive, difficult to scale, and unstable to iterate. We propose a lightweight and pragmatic post-training pipeline that targets such structured errors under realistic compute budgets. Starting from supervised fine-tuning (SFT) on MetaMathQA-style CoT data, we introduce a compact MathVerifier that decomposes a candidate solution into a six-dimensional error profile and aggregates it into interpretable wrongness and absurdity scores. These verifier signals serve two roles: (i) mining hard negatives that are near-correct yet structurally flawed, and (ii) defining per-sample importance weights that emphasize the most informative preference pairs. We integrate both into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model show that verifier-guided, weighted DPO yields more targeted improvements than vanilla SFT and unweighted DPO, particularly on problems where solutions are numerically close to correct but logically inconsistent, while avoiding the overhead of training large reward models or relying on external judges.

</details>


### [52] [High-Performance Self-Supervised Learning by Joint Training of Flow Matching](https://arxiv.org/abs/2512.19729)
*Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: FlowFM是一种基于流匹配的基础模型，通过联合训练表示编码器和条件流匹配生成器，解决了扩散模型在生成质量与判别性能之间的权衡问题，同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在自监督学习中展现出潜力，但面临生成质量与判别性能的权衡问题，且迭代采样过程计算成本高，阻碍了工业和边缘AI应用。

Method: 提出FlowFM模型，采用解耦设计联合训练表示编码器和条件流匹配生成器，通过流匹配学习更简单的速度场，加速训练并提高表示学习效率。

Result: 在可穿戴传感器数据上，FlowFM相比基于扩散的方法减少50.4%训练时间；在下游任务中超越最先进的SSL方法（SSL-Wearables），推理速度提升高达51.0倍，同时保持高生成质量。

Conclusion: FlowFM通过流匹配技术有效解决了扩散模型在自监督学习中的计算效率和性能权衡问题，为工业和边缘AI应用提供了高效解决方案。

Abstract: Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at https://github.com/Okita-Laboratory/jointOptimizationFlowMatching.

</details>


### [53] [ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures](https://arxiv.org/abs/2512.19730)
*Zhonghao Yang,Cheng Luo,Daojing He,Yiming Li,Yu Li*

Main category: cs.LG

TL;DR: 提出ArcGen方法，通过特征对齐层和两种对齐损失函数，实现跨架构的神经后门检测，在未见过的模型架构上检测性能提升高达42.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的神经后门检测方法在训练阶段未见过的模型架构上泛化能力不足，需要解决架构不变特征提取的问题。

Method: 提出ArcGen方法：1) 在特征提取函数中添加对齐层，减少架构信息对特征的直接影响；2) 设计两种对齐损失函数，要求具有相似后门行为但不同架构的模型在分布和样本级别对齐特征。

Result: 在涉及16,896个模型的大规模评估中，ArcGen在未见过的模型架构上检测性能（如AUC）提升高达42.5%，显著优于现有方法。

Conclusion: ArcGen通过提取架构不变特征有效解决了现有后门检测方法跨架构泛化不足的问题，为黑盒神经后门检测提供了更可靠的解决方案。

Abstract: Backdoor attacks pose a significant threat to the security and reliability of deep learning models. To mitigate such attacks, one promising approach is to learn to extract features from the target model and use these features for backdoor detection. However, we discover that existing learning-based neural backdoor detection methods do not generalize well to new architectures not seen during the learning phase. In this paper, we analyze the root cause of this issue and propose a novel black-box neural backdoor detection method called ArcGen. Our method aims to obtain architecture-invariant model features, i.e., aligned features, for effective backdoor detection. Specifically, in contrast to existing methods directly using model outputs as model features, we introduce an additional alignment layer in the feature extraction function to further process these features. This reduces the direct influence of architecture information on the features. Then, we design two alignment losses to train the feature extraction function. These losses explicitly require that features from models with similar backdoor behaviors but different architectures are aligned at both the distribution and sample levels. With these techniques, our method demonstrates up to 42.5% improvements in detection performance (e.g., AUC) on unseen model architectures. This is based on a large-scale evaluation involving 16,896 models trained on diverse datasets, subjected to various backdoor attacks, and utilizing different model architectures. Our code is available at https://github.com/SeRAlab/ArcGen.

</details>


### [54] [Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis](https://arxiv.org/abs/2512.19732)
*Gaurav Kumar Sharma*

Main category: cs.LG

TL;DR: 该研究对JARVIS-DFT带隙数据集进行系统分析，移除可能编码能带结构信息的描述符，创建了包含2280种材料的防泄漏数据集，并建立了三阶段建模框架。


<details>
  <summary>Details</summary>
Motivation: 解决带隙预测中描述符可能无意编码能带结构信息（如有效质量）的数据泄漏问题，为未来的防泄漏带隙预测研究提供基准数据集和性能指标。

Method: 1. 系统分析JARVIS-DFT带隙数据集，识别并移除可能编码能带结构信息的描述符；2. 创建包含2280种材料的防泄漏数据集；3. 实施三阶段建模框架：逐步引入基本物理描述符、工程特征和成分属性；4. 使用树基模型进行预测，并进行SHAP分析。

Result: 树基模型在所有三个阶段都达到R²值约0.88-0.90，表明在控制泄漏后，扩展描述符空间不会显著提高预测精度。SHAP分析一致显示介电张量分量是主要贡献者。

Conclusion: 该工作提供了经过筛选的防泄漏数据集和基准性能指标，强调了控制数据泄漏对带隙预测的重要性，并发现介电张量分量是预测带隙的关键描述符。

Abstract: In this study, we perform a systematic analysis of the JARVIS-DFT bandgap dataset and identify and remove descriptors that may inadvertently encode band-structure information, such as effective masses. This process yields a curated, leakage-controlled subset of 2280 materials. Using this dataset, a three-phase modeling framework is implemented that incrementally incorporates basic physical descriptors, engineered features, and compositional attributes. The results show that tree-based models achieve R2 values of approximately 0.88 to 0.90 across all phases, indicating that expanding the descriptor space does not substantially improve predictive accuracy when leakage is controlled. SHAP analysis consistently identifies the dielectric tensor components as the dominant contributors. This work provides a curated dataset and baseline performance metrics for future leakage-aware bandgap prediction studies.

</details>


### [55] [Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction](https://arxiv.org/abs/2512.19735)
*Gangxiong Zhang,Yongchao Long*

Main category: cs.LG

TL;DR: 提出CAP框架，通过训练无关的临床自适应提示改善ICU死亡率预测的公平性和准确性，减少人口统计学偏见。


<details>
  <summary>Details</summary>
Motivation: ICU患者死亡率预测对临床决策至关重要，但大型语言模型在预测时可能表现出性别、年龄和种族等人口统计学偏见，现有去偏方法往往降低预测性能，难以同时优化公平性和准确性。

Method: 开发多维偏见评估方案进行模型诊断，提出CAP框架，整合传统去偏提示与基于案例的推理，引导模型从类似历史误判案例及其正确结果中学习，纠正偏见推理模式。

Result: 在MIMIC-IV数据集上，CAP显著提高预测准确性和公平性：AUROC从0.806提升至0.873，AUPRC从0.497提升至0.694，性别和种族相关差异减少超过90%，特征依赖分析显示跨人口统计学群体的注意力模式高度一致（相似度得分>0.98）。

Conclusion: LLMs在ICU死亡率预测中存在可测量的偏见，精心设计的提示框架可以在不重新训练的情况下有效协同优化公平性和性能，为公平的临床决策支持提供可转移的范式。

Abstract: Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.

</details>


### [56] [OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting](https://arxiv.org/abs/2512.19738)
*Wilson Fung,Lu Guo,Drake Hilliard,Alessandro Casadei,Raj Ratan,Sreyoshi Bhaduri,Adi Surve,Nikhil Agarwal,Rohit Malshe,Pavan Mullapudi,Hungjen Wang,Saurabh Doodhwala,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: OpComm是一个结合监督学习和强化学习的包裹量预测框架，通过LightGBM回归预测需求，PPO智能体控制缓冲水平，生成式AI提供可解释性，在400多个站点将预测误差降低21.65%


<details>
  <summary>Details</summary>
Motivation: 最后一公里物流中包裹量的准确预测至关重要，预测错误会导致资源分配低效、成本增加和配送延误。现有方法缺乏将预测与决策支持有效结合的框架，且可解释性不足。

Method: 1. 使用LightGBM回归模型生成站点级需求预测；2. 基于PPO的强化学习智能体从离散动作集中选择缓冲水平，奖励函数对缓冲不足的惩罚大于缓冲过量；3. 通过蒙特卡洛更新机制反馈站点结果实现持续策略适应；4. 生成式AI层基于SHAP特征归因生成执行级摘要和场景分析。

Result: 在400多个站点上，OpComm将加权绝对百分比误差(WAPE)降低了21.65%，相比人工预测显著减少了缓冲不足事件，同时提高了决策者的透明度。

Conclusion: OpComm展示了上下文强化学习与预测建模相结合如何解决运营预测挑战，在高风险物流环境中将统计严谨性与实际决策制定连接起来，为操作预测提供了可扩展且可解释的解决方案。

Abstract: Accurate forecasting of package volumes at delivery stations is critical for last-mile logistics, where errors lead to inefficient resource allocation, higher costs, and delivery delays. We propose OpComm, a forecasting and decision-support framework that combines supervised learning with reinforcement learning-based buffer control and a generative AI-driven communication module. A LightGBM regression model generates station-level demand forecasts, which serve as context for a Proximal Policy Optimization (PPO) agent that selects buffer levels from a discrete action set. The reward function penalizes under-buffering more heavily than over-buffering, reflecting real-world trade-offs between unmet demand risks and resource inefficiency. Station outcomes are fed back through a Monte Carlo update mechanism, enabling continual policy adaptation. To enhance interpretability, a generative AI layer produces executive-level summaries and scenario analyses grounded in SHAP-based feature attributions. Across 400+ stations, OpComm reduced Weighted Absolute Percentage Error (WAPE) by 21.65% compared to manual forecasts, while lowering under-buffering incidents and improving transparency for decision-makers. This work shows how contextual reinforcement learning, coupled with predictive modeling, can address operational forecasting challenges and bridge statistical rigor with practical decision-making in high-stakes logistics environments.

</details>


### [57] [Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics](https://arxiv.org/abs/2512.19740)
*Kousar Raza,Faizan Ali*

Main category: cs.LG

TL;DR: 该论文创建并发布了2025年亚洲杯T20板球锦标赛的全面数据集，包含19场比赛的61个变量，用于支持板球分析和预测建模研究。


<details>
  <summary>Details</summary>
Motivation: 为板球分析研究提供标准化、全面的数据集，支持数据驱动的体育分析、预测建模和战略决策研究。

Method: 收集2025年亚洲杯T20锦标赛所有19场比赛数据，构建包含61个变量的结构化数据集，涵盖球队得分、三柱门、强力击球阶段统计、边界球数、掷硬币决定、场地和球员表现等指标。

Result: 创建了全面的板球数据集，通过探索性数据分析展示了球队表现指标、边界分布和得分模式的分析价值，数据集已在Zenodo平台公开发布，采用CC-BY 4.0许可。

Conclusion: 该工作为板球分析研究提供了开放、机器可读的基准数据集，支持可重复性研究和板球分析领域的进一步发展。

Abstract: This paper presents a structured and comprehensive dataset corresponding to the 2025 Asia Cup T20 cricket tournament, designed to facilitate data-driven research in sports analytics. The dataset comprises records from all 19 matches of the tournament and includes 61 variables covering team scores, wickets, powerplay statistics, boundary counts, toss decisions, venues, and player-specific highlights. To demonstrate its analytical value, we conduct an exploratory data analysis focusing on team performance indicators, boundary distributions, and scoring patterns. The dataset is publicly released through Zenodo under a CC-BY 4.0 license to support reproducibility and further research in cricket analytics, predictive modeling, and strategic decision-making. This work contributes an open, machine-readable benchmark dataset for advancing cricket analytics research.

</details>


### [58] [EdgeFlex-Transformer: Transformer Inference for Edge Devices](https://arxiv.org/abs/2512.19741)
*Shoaib Mohammad,Guanqun Song,Ting Zhu*

Main category: cs.LG

TL;DR: 提出一个轻量级多阶段优化流程，用于压缩和加速Vision Transformers在边缘设备上的部署，通过激活分析、内存感知剪枝、混合精度执行和AWQ量化，在CIFAR-10上实现76%内存减少和6倍延迟降低。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大规模Transformer模型面临内存、计算和延迟的严格约束，需要高效的压缩和加速方法。

Method: 采用多阶段优化流程：1) 通过前向钩子收集激活统计识别低重要性通道；2) 内存感知结构化剪枝缩小MLP层；3) 选择性FP16转换；4) 激活感知量化(AWQ)将权重和激活量化为INT8。

Result: 从6.32亿参数的ViT-Huge模型出发，在CIFAR-10上实现76%峰值内存使用减少，延迟降低超过6倍，同时保持甚至提升准确率。

Conclusion: 该框架为边缘平台上的高效Transformer推理提供了实用路径，并为集成动态稀疏性和MoE架构以进一步提升性能开辟了未来方向。

Abstract: Deploying large-scale transformer models on edge devices presents significant challenges due to strict constraints on memory, compute, and latency. In this work, we propose a lightweight yet effective multi-stage optimization pipeline designed to compress and accelerate Vision Transformers (ViTs) for deployment in resource-constrained environments. Our methodology combines activation profiling, memory-aware pruning, selective mixed-precision execution, and activation-aware quantization (AWQ) to reduce the model's memory footprint without requiring costly retraining or task-specific fine-tuning. Starting from a ViT-Huge backbone with 632 million parameters, we first identify low-importance channels using activation statistics collected via forward hooks, followed by structured pruning to shrink the MLP layers under a target memory budget. We further apply FP16 conversion to selected components and leverage AWQ to quantize the remaining model weights and activations to INT8 with minimal accuracy degradation. Our experiments on CIFAR-10 demonstrate that the fully optimized model achieves a 76% reduction in peak memory usage and over 6x lower latency, while retaining or even improving accuracy compared to the original FP32 baseline. This framework offers a practical path toward efficient transformer inference on edge platforms, and opens future avenues for integrating dynamic sparsity and Mixture-of-Experts (MoE) architectures to further scale performance across diverse tasks.

</details>


### [59] [On-device Large Multi-modal Agent for Human Activity Recognition](https://arxiv.org/abs/2512.19742)
*Md Shakhrul Iman Siam,Ishtiaque Ahmed Showmik,Guanqun Song,Ting Zhu*

Main category: cs.LG

TL;DR: 提出了一种用于人类活动识别的大规模多模态智能体，结合大语言模型提升分类性能和可解释性


<details>
  <summary>Details</summary>
Motivation: 人类活动识别在医疗保健和智能环境中有广泛应用，但传统方法缺乏可解释性和人机交互能力。大语言模型的发展为HAR带来了新机遇，不仅能进行分类，还能提供可解释性和类似人类的交互体验。

Method: 设计了一个大型多模态智能体框架，整合大语言模型的能力，通过推理和问答功能将技术输出转化为用户友好的洞察。该框架不仅能进行活动分类，还能提供解释和交互。

Result: 在HHAR、Shoaib、Motionsense等广泛使用的HAR数据集上进行评估，结果显示模型达到了与最先进方法相当的高分类准确率，同时通过推理和问答能力显著提升了可解释性。

Conclusion: 该研究成功开发了一个结合大语言模型的人类活动识别框架，在保持高分类性能的同时显著提升了系统的可解释性和用户参与度，为HAR领域提供了新的发展方向。

Abstract: Human Activity Recognition (HAR) has been an active area of research, with applications ranging from healthcare to smart environments. The recent advancements in Large Language Models (LLMs) have opened new possibilities to leverage their capabilities in HAR, enabling not just activity classification but also interpretability and human-like interaction. In this paper, we present a Large Multi-Modal Agent designed for HAR, which integrates the power of LLMs to enhance both performance and user engagement. The proposed framework not only delivers activity classification but also bridges the gap between technical outputs and user-friendly insights through its reasoning and question-answering capabilities. We conduct extensive evaluations using widely adopted HAR datasets, including HHAR, Shoaib, Motionsense to assess the performance of our framework. The results demonstrate that our model achieves high classification accuracy comparable to state-of-the-art methods while significantly improving interpretability through its reasoning and Q&A capabilities.

</details>


### [60] [From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning](https://arxiv.org/abs/2512.19743)
*Sasan Sharifipour,Constantino Álvarez Casado,Manuel Lage Cañellas,Miguel Bordallo López*

Main category: cs.LG

TL;DR: CUDA-APML是一种稀疏GPU实现，通过阈值化可忽略的分配和自适应softmax，在COO格式中直接运行，实现了近线性内存缩放，将峰值GPU内存减少99.9%，同时保持与密集APML相近的精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云损失函数存在权衡：Chamfer距离高效但允许多对一对应关系，Earth Mover距离能更好反映一对一传输但计算成本高，APML通过可微Sinkhorn迭代近似传输但密集公式存在二次内存缩放问题。

Method: 提出CUDA-APML稀疏GPU实现，通过阈值化可忽略的分配、运行自适应softmax、双向对称化和直接在COO格式中进行Sinkhorn归一化，实现近线性内存缩放，同时保留存储支持上的梯度。

Result: 在ShapeNet和MM-Fi数据集上，CUDA-APML在较小容差范围内匹配密集APML的性能，同时将峰值GPU内存减少99.9%，成对距离评估在当前实现中仍保持二次复杂度。

Conclusion: CUDA-APML通过稀疏GPU实现解决了APML的内存缩放问题，实现了高效的内存使用和计算性能，为3D点云学习提供了更实用的损失函数实现。

Abstract: Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml

</details>


### [61] [Learning to Reason in LLMs by Expectation Maximization](https://arxiv.org/abs/2512.20169)
*Junghyun Lee,Branislav Kveton,Sunav Choudhary,Subhojyoti Mukherjee,Anup Rao,Ryan A. Rossi,Alexa Siu*

Main category: cs.LG

TL;DR: 该研究将推理建模为隐变量模型，推导出EM学习目标，比较了不同采样方案对LLM推理能力的影响，发现简单的提示后验采样效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前LLM通过生成推理过程再回答问题的模式存在优化空间，需要系统化地研究如何通过采样策略提升推理模型的准确性。

Method: 将推理形式化为隐变量模型，推导EM目标，比较三种采样方案：带预算的拒绝采样、自教推理器（STaR）和仅保留推理阶段的提示后验采样（PPS）。

Result: 在ARC、MMLU和OpenBookQA数据集上使用Llama和Qwen模型的实验表明，采样方案显著影响推理模型的准确性，尽管简单，PPS优于其他采样方案。

Conclusion: 推理作为隐变量模型的视角连接了EM和现代基于奖励的优化，采样方案设计是提升推理能力的关键，简单的PPS方法表现出色。

Abstract: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.

</details>


### [62] [A K-Means, Ward and DBSCAN repeatability study](https://arxiv.org/abs/2512.19772)
*Anthony Bertrand,Engelbert Mephu Nguifo,Violaine Antoine,David Hill*

Main category: cs.LG

TL;DR: 该论文分析了K-Means、DBSCAN和Ward聚类算法的可重复性问题，发现当OpenMP线程数超过2时，K-Means会产生不一致的结果，旨在提高用户和开发者对此问题的认识。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的可重复性对于确保模型或实验得出相同科学结论至关重要。对于特定算法，比特级相同的可重复结果也是科学完整性的关键，因为它允许调试。作者旨在提高用户和开发者对聚类算法可重复性问题的认识。

Method: 将K-Means、DBSCAN和Ward聚类算法分解为基本步骤，识别每个阶段实现可重复性所需的条件。使用Python库scikit-learn的实现示例来检查每个方法的可重复性方面。

Result: 研究发现当OpenMP线程数超过2时，K-Means会产生不一致的结果。通过分析揭示了每个聚类算法在不同条件下的可重复性特征。

Conclusion: 这项工作旨在提高用户和开发者对聚类算法可重复性问题的认识，鼓励进一步调查和潜在修复，特别是在并行计算环境下确保算法的可重复性。

Abstract: Reproducibility is essential in machine learning because it ensures that a model or experiment yields the same scientific conclusion. For specific algorithms repeatability with bitwise identical results is also a key for scientific integrity because it allows debugging. We decomposed several very popular clustering algorithms: K-Means, DBSCAN and Ward into their fundamental steps, and we identify the conditions required to achieve repeatability at each stage. We use an implementation example with the Python library scikit-learn to examine the repeatable aspects of each method. Our results reveal inconsistent results with K-Means when the number of OpenMP threads exceeds two. This work aims to raise awareness of this issue among both users and developers, encouraging further investigation and potential fixes.

</details>


### [63] [Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy](https://arxiv.org/abs/2512.19805)
*Deepit Sapru*

Main category: cs.LG

TL;DR: 本文提出了一种营销决策框架，将异质性处理提升转化为约束性目标策略，在遵守业务护栏的同时最大化收入和留存率。


<details>
  <summary>Details</summary>
Motivation: 营销活动需要在最大化关键绩效指标（如收入和留存率）的同时，遵守预算限制、客户体验约束等业务护栏。传统方法如倾向性评分缺乏对异质性处理效应的考虑，无法在约束条件下优化目标策略。

Method: 框架首先使用提升学习器估计条件平均处理效应（CATE），然后通过约束分配优化来决定针对哪些用户以及部署何种优惠，同时满足预算、可接受的销售下降等限制条件。

Result: 在留存消息、事件奖励和消费阈值分配等应用中，该框架在离线评估中（使用提升AUC、逆倾向性评分IPS和自归一化IPS）持续优于倾向性评分和静态基线方法。在线A/B测试进一步验证了在保持客户体验约束的同时，对收入和完成率的战略提升。

Conclusion: 该框架为营销人员提供了一个可重复使用的操作手册，用于规模化实施因果目标策略、设置业务护栏，并使营销活动与战略关键绩效指标保持一致。

Abstract: This paper introduces a marketing decision framework that converts heterogeneous-treatment uplift into constrained targeting strategies to maximize revenue and retention while honoring business guardrails. The approach estimates Conditional Average Treatment Effects (CATE) with uplift learners and then solves a constrained allocation to decide who to target and which offer to deploy under limits such as budget or acceptable sales deterioration. Applied to retention messaging, event rewards, and spend-threshold assignment, the framework consistently outperforms propensity and static baselines in offline evaluations using uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). A production-scale online A/B test further validates strategic lift on revenue and completion while preserving customer-experience constraints. The result is a reusable playbook for marketers to operationalize causal targeting at scale, set guardrails, and align campaigns with strategic KPIs.

</details>


### [64] [Fine-Tuned In-Context Learners for Efficient Adaptation](https://arxiv.org/abs/2512.19879)
*Jorg Bornschein,Clare Lyle,Yazhe Li,Amal Rannen-Triki,Xu Owen He,Razvan Pascanu*

Main category: cs.LG

TL;DR: 该研究提出了一种统一方法，将上下文学习直接融入微调过程，通过用上下文示例增强任务特定数据来微调模型，结合了上下文学习的样本效率和微调的性能优势。


<details>
  <summary>Details</summary>
Motivation: 当前LLM适应下游任务主要有两种方法：提示工程（上下文少样本学习）和任务特定数据微调。提示方法在少样本场景表现好但数据增多后效果有限，微调方法数据多时效果好但少样本时表现不佳。需要一种能结合两者优势的统一方法。

Method: 提出统一方法：在微调过程中直接融入上下文学习，使用包含上下文示例的任务特定数据进行微调，模拟k-shot提示的结构。同时提出使用预序评估进行超参数选择，避免昂贵的交叉验证，充分利用所有可用数据。

Result: 该方法在少样本场景中结合了上下文学习的样本效率和微调的性能增益，在多个下游任务上一致匹配并显著超过两种基线方法（微调和上下文学习）。

Conclusion: 提出的统一方法有效结合了微调和上下文学习的优势，在少样本到多数据场景中都能提供优越的预测性能，为LLM适应下游任务提供了更有效的解决方案。

Abstract: When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.

</details>


### [65] [Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling](https://arxiv.org/abs/2512.19905)
*Indranil Halder,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: 论文研究了推理时间缩放的理论基础，通过贝叶斯线性回归模型分析LLM推理时采样选择对泛化误差的影响，发现适度奖励偏差下误差随采样数k单调下降，但严重偏差时存在最优k值，且存在最佳采样温度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型将计算资源从训练时间重新分配到推理时间，但推理时间缩放的基本原理尚不明确。需要建立可分析的理论模型来理解推理时间计算如何影响模型性能。

Method: 引入贝叶斯线性回归与奖励加权采样器的可分析模型，在高维状态下研究后验预测均值和方差。分析训练数据来自教师模型时的泛化误差，通过softmax温度应用于二次奖励进行推理时间采样选择。

Result: 当奖励与教师模型差异不大时，泛化误差随推理时间采样数k单调下降；但严重奖励偏差会导致存在有限最优k值，超过该值更多采样反而增加误差。对于固定k，存在最优采样温度。实验验证了这些结论。

Conclusion: 论文建立了推理时间缩放的理论框架，揭示了推理时间计算与数据收集之间的权衡关系，并表明任务难度增加时推理时间计算的优势会减弱。为确定何时扩展推理时间计算优于收集更多数据提供了理论依据。

Abstract: Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the "best-of-$k$" limit with the teacher as reward, we theoretically show that the generalization error decays as $Θ(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.

</details>


### [66] [Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra](https://arxiv.org/abs/2512.19909)
*Maxime Lacour,Pu Ren,Rie Nakata,Nori Nakata,Michael Mahoney*

Main category: cs.LG

TL;DR: 提出基于深度学习的CGM-FAS方法替代传统高斯过程建模非遍历地震动路径效应，实现高效多频率大空间域预测


<details>
  <summary>Details</summary>
Motivation: 传统基于高斯过程的非遍历地震动模型计算效率低，难以处理大规模预测问题，需要更高效的方法来建模空间变化和频率相关性

Method: 使用条件变分自编码器架构，以地震和台站地理坐标为条件变量，直接从数据中学习空间模式和频率间相关性

Result: CGM-FAS与现有GP方法预测结果一致，但具有无需预设相关函数、能捕获频率相关性、计算高效等优势，可在10秒内生成1万个站点1000个频率的预测图

Conclusion: CGM-FAS为多频率大空间域的非遍历地震动预测提供了高效可行的深度学习方法，展示了替代传统GP方法的潜力

Abstract: Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.

</details>


### [67] [Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning](https://arxiv.org/abs/2512.19920)
*Jiayun Wu,Jiashuo Liu,Zhiyuan Zeng,Tianyang Zhan,Wenhao Huang*

Main category: cs.LG

TL;DR: 该论文提出通过行为校准训练方法，让语言模型能够诚实表达不确定性，通过拒绝回答或标记不确定内容来减少幻觉，使小模型在不确定性量化方面超越大模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在关键领域部署受到幻觉问题的阻碍。传统强化学习范式使用二元奖励信号，无意中鼓励模型成为"好考生"而非诚实沟通者，只要正确概率超过零就会猜测。需要让模型能够诚实地表达不确定性。

Method: 提出行为校准训练方法，优化严格适当评分规则，让模型输出经过校准的正确概率。模型可以选择完全拒绝回答，或在不确定时标记具体主张。使用Qwen3-4B-Instruct进行实证分析。

Result: 行为校准强化学习使小模型在不确定性量化方面超越前沿大模型。在数学推理任务上，模型的Accuracy-to-Hallucination Ratio增益(0.806)超过GPT-5(0.207)。在事实QA任务中，4B模型实现与前沿模型相当的零样本校准误差，尽管其事实准确性低得多。

Conclusion: 行为校准训练方法能够有效减少LLM的幻觉问题，使模型能够诚实表达不确定性。这种不确定性量化能力是可转移的元技能，与原始预测准确性解耦，为LLM在关键领域的安全部署提供了新途径。

Abstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.

</details>


### [68] [The Seismic Wavefield Common Task Framework](https://arxiv.org/abs/2512.19927)
*Alexey Yermakov,Yue Zhao,Marine Denolle,Yiyu Ni,Philippe M. Wyder,Judah Goldfeder,Stefano Riva,Jan Williams,David Zoro,Amy Sara Rude,Matteo Tomasetto,Joe Germany,Joseph Bakarji,Georg Maierhofer,Miles Cranmer,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 该论文提出了一个用于地震波场机器学习的通用任务框架(CTF)，包含三个不同尺度的波场数据集和特定任务指标，旨在通过标准化评估提高科学机器学习的严谨性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 地震学在状态预测和重建（如地震早期预警和地面运动预测）以及管理参数变异性方面面临根本挑战。现有模拟方法受限于大规模计算，而真实数据方法受限于模型复杂性和稀疏传感器测量。机器学习虽有潜力，但缺乏适当的表征、公平报告和严格比较。

Method: 引入地震波场机器学习的通用任务框架(CTF)，包含三个不同尺度的波场数据集（全球、地壳和局部），以及针对预测、重建和泛化等任务的特定指标。该框架受自然语言处理等领域CTF启发，提供结构化、严格的算法评估基础。

Result: 在两个数据集上展示了评估过程，报告了各种方法和基础模型在从模拟和真实世界传感器测量中重建地震波场的性能。CTF评分揭示了不同方法在特定问题类别中的优势、局限性和适用性。

Conclusion: 该框架旨在用隐藏测试集上的标准化评估取代临时比较，提高科学机器学习的严谨性和可重复性。通过结构化评估，可以更清晰地了解不同机器学习方法在地震波场问题中的实际性能。

Abstract: Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.

</details>


### [69] [LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2512.20002)
*Jiacheng You,Jingcheng Yang,Yuhang Xie,Zhongxuan Wu,Xiucheng Li,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xinyang Chen*

Main category: cs.LG

TL;DR: LoFT-LLM是一个频率感知的时间序列预测框架，结合低频学习和LLM语义校准，在少样本和全数据场景下都显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中时间序列预测面临的挑战：训练数据有限、复杂噪声动态、现有方法使用全长度时间窗口监督预测包含大量高频噪声掩盖长期趋势、辅助变量信息利用不足

Method: 提出LoFT-LLM频率感知预测框架：1) PLFM模块从局部频谱块提取稳定低频趋势；2) 残差学习器建模高频变化；3) 微调LLM通过结构化自然语言提示整合辅助上下文和领域知识来精炼预测

Result: 在金融和能源数据集上的大量实验表明，LoFT-LLM在全数据和少样本情况下都显著优于强基线方法，提供更优的准确性、鲁棒性和可解释性

Conclusion: LoFT-LLM通过结合低频趋势提取、高频残差建模和LLM语义校准，有效解决了时间序列预测中的噪声干扰和少样本问题，为实际应用提供了准确、鲁棒且可解释的解决方案

Abstract: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.

</details>


### [70] [Control Variate Score Matching for Diffusion Models](https://arxiv.org/abs/2512.20003)
*Khaled Kahouli,Romuald Elie,Klaus-Robert Müller,Quentin Berthet,Oliver T. Unke,Arnaud Doucet*

Main category: cs.LG

TL;DR: 提出Control Variate Score Identity (CVSI)，通过控制变量法统一DSI和TSI两种分数估计器，实现全噪声谱方差最小化，提升扩散模型采样效率


<details>
  <summary>Details</summary>
Motivation: 扩散模型需要准确估计噪声扰动目标分布的分数。传统Denoising Score Identity (DSI)依赖数据样本，而Target Score Identity (TSI)利用目标能量函数，但两者存在根本性的方差权衡：DSI在低噪声区域方差高，TSI在高噪声区域方差高

Method: 提出Control Variate Score Identity (CVSI)，在控制变量法的理论框架下统一DSI和TSI两种估计器。推导出最优的时间依赖控制系数，理论上保证在整个噪声谱上实现方差最小化

Result: CVSI作为稳健的低方差插件估计器，显著提升了无数据采样器学习和推理时扩散采样的样本效率

Conclusion: 通过控制变量法统一DSI和TSI，解决了分数估计中的方差权衡问题，为扩散模型提供了更高效的采样方法

Abstract: Diffusion models offer a robust framework for sampling from unnormalized probability densities, which requires accurately estimating the score of the noise-perturbed target distribution. While the standard Denoising Score Identity (DSI) relies on data samples, access to the target energy function enables an alternative formulation via the Target Score Identity (TSI). However, these estimators face a fundamental variance trade-off: DSI exhibits high variance in low-noise regimes, whereas TSI suffers from high variance at high noise levels. In this work, we reconcile these approaches by unifying both estimators within the principled framework of control variates. We introduce the Control Variate Score Identity (CVSI), deriving an optimal, time-dependent control coefficient that theoretically guarantees variance minimization across the entire noise spectrum. We demonstrate that CVSI serves as a robust, low-variance plug-in estimator that significantly enhances sample efficiency in both data-free sampler learning and inference-time diffusion sampling.

</details>


### [71] [Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance](https://arxiv.org/abs/2512.20006)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: 提出了一种名为OGAB的新型激活函数，通过正交性和组感知偏置学习来缓解深度学习分类器中的类别不平衡问题，无需显式标签信息。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是机器学习和数据挖掘中的常见挑战，会导致分类器性能下降。尽管深度学习在特征提取方面表现出色，但在不平衡数据下性能仍然会恶化。现有方法主要通过数据预处理修改或后处理校正来解决类别不平衡，而本文希望在训练阶段直接在嵌入学习层面解决这一问题。

Method: 提出OGAB激活函数，结合正交变换和组感知偏置学习机制。正交变换通过保持特征独立性来保留少数类信息，防止多数类在嵌入空间中占据主导地位。组感知偏置机制自动识别数据聚类并调整嵌入以增强类别可分性，无需显式监督。

Result: 在真实世界和合成的不平衡数据集上验证了OGAB的有效性，相比传统和可学习激活函数，表现出持续的性能改进。

Conclusion: 激活函数可以通过引入强归纳偏置来解决复杂数据挑战，超越传统的非线性功能。OGAB在训练阶段直接解决类别不平衡问题，能够与学习过程无缝集成，为处理不平衡数据提供了新的有效方法。

Abstract: Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.

</details>


### [72] [An Optimal Policy for Learning Controllable Dynamics by Exploration](https://arxiv.org/abs/2512.20053)
*Peter N. Loxley*

Main category: cs.LG

TL;DR: 论文提出了一种用于在未知环境中学习可控动力学的非平稳最优探索策略，该策略通过有限时间范围内的贪婪信息增益最大化来实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 可控马尔可夫链描述了顺序决策任务的动态，是优化控制和强化学习的核心组件。然而，在未知环境中学习可控动力学时，存在某些状态（如瞬态状态、吸收状态和非回溯状态）会限制对动力学的控制，这使得非平稳策略对于实现最优探索至关重要。

Method: 提出了一种简单易实现、计算高效的最优策略，通过随时间变化的约束集选择控制，以贪婪方式最大化信息增益。给出了控制集的简单参数化方法，并提出了寻找最优策略的算法。

Result: 该策略在六个可控动力学的具体示例中得到了详细验证。通过计数论证、与次优策略的比较以及动态规划的序列改进特性，证明了策略的最优性。

Conclusion: 非平稳策略对于在存在限制控制的状态（如瞬态状态、吸收状态和非回溯状态）的环境中实现最优探索是必要的。提出的策略为在未知环境中学习可控动力学提供了一种高效实用的方法。

Abstract: Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.

</details>


### [73] [PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models](https://arxiv.org/abs/2512.20063)
*Mingue Park,Jisung Hwang,Seungwoo Yoo,Kyeongmin Yeo,Minhyuk Sung*

Main category: cs.LG

TL;DR: PairFlow是一种轻量级预处理方法，用于训练离散流模型，实现少步采样而无需预训练教师模型，计算成本仅为完整训练的1.7%


<details>
  <summary>Details</summary>
Motivation: 离散流模型在离散数据生成方面表现出色，但迭代采样速度慢。现有加速方法依赖微调，带来大量额外训练开销。需要一种轻量级解决方案来加速采样而不增加显著成本。

Method: 提出PairFlow预处理步骤，基于离散流模型的闭式反演，直接从源分布和目标分布的耦合样本训练模型，无需预训练教师。核心是闭式反演方法，能高效构建配对源-目标样本。

Result: PairFlow仅需完整训练计算量的1.7%，性能匹配甚至超越需要微调的两阶段训练。同时为后续蒸馏提供更强的基础模型，微调后能获得进一步加速。

Conclusion: PairFlow是一种高效、轻量级的离散流模型预处理方法，在分子数据、二值和RGB图像等多种应用场景中表现出广泛的适用性和有效性，显著降低了加速采样的计算成本。

Abstract: We introduce $\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.

</details>


### [74] [Information-directed sampling for bandits: a primer](https://arxiv.org/abs/2512.20096)
*Annika Hirling,Giorgio Nicoletti,Antonio Celani*

Main category: cs.LG

TL;DR: 该论文在双状态伯努利多臂老虎机框架下，系统研究了信息导向采样策略在探索与利用权衡中的表现，证明了在不同问题类别中的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为统计物理学家搭建强化学习与信息理论之间的桥梁，通过研究信息导向采样策略在多臂老虎机问题中的表现，为探索与利用的权衡提供理论分析。

Method: 将信息导向采样框架扩展到折扣无限时域设置，引入改进的信息度量和调优参数来调节决策行为。重点研究了对称老虎机和单公平硬币两种具体问题类别。

Result: 在对称情况下，IDS实现了有界的累积遗憾；在单公平硬币场景中，IDS策略产生的遗憾随时域对数增长，与经典渐近下界一致。

Conclusion: 该工作作为教学综合，成功展示了信息导向采样策略在多臂老虎机问题中的理论性能，为统计物理学家理解强化学习与信息理论的交叉提供了有价值的视角。

Abstract: The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.

</details>


### [75] [Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering](https://arxiv.org/abs/2512.20115)
*Yuanhao Chen,Qi Liu,Pengbin Chen,Zhongjian Qiao,Yanjie Li*

Main category: cs.LG

TL;DR: 本文提出了一种简单的样本过滤方法，通过筛选高奖励的转移样本来改进策略约束离线强化学习，解决了传统方法因数据集包含低质量转移而导致的性能受限问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布偏移问题，策略约束方法通过限制学习策略与行为策略的差异来解决此问题。但现有方法存在一个关键问题：如果数据集中包含大量低奖励转移，学习策略会被限制在次优的行为策略上，导致学习速度慢、样本效率低和性能不佳。

Method: 提出一种简单但高效的样本过滤方法：1）使用平均奖励和平均折扣奖励评估数据集中转移样本的得分；2）提取高得分的转移样本；3）使用这些高质量样本训练离线强化学习算法。

Result: 在一系列离线强化学习算法和基准任务上的实验结果表明，所提出的方法在样本效率和最终性能方面均优于基线方法。

Conclusion: 通过筛选高质量转移样本，可以有效改进策略约束离线强化学习的性能，解决了传统方法因数据集质量不均而导致的性能受限问题，提供了一种简单有效的改进方案。

Abstract: Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.

</details>


### [76] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: NeuralCrop是一种混合全球网格作物模型，结合了过程模型和机器学习优势，在产量预测和气候变化适应性方面优于传统模型


<details>
  <summary>Details</summary>
Motivation: 传统全球网格作物模型(GGCMs)在模拟复杂生物物理过程方面存在不确定性，而纯机器学习模型在气候变化条件下泛化能力不足，需要结合两者优势

Method: 开发NeuralCrop混合模型，先训练模拟先进的GGCM，再用观测数据进行微调，结合过程模型和机器学习组件

Result: NeuralCrop在站点和大尺度种植区域均优于现有GGCMs，在干旱极端条件下改进尤其显著，泛化到未见训练条件时仍保持稳健

Conclusion: 混合作物建模方法提供了整体改进的作物建模能力，在气候变化和极端天气加剧条件下能提供更可靠的产量预测

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [77] [Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud](https://arxiv.org/abs/2512.20218)
*Jixiao Yang,Jinyu Chen,Zixiao Huang,Chengda Xu,Chi Zhang,Sijia Li*

Main category: cs.LG

TL;DR: Cost-TrustFL是一个分层联邦学习框架，在非IID数据分布、恶意参与者检测和高昂跨云通信成本的多云环境中，联合优化模型性能和通信成本，同时提供对投毒攻击的鲁棒防御。


<details>
  <summary>Details</summary>
Motivation: 多云环境下的联邦学习面临三个关键挑战：非独立同分布数据分布、恶意参与者检测以及高昂的跨云通信成本（出口费用）。现有拜占庭鲁棒方法主要关注模型准确性，而忽视了跨云提供商数据传输的经济影响。

Method: 提出Cost-TrustFL分层联邦学习框架，包括：1）基于梯度的近似Shapley值计算方法，将复杂度从指数级降低到线性级，实现轻量级信誉评估；2）成本感知聚合策略，优先考虑云内通信以最小化昂贵的跨云数据传输。

Result: 在CIFAR-10和FEMNIST数据集上的实验表明，Cost-TrustFL在30%恶意客户端情况下达到86.7%的准确率，同时相比基线方法减少32%的通信成本。该框架在不同非IID程度和攻击强度下保持稳定性能。

Conclusion: Cost-TrustFL框架通过联合优化模型性能和通信成本，同时提供对投毒攻击的鲁棒防御，使其适用于现实世界的多云部署场景。

Abstract: Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.

</details>


### [78] [Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning](https://arxiv.org/abs/2512.20220)
*Kausthubh Manda,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: 该论文研究了离线多任务强化学习，其中多个任务共享其动作值函数的低秩表示。通过联合学习共享表示和任务特定值函数，分析表明跨任务数据池化能提高统计效率，并研究了表示重用对下游新任务的影响。


<details>
  <summary>Details</summary>
Motivation: 研究离线多任务强化学习中如何利用任务间共享的低秩表示结构来提高统计效率和泛化能力。在只能访问固定离线数据集、无法进行在线交互的情况下，探索如何通过跨任务数据共享来改善学习效果。

Method: 采用多任务版本的拟合Q迭代方法，通过贝尔曼误差最小化在离线数据上联合学习共享表示和任务特定值函数。在标准可实现性和覆盖性假设下进行分析，并研究下游任务中重用已学习表示的影响。

Result: 建立了学习值函数的有限样本泛化保证，明确表征了跨任务数据池化如何提高估计精度，得到1/√(nT)的样本依赖关系。同时表明，在下游任务中重用表示可以降低有效复杂度，相比从头学习有优势。

Conclusion: 研究阐明了共享表示在多任务离线Q学习中的作用，为理解多任务结构何时以及如何改善无模型、基于值的强化学习中的泛化提供了理论见解，表明跨任务表示共享能显著提高统计效率。

Abstract: We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.

</details>


### [79] [Adaptive Multi-task Learning for Probabilistic Load Forecasting](https://arxiv.org/abs/2512.20232)
*Onintze Zaballa,Verónica Álvarez,Santiago Mazuelas*

Main category: cs.LG

TL;DR: 本文提出了一种基于向量值隐马尔可夫模型的自适应多任务学习方法，用于多实体概率负荷预测，能够动态适应消费模式变化并评估负荷不确定性。


<details>
  <summary>Details</summary>
Motivation: 多实体负荷预测对电力系统高效可靠运行至关重要，但现有方法多为离线学习，无法捕捉消费模式变化，且多任务学习在负荷预测中的应用仍不充分。

Method: 基于向量值隐马尔可夫模型的自适应多任务学习方法，采用递归过程更新模型参数，使用最新参数进行预测，能够动态适应消费模式变化和实体间相关性。

Result: 实验结果表明，该方法在包含多实体负荷需求且具有多样动态消费模式的数据集上，在预测性能和不确定性评估方面均优于现有方法。

Conclusion: 提出的自适应多任务学习方法能够有效处理多实体负荷预测问题，动态适应消费模式变化，提供可靠的概率预测和不确定性评估。

Abstract: Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.

</details>


### [80] [How I Met Your Bias: Investigating Bias Amplification in Diffusion Models](https://arxiv.org/abs/2512.20233)
*Nathan Roos,Ekaterina Iakovleva,Ani Gjergji,Vito Paolo Pastore,Enzo Tartaglione*

Main category: cs.LG

TL;DR: 扩散生成模型在图像合成任务中表现出色，但会复制和放大数据集偏见。研究发现采样算法及其超参数对偏见放大有显著影响，可通过调整采样参数控制偏见程度。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型虽然性能优异，但倾向于复制和放大数据集中的偏见，这一问题尚未得到充分理解。以往研究认为偏见放大是扩散模型的固有特性，但本研究旨在探索采样算法及其超参数如何影响偏见放大。

Method: 通过控制实验，使用在Biased MNIST、Multi-Color MNIST和BFFHQ数据集上训练的模型以及Stable Diffusion，实证研究不同采样算法及其超参数对偏见放大的影响。

Result: 研究发现采样算法及其超参数对偏见放大有显著且可测量的影响。采样超参数可以诱导偏见的减少或放大，即使训练好的模型保持不变。这表明偏见放大并非扩散模型的固有特性，而是可以通过采样过程进行控制。

Conclusion: 扩散模型中的偏见放大问题不仅与训练数据相关，还受到采样算法和超参数的显著影响。通过调整采样参数可以控制偏见程度，这为减少生成模型中的偏见提供了新的途径。

Abstract: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.

</details>


### [81] [Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion](https://arxiv.org/abs/2512.20249)
*Xuanyu Hu*

Main category: cs.LG

TL;DR: 该论文提出BrainROI模型，通过软功能分区编码、可解释提示优化和参数化解码约束，在多模态脑解码任务中实现了跨被试的优越性能。


<details>
  <summary>Details</summary>
Motivation: 多模态脑解码面临跨被试泛化性和可解释性两大挑战。现有方法在处理不同被试的功能脑拓扑异质性时效果有限，且提示设计方法缺乏稳定性和透明度。

Method: 1. 设计新的fMRI编码器：使用多图谱软功能分区作为共享空间，将离散ROI拼接扩展为体素级门控融合机制，通过全局标签对齐确保ROI映射一致性。2. 引入可解释提示优化：在小样本闭环中使用本地部署的Qwen模型迭代生成和选择人类可读提示。3. 在推理时施加参数化解码约束。

Result: 在NSD数据集上实现领先水平的脑-字幕评估结果。在跨被试设置下，相比现有SOTA方法和代表性基线，BLEU-4和CIDEr等指标有明显提升。

Conclusion: BrainROI模型通过创新的fMRI编码器设计、可解释的提示优化过程和参数化解码约束，有效解决了多模态脑解码中的跨被试泛化和可解释性问题，取得了显著性能提升。

Abstract: Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.

</details>


### [82] [Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity](https://arxiv.org/abs/2512.20291)
*Yuxing Gan,Ziyu Lei*

Main category: cs.LG

TL;DR: CDSP-MoE通过冲突驱动的子空间剪枝方法，解决传统MoE架构中的参数隔离和指令过拟合问题，实现动态专家实例化和鲁棒的内容驱动路由


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构存在两个根本性限制：1）结构参数隔离导致灾难性遗忘；2）指令过拟合在无指令场景下性能下降。需要一种新范式来解决这些问题。

Method: 提出CDSP-MoE框架，基于通用权重子空间假设，维护超完备参数主干，通过可学习拓扑掩码在共享物理子空间中动态实例化逻辑专家。利用梯度冲突作为结构监督信号，通过滞后梯度博弈惩罚共享流形中的干扰连接，使拓扑能够自发剪枝冲突路径并演化出可解释的模块化结构。

Result: CDSP-MoE在没有人工定义任务标签的情况下实现了鲁棒的内容驱动路由，即使在严格的无指令盲推理协议下也能保持语义专业化。

Conclusion: CDSP-MoE通过从隔离专家容器到共享子空间中动态专家实例化的范式转变，有效解决了传统MoE架构的结构限制，实现了更好的参数效率和鲁棒性。

Abstract: Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts

</details>


### [83] [Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning](https://arxiv.org/abs/2512.20363)
*Daniel M. Jimenez-Gutierrez,Mehrdad Hassanzadeh,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.LG

TL;DR: 提出Clust-PSI-PFL框架，通过PSI指标量化客户端数据非独立同分布程度，使用聚类方法将客户端分组，在严重非IID数据下显著提升全局准确率和客户端公平性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的非独立同分布（non-IID）特性会导致模型更新偏差和性能下降，需要有效方法来缓解这一问题。

Method: 提出Clust-PSI-PFL框架：1) 使用加权PSI指标（WPSI^L）量化客户端数据分布差异；2) 基于PSI特征通过K-means++聚类形成分布同质的客户端组；3) 使用轮廓系数法确定最优聚类数量；4) 在聚类组内进行个性化联邦学习。

Result: 在6个数据集（表格、图像、文本）、2种数据划分协议（Dirichlet和Similarity）和多种客户端规模下，相比最先进基线方法：全局准确率提升高达18%；在严重非IID数据下，客户端公平性相对提升37%；WPSI^L指标比常用非IID度量指标（Hellinger、Jensen-Shannon、Earth Mover距离）更具信息性。

Conclusion: PSI引导的聚类是一种原理清晰、轻量级的机制，能够在标签偏斜情况下实现鲁棒的个性化联邦学习，有效解决非IID数据带来的挑战。

Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.

</details>


### [84] [FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning](https://arxiv.org/abs/2512.20329)
*Mrinmay Sen,Subhrajit Nag*

Main category: cs.LG

TL;DR: FedDPC是一种新的联邦学习方法，通过将本地更新投影到先前的全局更新上来同时解决数据异构性和部分客户端参与问题，从而减少方差并加速训练。


<details>
  <summary>Details</summary>
Motivation: 现代联邦学习中，数据异构性导致本地模型更新方差增大，使全局模型偏离真正的最优解。部分客户端参与进一步加剧了这一问题，使聚合偏向参与客户端的数据分布，导致全局模型更新方差增加，收敛偏离全局目标最优解，降低训练稳定性和模型性能。

Method: FedDPC通过两个关键技术：1) 将每个本地更新投影到先前的全局更新上，控制本地和全局更新的方差；2) 在聚合前对每个本地更新采用自适应缩放，进一步加速联邦学习训练。

Result: 在多个异构划分数据集上的图像分类任务实验表明，FedDPC优于现有最先进的联邦学习算法，在通信轮次中实现了更快的训练损失减少和更高的测试准确率。

Conclusion: FedDPC通过同时解决数据异构性和部分客户端参与问题，有效提高了联邦学习的训练效率和全局模型性能，为联邦学习中的方差控制提供了新的解决方案。

Abstract: Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.

</details>


### [85] [Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation](https://arxiv.org/abs/2512.20346)
*Emilia Majerz,Witold Dzwinel,Jacek Kitowski*

Main category: cs.LG

TL;DR: 该论文提出了一种基于物理的机器学习方法，用于加速CERN ALICE实验中零度量热器的粒子簇射模拟，通过新颖的损失函数和输出变异性缩放机制，实现了比传统方法更准确、更快速的模拟。


<details>
  <summary>Details</summary>
Motivation: 传统粒子探测器模拟计算成本高昂，需要加速模拟过程。物理机器学习方法能够将领域知识融入学习过程，提高模型的准确性和鲁棒性，特别适用于加速零度量热器的粒子簇射模拟。

Method: 采用基于物理的机器学习范式，引入新颖的损失函数和输出变异性缩放机制，使用归一化流在师生生成框架中构建模型，增强对粒子簇射空间分布和形态的准确表示，同时减少罕见伪影对训练的影响。

Result: 该方法不仅超越了经典的数据驱动模型同化方法，而且比现有ZDC模拟文献中的归一化流实现快421倍，显著提高了模拟速度和准确性。

Conclusion: 物理机器学习方法在粒子探测器模拟中具有显著优势，通过将领域知识嵌入学习过程，能够开发出既准确又高效的模拟模型，为高能物理实验的快速模拟提供了有效解决方案。

Abstract: Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.

</details>


### [86] [Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs](https://arxiv.org/abs/2512.20573)
*Rui Pan,Zhuofu Chen,Ravi Netravali*

Main category: cs.LG

TL;DR: FailFast是一个基于扩散大语言模型的推测解码框架，通过动态调整推测长度，在难以推测的区域快速失败以减少计算开销，在容易推测的区域积极扩展草案长度以减少验证延迟，实现了对自回归大语言模型的无损加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽然能实现快速的并行令牌生成，但单独使用时存在效率与质量之间的固有权衡。作者发现，如果精心应用，扩散大语言模型的特性可以成为推测解码中草案生成器的优势，其并行解码速度能显著降低代价高昂的拒绝风险。

Method: 提出FailFast框架，利用扩散大语言模型作为草案生成器，通过动态调整推测长度实现"快速失败"和"大胆获胜"策略。在难以推测的区域最小化计算开销以减少推测延迟，在容易推测的区域积极扩展草案长度（有时可达70个令牌）以减少验证延迟。

Result: 无需任何微调，FailFast实现了对自回归大语言模型的无损加速，相比原始解码达到最高4.9倍加速，相比最佳朴素扩散大语言模型草案生成器达到1.7倍加速，相比EAGLE-3达到1.4倍加速，在不同模型和工作负载上表现一致。

Conclusion: 扩散大语言模型在推测解码中作为草案生成器具有独特优势，FailFast框架通过动态调整推测长度有效解决了效率与质量的权衡问题，为自回归大语言模型提供了实用的无损加速解决方案。

Abstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.

</details>


### [87] [Field-Space Attention for Structure-Preserving Earth System Transformers](https://arxiv.org/abs/2512.20350)
*Maximilian Witte,Johannes Meuer,Étienne Plésiat,Christopher Kadow*

Main category: cs.LG

TL;DR: 本文提出Field-Space Attention机制，用于地球系统Transformer，在物理域而非潜在空间计算注意力，保持连续场表示，提高地球系统建模的物理一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确且物理一致的地球系统动力学建模需要能够直接在连续地球物理场上操作并保持其底层几何结构的机器学习架构。现有方法通常在潜在空间中操作，缺乏物理可解释性，难以融入科学约束。

Method: 提出Field-Space Attention机制，在物理域而非潜在空间计算注意力；采用固定的非学习多尺度分解；学习输入场的结构保持变形；保持所有中间表示为球面上的连续场；允许在架构中直接嵌入物理和统计先验。

Result: 在HEALPix网格上的全球温度超分辨率任务中，Field-Space Transformer比传统Vision Transformer和U-Net基线收敛更快更稳定，且需要更少的参数；保持了场结构，提高了保真度和可靠性。

Conclusion: Field-Space Attention作为一个紧凑、可解释且物理基础构建块，为下一代地球系统预测和生成建模框架提供了新方向，通过显式保持场结构实现物理约束的直接嵌入。

Abstract: Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.

</details>


### [88] [Performative Policy Gradient: Optimality in Performative Reinforcement Learning](https://arxiv.org/abs/2512.20576)
*Debabrota Basu,Udvas Das,Brahim Driss,Uddalak Mukherjee*

Main category: cs.LG

TL;DR: 本文提出了PePG算法，这是第一个考虑RL中执行性（performative）影响的策略梯度算法，能够收敛到在执行性分布偏移下保持最优的策略。


<details>
  <summary>Details</summary>
Motivation: 部署后的机器学习算法会影响其运行环境，从而改变底层动态，但标准强化学习方法忽略了这种执行性影响。虽然监督学习中已研究执行性设置下的最优算法设计，但强化学习中的对应研究仍不足。

Method: 提出了执行性策略梯度算法（PePG），这是第一个考虑执行性影响的策略梯度算法。在softmax参数化和有无熵正则化的情况下，证明了PePG能够收敛到执行性最优策略。

Result: 证明了PePG收敛到执行性最优策略，即那些在自身引起的分布偏移下仍保持最优的策略。在标准执行性RL环境中的实证分析表明，PePG优于标准策略梯度算法和现有的追求稳定性的执行性RL算法。

Conclusion: PePG算法首次在强化学习中考虑了执行性影响，能够收敛到执行性最优策略，显著扩展了先前仅追求执行性稳定性的工作，为执行性强化学习提供了有效的解决方案。

Abstract: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.

</details>


### [89] [Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning](https://arxiv.org/abs/2512.20605)
*Seijin Kobayashi,Yanick Schimpf,Maximilian Schlegel,Angelika Steger,Maciej Wolczyk,Johannes von Oswald,Nino Scherre,Kaitlin Maile,Guillaume Lajoie,Blake A. Richards,Rif A. Saurous,James Manyika,Blaise Agüera y Arcas,Alexander Meulemans,João Sacramento*

Main category: cs.LG

TL;DR: 该论文提出了一种在自回归模型内部表示中进行探索和行动的方法，通过高阶非因果序列模型控制基础模型的残差流激活，学习时间抽象动作，实现更高效的稀疏奖励学习。


<details>
  <summary>Details</summary>
Motivation: 当前基于下一个令牌预测预训练并通过强化学习微调的大规模自回归模型在探索时存在效率低下的问题，特别是在奖励稀疏的情况下。这是因为模型以令牌为单位进行采样，导致学习效率不高。

Method: 提出了一种内部强化学习方法，引入高阶非因果序列模型来控制基础自回归模型的残差流激活。该模型学习将长激活序列块压缩到内部控制器上，每个控制器执行一系列行为上有意义的动作，并包含学习到的终止条件。

Result: 在具有层次结构的网格世界和MuJoCo任务上，该方法能够学习到时间抽象动作，实现高效探索。内部控制器强化使得模型能够在标准RL微调失败的情况下从稀疏奖励中学习。

Conclusion: 内部强化学习是一种有前景的方法，可以在基础模型中实现层次强化学习，通过潜在动作生成和强化提高自回归模型在稀疏奖励环境中的学习效率。

Abstract: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>


### [90] [BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples](https://arxiv.org/abs/2512.20403)
*Xuan-An Le,Minh-Nam Tran,Son Nguyen*

Main category: cs.LG

TL;DR: BRIDGE框架通过两阶段知识蒸馏解决大模型到小模型的知识迁移问题，使用教师助理作为中介，在有限预算下实现高效知识传递


<details>
  <summary>Details</summary>
Motivation: 从大型专有模型（如GPT-4）向小型可部署模型（小于10亿参数）进行知识蒸馏面临容量预算陷阱：师生模型间1000倍容量差距阻碍直接迁移，而API成本又限制了数据收集

Method: 两阶段框架：第一阶段，中等规模教师助理（约70亿参数）在严格限制的数据子集（3-5%）上从黑盒教师学习，使用零API成本管道平衡熵难度和语义多样性；第二阶段，利用教师查询昂贵而TA推理免费的不对称性，TA为完整数据集生成合成推理来训练小型学生模型，并采用指令调优课程确保行为对齐

Result: 在医疗、法律和金融基准测试中，BRIDGE使学生性能提升28-41%，将能力差距缩小12-16%，同时使用10倍更少的教师查询；超越使用100%预算的直接蒸馏基线，仅消耗5%资源

Conclusion: BRIDGE框架突破了传统的成本-性能边界，通过战略性的中介和预算不对称性，在有限资源下实现了从大型专有模型到小型可部署模型的高效知识蒸馏

Abstract: Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.

</details>


### [91] [Machine Learning to Predict Digital Frustration from Clickstream Data](https://arxiv.org/abs/2512.20438)
*Jibin Joseph*

Main category: cs.LG

TL;DR: 使用电子商务网站点击流数据预测用户会话是否受挫，XGBoost准确率约90%，LSTM约91%，且仅需前20-30次交互即可可靠预测


<details>
  <summary>Details</summary>
Motivation: 移动应用和网站的用户受挫体验会导致销售损失和投诉，因此需要预测用户会话是否受挫以改善用户体验

Method: 使用基于愤怒爆发、来回导航、购物车流失、搜索挣扎和长时间徘徊等规则的受挫定义，处理540万原始点击流事件；构建表格特征训练标准分类器，同时使用完整事件序列训练判别式LSTM分类器

Result: XGBoost达到约90%准确率和0.9579 ROC AUC，LSTM表现最佳达到约91%准确率和0.9705 ROC AUC；研究显示仅需前20-30次交互，LSTM即可可靠预测受挫

Conclusion: 点击流数据能有效预测用户受挫，LSTM模型在序列数据处理上表现优异，且早期预测能力有助于及时干预改善用户体验

Abstract: Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.

</details>


### [92] [Explainable time-series forecasting with sampling-free SHAP for Transformers](https://arxiv.org/abs/2512.20514)
*Matthias Hertel,Sebastian Pütz,Ralf Mikut,Veit Hagenmeyer,Benjamin Schäfer*

Main category: cs.LG

TL;DR: SHAPformer：基于Transformer架构的快速、免采样的可解释时间序列预测模型，通过注意力机制操作生成SHAP解释，比传统SHAP方法快几个数量级。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在规划决策中至关重要，可解释性对建立用户信任和满足透明度要求很关键。现有SHAP框架缺乏针对时间序列的高效实现，且采样反事实时通常假设特征独立性。

Method: 基于Transformer架构，通过注意力机制操作使预测基于特征子集，实现免采样的SHAP解释生成。模型能够快速生成解释（1秒内完成）。

Result: 在具有真实解释的合成数据上，SHAPformer提供符合数据真实情况的解释。应用于真实世界电力负荷数据时，获得有竞争力的预测性能，并提供有意义的局部和全局洞察，如识别过去负荷为关键预测因子，并揭示圣诞节期间的独特模型行为。

Conclusion: SHAPformer是一个准确、快速且免采样的可解释时间序列预测模型，比传统SHAP置换解释器快几个数量级，能够提供真实可靠的时间序列预测解释。

Abstract: Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.

</details>


### [93] [Improving ML Training Data with Gold-Standard Quality Metrics](https://arxiv.org/abs/2512.20577)
*Leslie Barrett,Michael W. Sherman*

Main category: cs.LG

TL;DR: 该论文提出使用统计方法评估和提升人工标注训练数据的质量，通过测量标注一致性和一致性指标，发现多轮标注能提供更可靠的结果，且方差下降是数据质量提升的指标。


<details>
  <summary>Details</summary>
Motivation: 人工标注的训练数据对许多机器学习任务至关重要，但数据质量控制在文献中很少受到关注，尽管不同标注工作的数据质量差异很大。

Method: 提出使用统计方法评估和提升手标训练数据质量，通过测量标注一致性和一致性指标，采用多轮标注来获得更可靠的结果。

Result: 一致性指标在多轮标注中能提供更可靠的结果，方差下降是数据质量提升的指标；提出了一种无需为每个工作项进行多次标注就能收集高质量训练数据的方法；发现标注者适应期可能不足以最小化标注错误。

Conclusion: 需要更系统的方法来评估和提升人工标注训练数据的质量，多轮标注和统计一致性测量是有效的数据质量控制策略。

Abstract: Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.

</details>
