<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.LG](#cs.LG) [Total: 43]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 本文分析基于token补全的大语言模型的推理机制，认为其本质是随机生成而非真正的溯因推理，只是模仿了人类文本中的推理结构，这对LLM的评估和应用有重要影响。


<details>
  <summary>Details</summary>
Motivation: 探讨当前基于token补全的大语言模型的推理本质，澄清其看似溯因推理的表现实际上只是对训练数据中人类推理模式的模仿，而非真正的推理能力。

Method: 通过分析LLM的随机性本质和与人类溯因推理的相似性，使用具体例子展示LLM如何生成看似合理的想法、模仿常识推理和提供解释性答案，但缺乏真理基础、语义理解、验证能力和真实推理。

Result: LLM具有双重性质：随机性基础但表现出溯因推理的表象；能协助生成想法和支持人类思考，但输出需要批判性评估，因为无法识别真理或验证解释；回应了五个反对观点并承认分析局限性。

Conclusion: LLM的推理本质是模式匹配而非真实推理，这对评估和应用有重要影响：虽然能辅助人类思考，但输出必须批判性评估，因为它们缺乏真理识别和验证能力。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [2] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: 研究探索使用小型语言模型(SLMs)进行自动问题生成，作为大型语言模型在学情分析研究中的补充，提出了一种结合文本生成和概率推理能力的新型问题生成流程。


<details>
  <summary>Details</summary>
Motivation: 当前学情分析研究中主要依赖大型语言模型进行问题生成，但小型语言模型(SLMs)的潜力未被充分挖掘。本研究旨在探索SLMs在自动问题生成中的应用，作为大型模型的补充方案。

Method: 提出了一种"先生成后验证"的问题生成流程：首先通过扩展性生成创建大量候选问题，然后基于新颖的概率推理进行选择性验证和精炼。该方法充分利用了SLMs的文本生成和概率推理能力。

Result: 通过两项评估研究（7位人类专家评估和大型语言模型评估）显示，大多数评估者（人类或LLMs）认为生成的问题具有清晰的答案，并且与预期的学习目标基本一致。

Conclusion: 当采用精心设计的流程并充分利用其优势时，小型语言模型能够有效生成高质量的问题，为学情分析研究提供了大型语言模型之外的可行替代方案。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [3] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: DeepNews框架通过模拟金融记者的认知过程，解决了长文本生成中的"不可能三角"问题，在金融报道领域显著降低了幻觉率并提升了逻辑连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在垂直领域长文本生成中面临"不可能三角"困境：难以同时实现低幻觉、深度逻辑连贯和个性化表达。研究发现这是由于现有生成范式陷入了"统计平滑陷阱"，忽略了专家写作所需的高熵信息获取和结构化认知过程。

Method: 提出DeepNews框架，模拟资深金融记者的隐性认知过程，包含三个核心模块：1)基于信息觅食理论的双粒度检索机制，强制10:1饱和信息输入比；2)基于领域专家知识库（叙事模式）和原子块的模式引导战略规划；3)对抗性约束提示技术，使用节奏打破和逻辑迷雾等策略打破模型生成文本的概率平滑性。

Result: 实验发现金融深度报道存在明显的"知识悬崖"：当检索上下文低于15,000字符时，内容真实性崩溃；而超过30,000字符的高冗余输入能将无幻觉率稳定在85%以上。在顶级中文科技媒体的生态效度盲测中，基于上一代模型的DeepNews系统获得了25%的投稿接受率，显著优于SOTA模型的零样本生成（0%接受率）。

Conclusion: DeepNews框架通过显式建模专家认知过程，有效解决了长文本生成中的"不可能三角"问题，证明了结构化认知工作流在垂直领域内容生成中的优越性，为专业内容创作提供了新的技术路径。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [4] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 提出两阶段提示框架，从短评文本推断显性和隐性用户画像，用于生成个性化回复，无需微调模型


<details>
  <summary>Details</summary>
Motivation: 在用户信息有限（如外卖平台）的场景中，大语言模型因缺乏上下文用户数据而生成通用回复，降低了参与度和效果

Method: 两阶段提示框架：第一阶段从短评文本推断显性（用户声明的偏好）和隐性（人口统计或风格线索）用户画像；第二阶段将这些画像属性融入回复生成提示中；通过调整解码温度来鼓励多样且忠实的生成

Result: 在韩国外卖应用的真实数据集上评估，结果显示该方法在精确度、多样性和语义一致性方面均有提升，增强了自动回复的相关性和个性化

Conclusion: 用户画像增强的提示方法能有效提升自动回复的相关性和个性化，且无需模型微调

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [5] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 该论文研究了将英语训练的视觉语言模型扩展到其他语言的挑战，比较了翻译管道、LoRA微调和两阶段微调等方法的性能与计算成本，发现数据集翻译是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的进展主要局限于英语，限制了非英语用户的可访问性，需要将这种能力扩展到更广泛的语言范围。

Method: 探索并比较了三种方法：基于翻译的管道、LoRA微调，以及将视觉适应与语言适应分离的两阶段微调策略。使用翻译后的多模态基准测试和母语专家手动评估来评估这些方法。

Result: 结果显示数据集翻译是多语言VLM性能的主要瓶颈，数据质量限制了训练和评估的有效性。

Conclusion: 未来工作应聚焦于原生语言数据集的收集和改进的翻译策略。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [6] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: FA预训练的LLMs可以通过SWAA方法有效适应滑动窗口注意力，无需重新预训练，在保持长上下文性能的同时降低推理成本


<details>
  <summary>Details</summary>
Motivation: Transformer LLMs中的自注意力机制随输入长度呈二次方增长，导致长上下文推理成本高昂。滑动窗口注意力(SWA)可将复杂度降至线性，但直接将FA预训练的模型切换到SWA会导致严重的性能下降，因为存在训练-推理不匹配问题

Method: 提出滑动窗口注意力适应(SWAA)，包含五种方法的实用组合：(1)仅在预填充阶段应用SWA；(2)保留"sink"令牌；(3)交错FA/SWA层；(4)思维链(CoT)；(5)微调

Result: 实验表明SWA适应是可行但非平凡的：单一方法不足，但特定的协同组合能有效恢复原始长上下文性能。分析了不同SWAA配置的性能-效率权衡，并为不同场景提供推荐方案

Conclusion: FA预训练的LLMs可以通过SWAA方法有效适应滑动窗口注意力，无需重新预训练，在保持长上下文性能的同时降低推理成本，为实际应用提供了实用的解决方案

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [7] [Unsupervised Acquisition of Discrete Grammatical Categories](https://arxiv.org/abs/2503.18702)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: 该研究构建了一个多智能体语言习得计算实验室，通过母语模型生成语言示例，子代模型从中学习抽象语法知识，使用层次聚类分析从输入数据中提取离散语法规则。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何通过计算模型模拟语言习得过程，特别是子代智能体如何仅通过观察母语模型生成的语言示例（而非直接访问其内部知识）来获得抽象语法知识。

Method: 构建包含母语模型和子代语言模型的多智能体系统；子代模型仅能访问母语模型生成的话语示例；应用层次凝聚聚类分析对连续生成的话语进行模式分析；从输入数据中提取统计模式对应语法范畴；将发现的规则添加到子代模型的语法知识库中。

Result: 系统成功获得了类似自然语言学家提出的语法范畴结构；证明了非平凡语法知识的习得；通过训练数据确定的参数配置在测试集实验中得到验证，同样获得了非平凡范畴。

Conclusion: 该计算实验室环境能够有效模拟语言习得过程，子代智能体仅通过语言示例即可获得抽象语法知识，为理解语言习得机制提供了计算建模框架。

Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.

</details>


### [8] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: CoopRAG是一个新颖的检索增强生成框架，通过检索器和LLM的协同工作，以及检索器模型不同层之间的协同，提高问答任务中检索和生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在简单和多跳问答任务中仍然存在检索错误和幻觉问题，需要更有效的框架来缓解LLM生成事实不准确输出的倾向。

Method: 1) 将问题分解为子问题和推理链，并掩码不确定位置；2) 使用增强后的查询检索相关文档；3) 通过对比检索器不同层来重新排序文档；4) 利用LLM填充掩码位置重建推理链。

Result: CoopRAG在三个多跳QA数据集和一个简单QA数据集上，在检索和QA性能方面均优于最先进的QA方法。

Conclusion: CoopRAG通过检索器和LLM的协同工作，以及检索器模型内部的协同，有效提高了问答任务的准确性和可靠性。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [9] [T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/abs/2512.10430)
*Dmitrii Stoianov,Danil Taranets,Olga Tsymboi,Ramil Latypov,Almaz Dautov,Vladislav Kruglikov,Nikita Surkov,German Abramov,Pavel Gein,Dmitry Abulkhanov,Mikhail Gashkov,Viktor Zelenkovskiy,Artem Batalov,Aleksandr Medvedev,Anatolii Potapov*

Main category: cs.CL

TL;DR: T-pro 2.0是一个开源的俄语大语言模型，支持混合推理和高效推理，包含模型权重、指令语料库、数学推理基准和推理加速权重等完整资源。


<details>
  <summary>Details</summary>
Motivation: 为俄语社区提供一个可复现、可扩展的推理模型系统，支持俄语语言推理研究和高效推理应用开发。

Method: 使用西里尔字母密集分词器，采用适配的EAGLE推测解码流水线降低延迟，支持直接回答和推理轨迹生成两种模式。

Result: 发布了完整的开源资源包：模型权重、T-Wix 50万指令语料库、T-Math推理基准、EAGLE权重，并提供公开网页演示展示推理加速效果。

Conclusion: T-pro 2.0作为一个可访问的开放系统，为构建和评估高效实用的俄语大语言模型应用提供了基础。

Abstract: We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.

</details>


### [10] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: 提出SRAP框架，通过两阶段架构检测并恢复对抗性剽窃文本中的"折磨短语"，在科学文献中实现23.67%的术语恢复准确率。


<details>
  <summary>Details</summary>
Motivation: 科学文献完整性面临对抗性文本生成技术的威胁，特别是使用自动改写工具掩盖剽窃行为。现有检测方法依赖静态黑名单或通用语言模型，对新型混淆检测效果差且无法追溯剽窃来源。

Method: 提出SRAP框架：第一阶段使用领域特定掩码语言模型（SciBERT）进行统计异常检测；第二阶段使用密集向量检索（FAISS）和句子级对齐（SBERT）进行基于来源的语义重建。

Result: 在对抗性科学文本平行语料上的实验显示，零样本基线完全失败（0.00%恢复准确率），而检索增强方法达到23.67%恢复准确率，显著优于基线方法。静态决策边界在专业术语密集的科学文本中更稳健。

Conclusion: SRAP框架不仅能检测文本异常，还能数学重建原始术语，实现法医分析，将混淆表达链接回最可能的源文档，为科学文献完整性提供有效保护。

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [11] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 该研究通过将知识图谱与大型语言模型集成，使用KG-BERT增强模型的事实一致性和推理能力，在知识密集型任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理方面表现出色，但缺乏结构化知识，导致事实不一致问题。需要增强模型的事实可靠性和推理能力。

Method: 通过知识图谱集成，使用KG-BERT将结构化知识融入大型语言模型，增强模型的grounding和推理能力。

Result: 实验显示在问答和实体链接等知识密集型任务上取得显著提升，改善了事实可靠性。

Conclusion: 知识图谱集成方法能够提高大型语言模型的事实一致性，为下一代更具上下文感知能力的LLMs奠定基础。

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [12] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: 大型语言模型仅通过表层形式训练就能识别句法结构证据，如主语-助动词倒置和寄生空位许可，表明它们对句法结构敏感而不仅仅是线性顺序


<details>
  <summary>Details</summary>
Motivation: 传统生成语法中，主语-助动词倒置和寄生空位许可等系统性语法性对比被视为内部层级语法的证据。本研究旨在测试仅通过表层形式训练的大型语言模型是否能在这些对比中表现出隐含的结构表征

Method: 聚焦两个经典结构：主语-助动词倒置（测试主语边界识别）和寄生空位许可（测试抽象依赖结构）。使用GPT-4和LLaMA-3等模型，通过提示引出可接受性评分来评估

Result: 大型语言模型可靠地区分两种结构中的语法和非语法变体，表明它们对结构敏感而不仅仅是线性顺序。结构概括从表层形式的预测训练中涌现，显示出对句法的功能性敏感，无需显式编码

Conclusion: 仅通过表层形式训练的大型语言模型能够识别句法结构证据，支持它们具有结构敏感性而非仅依赖线性顺序。结构概括从预测训练中自然涌现，表明句法敏感性可以在没有显式编码的情况下功能性出现

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [13] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文研究了不同架构大语言模型在因果推理任务中的表现，发现仅依赖上下文学习（ICL）不足以实现可靠的因果推理，而经过微调的编码器和编码器-解码器架构在成本效益和鲁棒性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习（ICL）推动了大型语言模型（LLMs）的进展，但它在因果推理中的作用和性能仍不明确。因果推理需要多跳组合和严格的合取控制，而依赖输入的虚假词汇关系可能导致误导性结果。

Method: 比较了编码器、编码器-解码器和仅解码器架构的微调版本在零样本和少样本上下文学习中的表现，涵盖了自然语言和非自然语言场景。

Result: 研究发现：1）仅依赖ICL不足以实现可靠的因果推理，模型常过度关注无关输入特征；2）仅解码器模型对分布变化特别脆弱；3）微调的编码器和编码器-解码器模型在测试中（包括非自然语言场景）表现出更强的泛化能力；4）仅解码器架构需要达到很大规模才能匹配或超越前两者的性能。

Conclusion: 对于成本效益高、短期鲁棒的因果推理任务，经过针对性微调的编码器或编码器-解码器架构是更优选择，而仅解码器架构需要达到很大规模才能获得类似性能。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [14] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: 该论文针对角色扮演对话中的奖励建模问题，提出了首个系统性基准RoleRMBench和基于连续隐式偏好的奖励模型RoleRM，显著提升了主观领域奖励建模的性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在主观开放领域（如角色扮演）表现严重退化，难以捕捉基于角色的细微人类判断，需要专门针对角色扮演对话的奖励建模基准和方法。

Method: 提出RoleRMBench基准，涵盖叙事管理到角色一致性等七个细粒度能力；开发RoleRM模型，采用连续隐式偏好（CIP）方法，将主观评估重构为多重结构化策略下的连续一致成对监督。

Result: RoleRMBench评估显示通用奖励模型与人类判断存在显著差距，尤其在叙事和风格维度；RoleRM在平均性能上超越开源和闭源奖励模型超过24%，在叙事连贯性和风格保真度方面取得实质性提升。

Conclusion: 研究强调了连续偏好表示和标注一致性的重要性，为人本对话系统中的主观对齐奠定了基础，展示了针对特定领域奖励建模的有效性。

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [15] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: AgriGPT-Omni是一个农业全模态框架，整合语音、视觉和文本，通过三阶段训练范式实现跨语言和跨模态的统一推理，在农业多语言多模态任务上显著优于通用基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在农业应用中面临三大挑战：缺乏多语言语音数据、缺乏统一的多模态架构、缺乏全面的评估基准。这些限制阻碍了农业智能系统的发展，特别是在低资源地区。

Method: 1. 构建可扩展的数据合成和收集管道，将农业文本和图像转换为训练数据，创建了迄今最大的农业语音数据集（49.2万合成样本+1.4千真实样本，覆盖6种语言）。
2. 采用三阶段训练范式：文本知识注入、渐进式多模态对齐、基于GRPO的强化学习。
3. 提出AgriBench-Omni-2K，首个农业三模态基准测试，涵盖多样化的语音-视觉-文本任务和多语言切片。

Result: AgriGPT-Omni在多语言多模态推理以及真实世界语音理解任务上显著优于通用基线模型。实验证明了该框架在农业领域的有效性。

Conclusion: AgriGPT-Omni成功解决了农业多模态智能系统的关键挑战，通过统一框架整合语音、视觉和文本，为低资源地区提供了可持续的AI发展方案。所有模型、数据、基准和代码都将开源，促进可重复研究、包容性农业智能和可持续发展。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [16] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 该研究以塞尔维亚语为例，探讨低资源语言在AI时代面临的结构性挑战，提出基于CARE原则的Data Care框架，旨在将偏见缓解从技术修复转变为语料设计、标注和治理的核心组成部分。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英语等主导语言训练，对低资源语言的表征往往反映源语言材料的文化和语言偏见。研究以塞尔维亚语为案例，探讨AI时代低资源语言技术发展的结构、历史和社会技术因素。

Method: 采用半结构化访谈方法，采访了十位学者和从业者，包括语言学家、数字人文研究者和AI开发者，分析历史文本遗产破坏和当代问题如何导致还原性、工程优先的方法。

Result: 研究发现塞尔维亚语面临历史文本遗产破坏、浅层音译、依赖英语训练模型、数据偏见以及缺乏文化特异性的数据集策展等挑战。这些因素导致优先功能而忽视语言细微差别的工程优先方法。

Conclusion: 研究提出Data Care框架，基于CARE原则（集体利益、控制权、责任和伦理），将偏见缓解重新定义为语料设计、标注和治理的组成部分，为在传统LLM开发复制现有权力不平衡和文化盲点的背景下构建包容、可持续和文化基础的语言技术提供可复制模型。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [17] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出一个包含四个组件的全面数据偏见检测与缓解管道，针对可配置的敏感属性（如性别、宗教、年龄）处理两种数据偏见类型：代表性偏见和（显性）刻板印象，并通过实验验证了数据去偏的有效性，但发现模型偏见减少效果不一致。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据存在多方面偏见表现，包括有害语言和倾斜的人口分布。欧盟AI法案等法规要求识别和减轻对受保护群体的偏见，但缺乏实践指导和可操作化方法。需要开发系统性的数据偏见检测与缓解方案。

Method: 提出包含四个组件的管道：1）基于质量标准的LLM生成相关群体标签词表；2）使用人口统计代表性分数量化代表性偏见；3）基于社会语言学信息的过滤检测和缓解刻板印象；4）通过语法和上下文感知的反事实数据增强补偿代表性偏见。

Result: 在性别、宗教和年龄三个示例上的评估显示：1）数据去偏组件有效减少了文本数据集中的代表性偏见和显性刻板印象；2）在去偏数据上微调的LLMs（0.6B-8B参数）在偏见基准测试中并未一致表现改善，暴露了当前评估方法的缺陷。

Conclusion: 成功开发了数据偏见检测与缓解管道，能有效减少数据偏见，但数据去偏对模型偏见减少的效果有限，表明需要更有针对性的数据操作来解决模型偏见，同时当前评估方法存在关键差距。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [18] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出OPV（基于结果的流程验证器），通过验证长思维链中总结结果的推理过程，实现准确高效的验证，并支持大规模标注。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器无法检查长思维链中的不可靠中间步骤，而基于过程的验证器由于人工标注成本高昂导致高质量标注稀缺，难以可靠检测复杂长思维链中的错误。

Method: 提出OPV验证器，采用迭代主动学习框架，通过专家标注逐步提升验证能力。每轮迭代中，标注当前最佳OPV最不确定的案例，然后通过拒绝微调（RFT）和RLVR训练新的OPV。

Result: OPV在held-out benchmark上取得83.1的F1分数，优于Qwen3-Max-Preview的76.3。能有效检测合成数据集中的假阳性，与专家评估高度一致。与策略模型协作时，在AIME2025上将DeepSeek-R1-Distill-Qwen-32B的准确率从55.2%提升到73.3%。

Conclusion: OPV通过验证总结结果的推理过程，实现了准确高效的验证，支持大规模标注，在多个基准测试中表现出优越性能和广泛适用性。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [19] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT是一个三层调度员支持系统，旨在解决加勒比口音英语在紧急语音识别中的性能下降问题，通过结合口音调优的ASR、本地实体提取和生物声学痛苦检测，为调度员提供结构化输入以应用标准分诊协议。


<details>
  <summary>Details</summary>
Motivation: 紧急语音识别系统在非标准英语变体（特别是加勒比口音）上存在系统性性能下降，导致加勒比人群在紧急服务中存在关键差距，需要开发能够处理口音变化的系统以确保公平获得标准分诊协议。

Method: 三层架构设计：1) 加勒比口音调优的自动语音识别；2) 基于大语言模型的本地实体提取；3) 生物声学痛苦检测。系统将ASR低置信度视为有价值的队列优先级信号，特别是当与升高的声学痛苦标记结合时，表明呼叫者可能转向更基础的方言变体。

Result: 建立了口音弹性紧急AI框架，确保加勒比声音能够公平获得已建立的国家分诊协议（ESI用于常规操作，START用于大规模伤亡事件）。系统在ASR失败时仍能为调度员提供结构化输入，但加勒比紧急呼叫的实证验证仍待未来工作。

Conclusion: TRIDENT通过将ASR低置信度重新定义为优先级信号，结合实体提取和声学痛苦检测，创建了一个口音弹性的紧急响应系统，确保所有方言变体都能获得标准分诊协议，为灾难场景中的离线操作提供了部署考虑。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [20] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 提出OPV（基于结果的流程验证器），通过验证长推理链中总结结果的推理过程，实现准确高效的验证和大规模标注，超越现有验证器性能


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器无法检查长推理链中的不可靠中间步骤，而基于过程的验证器由于高质量标注稀缺且人工标注成本高昂，难以可靠检测复杂长推理链中的错误

Method: 提出OPV验证器，采用迭代主动学习框架，通过专家标注最不确定的案例，使用拒绝微调和RLVR训练新的OPV，逐步提升验证能力

Result: OPV在OPV-Bench上取得83.1的F1分数，超越Qwen3-Max-Preview的76.3；能有效检测合成数据集中的假阳性；与策略模型协作时显著提升性能，如将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升至73.3%

Conclusion: OPV通过验证总结结果的推理过程，实现了准确高效的验证和大规模标注，在多个基准测试中表现出优越性能和广泛适用性

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [21] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 通过模型缩放而非持续预训练来高效适配低资源语言，发现缩放能提升数据效率、减少灾难性遗忘，且缩放后的模型合并效果更好


<details>
  <summary>Details</summary>
Motivation: 当前大规模多语言模型在中小规模下表现不如语言特定适配模型，特别是在中低资源语言上。需要寻找更高效的预训练模型适配新目标语言的方法。

Method: 通过全面的缩放消融实验，使用近似FLOP匹配的模型，比较放大英语基础模型与标准持续预训练的效果。探索缩放后的语言特定模型是否能合并构建模块化多语言系统。

Result: 一旦接触足够的目标语言数据，更大的缩放模型能够匹配或超越在更多数据上持续预训练的小模型，显示缩放对数据效率的益处。缩放还有助于保持基础模型的英语能力，减少灾难性遗忘。合并缩放模型比合并小模型效果更好，但合并仍不如联合多语言训练有效。

Conclusion: 缩放是适配预训练模型到新目标语言的有效策略，能提高数据效率并减少灾难性遗忘。缩放后的模型合并效果更好，但合并方法仍有改进空间，需要专门针对语言级整合的合并方法。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [22] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: LLMs在印度临床应用中面临罗马化文本的性能下降问题，特别是在母婴健康分诊领域，罗马化文本的F1分数比原生脚本低5-12分，可能导致近200万额外错误。


<details>
  <summary>Details</summary>
Motivation: 在印度临床应用中，用户经常使用罗马化文本而非原生脚本进行交流，但现有研究很少使用真实世界数据评估这种书写变体对LLM可靠性的影响，特别是在母婴健康分诊这一关键领域。

Method: 使用包含五种印度语言和尼泊尔语的真实世界用户生成查询数据集，对领先的LLM进行基准测试，比较罗马化文本和原生脚本的性能差异，并分析错误原因。

Result: 罗马化消息的性能持续下降，F1分数比原生脚本低5-12分；在合作母婴健康组织中，这种差距可能导致近200万额外分诊错误；性能差距不是由于临床推理失败，LLM通常能正确推断罗马化查询的语义意图，但在罗马化输入的拼写噪声面前，最终分类输出仍然脆弱。

Conclusion: 研究揭示了LLM健康系统中的一个关键安全盲点：看似能理解罗马化输入的模型可能仍无法可靠地对其采取行动，这对印度等多元语言环境中的临床LLM部署具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [23] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: FACTS Leaderboard是一个综合评估语言模型事实准确性的在线评测套件，包含四个子榜单：多模态、参数化、搜索和基于文档的评测，通过自动评分模型评估模型在不同场景下生成事实准确文本的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估语言模型事实准确性的基准测试，需要开发一个能够衡量模型在不同场景下（包括多模态输入、闭卷问答、信息搜索和基于文档生成）生成事实准确文本能力的综合评测体系。

Method: 构建包含四个子榜单的评测套件：1) FACTS Multimodal：评估图像问答的事实准确性；2) FACTS Parametric：评估模型参数中的世界知识；3) FACTS Search：评估使用搜索API时的信息检索准确性；4) FACTS Grounding (v2)：评估长文本生成是否基于提供的文档。使用自动评分模型对模型响应进行评分，最终得分为四个子榜单的平均分。

Result: 开发了一个全面的事实准确性评测套件，包含公开和私有数据集以保护评测完整性，提供了在线平台供外部参与，并将在Kaggle上持续维护更新。

Conclusion: FACTS Leaderboard Suite为语言模型的事实准确性提供了全面、平衡和稳健的评估框架，通过多维度评测帮助研究人员和开发者更好地理解和改进模型的事实生成能力。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [24] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion是一个融合集成框架，通过结合传统Transformer分类器和大型语言模型，实现准确且成本感知的文本分类预测


<details>
  <summary>Details</summary>
Motivation: 传统Transformer分类器在文本分类任务中表现出色，但大型语言模型具有更强的推理能力。作者希望结合两者的优势，同时考虑实际部署中的准确性、延迟和成本权衡

Method: 通过拼接传统Transformer的嵌入表示和LLM生成的每类得分（通过结构化提示工程策略获得），然后将这个联合表示输入到紧凑的多层感知机（FusionMLP）中生成最终预测

Result: 在AG News数据集上达到92.4%准确率，在10类Reuters 21578主题分类上达到92.3%准确率，展现了跨领域的稳健性能

Conclusion: LabelFusion成功融合了LLM推理能力和传统Transformer分类器的优势，在保持高性能的同时实现了准确性、延迟和成本之间的实用权衡

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [25] [Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)](https://arxiv.org/abs/2512.09939)
*Stella C. Dong*

Main category: cs.MA

TL;DR: 该研究提出R-CMASP模型，将再保险决策建模为规范约束的多智能体系统，结合模拟器驱动动态和类型化通信，相比确定性自动化或单一LLM基线，能产生更稳定、规范遵从的行为。


<details>
  <summary>Details</summary>
Motivation: 再保险决策具有分布式信息、部分可观测性、异质认知责任、模拟器驱动动态以及监管约束等核心特性，确定性工作流自动化无法满足这些要求，缺乏认知灵活性、协作协调机制和规范敏感行为。

Method: 提出R-CMASP模型，扩展随机博弈和Dec-POMDPs，增加三个关键要素：(1)基于巨灾、资本和投资组合引擎的模拟器耦合转移动态；(2)具有结构化可观测性、信念更新和类型化通信的角色专业化智能体；(3)将偿付能力、监管和组织规则编码为联合行动可容许性约束的规范性可行性层。

Result: 在领域校准的合成环境中，使用具有工具访问和类型化消息协议的LLM智能体，显示受治理的多智能体协调比确定性自动化或单一LLM基线产生更稳定、一致和规范遵从的行为——降低定价方差、提高资本效率、增加条款解释准确性。

Conclusion: 结果表明，受监管的模拟器驱动决策环境最自然地建模为规范治理、模拟器耦合的多智能体系统，将审慎规范嵌入为可容许性约束并结构化通信为类型化行为，可显著增强均衡稳定性。

Abstract: Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.
  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.
  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.
  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.

</details>


### [26] [Empirical Hardness in Multi-Agent Pathfinding: Research Challenges and Opportunities](https://arxiv.org/abs/2512.10078)
*Jingyao Ren,Eric Ewing,T. K. Satish Kumar,Sven Koenig,Nora Ayanian*

Main category: cs.MA

TL;DR: 本文探讨多智能体路径规划（MAPF）实证难度的三个关键研究挑战：算法选择、影响难度的实例特征识别、以及生成困难实例或多样化基准数据集的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管MAPF问题是NP难问题，但具体实例的求解难度差异很大，这表明理论复杂性与实际难度之间存在差距。本文旨在理解这种实证难度现象，为未来研究奠定基础。

Method: 本文提出了MAPF实证难度研究的三个核心挑战框架：1）算法选择问题；2）识别影响MAPF难度的关键实例特征（如结构特性、相变、骨干/后门等）；3）利用对MAPF难度的理解来生成困难实例或多样化基准数据集。

Result: 建立了MAPF实证难度研究的基础框架，明确了三个关键研究方向，鼓励对这些有前景但尚未充分探索的领域进行深入研究。

Conclusion: 本文为未来MAPF实证硬度研究奠定了基础，并鼓励深入探索算法选择、实例特征分析和基准生成这三个关键挑战，以更好地理解MAPF问题的实际求解难度。

Abstract: Multi-agent pathfinding (MAPF) is the problem of finding collision-free paths for a team of agents on a map. Although MAPF is NP-hard, the hardness of solving individual instances varies significantly, revealing a gap between theoretical complexity and actual hardness. This paper outlines three key research challenges in MAPF empirical hardness to understand such phenomena. The first challenge, known as algorithm selection, is determining the best-performing algorithms for a given instance. The second challenge is understanding the key instance features that affect MAPF empirical hardness, such as structural properties like phase transition and backbone/backdoor. The third challenge is how to leverage our knowledge of MAPF empirical hardness to effectively generate hard MAPF instances or diverse benchmark datasets. This work establishes a foundation for future empirical hardness research and encourages deeper investigation into these promising and underexplored areas.

</details>


### [27] [Emergent Collective Memory in Decentralized Multi-Agent AI Systems](https://arxiv.org/abs/2512.10166)
*Khushiyant*

Main category: cs.MA

TL;DR: 该研究展示了去中心化多智能体系统中集体记忆如何通过个体智能体记忆与环境痕迹通信的相互作用而涌现。研究发现个体记忆单独提供68.7%的性能提升，而环境痕迹若无记忆则完全失效，表明记忆可独立运作但痕迹需要认知基础设施进行解释。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索去中心化多智能体系统中集体记忆的涌现机制，理解个体记忆与环境痕迹通信如何协同工作，以及在不同环境条件下哪种机制更有效。

Method: 方法包括：智能体维护内部记忆状态并沉积持久性环境痕迹；在五种环境条件下进行综合验证（20x20至50x50网格，5-20个智能体，每种配置50次运行）；系统密度扫描实验（rho在[0.049, 0.300]范围内，最多625个智能体）；理论相变预测验证。

Result: 结果显示：个体记忆单独提供68.7%的性能提升（1563.87 vs 927.23，p < 0.001）；环境痕迹若无记忆则完全失效；在现实大型网格（30x30, 50x50）上，当rho > 0.20时，痕迹通信主导，在综合指标上比记忆高出36-41%；实验交叉点确认预测临界密度rho_c = 0.230，误差在13%以内。

Conclusion: 结论表明集体记忆通过个体记忆与环境痕迹的相互作用而涌现，记忆可独立运作但痕迹需要认知基础设施。存在临界密度阈值，超过该阈值时痕迹通信变得主导，验证了理论相变预测。

Abstract: We demonstrate how collective memory emerges in decentralized multi-agent systems through the interplay between individual agent memory and environmental trace communication. Our agents maintain internal memory states while depositing persistent environmental traces, creating a spatially distributed collective memory without centralized control. Comprehensive validation across five environmental conditions (20x20 to 50x50 grids, 5-20 agents, 50 runs per configuration) reveals a critical asymmetry: individual memory alone provides 68.7% performance improvement over no-memory baselines (1563.87 vs 927.23, p < 0.001), while environmental traces without memory fail completely. This demonstrates that memory functions independently but traces require cognitive infrastructure for interpretation. Systematic density-sweep experiments (rho in [0.049, 0.300], up to 625 agents) validate our theoretical phase transition prediction. On realistic large grids (30x30, 50x50), stigmergic coordination dominates above rho ~ 0.20, with traces outperforming memory by 36-41% on composite metrics despite lower food efficiency. The experimental crossover confirms the predicted critical density rho_c = 0.230 within 13% error.

</details>


### [28] [Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing](https://arxiv.org/abs/2512.10610)
*Xiaopei Tan,Muyang Fan*

Main category: cs.MA

TL;DR: 提出"边驾驶边思考"框架，将LLM集成到图交通环境中，实现移动中实时路径规划，显著减少交叉口等待时间


<details>
  <summary>Details</summary>
Motivation: 传统方法需要智能体停止思考，导致交通效率低下。需要一种能让智能体在移动中同时进行路径规划的框架，以提高交通系统的实时响应能力和效率

Method: 采用非阻塞异步架构，使用Unity协程和专用请求管理器协调多个智能体。环境建模为带实时拥堵指标的加权无向图，智能体持续更新共享感知信息

Result: 在高流量下，智能体平均决策延迟仅0.75秒。LLM驱动的智能体能够动态适应交通状况，绕开拥堵重新规划路线，表现出超越静态路径规划的行为，同时保持实时性能

Conclusion: 该工作为自适应路由和多智能体协作的未来研究提供了可复现的框架，展示了LLM在实时交通系统中的实际应用潜力

Abstract: We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples](https://arxiv.org/abs/2512.09931)
*Akaash Chatterjee,Suman Kundu*

Main category: cs.AI

TL;DR: ExaCraft是一个AI驱动的个性化学习示例生成系统，通过分析学习者的动态上下文（包括位置、教育背景、专业、复杂度偏好和行为模式）来创建文化相关且个性化的学习示例。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI工具未能专注于生成个性化示例，也无法适应学习者不断变化的理解水平、学习困难和技能增长。学习最有效的方式是与学习者个人相关的、有共鸣的示例，但当前工具缺乏这种能力。

Method: 使用Google Gemini AI和Python Flask API构建的Chrome扩展系统，结合用户定义的个人资料（位置、教育、职业、复杂度偏好）和实时学习者行为分析，通过适应五个关键学习上下文方面来生成个性化示例。

Result: 系统能够生成从基础概念到高级技术实现不断演进的示例，响应主题重复、重新生成请求和主题进展模式，确保示例既文化相关又符合个人学习需求。

Conclusion: ExaCraft通过动态适应学习者的上下文，解决了现有教育AI工具在个性化示例生成方面的不足，为学习者提供更有效、更相关的学习体验。

Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.

</details>


### [30] [Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting](https://arxiv.org/abs/2512.09944)
*Moein Heidari,Mohammad Amin Roohi,Armin Khosravi,Ilker Hacihaliloglu*

Main category: cs.AI

TL;DR: Echo-CoPilot是一个多视图、多任务智能体，利用大语言模型协调专门的超声心动图工具，通过ReAct循环分解临床查询、调用工具并整合输出，在MIMIC-EchoQA基准测试中达到50.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 超声心动图是心血管诊疗的核心技术，但全研究解读仍然是认知密集型、多视图的手动任务。现有的基础模型虽然在单个感知子任务（如视图分类、分割或疾病预测）上表现良好，但通常孤立运行，无法提供统一、临床连贯的评估。

Method: 提出Echo-CoPilot多视图多任务智能体，使用大语言模型协调专门的超声心动图工具套件。在ReAct式循环中，智能体分解临床查询，调用视图识别、心脏结构分割、测量和疾病预测、报告合成等工具，并将输出整合为指南感知的答案和叙述性摘要。

Result: 在公开的MIMIC-EchoQA基准测试中达到50.8%的准确率，优于通用和生物医学视频视觉语言模型。定性分析显示智能体利用定量测量和生理学上下文解决临床决策阈值附近的挑战性病例，如临界左心室肥厚或心包积液严重程度。

Conclusion: Echo-CoPilot通过大语言模型协调专门工具，为超声心动图提供统一、临床连贯的评估框架，在基准测试中表现优异，能够处理临床决策阈值附近的复杂病例，代码将在论文接受后发布。

Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.

</details>


### [31] [Exploring LLMs for Scientific Information Extraction Using The SciEx Framework](https://arxiv.org/abs/2512.10004)
*Sha Li,Ayush Sadekar,Nathan Self,Yiqi Su,Lars Andersland,Mira Chaplin,Annabel Zhang,Hyoju Yang,James B Henderson,Krista Wigginton,Linsey Marr,T. M. Murali,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: SciEx是一个模块化、可组合的框架，用于从科学文献中提取细粒度信息，解决了现有LLM工具在处理长文档、多模态内容和快速变化的数据模式时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具在科学文献信息提取中存在多个挑战：处理长文档困难、难以整合多模态内容、无法有效协调不同出版物中的不一致信息，以及当数据模式快速变化时难以重新架构或微调系统。

Method: 提出SciEx框架，采用模块化设计，将PDF解析、多模态检索、信息提取和聚合等关键组件解耦，支持按需数据提取，并能灵活集成新模型、提示策略和推理机制。

Result: 在三个科学主题的数据集上评估了SciEx提取细粒度信息的准确性和一致性，为当前基于LLM的流水线提供了实践性的优势和局限性分析。

Conclusion: SciEx框架通过模块化设计有效解决了科学文献信息提取中的关键挑战，为快速变化的科学数据提取需求提供了灵活、可扩展的解决方案。

Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.

</details>


### [32] [SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration](https://arxiv.org/abs/2512.10046)
*Yan Zhuang,Jiawei Ren,Xiaokang Ye,Jianzhi Shen,Ruixuan Zhang,Tianai Yue,Muhammad Faayez,Xuhong He,Ziqiao Ma,Lianhui Qin,Zhiting Hu,Tianmin Shu*

Main category: cs.AI

TL;DR: SWR是一个基于虚幻引擎5构建的大规模、逼真城市环境仿真平台，支持多机器人控制和通信，并建立了两个具有挑战性的机器人基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在机器人领域的研究主要集中在室内家庭场景，缺乏针对大规模城市环境的仿真平台和评估基准。需要开发能够测试机器人在复杂城市环境中多模态感知、推理、规划等关键能力的平台。

Method: 基于虚幻引擎5构建SWR仿真平台，通过程序化生成无限扩展的逼真城市场景，包含行人、交通系统等动态元素。平台支持多机器人控制和通信。在此基础上建立了两个基准测试：1）多模态指令跟随任务；2）多智能体搜索任务。

Result: SWR平台在真实性、复杂性和可扩展性方面超越了现有的城市仿真。实验结果表明，包括视觉语言模型在内的最先进模型在SWR任务上表现不佳，缺乏城市环境所需的稳健感知、推理和规划能力。

Conclusion: SWR平台填补了大规模城市环境机器人仿真和评估的空白，为开发能够在复杂开放场景中工作的通用机器人提供了重要的测试平台。当前模型在城市环境任务上的不足揭示了未来研究需要关注的方向。

Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.

</details>


### [33] [Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning](https://arxiv.org/abs/2512.10054)
*Logan Robbins*

Main category: cs.AI

TL;DR: PDT是一种参数高效的并行解码架构，通过轻量级适配器让冻结预训练模型实现并行生成，解决传统方法中的连贯性漂移问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自回归解码本质上是顺序的，存在与输出长度线性相关的延迟瓶颈。现有的"分解-填充"方法虽然尝试通过外部编排实现并行化，但由于缺乏跨流通信而遭受连贯性漂移问题。

Method: 提出并行解码变换器（PDT），在冻结预训练模型中注入轻量级的"推测性笔记条件"适配器，允许并行解码流通过共享的动态潜在空间进行同步。将协调问题形式化为"推测共识"问题，其中兄弟流向全局总线广播语义"笔记"，由学习的验证头进行门控。

Result: 在冻结的200亿参数骨干模型上使用5万步课程进行验证，PDT实现了有效的自校正，在覆盖预测中达到77.8%的精确度，并在不修改主干权重的情况下恢复了近似的串行语义。

Conclusion: PDT为结构化并行生成提供了一个可扩展、高效的替代方案，无需进行完整的模型微调。

Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.

</details>


### [34] [Linear socio-demographic representations emerge in Large Language Models from indirect cues](https://arxiv.org/abs/2512.10065)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.AI

TL;DR: 研究发现LLMs在激活空间中形成用户社会人口属性的线性表示，这些表示基于姓名、职业等间接线索，并影响模型的下游行为如职业推荐。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs如何通过姓名、职业等间接线索编码人类对话伙伴的社会人口属性，以及这些隐式表示如何影响模型行为，这对大规模应用中的公平性问题有重要意义。

Method: 方法包括：1）在四个开源Transformer LLMs（Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B）的残差流中进行探针分析；2）使用显式人口统计披露提示；3）测试从姓名和职业等隐式线索预测人口统计特征；4）分析这些线性表示如何影响职业推荐等下游行为。

Result: 结果显示：1）LLMs在激活空间中形成社会人口属性的线性表示；2）姓名激活与人口普查一致的性别和种族表示；3）职业触发与现实世界劳动力统计数据相关的表示；4）这些隐式人口统计表示主动塑造下游行为；5）通过偏见基准测试的模型仍可能包含并利用隐式偏见。

Conclusion: 结论是LLMs基于间接线索形成隐式的人口统计表示，这些表示影响模型行为，即使通过偏见测试的模型仍可能包含隐式偏见，这对大规模应用的公平性有重要影响。

Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.

</details>


### [35] [Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit](https://arxiv.org/abs/2512.10092)
*Nick Jiang,Xiaoqing Sun,Lisa Dunlap,Lewis Smith,Neel Nanda*

Main category: cs.AI

TL;DR: 稀疏自编码器（SAE）嵌入比LLM更经济可靠，比稠密嵌入更可控，可用于大规模文本分析，揭示数据集差异、概念相关性等洞察。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本分析主要依赖昂贵的LLM技术或缺乏可控性的稠密嵌入模型，需要一种更经济、可靠且可控的表示学习方法。

Method: 使用稀疏自编码器（SAEs）创建SAE嵌入表示，其维度映射到可解释的概念，通过四个数据分析任务验证其有效性。

Result: SAE嵌入比LLM成本低2-8倍且更可靠，比稠密嵌入更可控，能发现数据集语义差异、概念相关性，在基于属性的检索任务中表现更优。

Conclusion: SAE是用于非结构化数据分析的多功能工具，强调了通过数据解释模型的重要性，为大规模文本分析提供了经济、可靠且可控的解决方案。

Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.

</details>


### [36] [Robust AI Security and Alignment: A Sisyphean Endeavor?](https://arxiv.org/abs/2512.10100)
*Apostol Vassilev*

Main category: cs.AI

TL;DR: 该论文将哥德尔不完备性定理扩展到AI领域，建立了AI安全性和对齐性的信息论限制，并提供了应对这些挑战的实用方法。


<details>
  <summary>Details</summary>
Motivation: 认识到AI安全和对齐存在根本性限制对于负责任地采用AI技术至关重要，需要了解这些限制并为相关挑战做好准备。

Method: 通过将哥德尔不完备性定理扩展到AI领域，建立信息论框架来分析AI安全性和对齐性的根本限制。

Result: 证明了AI系统在安全性和对齐性方面存在无法避免的信息论限制，这些限制源于计算和逻辑的基本约束。

Conclusion: AI安全和对齐存在根本性限制，需要承认这些限制并采用实用方法来应对相关挑战，这对AI系统的认知推理能力也有更广泛的影响。

Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending Gödel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.

</details>


### [37] [AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice](https://arxiv.org/abs/2512.10114)
*Mesafint Fanuel,Mahmoud Nabil Mahmoud,Crystal Cook Marshal,Vishal Lakhotia,Biswanath Dari,Kaushik Roy,Shaohu Zhang*

Main category: cs.AI

TL;DR: AgriRegion是一个专门为农业咨询设计的检索增强生成框架，通过地理空间元数据注入和区域优先重排序机制，减少LLM在农业领域的上下文幻觉问题，提供区域感知的精准农业建议。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域经常出现上下文幻觉问题，提供的建议在一个地区科学合理但在另一个地区可能造成灾难性后果，这是由于土壤、气候和地方法规的差异造成的。需要专门针对农业领域的高保真、区域感知的咨询系统。

Method: 提出了AgriRegion框架，这是一个专门为农业设计的检索增强生成系统。与仅依赖语义相似性的标准RAG方法不同，AgriRegion包含地理空间元数据注入层和区域优先重排序机制。通过将知识库限制在已验证的本地农业推广服务，并在检索过程中强制执行地理空间约束，确保种植计划、害虫控制和施肥建议的本地准确性。

Result: 创建了新的基准数据集AgriRegion-Eval，包含12个农业子领域的160个领域特定问题。实验表明，AgriRegion相比最先进的LLM系统减少了10-20%的幻觉，并显著提高了综合评估中的信任分数。

Conclusion: AgriRegion框架通过结合地理空间约束和区域优先检索，有效解决了农业领域LLM的上下文幻觉问题，为农民和农业从业者提供了更可靠、区域特定的农业建议，提高了农业咨询的准确性和可信度。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.

</details>


### [38] [The 2025 Foundation Model Transparency Index](https://arxiv.org/abs/2512.10169)
*Alexander Wan,Kevin Klyman,Sayash Kapoor,Nestor Maslej,Shayne Longpre,Betty Xiong,Percy Liang,Rishi Bommasani*

Main category: cs.AI

TL;DR: 2025年基础模型透明度指数显示，主要AI公司的透明度从2024年的58分降至2025年的40分，透明度恶化，公司在训练数据和模型使用影响方面最不透明。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型开发公司的影响力日益增强，需要评估其透明度实践的变化情况，为政策制定者提供参考，推动AI行业的透明化发展。

Method: 采用年度基础模型透明度指数评估方法，新增数据获取、使用数据和监控等指标，首次评估阿里巴巴、DeepSeek和xAI等公司，对多家公司进行量化评分。

Result: 透明度整体恶化，平均分从58降至40；IBM表现最佳（95分），xAI和Midjourney最差（14分）；公司在训练数据、计算资源和模型部署后影响方面最不透明。

Conclusion: 基础模型开发公司的透明度在下降，需要更积极的政策干预来解决关键信息缺失问题，特别是训练数据和模型影响方面的透明度需要加强监管。

Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.

</details>


### [39] [An exploration for higher efficiency in multi objective optimisation with reinforcement learning](https://arxiv.org/abs/2512.10208)
*Mehmet Emin Aydin*

Main category: cs.AI

TL;DR: 本文提出了一种基于多目标强化学习的广义方法，用于优化搜索过程中的算子序列选择问题，旨在提高多目标优化算法的效率。


<details>
  <summary>Details</summary>
Motivation: 优化和搜索过程中的效率问题仍然是影响算法性能的主要挑战。虽然使用算子池而非单一算子来处理邻域移动操作具有潜力，但最优或接近最优的算子序列需要进一步研究。多目标优化领域在这方面研究较少，需要一种能够推广经验并有效利用的方法。

Method: 提出基于多目标强化学习的广义方法，通过强化学习框架来学习和选择最优算子序列。该方法包含已完成和待完成的多个阶段，旨在系统性地展示多目标强化学习在优化中的效率。

Result: 论文概述了提出的广义方法框架，其中某些阶段已完成，某些阶段仍在进行中。该方法旨在证明多目标强化学习在提高优化效率方面的潜力。

Conclusion: 基于多目标强化学习的广义方法为解决多目标优化中的算子序列选择问题提供了有前景的解决方案，有望显著提高优化算法的效率和性能。

Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.

</details>


### [40] [ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs](https://arxiv.org/abs/2512.10211)
*Junyang Cai,El Mehdi Er Raqabi,Pascal Van Hentenryck,Bistra Dilkina*

Main category: cs.AI

TL;DR: 将预测-搜索框架扩展到参数化混合整数线性规划，提出ID-PaS身份感知学习框架，有效处理异构变量，在多个现实大规模问题上优于Gurobi和传统PaS方法。


<details>
  <summary>Details</summary>
Motivation: 现有预测-搜索方法主要针对二元问题，忽略了实际应用中常见的固定变量问题，且无法有效处理异构变量，限制了在参数化混合整数线性规划中的应用。

Method: 提出ID-PaS（身份感知学习框架），扩展预测-搜索框架到参数化混合整数线性规划，使机器学习模型能够更有效地处理异构变量。

Result: 在多个现实世界大规模问题上进行实验，ID-PaS始终表现出优于最先进求解器Gurobi和传统预测-搜索方法的性能。

Conclusion: ID-PaS框架成功将预测-搜索方法扩展到参数化混合整数线性规划，通过身份感知学习有效处理异构变量，为实际应用中的组合优化问题提供了更强大的解决方案。

Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.

</details>


### [41] [Reverse Thinking Enhances Missing Information Detection in Large Language Models](https://arxiv.org/abs/2512.10273)
*Yuxin Liu,Chaojie Gu,Yihang Zhang,Bin Qian,Shibo He*

Main category: cs.AI

TL;DR: 论文提出了一种基于逆向思维的新框架，通过引导大语言模型进行反向推理来识别缺失信息，相比传统前向推理方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理缺失信息问题时表现不佳，容易出现回答不完整、事实错误和幻觉等问题。传统的链式思维和树状思维等前向推理方法在结构化问题解决中虽然成功，但往往无法系统性地识别和恢复被省略的信息。

Method: 受反向推理研究启发，提出了一个新颖的逆向思维框架，引导大语言模型通过反向思考来识别必要条件和定位缺失元素。该方法将具有挑战性的缺失信息识别任务转化为更易管理的反向推理问题。

Result: 实验结果表明，逆向思维方法相比传统前向推理方法取得了显著的性能提升，为增强大语言模型的逻辑完整性和推理鲁棒性提供了有前景的方向。

Conclusion: 逆向思维方法能够有效提升大语言模型在缺失信息检测任务上的表现，为解决模型在处理不完整信息时的局限性提供了新的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.

</details>


### [42] [Neuronal Attention Circuit (NAC) for Representation Learning](https://arxiv.org/abs/2512.10282)
*Waleed Razzaq,Izis Kankaraway,Yun-Bo Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的生物启发式连续时间注意力机制NAC，通过将注意力对数计算重新表述为线性一阶ODE的解决方案，实现了高效的适应性动态建模。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制虽然改进了RNN的表示学习，但其离散性质限制了连续时间建模能力。需要一种生物可信的连续时间注意力机制来解决这一问题。

Method: 提出神经注意力电路(NAC)，将注意力对数计算重新表述为线性一阶ODE的解，使用源自秀丽隐杆线虫神经元电路策略的稀疏门控机制，支持三种计算模式：显式欧拉积分、精确闭式解和稳态近似。

Result: NAC在多个领域（不规则时间序列分类、自动驾驶车道保持、工业预测）中匹配或优于基线方法，在运行时间和内存效率方面处于中等位置，同时提供了理论保证。

Conclusion: NAC是一种有效的生物启发式连续时间注意力机制，通过ODE框架和稀疏门控设计，在保持理论保证的同时实现了实际应用的性能提升。

Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.

</details>


### [43] [Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules](https://arxiv.org/abs/2512.10300)
*Yanbei Jiang,Xueqi Ma,Shu Liu,Sarah Monazam Erfani,Tongliang Liu,James Bailey,Jey Han Lau,Krista A. Ehinger*

Main category: cs.AI

TL;DR: 本文提出了一个新颖的可解释性框架来系统分析视觉语言模型（VLM）的内部机制，重点关注注意力头在多模态推理中的功能角色。通过CogVision数据集和探测方法，识别出专门执行特定认知功能的注意力头，并发现这些功能头具有稀疏性、可变分布和层次组织等特征。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态基准测试中表现出色，但它们很大程度上仍然是一个黑箱。为了理解VLM的内部工作机制，特别是注意力头在多模态推理中的功能角色，需要开发系统化的可解释性分析方法。

Method: 1. 引入CogVision数据集，将复杂的多模态问题分解为逐步的子问题，模拟人类链式思维推理范式；2. 每个子问题关联特定的感知或认知功能（如高级视觉感知和推理）；3. 采用基于探测的方法识别专门执行这些功能的注意力头；4. 在不同VLM家族中进行分析；5. 通过干预实验验证功能头的重要性。

Result: 1. 功能头普遍稀疏存在；2. 不同功能的功能头数量和分布存在差异；3. 功能头介导交互和层次组织；4. 干预实验表明：移除功能头导致性能下降，强调功能头则提高准确性；5. 这些发现在不同VLM家族中具有普遍性。

Conclusion: 研究揭示了VLM的认知组织结构，为设计具有更符合人类感知和推理能力的模型提供了新见解和方向。功能头的识别和分析有助于理解VLM的内部工作机制，推动模型向更透明、可解释的方向发展。

Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

</details>


### [44] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 该论文提出了"可信编排AI十大标准"框架，将治理架构嵌入AI生态系统执行层，确保AI系统可验证、透明、可重现并受有意义的人类控制。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键决策中扮演越来越重要的角色，技术能力与制度问责之间出现了日益扩大的差距。仅靠伦理指导不足以应对这一挑战，需要将治理嵌入生态系统执行层的架构。

Method: 提出了"可信编排AI十大标准"这一全面的保障框架，整合了人类输入、语义一致性、审计和溯源完整性，构建了统一的控制面板架构。该框架从国际标准和澳大利亚国家AI保障框架中汲取灵感，为整个AI组件、消费者和人类参与者提供治理覆盖。

Result: 该框架表明可信度可以通过工程方法系统地融入AI系统，确保执行层保持可验证、透明、可重现，并处于有意义的人类控制之下。

Conclusion: 该研究提出了一种将治理架构嵌入AI生态系统执行层的系统性方法，通过"可信编排AI十大标准"框架，为AI系统的可信度提供了工程化解决方案，弥补了技术能力与制度问责之间的差距。

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [45] [EpiPlanAgent: Agentic Automated Epidemic Response Planning](https://arxiv.org/abs/2512.10313)
*Kangkun Mao,Fang Xu,Jinru Ding,Yidong Jiang,Yujun Yao,Yirong Chen,Junming Liu,Xiaoqin Wu,Qian Wu,Xiaoyan Huang,Jie Xu*

Main category: cs.AI

TL;DR: EpiPlanAgent是一个基于大型语言模型的多智能体系统，能够自动生成和验证数字应急响应计划，显著提高计划完整性和指南对齐性，同时大幅缩短开发时间。


<details>
  <summary>Details</summary>
Motivation: 传统的流行病应对规划依赖劳动密集型的手工方法，效率低下且难以规模化。本研究旨在设计一个自动化系统来改进这一过程，利用AI技术提升公共卫生应急准备能力。

Method: 开发了基于大型语言模型的多智能体框架EpiPlanAgent，集成了任务分解、知识基础和模拟模块。公共卫生专业人员使用真实世界疫情场景在受控环境中测试该系统。

Result: EpiPlanAgent显著提高了计划的完整性和指南对齐性，同时大幅减少了开发时间。专家评估确认AI生成内容与人工编写内容高度一致，用户反馈显示系统具有很高的感知效用。

Conclusion: EpiPlanAgent为智能流行病应对规划提供了有效、可扩展的解决方案，展示了智能体AI在转变公共卫生准备方面的潜力。

Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.

</details>


### [46] [User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation](https://arxiv.org/abs/2512.10322)
*Yongqiang Yu,Xuhui Li,Hazza Mahmood,Jinxing Zhou,Haodong Hong,Longtao Jiang,Zhiqiang Xu,Qi Wu,Xiaojun Chang*

Main category: cs.AI

TL;DR: 提出了一种用户反馈驱动的视觉语言导航自适应框架，通过系统整合人类交互来增强GSA-VLN，将用户反馈转化为高质量训练数据，并采用记忆库预热机制提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 当前GSA-VLN框架仅依赖无监督环境适应，忽略了用户反馈这一自然且有价值的监督信号。为了缩小静态基准测试与现实部署之间的差距，需要将用户反馈系统性地整合到持续学习中。

Method: 1. 用户反馈驱动的自适应框架：将用户提供的导航指令和纠正信号转化为高质量、环境对齐的训练数据；2. 记忆库预热机制：重用先前获取的环境知识，缓解冷启动问题，确保稳定重新部署。

Result: 在GSA-R2R基准测试中，该方法持续超越GR-DUET等强基线，提高了导航成功率和路径效率。记忆库预热机制稳定了早期导航性能，减少了更新后的性能下降。在持续和混合自适应设置下均表现出鲁棒性和通用性。

Conclusion: 通过系统整合用户反馈到GSA-VLN框架中，能够显著提升自适应质量，实现更高效、更现实的导航适应。记忆库预热机制有效解决了冷启动问题，为现实世界部署提供了稳定可靠的解决方案。

Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.

</details>


### [47] [On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering](https://arxiv.org/abs/2512.10339)
*Ziseok Lee,Minyeong Hwang,Sanghyun Jo,Wooyeol Lee,Jihyung Ko,Young Bin Park,Jae-Mun Choi,Eunho Yang,Kyungsu Kim*

Main category: cs.AI

TL;DR: 论文提出ACE方法解决概率密度比方法中的边缘路径崩溃问题，使异构扩散模型能够稳定组合生成


<details>
  <summary>Details</summary>
Motivation: 现有的概率密度比方法在组合异构扩散模型时会出现边缘路径崩溃问题，导致中间密度不可归一化，限制了模型在分子设计等任务中的应用

Method: 提出ACE方法：1) 推导路径存在性判据，仅从噪声调度和指数预测崩溃；2) 扩展Feynman-Kac引导到时变指数，保证有效概率路径

Result: 在合成2D基准和柔性姿态支架装饰任务中，ACE消除崩溃，实现高引导组合生成，在分布和对接指标上优于恒定指数基线和专用模型

Conclusion: ACE将概率密度比引导从不稳定启发式方法转变为可控生成的可靠工具，特别适用于异构专家模型组合

Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.

</details>


### [48] [Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning](https://arxiv.org/abs/2412.20505)
*Hang Ni,Yuzhi Wang,Hao Liu*

Main category: cs.AI

TL;DR: 提出循环城市规划（CUP）新范式，利用大语言模型多智能体框架实现城市计划的持续生成、评估和优化，形成闭环规划系统。


<details>
  <summary>Details</summary>
Motivation: 城市化背景下的城市更新面临重大挑战，需要适应性方法来应对不断变化的需求。利用大语言模型的最新进展，为城市规划提供动态响应方案。

Method: 提出循环城市规划（CUP）范式，构建基于大语言模型的多智能体框架，包含三个核心组件：1）规划组件：LLM智能体基于上下文数据生成和优化城市计划；2）生活组件：智能体模拟居民行为和互动，建模城市环境中的生活；3）评判组件：评估计划有效性并提供迭代改进反馈。

Result: 在真实世界数据集上的实验证明了该框架作为连续自适应规划过程的有效性。

Conclusion: 循环城市规划（CUP）通过大语言模型多智能体框架实现了动态响应式的城市规划方法，为城市更新提供了持续优化的闭环解决方案。

Abstract: Urban regeneration presents significant challenges within the context of urbanization, requiring adaptive approaches to tackle evolving needs. Leveraging advancements in large language models (LLMs), we propose Cyclical Urban Planning (CUP), a new paradigm that continuously generates, evaluates, and refines urban plans in a closed-loop. Specifically, our multi-agent LLM-based framework consists of three key components: (1) Planning, where LLM agents generate and refine urban plans based on contextual data; (2) Living, where agents simulate the behaviors and interactions of residents, modeling life in the urban environment; and (3) Judging, which involves evaluating plan effectiveness and providing iterative feedback for improvement. The cyclical process enables a dynamic and responsive planning approach. Experiments on the real-world dataset demonstrate the effectiveness of our framework as a continuous and adaptive planning process.

</details>


### [49] [REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature](https://arxiv.org/abs/2512.10348)
*Wenhan Wu,Zhili He,Huanghuang Liang,Yili Gong,Jiawei Jiang,Chuang Hu,Dazhao Cheng*

Main category: cs.AI

TL;DR: REMISVFU是一个用于垂直联邦学习的表示误导框架，通过将遗忘方的编码器输出坍缩到单位球面上的随机锚点来实现快速客户端级遗忘，同时通过正交投影对齐梯度来保持剩余方的模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘技术主要针对水平联邦学习（HFL），而垂直联邦学习（VFL）中数据按特征划分的架构使得HFL的遗忘方法失效。数据保护法规（如GDPR）要求联邦系统支持参与方的被遗忘权，因此需要专门针对VFL的遗忘解决方案。

Method: 提出REMISVFU框架：当收到删除请求时，遗忘方将其编码器输出坍缩到单位球面上的随机锚点，切断其特征与全局模型的统计联系。服务器端联合优化保留损失和遗忘损失，通过正交投影对齐梯度以消除破坏性干扰，保持剩余方的模型效用。

Result: 在公开基准测试中，REMISVFU将后门攻击成功率抑制到自然类先验水平，仅牺牲约2.5个百分点的干净准确率，优于现有最先进的基线方法。

Conclusion: REMISVFU是一个即插即用的表示误导框架，能够在splitVFL系统中实现快速、客户端级的遗忘，有效平衡遗忘效果和模型效用，为垂直联邦学习提供了实用的遗忘解决方案。

Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

</details>


### [50] [LLM-Empowered Representation Learning for Emerging Item Recommendation](https://arxiv.org/abs/2512.10370)
*Ziying Zhang,Quanming Yao,Yaqing Wang*

Main category: cs.AI

TL;DR: EmerFlow是一个基于大语言模型的表示学习框架，专门用于推荐新兴物品，通过LLM推理丰富原始特征，对齐现有推荐模型的嵌入空间，并利用元学习优化嵌入表示。


<details>
  <summary>Details</summary>
Motivation: 现有推荐方法通常假设新兴物品只有很少甚至没有历史交互，这种假设过于简化了问题。实际上，好的模型需要保持新兴物品的独特性，同时利用它们与成熟物品的共享模式。

Method: EmerFlow框架：1）通过LLM推理丰富新兴物品的原始特征；2）将这些表示与现有推荐模型的嵌入空间对齐；3）通过元学习整合新的交互来优化嵌入。

Result: 在电影和医药等多个领域的广泛实验中，EmerFlow始终优于现有方法，能够从有限的交互中学习到新兴物品的表达性嵌入。

Conclusion: EmerFlow成功解决了新兴物品推荐中的动态过程挑战，通过LLM增强的表示学习框架，在保持物品独特性的同时利用共享模式，显著提升了推荐性能。

Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.

</details>


### [51] [AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management](https://arxiv.org/abs/2512.10371)
*Shizuo Tian,Hao Wen,Yuxuan Chen,Jiacheng Liu,Shanhui Zhao,Guohong Liu,Ju Ren,Yunxin Liu,Yuanchun Li*

Main category: cs.AI

TL;DR: AgentProg：一种程序引导的移动GUI智能体上下文管理方法，通过将交互历史重构为带变量和控制流的程序来减少上下文开销，在长时任务中保持稳定性能


<details>
  <summary>Details</summary>
Motivation: 移动GUI智能体的快速发展推动了长时任务自动化的研究，但现有方法依赖不断扩展的交互历史导致巨大的上下文开销，现有上下文管理和压缩技术往往无法保留关键语义信息，导致任务性能下降

Method: 提出AgentProg，一种程序引导的智能体上下文管理方法，将交互历史重构为带变量和控制流的程序，基于程序结构确定哪些信息应保留、哪些可丢弃，并集成受信念MDP框架启发的全局信念状态机制来处理部分可观测性和适应环境变化

Result: 在AndroidWorld和扩展的长时任务套件上的实验表明，AgentProg在这些基准测试中达到了最先进的成功率，更重要的是，在长时任务中保持稳健性能，而基线方法则出现灾难性性能下降

Conclusion: AgentProg通过程序化组织交互历史有效解决了移动GUI智能体长时任务中的上下文管理问题，在减少开销的同时保持任务性能，为长时任务自动化提供了有效的解决方案

Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.

</details>


### [52] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 本文提出SaEI方法，通过选择性对抗熵干预增强视觉语言模型的推理能力，在RL采样阶段引入熵干预来提升响应多样性


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的微调方法主要通过控制特定token的更新来干预熵，但忽略了RL采样阶段的熵干预，这可以通过提高响应多样性来增强GRPO性能

Method: 提出选择性对抗熵干预(SaEI)：1) 熵引导对抗采样(EgAS)，将采样响应的熵作为对抗目标，用对抗梯度攻击视觉输入生成对抗样本；2) token选择性熵计算(TsEC)，在不扭曲VLM事实知识的前提下最大化对抗攻击效果

Result: 在领域内和领域外数据集上的广泛实验表明，该方法能显著提升策略探索能力，增强推理能力

Conclusion: 通过选择性对抗熵干预方法，在RL采样阶段引入熵干预能有效提升视觉语言模型的推理能力，代码将在论文接受后发布

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [53] [Representation of the structure of graphs by sequences of instructions](https://arxiv.org/abs/2512.10429)
*Ezequiel Lopez-Rubio*

Main category: cs.AI

TL;DR: 提出一种将图邻接矩阵转换为指令字符串的新表示方法，使图结构能适配深度学习语言模型处理


<details>
  <summary>Details</summary>
Motivation: 当前图表示方法（基于邻接矩阵）不适合深度学习语言模型处理，而深度学习模型在文本处理方面表现出强大能力，需要一种能兼容深度学习语言模型的图表示方法

Method: 将图的邻接矩阵转换为一系列简单指令构成的字符串，这些指令逐步构建邻接矩阵，该转换是可逆的，既能从图生成字符串，也能从字符串重建图

Result: 提出的表示方法紧凑且能保持图的局部结构模式，初步计算实验显示出有利结果

Conclusion: 这种新的图表示方法有望提升深度学习模型对图的处理能力，为图处理与深度学习语言模型的结合提供了新途径

Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.

</details>


### [54] [Targeted Data Protection for Diffusion Model by Matching Training Trajectory](https://arxiv.org/abs/2512.10433)
*Hojun Lee,Mijin Koo,Yeji Song,Nojun Kwak*

Main category: cs.AI

TL;DR: TAFAP是一种通过轨迹对齐实现目标数据保护的新方法，能够有效控制扩散模型微调过程，将输出重定向到用户指定的目标概念，同时保持高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型微调技术虽然使个性化图像生成更加便捷，但也带来了未经授权数据使用和隐私侵犯的严重问题。现有的保护方法仅限于被动降低图像质量，无法实现稳定控制。虽然目标数据保护（TDP）为主动重定向到用户指定概念提供了有前景的范式，但现有TDP方法由于采用快照匹配方式而控制性差，未能考虑完整的学习动态。

Method: TAFAP（基于对抗性扰动的微调轨迹对齐）是首个通过控制整个训练轨迹成功实现有效TDP的方法。与快照匹配方法不同，TAFAP采用受数据集蒸馏启发的轨迹匹配技术，在整个微调过程中强制执行持久、可验证的转换。该方法能够同时控制身份和视觉模式。

Result: 通过大量实验验证，TAFAP首次在扩散模型中成功实现了目标转换，同时控制身份和视觉模式。TAFAP显著优于现有TDP尝试，实现了向目标概念的稳健重定向，同时保持高图像质量。

Conclusion: TAFAP实现了可验证的安全保障，并为控制和追踪扩散模型输出的改变提供了新框架。该方法能够有效保护数据隐私，防止未经授权的模型微调使用。

Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.

</details>


### [55] [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Jahnvi Singh,Vinay Chamola,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.AI

TL;DR: 论文研究了科学同行评审中LLM评估系统的对抗性PDF操纵脆弱性，开发了WAVS评估指标，发现多种攻击策略能成功翻转评审决策


<details>
  <summary>Details</summary>
Motivation: 科学同行评审领域正在快速整合大型语言模型，存在两种趋势：评审员个人使用LLM减轻工作负担（"懒惰评审员"假设）和机构正式部署AI评估系统。本研究旨在探究这些LLM评估系统对对抗性PDF操纵的鲁棒性

Method: 开发了WAVS（加权对抗脆弱性评分）评估指标，收集了200篇科学论文数据集，针对评审任务调整了15种领域特定攻击策略，在包括GPT-5、Claude Haiku和DeepSeek在内的13个语言模型上进行了评估

Result: 研究发现混淆策略如"Maximum Mark Magyk"能成功操纵评分，即使在大型模型中也能实现令人担忧的决策翻转率。攻击策略能够将"拒绝"决定翻转为"接受"

Conclusion: LLM评估系统对对抗性PDF操纵存在显著脆弱性，需要更多研究来加强这些系统的安全性。作者将发布完整数据集和注入框架以促进该领域研究

Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

</details>


### [56] [Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution](https://arxiv.org/abs/2512.10696)
*Zouying Cao,Jiaji Deng,Li Yu,Weikang Zhou,Zhaoyang Liu,Bolin Ding,Hai Zhao*

Main category: cs.AI

TL;DR: ReMe是一个用于LLM智能体经验驱动进化的记忆框架，通过多层面提炼、上下文自适应重用和基于效用的精炼机制，实现从静态存储到动态推理的转变，显著提升智能体性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆框架主要采用"被动积累"范式，将记忆视为静态的只追加档案，无法实现动态推理和有效重用，导致冗余试错和效率低下。

Method: 提出ReMe框架，包含三个创新机制：1) 多层面提炼 - 识别成功模式、分析失败触发因素并生成比较性见解；2) 上下文自适应重用 - 通过场景感知索引将历史见解适配到新情境；3) 基于效用的精炼 - 自主添加有效记忆并修剪过时记忆，保持紧凑高质量的经验池。

Result: 在BFCL-V3和AppWorld上的实验表明，ReMe建立了智能体记忆系统的新SOTA。关键发现：配备ReMe的Qwen3-8B超越了更大的无记忆Qwen3-14B，显示出显著的内存缩放效应，表明自进化记忆为终身学习提供了计算高效的途径。

Conclusion: ReMe框架成功弥合了静态存储与动态推理之间的差距，通过经验驱动的智能体进化机制，为LLM智能体提供了高效、自适应的记忆系统，推动了终身学习的发展。

Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.

</details>


### [57] [Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly](https://arxiv.org/abs/2512.10787)
*Moshe Lahmy,Roi Yozevitch*

Main category: cs.AI

TL;DR: SEAL-RAG提出了一种"替换而非扩展"的策略来解决RAG系统中多跳查询的上下文稀释问题，通过搜索-提取-评估-循环的机制，在固定检索深度下主动替换无关信息，显著提升了答案正确性和证据精度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在处理多跳查询时，当初始检索遗漏桥接事实时容易失败。传统的纠正方法（如Self-RAG、CRAG、Adaptive-k）通常通过添加更多上下文或修剪现有列表来解决问题，但这往往导致"上下文稀释"问题，即无关信息挤占了相关信息的空间。

Method: SEAL-RAG采用训练免费的控制器，执行搜索→提取→评估→循环的周期：1）进行实时的实体锚定提取，构建实时"间隙规范"（缺失的实体/关系）；2）触发有针对性的微查询；3）使用"实体优先排名"主动将无关信息替换为填补间隙的证据，所有操作都在固定检索深度k下完成。

Result: 在HotpotQA（k=3）上，SEAL比Self-RAG在答案正确性上提升3-13个百分点，证据精度提升12-18个百分点。在2WikiMultiHopQA（k=5）上，比Adaptive-k在准确率上提升8.0个百分点，证据精度保持在96%（而CRAG仅为22%）。所有提升都具有统计显著性（p<0.001）。

Conclusion: SEAL-RAG通过强制固定k替换策略，在保持可预测成本配置的同时，确保top-k位置被优化用于精度而非仅仅广度。该方法有效解决了RAG系统中的上下文稀释问题，显著提升了多跳查询的性能。

Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.

</details>


### [58] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: NormCode是一种半正式语言，通过数据隔离和显式输入传递消除多步LLM工作流中的上下文污染问题，实现可审计的AI工作流。


<details>
  <summary>Details</summary>
Motivation: 多步LLM工作流存在上下文污染问题：随着信息在步骤间积累，模型会产生幻觉、混淆中间输出、丢失任务约束。需要一种能够消除跨步骤污染的设计方案。

Method: NormCode是一种半正式语言，用于构建推理计划的结构化分解。它强制分离语义操作（LLM驱动的推理，非确定性）和句法操作（确定性数据重构），支持三种同构格式：.ncds（人工编写）、.ncd（机器执行）、.ncn（人工验证）。

Result: 通过两个演示验证：1）任意长度输入的base X加法算法达到100%准确率；2）NormCode自身五阶段编译器管道的自托管执行。工作编排器提供依赖驱动调度、SQLite支持的检查点和循环管理。

Conclusion: NormCode通过设计消除跨步骤污染，使AI工作流可审计，解决了法律推理、医疗决策和金融分析等高风险领域对透明度的关键需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [59] [Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs](https://arxiv.org/abs/2512.10611)
*Minghao LI,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.AI

TL;DR: Phythesis框架结合大语言模型和物理引导的进化优化，自动化生成数据中心仿真就绪场景，提升能源效率设计


<details>
  <summary>Details</summary>
Motivation: 传统数据中心设计方法依赖人工经验和专业仿真工具，难以应对日益增长的系统复杂性。现有生成式AI方法不考虑底层物理原理，无法满足数据中心可量化运营目标和严格物理约束的设计需求。

Method: 提出Phythesis框架，采用迭代双层优化架构：1) LLM驱动的优化层生成物理合理的三维布局并进行自我批评以优化场景拓扑；2) 物理信息优化层识别最优资产参数并选择最佳资产组合。

Result: 在三种生成规模上的实验表明，与基于纯LLM的解决方案相比，Phythesis实现了57.3%的生成成功率提升和11.5%的电力使用效率(PUE)改进。

Conclusion: Phythesis成功将大语言模型与物理引导优化相结合，为能源高效的数据中心设计提供自动化仿真就绪场景合成解决方案，有效解决了传统方法和现有AI方法的局限性。

Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.

</details>


### [60] [Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification](https://arxiv.org/abs/2512.10640)
*Liang Peng,Haopeng Liu,Yixuan Ye,Cheng Liu,Wenjun Shen,Si Wu,Hau-San Wong*

Main category: cs.AI

TL;DR: scRCL是一个用于单细胞组学研究的无监督细胞类型识别框架，通过结合细胞-基因相互作用来获得更具信息量的表示，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单细胞聚类方法大多只关注细胞内在结构，忽略了细胞-基因关联的关键作用，这限制了它们区分密切相关的细胞类型的能力。

Method: 提出了Refinement Contrastive Learning框架(scRCL)，包含两个对比分布对齐组件来揭示可靠的细胞内在结构，以及一个整合基因相关性结构学习的细化模块来增强细胞嵌入。

Result: 在多个单细胞RNA-seq和空间转录组学基准数据集上的实验表明，该方法在细胞类型识别准确性方面始终优于最先进的基线方法。

Conclusion: scRCL通过显式结合细胞-基因相互作用，能够获得更具生物学意义的细胞表示，下游分析证实恢复的细胞群体表现出连贯的基因表达特征，验证了该方法的生物学相关性。

Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.

</details>


### [61] [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)
*Tong Zhang,Carlos Hinojosa,Bernard Ghanem*

Main category: cs.AI

TL;DR: CAPTAIN是一个无需训练的框架，通过在去噪过程中直接修改潜在特征来减少扩散模型的记忆化问题，同时保持与提示的对齐和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能会无意中复制训练示例，引发隐私和版权问题。现有的推理时缓解方法通常操纵无分类器指导或扰动提示嵌入，但往往难以在不损害与条件提示对齐的情况下减少记忆化。

Method: CAPTAIN采用三阶段方法：1) 基于频率的噪声初始化，减少去噪早期复制记忆模式的倾向；2) 识别特征注入的最佳去噪时间步并定位记忆区域；3) 将非记忆参考图像的语义对齐特征注入到定位的潜在区域中。

Result: 实验表明，CAPTAIN相比基于CFG的基线方法显著减少了记忆化，同时保持了与预期提示的强对齐。

Conclusion: CAPTAIN提供了一个有效的训练免费框架，能够在减少扩散模型记忆化的同时保持提示对齐和视觉质量，解决了现有方法在平衡记忆化减少和提示对齐方面的挑战。

Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.

</details>


### [62] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 研究大型语言模型多智能体系统中价值多样性如何影响集体行为，发现价值多样性增强价值稳定性、促进涌现行为、带来更多创造性原则，但存在边际递减效应


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型的多智能体系统日益普及，这些人工社区的集体行为（如集体智能）受到越来越多的关注。本研究旨在回答一个基本问题：价值多样性如何塑造AI社区的集体行为？

Method: 使用基于施瓦茨基本人类价值理论的自然主义价值引出方法，构建了多智能体模拟，让不同规模的社区参与开放式互动和宪法制定过程

Result: 结果显示价值多样性增强了价值稳定性，促进了涌现行为，并带来了更多由智能体自身开发、无需外部指导的创造性原则。但这些效应也显示出边际递减：极端异质性会导致不稳定性

Conclusion: 本研究将价值多样性定位为未来AI能力的新维度，连接了AI能力与社会学研究中制度涌现的研究

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [63] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: 该研究挑战了传统大语言模型安全评估只关注普遍风险的局限性，提出需要考虑用户特定情境的个体化安全评估方法，特别是在金融和健康等高风险建议领域。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全评估主要关注普遍风险（如危险能力或不良倾向），但数百万用户使用LLM获取金融、健康等高风险个人建议，这些危害是情境依赖而非普遍的。虽然OECD等框架认识到需要评估个体风险，但用户福利安全评估仍然不成熟。

Method: 研究采用探索性方法，评估GPT-5、Claude Sonnet 4和Gemini 2.5 Pro在金融和健康建议上的表现。首先比较了有无用户情境信息的评估者评分差异；其次测试了包含用户报告会披露的情境信息的提示是否能改善评估效果。

Result: 1. 评估者必须访问丰富的用户情境信息：相同LLM回答，了解用户情境的评估者给出的安全评分显著低于不了解情境的评估者（对高脆弱性用户，安全评分从5/7降至3/7）。2. 仅包含用户报告会披露的情境信息并不能显著改善评估效果，特别是对脆弱人群。

Conclusion: 有效的用户福利安全评估需要评估者基于多样化的用户档案来评估回答，仅靠现实用户情境披露是不够的。该研究提供了情境感知评估的方法论起点，证明评估个体福利需要不同于现有普遍风险框架的方法。

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [64] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: 该研究探讨了强化学习（RL）和显式推理（thinking）在胸部X光视觉语言模型中的应用效果，发现RL能提升报告生成和视觉定位性能，但显式推理并未带来额外改进。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型主要依赖监督微调（SFT），但SFT仅优化下一个标记预测而不评估答案质量。强化学习能结合任务特定反馈，且在数学和编程任务中结合显式推理已显示出显著优势，因此研究团队希望探索RL和显式推理在胸部X光VLM中的效果。

Method: 基于Qwen3-VL构建RadVLM模型，首先进行大规模CXR数据的SFT，然后通过冷启动SFT阶段赋予模型基本推理能力。接着应用Group Relative Policy Optimization（GRPO）结合临床基础的任务特定奖励进行报告生成和视觉定位，并在有/无显式推理的领域特定和通用领域Qwen3-VL变体上进行匹配的RL实验。

Result: 研究发现：1）强大的SFT对高基础性能仍然至关重要；2）RL在两个任务上都提供了额外增益；3）显式推理并未进一步改善结果。在统一评估流程下，RL优化的RadVLM模型在报告生成和视觉定位上都超越了基线对应模型，并达到了最先进的性能。

Conclusion: 临床对齐的强化学习是医学视觉语言模型中监督微调的有力补充，能显著提升胸部X光解释的性能，但显式推理在当前设置下并未显示出额外优势。

Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

</details>


### [65] [COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators](https://arxiv.org/abs/2512.10702)
*Wei Fang,Chiyao Wang,Wenshuai Ma,Hui Liu,Jianqiang Hu,Xiaona Niu,Yi Chu,Mingming Zhang,Jingxiao Yang,Dongwei Zhang,Zelin Li,Pengyun Liu,Jiawei Zheng,Pengke Zhang,Chaoshi Qin,Wangang Guo,Bin Wang,Yugang Xue,Wei Zhang,Zikuan Wang,Rui Zhu,Yihui Cao,Quanmao Lu,Rui Meng,Yan Li*

Main category: cs.AI

TL;DR: CA-GPT AI-OCT系统在OCT引导的PCI规划和评估中，显著优于通用ChatGPT-5和初级医师，提供标准化可靠的血管内影像解读方法。


<details>
  <summary>Details</summary>
Motivation: 虽然血管内成像（特别是OCT）改善了PCI结果，但其解读依赖于操作者经验。通用AI虽有潜力但缺乏领域特异性可靠性，需要评估专门针对OCT-PCI的AI模型性能。

Method: 单中心分析96例接受OCT引导PCI的患者，比较CA-GPT、ChatGPT-5和初级医师生成的程序决策与专家记录的一致性，使用10个预设指标评估PCI前和PCI后阶段。

Result: PCI前规划：CA-GPT中位一致性评分（5[3.75-5]）显著高于ChatGPT-5（3[2-4]）和初级医师（4[3-4]）。在支架直径（90.3% vs 72.2%）和长度选择（80.6% vs 52.8%）上优于初级医师。PCI后评估：CA-GPT总体一致性（5[4.75-5]）显著高于ChatGPT-5（4[4-5]）和初级医师（5[4-5]）。亚组分析显示CA-GPT在复杂场景中保持优势。

Conclusion: 基于CA-GPT的AI-OCT系统在PCI规划和评估阶段均优于通用大语言模型和初级医师，为血管内影像解读提供了标准化可靠方法，具有增强操作者专业知识和优化OCT引导PCI的显著潜力。

Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.

</details>


### [66] [HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition](https://arxiv.org/abs/2512.10807)
*Wang Lu,Yao Zhu,Jindong Wang*

Main category: cs.AI

TL;DR: HAROOD：一个用于人类活动识别（HAR）在分布外（OOD）场景下的综合基准测试框架，涵盖4种OOD场景、6个数据集、16种方法，旨在评估现有OOD算法在HAR任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，个体、设备、环境和时间的变化导致相同活动的时间序列传感器数据存在显著分布偏移。现有研究仅在特定分布偏移场景（如跨设备或跨位置）中应用或调整现有OOD算法，缺乏对这些算法有效性的全面洞察。

Method: 提出HAROOD基准测试框架，定义4种OOD场景：跨人员、跨位置、跨数据集和跨时间。构建包含6个数据集、16种比较方法（基于CNN和Transformer架构实现）以及两种模型选择协议的测试平台。

Result: 通过大量实验发现：没有单一方法在所有情况下始终优于其他方法，这突显了该领域仍有巨大的改进空间。代码库高度模块化，易于扩展新数据集、算法和比较分析。

Conclusion: HAROOD为OOD-based HAR研究提供了全面的基准测试框架，揭示了现有方法的局限性，并为进一步研究提供了重要参考。开源代码旨在促进该领域的研究发展。

Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.

</details>


### [67] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 提出Agile Deliberation框架，通过概念界定和迭代两个阶段，支持用户在模糊概念下通过边界案例交互式定义视觉概念分类器


<details>
  <summary>Details</summary>
Motivation: 现有的人机协同方法假设用户已有清晰稳定的概念理解，但现实中用户常从模糊概念开始，需要通过"概念审议"逐步细化。内容审核专家的实践表明需要支持演化、主观概念的框架

Method: 提出Agile Deliberation框架，包含两个阶段：1) 概念界定 - 将初始概念分解为结构化子概念层次；2) 概念迭代 - 展示语义边界案例供用户反思反馈，迭代对齐图像分类器与用户意图

Result: 通过18个用户会话（每个1.5小时）评估，Agile Deliberation比自动分解基线F1分数高7.5%，比手动审议高3%以上，参与者报告概念理解更清晰且认知负担更低

Conclusion: Agile Deliberation有效支持演化、主观概念的视觉分类器构建，通过结构化审议过程帮助用户从模糊概念出发，逐步精确定义自己的概念理解

Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

</details>


### [68] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: V-OCBF：从离线演示中学习神经控制屏障函数，实现无模型的安全控制器合成，避免在线交互和手工设计屏障函数


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法通常只强制执行软期望成本约束，无法保证前向不变性；而控制屏障函数（CBFs）虽然提供严格安全保证，但依赖专家设计的屏障函数或完整的系统动力学知识。需要一种能从离线数据学习安全控制器的方法。

Method: 提出价值引导离线控制屏障函数（V-OCBF）框架：1）从离线演示中学习神经CBF，不假设动力学模型可用；2）推导递归有限差分屏障更新，实现无模型学习；3）采用期望分位数目标，避免在分布外动作上查询屏障；4）将学习到的屏障与二次规划（QP）结合合成实时安全控制。

Result: 在多个案例研究中，V-OCBF相比基线方法显著减少了安全违规，同时保持了强大的任务性能，展示了其在无需在线交互或手工设计屏障的情况下合成安全关键控制器的可扩展性。

Conclusion: V-OCBF框架成功实现了从离线演示中学习神经控制屏障函数，为安全关键系统提供了无需在线交互、无模型、无需手工设计屏障的安全控制器合成方法，在保证任务性能的同时显著提升了安全性。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization](https://arxiv.org/abs/2512.09972)
*Kesheng Chen,Wenjian Luo,Zhenqian Zhu,Yamin Hu,Yiya Xi*

Main category: cs.LG

TL;DR: BAMBO是一个贝叶斯自适应多目标块级优化框架，用于自动构建LLM的帕累托前沿，通过混合最优块划分策略解决维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型合并技术在构建帕累托前沿方面存在不足：粗粒度的模型级方法只能产生稀疏的次优解集，而细粒度的层级方法则面临维度灾难，导致搜索空间计算不可行。

Method: 提出BAMBO框架，采用混合最优块划分策略，将其表述为一维聚类问题，使用动态规划方法平衡块内同质性和块间信息分布，显著降低维度。整个过程在由q-期望超体积改进获取函数驱动的进化循环中自动化进行。

Result: 实验表明，BAMBO能够发现比基线方法更优越、更全面的帕累托前沿，使模型选择能够根据不同的操作约束进行敏捷调整。

Conclusion: BAMBO通过创新的块级优化方法有效解决了LLM能力-效率权衡中的帕累托前沿构建问题，为模型选择提供了灵活高效的解决方案。

Abstract: Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.

</details>


### [70] [Latent Action World Models for Control with Unlabeled Trajectories](https://arxiv.org/abs/2512.10016)
*Marvin Alles,Xingyuan Zhang,Patrick van der Smagt,Philip Becker-Ehmck*

Main category: cs.LG

TL;DR: 本文提出了一种潜在动作世界模型，能够同时利用动作标注和无动作数据来学习，显著减少对标注动作样本的需求，在DeepMind Control Suite上仅需十分之一的标注样本即可达到强大性能。


<details>
  <summary>Details</summary>
Motivation: 受人类结合直接交互和无动作经验（如视频）的启发，研究者希望解决标准世界模型依赖动作标注轨迹的问题。当动作标签稀缺时，传统方法的效率受限，因此需要一种能同时利用动作标注和无动作数据的模型。

Method: 引入潜在动作世界模型家族，通过共享潜在动作表示来联合使用动作标注和无动作数据。该方法学习一个潜在空间，将观察到的控制信号与从被动观察推断的动作对齐，使单个动力学模型能够在大规模无标签轨迹上训练，同时仅需少量动作标注样本。使用潜在动作世界模型通过离线强化学习学习潜在动作策略。

Result: 在DeepMind Control Suite上，该方法仅需约十分之一的动作标注样本就能达到与纯动作标注基线相当的强大性能。这证明了潜在动作能够有效结合被动和交互数据进行训练。

Conclusion: 潜在动作世界模型成功连接了离线强化学习和无动作训练这两个传统上分离的领域，使世界模型能够更高效地学习，显著减少了对动作标注数据的依赖。

Abstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.

</details>


### [71] [Cluster-Dags as Powerful Background Knowledge For Causal Discovery](https://arxiv.org/abs/2512.10032)
*Jan Marco Ruiz de Vargas,Kirtan Padh,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出基于Cluster-DAG先验知识的因果发现方法，通过Cluster-PC和Cluster-FCI算法在完全和部分可观测场景中提升性能


<details>
  <summary>Details</summary>
Motivation: 当前因果发现方法在处理高维数据和复杂依赖关系时面临挑战，而融入系统先验知识可以辅助因果发现。研究者希望利用Cluster-DAG作为先验知识框架来"预热启动"因果发现过程

Method: 提出Cluster-DAG作为先验知识框架，比现有的基于分层背景知识的方法更灵活。在此基础上开发了两种改进的基于约束的算法：Cluster-PC用于完全可观测场景，Cluster-FCI用于部分可观测场景

Result: 在模拟数据上的实证评估表明，Cluster-PC和Cluster-FCI算法在各自场景下都优于没有先验知识的基线方法

Conclusion: Cluster-DAG作为先验知识框架能够有效提升因果发现算法的性能，特别是在处理高维数据和复杂依赖关系时，为因果发现提供了更灵活和有效的解决方案

Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.

</details>


### [72] [Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation](https://arxiv.org/abs/2512.10033)
*Sarwan Ali*

Main category: cs.LG

TL;DR: HB-SGE是一种结合重球动量和预测梯度外推的鲁棒一阶优化方法，在病态和非凸问题上比NAG和标准动量方法更稳定


<details>
  <summary>Details</summary>
Motivation: 传统加速梯度方法（如NAG）在病态或非凸问题上容易发散，需要一种既能保持加速效果又能在复杂优化地形中保持稳定的方法

Method: 结合重球动量和基于局部泰勒近似的预测梯度外推，估计未来梯度方向，提供自适应加速同时保持稳定性

Result: 在病态二次问题（条件数κ=50）上，HB-SGE在119次迭代收敛，而SGD和NAG发散；在非凸Rosenbrock函数上，HB-SGE在2,718次迭代收敛，而经典动量方法在10步内发散

Conclusion: HB-SGE为病态和非凸优化问题提供了鲁棒的替代方案，在保持O(d)内存开销和标准动量相同超参数的同时，比SGD更快且比NAG更稳定

Abstract: Accelerated gradient methods like Nesterov's Accelerated Gradient (NAG) achieve faster convergence on well-conditioned problems but often diverge on ill-conditioned or non-convex landscapes due to aggressive momentum accumulation. We propose Heavy-Ball Synthetic Gradient Extrapolation (HB-SGE), a robust first-order method that combines heavy-ball momentum with predictive gradient extrapolation. Unlike classical momentum methods that accumulate historical gradients, HB-SGE estimates future gradient directions using local Taylor approximations, providing adaptive acceleration while maintaining stability. We prove convergence guarantees for strongly convex functions and demonstrate empirically that HB-SGE prevents divergence on problems where NAG and standard momentum fail. On ill-conditioned quadratics (condition number $κ=50$), HB-SGE converges in 119 iterations while both SGD and NAG diverge. On the non-convex Rosenbrock function, HB-SGE achieves convergence in 2,718 iterations where classical momentum methods diverge within 10 steps. While NAG remains faster on well-conditioned problems, HB-SGE provides a robust alternative with speedup over SGD across diverse landscapes, requiring only $O(d)$ memory overhead and the same hyperparameters as standard momentum.

</details>


### [73] [Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs](https://arxiv.org/abs/2512.10040)
*Skyler Wu,Aymen Echarghaoui*

Main category: cs.LG

TL;DR: 该论文提出了四种新的参考权重设置策略来改进多参考偏好优化(MRPO)，但实验发现单参考DPO通常优于多参考方法


<details>
  <summary>Details</summary>
Motivation: 当前多参考偏好优化(MRPO)方法中的参考权重设置是临时且统计上不合理的，导致性能不可靠，需要更系统的权重设置策略

Method: 提出了四种新的权重设置策略：两种利用验证信号的离线方法；一种使用滑动窗口估计器减少过拟合的在线方法；以及一种将参考权重视为K臂老虎机并通过Thompson Sampling处理的在线方法

Result: 实验显示所有四种新策略在UltraFeedback和SafeRLHF数据集上的偏好准确率都优于当前MRPO权重方法。但更引人深思的是，使用7个参考模型中任意6个的单参考DPO都持续优于所有测试的多参考方法

Conclusion: 虽然提出的新权重策略改进了MRPO性能，但单参考DPO通常优于多参考方法，这质疑了多参考方法在实际应用中的吸引力

Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.

</details>


### [74] [SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation](https://arxiv.org/abs/2512.10042)
*Jongmin Lee,Meiqi Sun,Pieter Abbeel*

Main category: cs.LG

TL;DR: SEMDICE是一种无监督强化学习预训练算法，通过最大化状态熵从任意离策略数据集中学习先验策略，在下游任务中表现出优越的适应效率。


<details>
  <summary>Details</summary>
Motivation: 在无监督强化学习预训练中，智能体需要在不依赖任务特定奖励函数的情况下学习先验策略。状态熵最大化（SEM）是一种有效方法，但现有方法在从离策略数据集中学习SEM策略方面存在局限性。

Method: 提出SEMDICE算法，这是一种基于原理的离策略算法，直接从任意离策略数据集中计算SEM策略。该方法在平稳分布空间中直接优化策略，计算单一、平稳的马尔可夫状态熵最大化策略。

Result: 实验结果表明，SEMDICE在最大化状态熵方面优于基线算法，同时在基于SEM的无监督RL预训练方法中实现了最佳的下游任务适应效率。

Conclusion: SEMDICE提供了一种有效的无监督预训练方法，能够从任意离策略数据集中学习状态熵最大化策略，为下游任务提供了优越的适应能力。

Abstract: In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.

</details>


### [75] [Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition](https://arxiv.org/abs/2512.10043)
*João Lucas Luz Lima Sarcinelli,Diego Furtado Silva*

Main category: cs.LG

TL;DR: 提出一种用于零样本命名实体识别的三阶段集成方法，使用本地运行的大语言模型，在葡萄牙语NER任务上优于单个模型


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中表现出色，但在命名实体识别（尤其是低资源语言如葡萄牙语）方面表现不佳。虽然开源权重模型支持本地部署，但没有单一模型在所有任务上占优，因此需要集成方法。现有的大语言模型集成主要关注文本生成或分类，NER任务研究不足。

Method: 提出一种新颖的三阶段集成流程：1）使用相似能力的本地运行大语言模型；2）通过启发式方法选择最优模型组合；3）利用少量标注数据优化集成效果。该方法不需要对模型进行微调。

Result: 在五个葡萄牙语NER数据集中，该方法在四个数据集上优于单个大语言模型。此外，在不同源数据集上获得的集成模型在跨数据集配置中通常优于单个模型，可能消除对当前任务标注数据的需求。

Conclusion: 该工作通过有效结合多个小型大语言模型而不需要微调，推进了可扩展、低资源和零样本命名实体识别的发展。代码已开源。

Abstract: Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs enable local deployment, no single model dominates all tasks, motivating ensemble approaches. However, existing LLM ensembles focus on text generation or classification, leaving NER under-explored. In this context, this work proposes a novel three-step ensemble pipeline for zero-shot NER using similarly capable, locally run LLMs. Our method outperforms individual LLMs in four out of five Portuguese NER datasets by leveraging a heuristic to select optimal model combinations with minimal annotated data. Moreover, we show that ensembles obtained on different source datasets generally outperform individual LLMs in cross-dataset configurations, potentially eliminating the need for annotated data for the current task. Our work advances scalable, low-resource, and zero-shot NER by effectively combining multiple small LLMs without fine-tuning. Code is available at https://github.com/Joao-Luz/local-llm-ner-ensemble.

</details>


### [76] [Detailed balance in large language model-driven agents](https://arxiv.org/abs/2512.10047)
*Zhuo-Yang Song,Qing-Hong Cao,Ming-xing Luo,Hua Xing Zhu*

Main category: cs.LG

TL;DR: 该论文发现大语言模型驱动的智能体在状态转移中存在详细平衡，表明LLM生成可能不是通过学习规则集和策略，而是通过隐式学习一类超越不同架构和提示模板的潜在函数。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的智能体在解决复杂问题方面取得了经验性成功，但缺乏理解其宏观动力学的理论框架。作者希望建立复杂AI系统的宏观动力学理论，将AI智能体研究从工程实践提升为可预测和可量化的科学。

Method: 基于最小作用原理的方法来估计LLM在智能体中的潜在生成方向性。通过实验测量LLM生成状态之间的转移概率，统计发现LLM生成转换中存在详细平衡。

Result: 发现LLM生成转换中存在详细平衡，这表明LLM生成可能不是通过学习规则集和策略，而是通过隐式学习一类潜在的势函数，这些势函数可能超越不同的LLM架构和提示模板。这是首次发现的LLM生成动力学中不依赖于具体模型细节的宏观物理定律。

Conclusion: 这项工作试图建立复杂AI系统的宏观动力学理论，旨在将AI智能体研究从工程实践集合提升为基于有效测量的科学，这些测量是可预测和可量化的。这一发现为理解LLM驱动的智能体提供了新的理论视角。

Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.

</details>


### [77] [DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting](https://arxiv.org/abs/2512.10051)
*Moulik Gupta,Achyut Mani Tripathi*

Main category: cs.LG

TL;DR: DB2-TransF是一种新型Transformer架构，用可学习的Daubechies小波系数层替代自注意力机制，在时间序列预测中实现高效的多尺度模式捕获，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在时间序列预测中能有效建模长程依赖，但其二次计算复杂度限制了在大规模高维场景下的可扩展性和适应性。需要一种既能保持强大建模能力又能降低计算成本的替代方案。

Method: 提出DB2-TransF架构，核心创新是用可学习的Daubechies小波系数层替代传统的自注意力机制。这个小波基础模块能高效捕获多尺度局部和全局模式，并增强多个时间序列之间的相关性建模。

Result: 在13个标准预测基准测试上的实验表明，DB2-TransF在预测准确性上与传统Transformer相当或更优，同时显著降低了内存使用量，实现了资源高效的时间序列预测。

Conclusion: DB2-TransF为时间序列预测提供了一个可扩展且资源高效的框架，通过小波变换替代自注意力机制，在保持强大建模能力的同时解决了Transformer的计算复杂度问题。

Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF

</details>


### [78] [Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens](https://arxiv.org/abs/2512.10056)
*Alireza Namazi,Amirreza Dolatpour Fathkouhi,Heman Shakeri*

Main category: cs.LG

TL;DR: SoTra方法通过传播连续概率分布来减少暴露偏差，学习校准的、不确定性感知的轨迹，结合风险感知解码模块最小化临床风险，在血糖和血压预测中显著降低临床风险。


<details>
  <summary>Details</summary>
Motivation: 在糖尿病和血流动力学管理的预测控制中，不同操作区域具有不同的临床风险。标准模型使用教师强制训练存在暴露偏差，导致闭环使用时多步预测不稳定。

Method: 提出Soft-Token Trajectory Forecasting (SoTra)方法，传播连续概率分布（"软标记"）来减轻暴露偏差，学习校准的、不确定性感知的轨迹，并加入风险感知解码模块最小化预期临床危害。

Result: 在血糖预测中，SoTra将平均区域风险降低18%；在血压预测中，将有效临床风险降低约15%。

Conclusion: SoTra在安全关键的预测控制中表现出色，能够显著降低临床风险，支持其在医疗预测控制中的应用。

Abstract: Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\%. These improvements support its use in safety-critical predictive control.

</details>


### [79] [MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis](https://arxiv.org/abs/2512.10098)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.LG

TL;DR: MedXAI是一个可解释的医学影像分类框架，通过整合深度学习模型与临床专家知识，提高跨域泛化能力，减少罕见类别偏见，并提供临床可理解的解释。


<details>
  <summary>Details</summary>
Motivation: 医学AI面临三大挑战：1）在域偏移下泛化能力差；2）对罕见病理类别存在偏见；3）缺乏临床部署所需的透明度。现有深度学习模型在真实世界分布变化中表现不佳，且缺乏可解释性。

Method: MedXAI是一个统一的专家知识驱动框架，将深度视觉模型与临床专家知识相结合。它通过定位相关诊断特征来提供人类可理解的解释，而不是依赖技术性的后处理方法（如显著性图、LIME）。框架包含符号组件作为有效的临床先验和正则化器。

Result: 在10个多中心数据集上的实验显示：跨域泛化能力提升3%，罕见类别的F1分数提升10%，显著优于强深度学习基线。消融实验证实符号组件作为有效的临床先验和正则化器，提高了分布偏移下的鲁棒性。

Conclusion: MedXAI在提供临床对齐解释的同时，实现了优越的域内和跨域性能，特别是在多模态医学AI中的罕见疾病诊断方面。该框架解决了医学AI中的泛化、偏见和可解释性三大核心挑战。

Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly un- der domain shifts and rare-class conditions. Deep learning mod- els often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Med- ical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician- derived expert knowledge to improve generalization, reduce rare- class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.

</details>


### [80] [CHyLL: Learning Continuous Neural Representations of Hybrid Systems](https://arxiv.org/abs/2512.10117)
*Sangli Teng,Hang Liu,Jingyu Song,Koushil Sreenath*

Main category: cs.LG

TL;DR: CHyLL提出了一种在潜在空间中学习混合系统连续神经表示的新方法，避免了轨迹分割、事件函数或模式切换，通过将状态空间重构为分段光滑商流形使流变得空间连续。


<details>
  <summary>Details</summary>
Motivation: 学习同时具有连续和离散时间动态的混合系统的流具有挑战性。现有方法学习每个离散模式中的动态，但受到模式切换和流中不连续性的组合影响。

Method: CHyLL的关键见解是重置映射在守卫表面粘合状态空间，将状态空间重构为分段光滑商流形，使流变得空间连续。基于微分拓扑的嵌入定理，CHyLL同时学习高维空间中的无奇点神经嵌入和其中的连续流。

Result: CHyLL能够准确预测混合系统的流，具有优越的准确性，并能识别混合系统的拓扑不变量。最后，该方法成功应用于随机最优控制问题。

Conclusion: CHyLL提供了一种无需轨迹分割、事件函数或模式切换的连续混合系统学习方法，通过将状态空间重构为商流形，在潜在空间中实现了连续流的准确学习和拓扑不变量的识别。

Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.

</details>


### [81] [Partitioning the Sample Space for a More Precise Shannon Entropy Estimation](https://arxiv.org/abs/2512.10133)
*Gabriel F. A. Bastos,Jugurta Montalvão*

Main category: cs.LG

TL;DR: 提出一种新的离散熵估计器，通过结合缺失质量和未见结果数量的估计来补偿小数据集中的负偏差，在欠采样情况下优于经典估计器


<details>
  <summary>Details</summary>
Motivation: 从小数据集中可靠地估计香农熵是一个关键问题，特别是在样本数量可能少于可能结果数量的情况下，这在多个应用中都很重要

Method: 提出一种离散熵估计器，利用可分解性特性，结合缺失质量和未见结果数量的估计来补偿它们引起的负偏差

Result: 实验结果表明，该方法在欠采样情况下优于一些经典估计器，与一些成熟的最先进估计器性能相当

Conclusion: 该方法为小数据集中的熵估计提供了一种有效的解决方案，特别是在样本数量有限的情况下

Abstract: Reliable data-driven estimation of Shannon entropy from small data sets, where the number of examples is potentially smaller than the number of possible outcomes, is a critical matter in several applications. In this paper, we introduce a discrete entropy estimator, where we use the decomposability property in combination with estimations of the missing mass and the number of unseen outcomes to compensate for the negative bias induced by them. Experimental results show that the proposed method outperforms some classical estimators in undersampled regimes, and performs comparably with some well-established state-of-the-art estimators.

</details>


### [82] [Sequence-to-Image Transformation for Sequence Classification Using Rips Complex Construction and Chaos Game Representation](https://arxiv.org/abs/2512.10141)
*Sarwan Ali,Taslim Murad,Imdadullah Khan*

Main category: cs.LG

TL;DR: 该论文提出了一种将分子序列转化为图像的新拓扑方法，结合混沌游戏表示和Rips复形构建，用于分子序列分类，在抗癌肽数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统分子序列分类的特征工程方法存在稀疏性和计算复杂度问题，而深度学习模型在表格化生物数据上表现不佳。需要一种能有效捕捉分子序列结构特征并兼容深度学习架构的新方法。

Method: 结合混沌游戏表示将分子序列元素映射到2D坐标，计算成对距离，构建Rips复形来捕捉局部结构和全局拓扑特征。该方法提供了表示唯一性、拓扑稳定性和信息保留的形式保证。

Result: 在抗癌肽数据集上的广泛实验表明，该方法优于基于向量的方法、序列语言模型和现有的基于图像的方法，在乳腺癌和肺癌数据集上分别达到86.8%和94.5%的准确率。

Conclusion: 拓扑表示保留了关键的序列信息，同时能够有效利用基于视觉的深度学习架构进行分子序列分析，为解决分子序列分类问题提供了新的有效途径。

Abstract: Traditional feature engineering approaches for molecular sequence classification suffer from sparsity issues and computational complexity, while deep learning models often underperform on tabular biological data. This paper introduces a novel topological approach that transforms molecular sequences into images by combining Chaos Game Representation (CGR) with Rips complex construction from algebraic topology. Our method maps sequence elements to 2D coordinates via CGR, computes pairwise distances, and constructs Rips complexes to capture both local structural and global topological features. We provide formal guarantees on representation uniqueness, topological stability, and information preservation. Extensive experiments on anticancer peptide datasets demonstrate superior performance over vector-based, sequence language models, and existing image-based methods, achieving 86.8\% and 94.5\% accuracy on breast and lung cancer datasets, respectively. The topological representation preserves critical sequence information while enabling effective utilization of vision-based deep learning architectures for molecular sequence analysis.

</details>


### [83] [Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences](https://arxiv.org/abs/2512.10147)
*Sarwan Ali,Taslim Murad*

Main category: cs.LG

TL;DR: 提出一种基于哈希的SARS-CoV-2刺突蛋白序列嵌入方法，用于高效的大规模病毒谱系分类，相比现有方法显著提升效率。


<details>
  <summary>Details</summary>
Motivation: COVID-19的早期检测和表征对临床响应和公共卫生规划至关重要。现有方法存在局限性：系统发育树方法计算量大，难以扩展到数百万序列的数据集；现有嵌入方法要么依赖序列比对，要么预测性能不佳且运行成本高，阻碍了大规模分析。

Method: 针对SARS-CoV-2刺突蛋白区域的最常见谱系，提出一种可扩展的嵌入方法，利用哈希技术生成紧凑的低维刺突序列表示。这些嵌入用于训练多种机器学习模型进行监督谱系分类。

Result: 与多种基线方法和最先进的生物序列嵌入方法进行广泛评估。提出的嵌入方法在效率方面有显著改进，达到86.4%的分类准确率，同时将嵌入生成时间减少高达99.81%。

Conclusion: 该方法作为快速、有效且可扩展的大规模病毒序列分析解决方案具有巨大潜力。

Abstract: Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.

</details>


### [84] [CIEGAD: Cluster-Conditioned Interpolative and Extrapolative Framework for Geometry-Aware and Domain-Aligned Data Augmentation](https://arxiv.org/abs/2512.10178)
*Keito Inoshita,Xiaokang Zhou,Akira Kawai,Katsutoshi Yada*

Main category: cs.LG

TL;DR: CIEGAD是一个几何感知和领域对齐的数据增强框架，通过聚类条件、层次频率-几何分配以及插值和外推合成，系统性地补充分布内和分布外的语义未覆盖区域，解决数据稀缺和标签不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 实际深度学习部署中，数据稀缺和标签分布不平衡导致真实数据分布中存在语义未覆盖区域，这阻碍模型训练，导致类边界附近误分类以及边缘区域不稳定行为。虽然大语言模型在数据增强方面有潜力，但尚未建立能够同时实现生成方向控制、领域对齐和质量控制的集成框架。

Method: 提出CIEGAD框架：1) 通过聚类条件构建领域配置文件；2) 使用整合类别频率和几何指标的层次频率-几何分配进行生成分配；3) 通过插值和外推合成的共存精细控制生成方向；4) 通过几何约束过滤结合LLM-as-a-Judge机制进行质量控制。

Result: 在多个分类任务上的实验表明，CIEGAD有效扩展了真实数据分布的边缘，同时保持了生成数据与真实数据的高对齐性以及语义多样性。特别是在长尾和多类分类任务中，CIEGAD持续提高了F1分数和召回率，验证了分布一致性、多样性和质量的三重和谐。

Conclusion: CIEGAD作为一个面向实际应用的数据增强框架，能够补充代表性不足的区域，同时保持与真实数据的对齐，为解决数据稀缺和标签不平衡问题提供了有效的解决方案。

Abstract: In practical deep learning deployment, the scarcity of data and the imbalance of label distributions often lead to semantically uncovered regions within the real-world data distribution, hindering model training and causing misclassification near class boundaries as well as unstable behaviors in peripheral areas. Although recent large language models (LLMs) show promise for data augmentation, an integrated framework that simultaneously achieves directional control of generation, domain alignment, and quality control has not yet been fully established. To address these challenges, we propose a Cluster-conditioned Interpolative and Extrapolative framework for Geometry-Aware and Domain-aligned data augmentation (CIEGAD), which systematically complements both in-distribution and out-of-distribution semantically uncovered regions. CIEGAD constructs domain profiles through cluster conditioning, allocates generation with a hierarchical frequency-geometric allocation integrating class frequency and geometric indicators, and finely controls generation directions via the coexistence of interpolative and extrapolative synthesis. It further performs quality control through geometry-constrained filtering combined with an LLM-as-a-Judge mechanism. Experiments on multiple classification tasks demonstrate that CIEGAD effectively extends the periphery of real-world data distributions while maintaining high alignment between generated and real-world data as well as semantic diversity. In particular, for long-tailed and multi-class classification tasks, CIEGAD consistently improves F1 and recall, validating the triple harmony of distributional consistency, diversity, and quality. These results indicate that CIEGAD serves as a practically oriented data augmentation framework that complements underrepresented regions while preserving alignment with real-world data.

</details>


### [85] [MiniF2F-Dafny: LLM-Guided Mathematical Theorem Proving via Auto-Active Verification](https://arxiv.org/abs/2512.10187)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou*

Main category: cs.LG

TL;DR: 将数学推理基准miniF2F首次翻译到自动化定理证明器Dafny，评估其自动验证能力和LLM辅助证明效果


<details>
  <summary>Details</summary>
Motivation: 将数学推理基准miniF2F从交互式定理证明器扩展到自动化定理证明器Dafny，探索自动化验证与LLM辅助证明的结合

Method: 1) 创建miniF2F-Dafny基准；2) 测试Dafny自动验证能力；3) 对需要人工干预的问题，评估12个现成LLM提供证明提示的效果；4) 采用迭代错误修正策略

Result: Dafny自动验证了测试集40.6%和验证集44.7%的问题（空证明）；最佳LLM在pass@4指标下达到55.7%成功率；展示了LLM提供高层指导与自动化处理底层细节的有效分工

Conclusion: 首次将miniF2F基准扩展到自动化定理证明器，证明了LLM与自动化定理证明器结合的有效性，为数学推理自动化提供了新方向

Abstract: We present miniF2F-Dafny, the first translation of the mathematical reasoning benchmark miniF2F to an automated theorem prover: Dafny. Previously, the benchmark existed only in interactive theorem provers (Lean, Isabelle, HOL Light, Metamath). We find that Dafny's automation verifies 99/244 (40.6%) of the test set and 109/244 (44.7%) of the validation set with empty proofs--requiring no manual proof steps. For problems where empty proofs fail, we evaluate 12 off-the-shelf LLMs on providing proof hints. The best model we test achieves 55.7% pass@4 success rate employing iterative error correction. These preliminary results highlight an effective division of labor: LLMs provide high-level guidance while automation handles low-level details. Our benchmark can be found on GitHub at http://github.com/dafny-lang/miniF2F .

</details>


### [86] [Federated Domain Generalization with Latent Space Inversion](https://arxiv.org/abs/2512.10224)
*Ragja Palakkadavath,Hung Le,Thanh Nguyen-Tang,Svetha Venkatesh,Sunil Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦域泛化方法，通过潜在空间反转技术保护客户端隐私，并使用重要性权重聚合策略处理非独立同分布数据，在减少通信开销的同时实现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦域泛化方法虽然提升了全局模型的泛化能力，但通过共享客户端数据统计信息的方式损害了隐私保护。同时，当客户端数据非独立同分布时，简单的模型聚合可能会丢弃重要的本地适配信息。

Method: 1. 提出潜在空间反转技术，在保护隐私的前提下增强本地模型间的域不变性；2. 设计重要性权重聚合策略，在模型聚合时优先考虑对本地模型预测有显著影响的参数。

Result: 实验表明，该方法在保持较少通信开销的同时，取得了优于现有最先进方法的性能表现。

Conclusion: 该方法有效解决了联邦域泛化中的隐私保护问题和非独立同分布数据聚合难题，为联邦学习中的域泛化提供了更安全高效的解决方案。

Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.

</details>


### [87] [Adaptive Information Routing for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.10229)
*Jun Seo,Hyeokjun Choe,Seohui Bae,Soyeon Park,Wonbin Ahn,Taeyoon Lim,Junhyuk Kang,Sangjun Han,Jaehoon Lee,Dongwan Kang,Minjae Kim,Sungdong Yoo,Soonyoung Lee*

Main category: cs.LG

TL;DR: 本文提出了自适应信息路由（AIR）框架，用于多模态时间序列预测，通过文本信息动态指导时间序列模型，而非将文本数据视为可互换的辅助特征。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测主要依赖历史数据，但在实际场景中，仅凭时间序列数据往往信息有限，难以做出准确预测。多模态方法虽然引入了文本数据等额外模态，但现有方法通常将文本数据与时间序列数据同等对待，未能充分利用文本信息来指导时间序列模型的动态行为。

Method: 提出了自适应信息路由（AIR）框架，该框架利用文本信息动态控制多元时间序列信息的组合方式和程度。同时开发了文本精炼流水线，使用大型语言模型将原始文本数据转换为适合多模态预测的形式，并建立了基于此流水线的基准测试。

Result: 在原油价格和汇率等真实市场数据上的实验结果表明，AIR能够有效利用文本输入调节时间序列模型的行为，在各种时间序列预测任务中显著提高了预测准确性。

Conclusion: AIR框架通过文本信息动态指导时间序列模型，而非简单地将文本作为辅助特征，在多模态时间序列预测中取得了显著效果提升，为解决传统时间序列预测信息有限的问题提供了新思路。

Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.

</details>


### [88] [R^2-HGP: A Double-Regularized Gaussian Process for Heterogeneous Transfer Learning](https://arxiv.org/abs/2512.10258)
*Duo Wang,Xinming Wang,Chao Wang,Xiaowei Yue,Jianguo Wu*

Main category: cs.LG

TL;DR: 本文提出了一种双正则化异构高斯过程框架（R²-HGP），用于解决多源迁移学习中输入空间异构、忽略物理先验知识和负迁移等问题。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程模型在多源迁移学习中面临三大挑战：1）源域和目标域输入空间异构导致直接知识迁移困难；2）异构迁移中忽略物理先验知识，导致映射不稳定；3）源与目标间不当信息共享容易引发负迁移。传统模型无法统一解决这些问题。

Method: 提出R²-HGP框架：1）首先提出可训练的先验概率映射模型对齐异构输入域；2）将对齐后的输入作为隐变量，构建多源迁移GP模型，并集成到条件变分自编码器框架中；3）加入物理知识作为正则项，确保对齐结果符合已知物理规律；4）在多源迁移GP模型中施加稀疏惩罚，自适应选择最有信息量的源输出并抑制负迁移。

Result: 通过大量仿真和真实工程案例验证，R²-HGP在多种评估指标上均优于现有最先进基准方法，证明了其有效性。

Conclusion: R²-HGP框架成功解决了多源迁移学习中的异构输入对齐、物理知识整合和负迁移抑制等关键问题，为复杂工程应用提供了有效的解决方案。

Abstract: Multi-output Gaussian process (MGP) models have attracted significant attention for their flexibility and uncertainty-quantification capabilities, and have been widely adopted in multi-source transfer learning scenarios due to their ability to capture inter-task correlations. However, they still face several challenges in transfer learning. First, the input spaces of the source and target domains are often heterogeneous, which makes direct knowledge transfer difficult. Second, potential prior knowledge and physical information are typically ignored during heterogeneous transfer, hampering the utilization of domain-specific insights and leading to unstable mappings. Third, inappropriate information sharing among target and sources can easily lead to negative transfer. Traditional models fail to address these issues in a unified way. To overcome these limitations, this paper proposes a Double-Regularized Heterogeneous Gaussian Process framework (R^2-HGP). Specifically, a trainable prior probability mapping model is first proposed to align the heterogeneous input domains. The resulting aligned inputs are treated as latent variables, upon which a multi-source transfer GP model is constructed and the entire structure is integrated into a novel conditional variational autoencoder (CVAE) based framework. Physical insights is further incorporated as a regularization term to ensure that the alignment results adhere to known physical knowledge. Next, within the multi-source transfer GP model, a sparsity penalty is imposed on the transfer coefficients, enabling the model to adaptively select the most informative source outputs and suppress negative transfer. Extensive simulations and real-world engineering case studies validate the effectiveness of our R^2-HGP, demonstrating consistent superiority over state-of-the-art benchmarks across diverse evaluation metrics.

</details>


### [89] [An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis](https://arxiv.org/abs/2512.10308)
*Vasiliki Stoumpou,Maciej Tysarowski,Talhat Azemi,Jawad Haider,Howard L. Haronian,Robert C. Hagberg,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 开发了一个可解释的处方框架，为主动脉瓣狭窄患者提供个体化的TAVR或SAVR治疗建议，通过优化5年死亡率来改善临床决策。


<details>
  <summary>Details</summary>
Motivation: 目前临床实践中，对于低至中危严重主动脉瓣狭窄患者的治疗选择（外科手术SAVR vs 经导管TAVR）存在差异，主要受患者异质性和机构偏好影响。现有模型仅能预测术后风险，缺乏能够直接优化长期结果的可解释、个体化治疗推荐。

Method: 引入了一个可解释的处方框架，整合预后匹配、反事实结果建模和最优策略树（OPT）。通过Hartford医院和St. Vincent's医院的数据，使用预后匹配和样本加权模拟随机化，估计患者在SAVR和TAVR两种治疗下的反事实死亡率。策略模型基于这些反事实预测进行训练，将患者划分为临床上一致的亚组，并推荐估计风险较低的治疗方案。

Result: 如果应用OPT推荐的治疗方案，反事实评估显示，与真实临床实践中的治疗方案相比，Hartford医院的5年死亡率估计降低20.3%，St. Vincent's医院降低13.8%。这表明模型在不同机构的未见数据上具有良好的泛化能力。学习到的决策边界与现实世界结果和临床观察一致。

Conclusion: 该可解释的处方框架首次为TAVR与SAVR的选择提供了透明、数据驱动的推荐，在内部和外部队列中都能改善估计的长期结果，同时保持临床合理性，为结构性心脏病精准医学提供了更系统、循证的方法。

Abstract: Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.
  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.
  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\% in Hartford and 13.8\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.
  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.

</details>


### [90] [A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale](https://arxiv.org/abs/2512.10341)
*Vinoth Punniyamoorthy,Ashok Gadi Parthi,Mayilsamy Palanigounder,Ravi Kiran Kodali,Bikesh Kumar,Kabilan Kannan*

Main category: cs.LG

TL;DR: 提出一个云原生隐私保护架构，整合联邦学习、差分隐私、零知识合规证明和强化学习驱动的自适应治理，用于分布式机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习系统需要强大的隐私保证、可验证的合规性以及在异构多云环境中的可扩展部署。

Method: 集成联邦学习、差分隐私、零知识合规证明和强化学习驱动的自适应治理，构建云原生隐私保护架构，支持安全模型训练和推理，无需集中敏感数据。

Result: 在混合Kubernetes集群部署的原型显示减少了成员推理风险，保持一致的正式隐私预算执行，并在差分隐私下保持稳定的模型性能。多机构工作负载实验表明，该架构在最小开销下保持效用，并提供持续的风险感知治理。

Conclusion: 该框架为大规模部署可信赖且合规的分布式机器学习系统建立了实用基础。

Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deploy- ment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero- knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership- inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Ex- perimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The pro- posed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.

</details>


### [91] [Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories](https://arxiv.org/abs/2512.10350)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 本文提出了一种几何框架来分析智能体循环在语义嵌入空间中的行为，将迭代变换视为离散动力系统，揭示了智能体循环的收敛与发散机制。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体系统通过递归反馈循环运行，但人们对这些智能体循环的几何行为（收敛、发散或更复杂的动态）了解甚少。需要一种系统方法来理解和控制这些迭代变换的动态特性。

Method: 引入几何框架分析智能体轨迹在语义嵌入空间中的行为，区分语言变换发生的工件空间和进行几何测量的嵌入空间。提出等张校准方法消除余弦相似度的各向异性偏差，使相似度与人类语义判断对齐并保持高局部稳定性。通过受控实验研究单一智能体循环。

Result: 识别出两种基本机制：收缩性重写循环收敛于稳定吸引子且分散度减小；探索性总结与否定循环产生无界发散且无聚类形成。这些机制显示出收缩和扩张的定性不同几何特征。

Conclusion: 提示设计直接控制智能体循环的动态机制，使得能够系统控制大语言模型迭代变换中的收敛、发散和轨迹结构。这为理解和设计智能体系统的行为提供了理论基础。

Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems.
  We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors.
  Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion.
  Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.

</details>


### [92] [GPG: Generalized Policy Gradient Theorem for Transformer-based Policies](https://arxiv.org/abs/2512.10365)
*Hangyu Mao,Guangting Dong,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出了专门针对Transformer策略的广义策略梯度定理，将标准策略梯度定理和GRPO作为其特例，并探索了在LLM训练中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 针对Transformer架构的策略优化问题，现有策略梯度方法可能不够通用或高效，需要建立更统一的框架来理解和改进基于Transformer的策略学习。

Method: 提出了广义策略梯度定理，专门针对Transformer策略设计，该框架能够包含标准策略梯度定理和GRPO作为特例，并应用于大型语言模型的训练优化。

Result: 成功建立了Transformer策略的广义策略梯度框架，证明了标准策略梯度定理和GRPO都是该框架的特殊情况，为LLM训练提供了新的优化视角。

Conclusion: 广义策略梯度定理为Transformer策略优化提供了统一的理论框架，有助于更高效地训练大型语言模型，并为策略优化方法的发展提供了新的理论基础。

Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.

</details>


### [93] [Fitting magnetization data using continued fraction of straight lines](https://arxiv.org/abs/2512.10390)
*Vijay Prakash S*

Main category: cs.LG

TL;DR: 该论文提出使用直线连分式来近似铁磁物质磁化过程中的非线性函数，用于解释磁畴在外部磁场作用下的生长和收缩行为。


<details>
  <summary>Details</summary>
Motivation: 铁磁物质的磁化过程具有非线性特征，这种非线性源于磁畴在外部磁场作用下的重新排列。传统方法难以准确描述这种复杂的非线性行为，需要一种有效的数学工具来近似和解释磁化过程的非线性特性。

Method: 采用直线连分式（continued fraction of straight lines）作为代数表达式来近似磁化过程的非线性函数。通过非线性回归方法估计模型参数，该表达式能够描述磁畴在外部磁场作用下的对齐和重新排列过程。

Result: 直线连分式方法成功近似了铁磁物质的磁化非线性函数，能够有效解释磁畴在外部磁场作用下的生长和收缩行为。该方法为理解磁化过程的微观机制提供了数学工具。

Conclusion: 直线连分式是一种有效的数学工具，可用于近似和解释铁磁物质磁化过程的非线性行为。该方法不仅能够描述磁畴的对齐过程，还能解释磁畴的生长和收缩机制，为磁化过程的研究提供了新的分析框架。

Abstract: Magnetization of a ferromagnetic substance in response to an externally applied magnetic field increases with the strength of the field. This is because at the microscopic level, magnetic moments in certain regions or domains of the substance increasingly align with the applied field, while the amount of misaligned domains decreases. The alignment of such magnetic domains with an applied magnetic field forms the physical basis for the nonlinearity of magnetization. In this paper, the nonlinear function is approximated as a combination of continued fraction of straight lines. The resulting fit is used to interpret the nonlinear behavior in both growing and shrinking magnetic domains. The continued fraction of straight lines used here is an algebraic expression which can be used to estimate parameters using nonlinear regression.

</details>


### [94] [Metacognitive Sensitivity for Test-Time Dynamic Model Selection](https://arxiv.org/abs/2512.10451)
*Le Tuan Minh Trinh,Le Minh Vu Pham,Thi Minh Anh Pham,An Duc Nguyen*

Main category: cs.LG

TL;DR: 论文提出基于人类元认知的AI元认知评估框架，使用meta-d'指标衡量模型置信度预测自身准确性的可靠性，并基于此进行动态模型选择，提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然能表达预测置信度，但往往存在校准不良的问题，即置信度不能真实反映模型能力。受人类认知科学启发，需要评估AI是否真正"知道自己的知识"，建立元认知评估框架。

Method: 提出meta-d'指标作为元认知敏感性的心理学基础度量，衡量模型置信度预测自身准确性的可靠性。使用动态敏感性分数作为上下文，结合基于多臂老虎机的仲裁器进行测试时模型选择，学习在给定任务中信任哪个专家模型。

Result: 在多个数据集和深度学习模型组合（包括CNN和VLM）上的实验表明，这种元认知方法相比组成模型提高了联合推理准确性。

Conclusion: 该工作为AI模型提供了新颖的行为描述，将集成选择重新定义为评估短期信号（置信度预测分数）和中期特征（元认知敏感性）的问题。

Abstract: A key aspect of human cognition is metacognition - the ability to assess one's own knowledge and judgment reliability. While deep learning models can express confidence in their predictions, they often suffer from poor calibration, a cognitive bias where expressed confidence does not reflect true competence. Do models truly know what they know? Drawing from human cognitive science, we propose a new framework for evaluating and leveraging AI metacognition. We introduce meta-d', a psychologically-grounded measure of metacognitive sensitivity, to characterise how reliably a model's confidence predicts its own accuracy. We then use this dynamic sensitivity score as context for a bandit-based arbiter that performs test-time model selection, learning which of several expert models to trust for a given task. Our experiments across multiple datasets and deep learning model combinations (including CNNs and VLMs) demonstrate that this metacognitive approach improves joint-inference accuracy over constituent models. This work provides a novel behavioural account of AI models, recasting ensemble selection as a problem of evaluating both short-term signals (confidence prediction scores) and medium-term traits (metacognitive sensitivity).

</details>


### [95] [Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2512.10510)
*Chihyeon Song,Jaewoo Lee,Jinkyoo Park*

Main category: cs.LG

TL;DR: 本文提出了一种自适应回放缓冲区（ARB）方法，通过动态调整离线与在线数据的采样优先级来解决离线到在线强化学习中的稳定性与性能平衡问题。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习面临一个关键困境：如何在固定离线数据集和新收集的在线经验之间取得平衡。标准方法通常依赖固定的数据混合比例，难以在早期学习稳定性和渐进性能之间进行有效权衡。

Method: 提出自适应回放缓冲区（ARB），这是一种基于轻量级指标"on-policyness"的动态数据采样优先级方法。ARB评估收集的轨迹与当前策略行为的对齐程度，并为该轨迹中的每个转换分配相应的采样权重。该方法无需复杂学习过程，易于实现，可无缝集成到现有O2O RL算法中。

Result: 在D4RL基准测试上的广泛实验表明，ARB能够持续缓解早期性能下降问题，并显著提高各种O2O RL算法的最终性能。

Conclusion: ARB方法通过自适应、行为感知的回放缓冲区设计，有效解决了离线到在线强化学习中的数据平衡问题，证明了这种设计在提升学习稳定性和最终性能方面的重要性。

Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.

</details>


### [96] [Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees](https://arxiv.org/abs/2512.10522)
*Zahra Rahiminasab,Michael Yuhas,Arvind Easwaran*

Main category: cs.LG

TL;DR: 提出DDE框架，通过师生蒸馏压缩变分自编码器的解耦潜在空间，减少OOD推理器尺寸以适应资源受限设备，同时保持解耦特性


<details>
  <summary>Details</summary>
Motivation: 解耦潜在空间可用于多标签OOD样本推理，但现有模型尺寸较大，难以部署在资源受限设备上，需要压缩模型同时保持解耦特性

Method: 提出解耦蒸馏编码器(DDE)框架，将师生蒸馏形式化为带约束的优化问题，通过解耦约束保持解耦特性，并基于Rademacher复杂度建立理论保证

Result: 在NVIDIA设备上实证评估，压缩模型在保持解耦特性的同时减小了模型尺寸，适合资源受限环境部署

Conclusion: DDE框架能有效压缩解耦推理模型，保持解耦特性，为资源受限设备上的OOD推理提供可行解决方案

Abstract: Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA

</details>


### [97] [Mode-Seeking for Inverse Problems with Diffusion Models](https://arxiv.org/abs/2512.10524)
*Sai Bharath Chandra Gutha,Ricardo Vinuesa,Hossein Azizpour*

Main category: cs.LG

TL;DR: 提出VML-MAP算法，通过变分模式寻求损失引导扩散模型解决逆问题，无需任务特定训练，性能优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练扩散模型的后验采样或MAP估计方法存在建模近似和计算复杂度高的问题，需要更有效的方法来解决逆问题

Method: 提出变分模式寻求损失(VML)，通过最小化扩散后验与测量后验之间的KL散度来引导生成样本趋向MAP估计；对于线性逆问题，VML可解析推导无需近似；基于此提出VML-MAP算法

Result: 在多个数据集上的多样化图像恢复任务中，VML-MAP在性能和计算时间上均优于现有方法

Conclusion: VML-MAP为使用预训练扩散模型解决逆问题提供了一种有效且计算高效的方法，无需任务特定训练或微调

Abstract: A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\mathbf{x}_0|\mathbf{x}_t)$ and the measurement posterior $p(\mathbf{x}_0|\mathbf{y})$, where $\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.

</details>


### [98] [Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders](https://arxiv.org/abs/2512.10547)
*Qingsen Ma,Dianyun Wang,Jiaming Lyu,Yaoye Wang,Lechen Ning,Sujie Zhu,Zhenbo Xu,Liuyu Xiang,Huining Li,Huijia Wu,Zhaofeng He*

Main category: cs.LG

TL;DR: STA-Attention框架使用Top-K稀疏自编码器将KV缓存分解为可解释的语义原子，通过双预算策略选择性保留信息量最大的语义成分，在保持模型性能的同时提升可解释性。


<details>
  <summary>Details</summary>
Motivation: KV缓存是长上下文大语言模型的主要内存瓶颈，但通常被当作不透明的数值张量处理。需要一种方法既能减少内存占用，又能提供对KV缓存的语义解释。

Method: 提出STA-Attention框架，使用Top-K稀疏自编码器分解KV缓存为语义原子；发现键值不对称性（Key稀疏路由，Value密集内容）；引入双预算策略选择性保留信息量最大的语义成分。

Result: 在Yi-6B、Mistral-7B、Qwen2.5-32B等模型上的实验表明，语义重建保持困惑度和零样本性能与原模型相当，有效弥合了机制可解释性与忠实注意力建模之间的差距。

Conclusion: STA-Attention通过语义分解KV缓存，在保持模型性能的同时提供了可解释性，揭示了键值向量的不对称结构，为长上下文模型的内存优化和可解释性提供了新思路。

Abstract: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.

</details>


### [99] [Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning](https://arxiv.org/abs/2512.10573)
*Yi Huang,Qingyun Sun,Yisen Gao,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: LaT-IB是一种抗标签噪声的信息瓶颈方法，通过引入"最小充分干净"准则和噪声感知潜在解耦，在标签噪声场景下实现鲁棒的表示学习。


<details>
  <summary>Details</summary>
Motivation: 传统信息瓶颈方法严重依赖准确标签，在现实场景中标签噪声普遍存在时容易导致性能下降和过拟合，需要开发抗标签噪声的表示学习方法。

Method: 提出LaT-IB方法，引入"最小充分干净"准则作为互信息正则化器，采用噪声感知潜在解耦将潜在表示分解为干净标签空间和噪声空间对齐的组件，并设计三阶段训练框架：预热、知识注入和鲁棒训练。

Result: 理论推导了各组件互信息边界，证明优化能鼓励对输入噪声不变的表示并分离干净和噪声标签信息。实验表明LaT-IB在标签噪声下实现了优越的鲁棒性和效率。

Conclusion: LaT-IB有效解决了标准信息瓶颈对标签噪声的脆弱性，显著增强了在现实标签噪声场景下的鲁棒性和适用性。

Abstract: The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a "Minimal-Sufficient-Clean" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.

</details>


### [100] [Multi-Objective Reward and Preference Optimization: Theory and Algorithms](https://arxiv.org/abs/2512.10601)
*Akhil Agnihotri*

Main category: cs.LG

TL;DR: 该论文在约束强化学习领域提出了一系列理论框架和算法，涵盖平均成本CMDPs、有限时域设置、人类偏好学习以及大语言模型对齐，提供了理论保证和实际应用工具。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理安全约束、人类偏好和模型对齐方面存在不足，需要开发能够同时满足性能要求和约束条件的算法框架，特别是在安全关键环境和大型语言模型对齐中。

Method: 1. ACPO算法：将敏感性分析与信任域更新结合处理平均成本CMDPs；2. e-COP算法：基于episodic策略差异引理处理有限时域CMDPs；3. warmPref-PS：在线性bandits中集成异构评分者的离线偏好数据；4. PSPL算法：从成对轨迹比较中联合采样奖励模型和转移动态；5. MOPO算法：多目标约束优化方法，适用于数十亿参数的语言模型对齐。

Result: ACPO在平均成本CMDPs中实现了最先进的实证性能；e-COP在安全关键环境中提供了可证明的性能和可扩展性；warmPref-PS显著减少了遗憾并提高了RLHF的数据收集效率；PSPL提供了贝叶斯简单遗憾保证并稳健识别最优策略；MOPO能够扩展到数十亿参数模型并在各种对齐设置中保持鲁棒性。

Conclusion: 该论文统一了平均成本、episodic和偏好驱动的约束强化学习范式，为安全和对齐的决策制定提供了理论进展和实用工具，推动了约束RL在控制、偏好学习和大型语言模型对齐等多个领域的应用。

Abstract: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.

</details>


### [101] [Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach](https://arxiv.org/abs/2512.10633)
*C. Bosco,U. Minora,D. de Rigo,J. Pingsdorf,R. Cortinovis*

Main category: cs.LG

TL;DR: 提出一种混合方法预测欧洲非法边境过境，结合机器学习与专家定性洞察，为欧盟移民政策提供一年期预测支持


<details>
  <summary>Details</summary>
Motivation: 应对移民模式突然变化和传统数据局限性的挑战，满足欧盟《移民与庇护公约》的预测需求，支持庇护与移民管理法规

Method: 混合方法整合机器学习技术与移民专家的定性洞察，引入人工评估协变量，测试验证已知数据

Result: 开发出适用于欧盟移民治理的新型操作工具，提供政策相关预测，支持战略决策、预警系统和成员国团结机制

Conclusion: 通过数据驱动建模与专家判断相结合，提高了预测能力，为欧盟移民政策制定提供了可靠的应用工具

Abstract: This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.

</details>


### [102] [Token Sample Complexity of Attention](https://arxiv.org/abs/2512.10656)
*Léa Bohbot,Cyril Letrouit,Gabriel Peyré,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型中注意力机制在极长序列下的收敛行为，提出了token-sample复杂度的概念，分析了注意力映射在不同条件下的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的上下文窗口不断扩大，需要研究注意力机制在极长序列长度下的行为特征，特别是注意力如何随着token数量增加而收敛到其无限token极限。

Method: 引入token-sample复杂度概念，从两个层面分析收敛性：1) 注意力映射的点态一致收敛；2) 变换后token分布的矩收敛。针对紧支撑分布和亚高斯分布，推导了收敛速率界限，并研究了注意力参数趋于无穷大时softmax接近hardmax的情况。

Result: 对于紧支撑分布，注意力映射在半径为R的球上以C(R)/√n的速率一致收敛，其中C(R)随R指数增长。对于变换分布的矩收敛，速率为C'(R)/n^β，其中β<1/2，C'(R)与分布支撑大小呈多项式关系。在注意力参数趋于无穷时，获得了对数收敛速率。实验在合成高斯数据和真实BERT模型上验证了理论预测。

Conclusion: 该研究为理解大型语言模型中注意力机制在长序列下的行为提供了理论框架，揭示了注意力收敛的复杂特性，对模型设计和优化具有指导意义。

Abstract: As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^β$ with $β<\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $β$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.

</details>


### [103] [DCFO Additional Material](https://arxiv.org/abs/2512.10659)
*Tommaso Amico,Pernille Matthews,Lena Krieger,Arthur Zimek,Ira Assent*

Main category: cs.LG

TL;DR: 提出DCFO方法，为LOF异常检测算法生成反事实解释，通过数据空间分区实现梯度优化，在50个数据集上验证优于基准方法


<details>
  <summary>Details</summary>
Motivation: 异常检测需要解释性来理解异常原因、验证显著性、识别偏差。现有反事实解释方法忽视异常检测的特殊挑战，且不针对经典算法如LOF。LOF作为最流行的无监督异常检测方法之一，缺乏可解释性

Method: 提出DCFO方法，专门为LOF生成反事实解释。通过将数据空间分区为LOF行为平滑的区域，实现高效的基于梯度的优化

Result: 在50个OpenML数据集上进行广泛实验验证，DCFO一致优于基准竞争对手，在生成的反事实的接近度和有效性方面表现更优

Conclusion: DCFO填补了LOF异常检测算法缺乏可解释性的空白，通过专门设计的反事实解释方法，为理解异常提供了有效的工具

Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.

</details>


### [104] [Learning by Analogy: A Causal Framework for Composition Generalization](https://arxiv.org/abs/2512.10669)
*Lingjing Kong,Shaoan Xie,Yang Jiao,Yetian Chen,Yanhui Guo,Simone Shao,Yan Gao,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出基于因果模块化和最小变化原则的形式化框架，通过分层数据生成过程实现组合泛化，证明了潜在分层结构的可识别性，并在基准数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 组合泛化能力使模型能够超越有限经验进行理解和生成，但目前支撑这一关键能力的数据结构和原理仍不清楚。论文旨在形式化人类通过类比进行概念重组的直观过程，建立理论框架来解释组合泛化。

Method: 引入基于因果模块化和最小变化原则的分层数据生成过程，将高层概念分解为可重组的低层概念。理论证明了潜在分层结构可以从文本-图像对等可观测数据中可识别地恢复。

Result: 理论框架支持复杂概念关系的组合泛化，超越了先前假设简单交互（如加性效应）的工作。在基准数据集上应用理论见解取得了显著改进。

Conclusion: 组合泛化需要将高层概念分解为可跨相似语境重组的低层概念，通过因果模块化和分层生成过程的形式化框架，能够实现并理论保证复杂概念关系的组合泛化能力。

Abstract: Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.
  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.

</details>


### [105] [HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification](https://arxiv.org/abs/2512.10701)
*Mostafa Anoosha,Zeinab Dehghani,Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker*

Main category: cs.LG

TL;DR: HybridVFL框架通过客户端特征解耦和服务器端跨模态Transformer融合，在边缘AI场景中显著提升垂直联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 标准垂直联邦学习(VFL)在边缘AI场景（如移动健康诊断）中因简单的特征融合而存在性能限制，需要更先进的融合机制来提升隐私保护系统的鲁棒性

Method: 提出HybridVFL框架，包含客户端特征解耦和服务器端跨模态Transformer，实现上下文感知的特征融合

Result: 在多模态HAM10000皮肤病变数据集上的系统评估显示，HybridVFL显著优于标准联邦学习基线方法

Conclusion: 先进的融合机制对于构建鲁棒、隐私保护的边缘AI系统至关重要，HybridVFL为垂直联邦学习提供了有效的解决方案

Abstract: Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.

</details>


### [106] [Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality](https://arxiv.org/abs/2512.10720)
*Lingjing Kong,Shaoan Xie,Guangyi Chen,Yuewen Sun,Xiangchen Song,Eric P. Xing,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出基于因果最小化原则的理论框架，为深度生成模型提供可解释性基础，通过稀疏性约束使学习到的表示等价于数据生成过程的真实潜在变量，从而实现对扩散视觉模型和自回归语言模型的因果解释和细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型虽然革命性地改变了图像和文本生成领域，但通常作为不透明的黑盒运行，阻碍了人类的理解、控制和对齐。现有的方法如稀疏自编码器虽然经验上成功，但缺乏理论保证，可能导致主观见解。主要目标是建立可解释生成模型的原则性基础。

Method: 引入基于因果最小化原则的理论框架——偏好最简单的因果解释。提出分层选择模型，其中高层概念由低层变量的受限组合产生，更好地捕捉数据生成中的复杂依赖关系。在理论上推导的最小化条件（表现为稀疏性或压缩约束）下，学习到的表示可以等价于数据生成过程的真实潜在变量。

Result: 将因果最小化约束应用于领先的生成模型，能够提取其固有的分层概念图，为模型内部知识组织提供新见解。这些基于因果的概念可作为杠杆实现细粒度的模型控制，为透明可靠的系统铺平道路。

Conclusion: 因果最小化原则能够为扩散视觉模型和自回归语言模型的潜在表示提供清晰的因果解释和鲁棒的组件级可识别控制。该理论框架为可解释生成模型建立了原则性基础，支持透明、可靠的系统开发。

Abstract: Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.

</details>


### [107] [Generalized Spherical Neural Operators: Green's Function Formulation](https://arxiv.org/abs/2512.10723)
*Hao Tang,Hao Chen,Chao Li*

Main category: cs.LG

TL;DR: 提出基于可设计球面格林函数及其谐波展开的通用算子设计框架，建立球面学习的算子理论基础，并开发GSNO算子和GSHNet架构，在多个实际应用中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经算子在求解参数偏微分方程方面具有强大能力，但扩展到球面域面临挑战，需要保持内在几何特性同时避免破坏旋转一致性的失真。现有球面算子依赖旋转等变性但缺乏处理现实世界复杂性的灵活性。

Method: 提出基于可设计球面格林函数及其谐波展开的通用算子设计框架，建立球面学习的算子理论基础。基于此提出绝对和相对位置依赖的格林函数，实现等变性和不变性的灵活平衡。开发GSNO算子和新颖的谱学习方法，适应各向异性、约束丰富的系统同时保持谱效率。构建GSHNet分层架构，结合多尺度谱建模与球面上-下采样，增强全局特征表示。

Result: 在扩散MRI、浅水动力学和全球天气预报等任务上评估，GSNO和GSHNet始终优于最先进方法。

Conclusion: GSNO为球面算子学习提供了一个有原则且通用的框架，将严格理论与现实世界复杂性连接起来。

Abstract: Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.

</details>


### [108] [Interpretable and Steerable Concept Bottleneck Sparse Autoencoders](https://arxiv.org/abs/2512.10805)
*Akshay Kulkarni,Tsui-Wei Weng,Vivek Narayanaswamy,Shusen Liu,Wesam A. Sakla,Kowshik Thopalli*

Main category: cs.LG

TL;DR: 论文提出CB-SAE框架，通过剪枝低效神经元和添加轻量级概念瓶颈层，显著提升稀疏自编码器的可解释性和可操控性


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAE）存在两个主要问题：1）大多数神经元要么可解释性低，要么可操控性低，要么两者都低，导致下游应用效果不佳；2）由于SAE的无监督特性，用户期望的概念往往不在学习到的字典中，限制了实际应用价值

Method: 提出概念瓶颈稀疏自编码器（CB-SAE）框架：1）剪枝低效用神经元；2）在潜在空间中添加轻量级概念瓶颈层，与用户定义的概念集对齐

Result: CB-SAE在大型视觉语言模型和图像生成任务中，可解释性提升32.1%，可操控性提升14.5%

Conclusion: CB-SAE通过结合剪枝和概念瓶颈增强，有效解决了SAE在可解释性和可操控性方面的局限性，为机制可解释性、概念发现和模型操控提供了更实用的解决方案

Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.

</details>


### [109] [Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments](https://arxiv.org/abs/2512.10835)
*Atahan Cilan,Atay Özgövde*

Main category: cs.LG

TL;DR: 提出强化学习框架，无需人类游戏数据即可生成可控多样的玩家行为，通过行为向量距离奖励实现平滑控制


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量玩家轨迹数据、需为不同玩家类型训练单独模型，或缺乏可解释行为参数与学习策略的直接映射，限制了可扩展性和可控性

Method: 在N维连续空间定义玩家行为，从包含真实人类风格子集的区域均匀采样目标行为向量；训练时每个智能体接收当前和目标行为向量作为输入，奖励基于两者距离的归一化减少；使用PPO-based多智能体策略

Result: 在自定义多玩家Unity游戏中，相比仅以胜利为目标的基线，该方法产生显著更大的行为多样性，并能可靠匹配不同目标行为向量；单个策略可重现新或未见过的游戏风格而无需重新训练

Conclusion: 该方法为自动化游戏测试、游戏平衡、人类行为模拟和在线游戏中替换断线玩家提供了可扩展解决方案

Abstract: This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.

</details>


### [110] [Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants](https://arxiv.org/abs/2512.10857)
*Chirag Modi,Jiequn Han,Eric Vanden-Eijnden,Joan Bruna*

Main category: cs.LG

TL;DR: 提出一种基于随机插值的自洽生成模型方法，用于从噪声观测数据中学习干净数据的分布，通过迭代更新传输映射来反演数据退化过程。


<details>
  <summary>Details</summary>
Motivation: 在许多科学和工程领域中，我们通常只能获得经过噪声和病态信道退化的观测数据，而无法直接获取干净数据。现有的传输方法主要针对干净数据集，无法直接处理这种需要从退化数据中学习原始数据分布的反问题。

Method: 提出自洽随机插值（SCSI）方法：通过迭代更新退化数据与干净数据之间的传输映射，仅使用退化数据集和对退化信道的黑盒访问。该方法在适当条件下收敛到自洽的传输映射，有效反演退化信道。

Result: SCSI方法在计算效率上优于变分替代方法，具有高度灵活性（可处理任意非线性前向模型），并享有理论保证。在自然图像处理和科学重建的反问题中表现出优越性能。

Conclusion: SCSI方法为从退化观测数据中构建干净数据的生成模型提供了一种高效、灵活且理论可靠的新框架，在科学和工程应用中具有重要价值。

Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.

</details>


### [111] [Scaling Behavior of Discrete Diffusion Language Models](https://arxiv.org/abs/2512.10858)
*Dimitri von Rütte,Janis Fluri,Omead Pooladzandi,Bernhard Schölkopf,Thomas Hofmann,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文研究了离散扩散语言模型（DLMs）的缩放规律，发现其缩放行为与噪声类型密切相关，且与自回归语言模型（ALMs）显著不同。均匀扩散在计算效率训练中需要更多参数但更少数据，适合数据受限场景。


<details>
  <summary>Details</summary>
Motivation: 现代LLM预训练消耗大量计算资源和训练数据，不同模型的缩放规律成为关键区分因素。离散扩散语言模型作为自回归语言模型的替代方案，其缩放行为尚未得到充分探索，先前研究表明DLMs需要更多数据和计算才能达到ALMs的性能水平。

Method: 通过平滑插值掩码扩散和均匀扩散来研究不同噪声类型下DLMs的缩放行为，同时密切关注批量大小和学习率等关键超参数。实验规模扩展到100亿参数，训练计算量达10^22 FLOPs。

Result: DLMs的缩放行为强烈依赖于噪声类型，与ALMs显著不同。所有噪声类型在计算受限的缩放中收敛到相似的损失值，但均匀扩散在计算效率训练中需要更多参数和更少数据，而掩码扩散则相反。成功将均匀扩散模型扩展到100亿参数，验证了预测的缩放规律。

Conclusion: 均匀扩散模型在数据受限场景下具有优势，需要更多参数但更少数据，使其成为计算效率训练的有前景候选方案。研究证实了DLMs缩放行为对噪声类型的依赖性，并建立了迄今为止最大的公开均匀扩散模型。

Abstract: Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.
  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.

</details>
