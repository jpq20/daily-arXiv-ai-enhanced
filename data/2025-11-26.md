<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [cs.LG](#cs.LG) [Total: 81]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search](https://arxiv.org/abs/2511.19648)
*Manil Shrestha,Edward Kim*

Main category: cs.CL

TL;DR: 提出了两种混合算法来解决知识图谱多跳问答中的效率和可验证性问题：LLM引导规划使用单次LLM调用预测关系序列，嵌入引导神经搜索完全消除LLM调用，通过轻量级边评分器融合文本和图嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱多跳问答中的组合爆炸问题，减少对昂贵LLM推理的依赖，确保答案在结构化知识中的可验证性。

Method: 1) LLM引导规划：单次LLM调用预测关系序列，通过广度优先搜索执行；2) 嵌入引导神经搜索：使用6.7M参数边评分器融合文本和图嵌入，完全消除LLM调用；通过知识蒸馏将规划能力压缩到4B参数模型。

Result: LLM引导规划达到接近完美的准确率（micro-F1 > 0.90），嵌入引导神经搜索实现100倍以上加速且保持竞争性准确率。在MetaQA上的评估显示，基于结构的规划比直接答案生成更具可迁移性。

Conclusion: 可验证的多跳推理不需要在推理时使用大规模模型，而是需要结合符号结构和学习表示的适当架构归纳偏差。

Abstract: Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.

</details>


### [2] [Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian](https://arxiv.org/abs/2511.19719)
*Mobina Mehrazar,Mohammad Amin Yousefi,Parisa Abolfath Beygi,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本研究评估了波斯语情感分类任务中LLM生成解释的忠实性，发现尽管模型分类性能良好，但其解释与人类判断存在显著差异，提示当前解释方法和指标存在局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地生成自解释，特别是在低资源语言中，这些解释的忠实性引发担忧。本研究旨在评估波斯语情感分类中LLM生成解释的忠实性。

Method: 通过比较模型识别的影响词与人类标注者的识别结果，使用基于token级对数概率的置信度分数评估忠实性，并测试两种提示策略（预测-解释和解释-预测）对解释忠实性的影响。

Result: LLM在分类性能上表现强劲，但生成的解释往往偏离忠实推理，模型间的解释一致性高于与人类判断的一致性。

Conclusion: 当前解释方法和指标存在局限性，需要更稳健的方法来确保LLM在多语言和低资源环境中的可靠性。

Abstract: Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.

</details>


### [3] [Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation](https://arxiv.org/abs/2511.19739)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 本研究评估了10种基于transformer的嵌入模型在心脏病学领域的表现，发现编码器架构（特别是BioLinkBERT）在领域特定性能上优于更大的解码器模型，且计算资源需求更少。


<details>
  <summary>Details</summary>
Motivation: 领域特定文本嵌入对临床自然语言处理至关重要，但不同模型架构之间的系统比较仍然有限。

Method: 通过低秩适应（LoRA）微调方法，在106,535个心脏病学文本对（来自权威医学教材）上适配了10种transformer嵌入模型。

Result: 编码器架构（特别是BioLinkBERT）获得最佳领域特定性能（分离分数：0.510），优于更大的解码器模型，且计算资源需求显著更少。

Conclusion: 研究挑战了更大语言模型必然产生更好领域特定嵌入的假设，为临床NLP系统开发提供了实用指导，所有模型和数据集已公开以支持可重复研究。

Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.

</details>


### [4] [What does it mean to understand language?](https://arxiv.org/abs/2511.19757)
*Colton Casto,Anna Ivanova,Evelina Fedorenko,Nancy Kanwisher*

Main category: cs.CL

TL;DR: 该论文提出语言理解需要将信息从核心语言系统导出到其他脑区，以构建丰富的心理模型，并回顾了支持这一假说的证据。


<details>
  <summary>Details</summary>
Motivation: 探讨语言理解的深层机制，超越表面意义提取，研究大脑如何构建情境的心理模型。

Method: 回顾现有证据，利用认知神经科学的最新进展作为概念基础和方法来直接检验假说。

Result: 提出了语言理解需要信息从语言系统导出到其他脑区的假说，并提供了支持证据。

Conclusion: 认知神经科学的进展为揭示语言理解的认知和神经机制提供了新策略，开启了研究语言深层理解的新途径。

Abstract: Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.

</details>


### [5] [Gender Bias in Emotion Recognition by Large Language Models](https://arxiv.org/abs/2511.19785)
*Maureen Herbert,Katie Sun,Angelica Lim,Yasaman Etesam*

Main category: cs.CL

TL;DR: 本文研究大型语言模型在情感心理理论中的性别偏见问题，并评估了多种去偏策略，发现基于训练的方法比仅靠提示工程更有效。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展和日常应用日益增多，评估和确保其公平性变得至关重要，特别是在情感心理理论领域是否存在性别偏见。

Method: 通过向模型描述人物及其环境后询问"这个人感觉如何？"来评估性别偏见，并提出并评估了多种去偏策略，包括基于训练的方法和推理时的提示工程方法。

Result: 研究表明，实现有意义的偏见减少需要基于训练的方法，而不仅仅依赖推理时的提示工程方法。

Conclusion: 在大型语言模型的公平性评估中，基于训练的去偏策略比仅靠提示工程更有效，这对确保模型公平性具有重要意义。

Abstract: The rapid advancement of large language models (LLMs) and their growing integration into daily life underscore the importance of evaluating and ensuring their fairness. In this work, we examine fairness within the domain of emotional theory of mind, investigating whether LLMs exhibit gender biases when presented with a description of a person and their environment and asked, "How does this person feel?". Furthermore, we propose and evaluate several debiasing strategies, demonstrating that achieving meaningful reductions in bias requires training based interventions rather than relying solely on inference-time prompt-based approaches such as prompt engineering.

</details>


### [6] [Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions](https://arxiv.org/abs/2511.19816)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 本文介绍了NRC VAD Lexicon v2，这是一个包含10,000个英语多词表达和25,000个单词的情感关联评分词典，扩展了2018年发布的v1版本。


<details>
  <summary>Details</summary>
Motivation: 现有词典如2018年发布的NRC VAD Lexicon仅包含单词的情感关联评分，缺乏对多词表达的情感分析，且覆盖范围有限。

Method: 通过人工标注的方式，为多词表达及其组成词提供效价、唤醒度和支配度的情感评分，并增加2018年以来更常见的单词覆盖。

Result: 新词典包含10,000个多词表达和25,000个单词的情感关联评分，数据可靠性高，可用于分析多词表达的情感特征和情感组合性。

Conclusion: NRC VAD Lexicon v2为NLP、心理学、公共卫生、数字人文和社会科学等领域的研究提供了丰富的情感分析资源。

Abstract: Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html

</details>


### [7] [Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana](https://arxiv.org/abs/2511.19818)
*Koena Ronny Mabokela,Tim Schlippe,Mpho Raborife,Turgay Celik*

Main category: cs.CL

TL;DR: 本文提出了一种利用表情符号和情感词汇的自动语言无关情感标注方法，在英语、Sepedi和Setswana三种语言的推文上进行测试，平均只需修正34%的自动生成标签。


<details>
  <summary>Details</summary>
Motivation: 非洲语言由于缺乏数字语言资源而被归类为低资源语言，手动标注文本数据耗时且昂贵，需要自动化的快速流程来减少人工工作量。

Method: 利用表情符号和情感词汇的自动语言无关情感标注方法，在SAfriSenti多语言情感语料库中的英语、Sepedi和Setswana推文上进行实验。

Result: 英语推文标注准确率为66%，Sepedi为69%，Setswana为63%，平均只需修正34%的自动生成标签。

Conclusion: 该方法能够有效减少情感标注的人工工作量，为低资源语言的情感分析提供了可行的自动化解决方案。

Abstract: Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.

</details>


### [8] [Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs](https://arxiv.org/abs/2511.19852)
*Shi-Wei Dai,Yan-Wei Shie,Tsung-Huan Yang,Lun-Wei Ku,Yung-Hui Li*

Main category: cs.CL

TL;DR: 提出了PersonaPulse框架，通过迭代优化角色扮演提示词来增强LLM的个性表达能力，利用情境响应基准作为评分工具，实现更真实的个性表达。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然探索了使用提示词来激发LLM的特定个性特征，但未能优化这些提示词以最大化个性表达。

Method: PersonaPulse框架利用LLM对个性特征的固有知识，迭代增强角色扮演提示词，同时整合情境响应基准作为评分工具，确保更真实和情境化的评估来指导优化过程。

Result: 定量评估显示，PersonaPulse生成的提示词优于基于心理学研究设计的先前工作；探索了模型大小与个性建模的关系；发现某些个性特征的激发程度可以通过暂停优化过程来部分控制。

Conclusion: 这些发现强调了提示词优化在塑造LLM个性表达中的重要性，为未来自适应AI交互研究提供了宝贵见解。

Abstract: Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.

</details>


### [9] [A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction](https://arxiv.org/abs/2511.19858)
*Farzad Ahmed,Joniel Augustine Jerome,Meliha Yetisgen,Özlem Uzuner*

Main category: cs.CL

TL;DR: 评估三种提示策略（零样本提示、静态随机示例提示、检索增强动态提示）在医疗错误处理任务中的表现，发现检索增强动态提示在多个指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 临床文档中存在可能影响患者安全的事实性、诊断性和管理性错误，大型语言模型可能帮助检测和纠正这些错误，但不同提示策略下的表现尚不明确。

Method: 使用MEDEC数据集评估9个指令调优的大型语言模型，采用零样本提示、静态随机示例提示和检索增强动态提示三种策略，测量准确性、召回率、假阳性率等指标。

Result: 检索增强动态提示在所有9个模型中均表现最佳，将假阳性率降低约15%，在错误句子检测中召回率提高5-10%，并生成更准确的纠正内容。

Conclusion: 检索增强动态提示优于零样本和静态随机示例提示，使用检索示例可提高检测准确性、减少假阳性并增强医疗错误纠正的可靠性。

Abstract: Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.

</details>


### [10] [$\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers](https://arxiv.org/abs/2511.19987)
*Xinyu Wang,Hanwei Wu,Qingchen Hu,Zhenghan Tai,Jingrui Tian,Lei Ding,Jijun Chi,Hailin He,Tung Sum Thomas Kwok,Yufei Cui,Sicheng Lyu,Muzhi Li,Mingze Li,Xinyue Yu,Ling Zhou,Peng Lu*

Main category: cs.CL

TL;DR: R2R是一个领域感知的重新排序框架，通过动态专家路由和两阶段训练策略解决领域专业化问题，在金融、法律等高风险领域超越通用模型和单领域微调基线。


<details>
  <summary>Details</summary>
Motivation: 通用模型在高风险领域（如金融、法律）中缺乏领域特定知识，而简单微调会导致表面形式过拟合和灾难性遗忘。

Method: 采用动态专家路由和两阶段训练策略（EAG），通过掩码最具预测性的表面线索，强制重新排序器学习领域不变的相关性模式。使用轻量级潜在语义路由器选择最优LoRA专家。

Result: 在多个重新排序器骨干和不同领域（法律、医疗、金融）上的广泛实验表明，R2R始终优于通用模型和单领域微调基线。

Conclusion: R2R是一种模型无关且模块化的领域专业化方法，具有强大的跨领域鲁棒性。

Abstract: Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.

</details>


### [11] [Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test](https://arxiv.org/abs/2511.19997)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文通过合成基准测试发现，因果Transformer在方向性学习上存在固有优化差距，即使在没有语言先验的随机字符串映射任务中，正向任务表现也明显优于逆向任务。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer架构中方向性失败的根本原因问题：是源于语言统计特性还是架构本身的内在限制。

Method: 使用完全合成的、熵控制的基准测试，构建具有可调分支因子K的随机字符串映射任务，包括零条件熵的正向任务和具有分析确定熵底线的逆向任务。

Result: 即使是从头训练的GPT-2模型也表现出强烈且可复现的方向性优化差距（如K=5时1.16 nats），远大于在相同数据上训练的MLP。预训练初始化改变了优化行为但未消除此差距，而LoRA在高熵逆向映射上遇到尖锐的能力瓶颈。

Conclusion: 研究分离出了因果Transformer训练中固有的方向性摩擦的最小、无语义特征，这种特征在移除语言先验、词频和语料级时间不对称性后仍然存在。

Abstract: Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a "reversal curse," and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.

</details>


### [12] [Online-PVLM: Advancing Personalized VLMs with Online Concept Learning](https://arxiv.org/abs/2511.20056)
*Huiyu Bai,Runze Wang,Zhuoyun Du,Yiyang Zhao,Fengji Zhang,Haoyu Chen,Xiaoyong Zhu,Bo Zheng,Xuejiao Zhao*

Main category: cs.CL

TL;DR: 本文提出了Online-PVLM框架，利用双曲表示实现个性化视觉语言模型的在线概念学习，无需训练即可在测试时生成概念嵌入，解决了现有方法无法支持实时适应和大规模场景的问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化视觉语言模型方法需要为每个新概念学习单独的嵌入，无法支持测试时的实时适应，在大规模场景下无法高效检索概念嵌入。

Method: 提出Online-PVLM框架，利用双曲表示实现免训练的概念嵌入生成，在测试时进行在线概念学习。

Result: 开发了OP-Eval基准测试，包含1,292个概念和超过30K高质量实例，实验证明该框架达到了最先进的性能。

Conclusion: 该框架使个性化视觉语言模型的使用既具有可扩展性又高效，源代码和数据集将公开。

Abstract: Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.

</details>


### [13] [MTA: A Merge-then-Adapt Framework for Personalized Large Language Model](https://arxiv.org/abs/2511.20072)
*Xiaopeng Li,Yuanjin Zheng,Wanyu Wang,wenlin zhang,Pengyue Jia,Yiqi Wang,Maolin Wang,Xuetao Wei,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 提出了MTA框架解决个性化大语言模型存储成本高和稀疏数据性能差的问题，通过元LoRA银行构建、自适应LoRA融合和LoRA堆叠实现高效个性化。


<details>
  <summary>Details</summary>
Motivation: 现有个性化大语言模型方法存在两个主要限制：存储成本随用户数量线性增长导致不可扩展；为稀疏数据用户从头微调静态模型性能不佳。

Method: MTA框架包含三个阶段：1) 构建共享元LoRA银行，预训练元个性化特征；2) 自适应LoRA融合，动态合并相关锚点元LoRA合成用户特定模块；3) LoRA堆叠用于少样本个性化，在合并LoRA上添加超低秩轻量级模块进行微调。

Result: 在LaMP基准测试上的广泛实验表明，该方法在多个任务上优于现有最先进方法。

Conclusion: MTA框架通过元LoRA银行、动态融合和堆叠技术，有效解决了PLLMs的存储扩展性和稀疏数据性能问题，实现了高效个性化。

Abstract: Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.

</details>


### [14] [More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering](https://arxiv.org/abs/2511.20086)
*Duc Anh Vu,Thong Nguyen,Cong-Duy Nguyen,Viet Anh Nguyen,Anh Tuan Luu*

Main category: cs.CL

TL;DR: BiasPrompting是一个新颖的推理框架，通过引导LLMs为所有可能的答案选项生成并批判性评估推理，来提升多项选择题任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多项选择题任务中存在局限性：答案选项通常缺乏上下文基础或解释，导致无法充分探索所有可能答案，从而降低模型的推理能力。

Method: BiasPrompting包含两个组件：1) 推理生成阶段，引导模型为每个答案选项生成支持性推理；2) 推理引导的一致性阶段，综合生成的推理来选择最合理的答案。

Result: 在五个广泛使用的多项选择题回答基准测试中，BiasPrompting展现出显著改进。实验表明该方法增强了LLMs的推理能力，特别是在现有方法表现不佳的复杂和具有挑战性问题场景中。

Conclusion: BiasPrompting为处理复杂和具有挑战性的问题提供了强大基础，特别是在现有方法表现不佳的设置中，有效提升了LLMs在多项选择题任务中的推理能力。

Abstract: With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models' reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.

</details>


### [15] [SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space](https://arxiv.org/abs/2511.20102)
*Zhenyi Shen,Junru Lu,Lin Gui,Jiazheng Li,Yulan He,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: SSA是一种统一的稀疏注意力训练框架，通过双向对齐稀疏和全注意力，解决现有稀疏注意力方法中梯度更新不足和稀疏度低的问题，在保持性能的同时实现更强的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法存在梯度更新不足的悖论：被排除在稀疏训练之外的低秩键值对既无前向贡献也无反向梯度，无法学习适当的抑制，导致稀疏度低于全注意力模型。

Method: 提出SSA框架，同时考虑稀疏和全注意力，在每一层强制执行双向对齐，保持所有token的梯度流动，同时显式鼓励稀疏注意力输出与其全注意力对应物对齐。

Result: SSA在多个常识基准测试中实现了最先进的性能，支持灵活的稀疏预算调整，性能随可关注token数量增加而持续提升，并展现出最强的长上下文外推能力。

Conclusion: SSA通过双向对齐机制有效解决了稀疏注意力训练中的梯度缺陷问题，实现了更好的稀疏性和性能平衡，同时改善了长上下文外推能力。

Abstract: The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.

</details>


### [16] [EM2LDL: A Multilingual Speech Corpus for Mixed Emotion Recognition through Label Distribution Learning](https://arxiv.org/abs/2511.20106)
*Xingfeng Li,Xiaohan Shi,Junjie Li,Yongwei Li,Masashi Unoki,Tomoki Toda,Masato Akagi*

Main category: cs.CL

TL;DR: EM2LDL是一个新颖的多语言语音语料库，通过标签分布学习推进混合情感识别，包含英语、普通话和粤语表达性话语，捕捉多语言地区语码转换现象。


<details>
  <summary>Details</summary>
Motivation: 解决现有语料库主要为单语言和单标签情感标注的限制，这些限制阻碍了语言多样性建模、混合情感表达以及生态效度。

Method: 整合来自在线平台的自发情感表达，使用32个类别的细粒度情感分布进行标注，采用自监督学习模型建立实验基线。

Result: HuBERT-large-EN模型在说话人无关的性别、年龄和个性评估中表现最优，展示了稳健的性能。

Conclusion: EM2LDL通过融入语言多样性和生态效度，为多语言环境中复杂情感动态的探索提供了可能，为开发适应性强的共情系统提供了多功能测试平台。

Abstract: This study introduces EM2LDL, a novel multilingual speech corpus designed to advance mixed emotion recognition through label distribution learning. Addressing the limitations of predominantly monolingual and single-label emotion corpora \textcolor{black}{that restrict linguistic diversity, are unable to model mixed emotions, and lack ecological validity}, EM2LDL comprises expressive utterances in English, Mandarin, and Cantonese, capturing the intra-utterance code-switching prevalent in multilingual regions like Hong Kong and Macao. The corpus integrates spontaneous emotional expressions from online platforms, annotated with fine-grained emotion distributions across 32 categories. Experimental baselines using self-supervised learning models demonstrate robust performance in speaker-independent gender-, age-, and personality-based evaluations, with HuBERT-large-EN achieving optimal results. By incorporating linguistic diversity and ecological validity, EM2LDL enables the exploration of complex emotional dynamics in multilingual settings. This work provides a versatile testbed for developing adaptive, empathetic systems for applications in affective computing, including mental health monitoring and cross-cultural communication. The dataset, annotations, and baseline codes are publicly available at https://github.com/xingfengli/EM2LDL.

</details>


### [17] [Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach](https://arxiv.org/abs/2511.20107)
*Huu Tuong Tu,Ha Viet Khanh,Tran Tien Dat,Vu Huan,Thien Van Luong,Nguyen Tien Cuong,Nguyen Thi Thu Trang*

Main category: cs.CL

TL;DR: 提出了一种基于检索技术和预训练ASR模型的免训练框架，用于发音错误检测和诊断，无需音素级建模或额外训练。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要评分模型或音素级模型训练，过程复杂且资源消耗大，需要更简单有效的解决方案。

Method: 利用预训练自动语音识别模型和检索技术，避免音素特定建模和任务特定训练，实现免训练框架。

Result: 在L2-ARCTIC数据集上达到69.60%的F1分数，优于传统方法，同时避免了模型训练的复杂性。

Conclusion: 该方法提供了一种简单有效的发音错误检测和诊断方案，无需复杂训练过程，具有实用价值。

Abstract: Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.

</details>


### [18] ["When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings](https://arxiv.org/abs/2511.20120)
*Somsubhra De,Harsh Kumar,Arun Prakash A*

Main category: cs.CL

TL;DR: 该论文探索了使用大型语言模型（如GPT-4.1、Gemini-2.5和LLaMA-4）结合少量样本策略进行印地语系语言的语法错误校正，在资源有限的情况下取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 针对印地语系语言语法错误校正任务资源有限、语言多样性复杂和形态学复杂的问题，研究如何利用大型语言模型的多语言泛化能力来弥补资源差距。

Method: 采用基于提示的方法，结合零样本和少量样本策略，使用最先进的大型语言模型进行语法错误校正，并通过精心设计的提示和轻量级适配来提升校正质量。

Result: 在共享任务中取得了领先成绩：泰米尔语排名第1（GLEU: 91.57）、印地语排名第1（GLEU: 85.69）、泰卢固语排名第2（GLEU: 85.22）、孟加拉语排名第4（GLEU: 92.86）、马拉雅拉姆语排名第5（GLEU: 92.97）。

Conclusion: 研究表明基于提示的自然语言处理技术非常有效，大型语言模型具有填补多语言语法错误校正资源差距的巨大潜力，即使基本的提示策略也能显著优于专门针对印地语系语言微调的模型。

Abstract: Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.

</details>


### [19] [SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models](https://arxiv.org/abs/2511.20143)
*Wen-Fang Su,Hsiao-Wei Chou,Wen-Yang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种结合图像数据增强技术的网格标记方法，用于解决不连续命名实体识别中的分割和遗漏问题，在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别中的不连续实体识别具有挑战性，传统方法在文本分割时容易错误分割或完全遗漏跨句子的不连续实体，严重影响识别准确性。

Method: 基于网格标记方法，集成图像数据增强技术（如裁剪、缩放和填充）到网格模型中，增强对不连续实体的识别能力和处理分割挑战的能力。

Result: 实验结果显示，传统分割方法在捕捉跨句子不连续实体方面表现不佳，而增强的网格模型在CADEC、ShARe13和ShARe14数据集上整体F1分数提升1-2.5%，不连续实体识别提升3.7-8.4%。

Conclusion: 结合图像数据增强的网格标记方法能有效提升不连续命名实体识别的性能，特别是在处理跨句子不连续实体方面表现出色。

Abstract: Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.

</details>


### [20] [KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP](https://arxiv.org/abs/2511.20182)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.CL

TL;DR: 提出了首个公开可用的吉尔吉斯语单语BERT模型KyrgyzBERT，该模型有3590万参数，使用针对吉尔吉斯语形态结构设计的自定义分词器。通过创建kyrgyz-sst2情感分析基准进行评估，KyrgyzBERT在该数据集上微调后达到0.8280的F1分数，与五倍大的mBERT模型性能相当。


<details>
  <summary>Details</summary>
Motivation: 吉尔吉斯语作为低资源语言，缺乏基础NLP工具。为了解决这一差距，需要开发专门针对吉尔吉斯语的语言模型。

Method: 开发了KyrgyzBERT模型，使用自定义分词器适应吉尔吉斯语的形态结构。通过翻译斯坦福情感树库并手动标注完整测试集，创建了kyrgyz-sst2情感分析基准数据集。

Result: KyrgyzBERT在kyrgyz-sst2数据集上微调后获得0.8280的F1分数，与五倍大的多语言BERT模型性能竞争。

Conclusion: KyrgyzBERT是首个公开可用的吉尔吉斯语单语BERT模型，为吉尔吉斯语NLP研究提供了重要基础工具，所有模型、数据和代码都已发布以支持未来研究。

Abstract: Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.

</details>


### [21] [REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance](https://arxiv.org/abs/2511.20233)
*Chuyi Kong,Gao Wei,Jing Ma,Hongzhan Lin,Zhiyuan Fan*

Main category: cs.CL

TL;DR: REFLEX是一种基于大语言模型的自优化事实核查范式，通过角色扮演对话将事实核查重新表述为联合训练判决预测和解释生成，利用内部知识提高准确性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于大语言模型的事实核查系统过度依赖外部知识源导致的延迟、幻觉问题，提升可靠性、可解释性和实时响应能力。

Method: 提出REFLEX范式，通过角色扮演对话联合训练判决预测和解释生成，自适应提取对比激活对构建转向向量，将真相分解为风格和实质，利用激活级信号指导推理并抑制噪声解释。

Result: 在真实世界数据集上的实验表明，REFLEX优于先前方法，仅用465个自优化训练样本就达到最先进性能，解释性目标训练的模型可有效指导无此目标的模型，提升达7.57%。

Conclusion: REFLEX证明了内部解释信号在事实推理中具有解释和增强的双重作用，能够更忠实、高效地进行推理，解决了传统方法在处理细微、人类未知真相时的挑战。

Abstract: The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

</details>


### [22] [Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios](https://arxiv.org/abs/2511.20340)
*Luohe Shi,Zuchao Li,Lefei Zhang,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: SpecFormer是一种新颖的架构，通过结合单向和双向注意力机制，在低验证资源和低调度成本下实现LLM推理加速，消除了对大型前缀树的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法假设有大量可用计算能力，但在批处理等主流推理系统中，可用空闲计算能力被压缩，因此需要在低验证资源和低调度成本下进行推测解码。

Method: 提出SpecFormer架构，集成单向和双向注意力机制，结合自回归模型从整个输入序列提取信息的能力和非自回归模型的并行生成优势。

Result: 通过在不同规模模型上的无损推测解码实验，证明SpecFormer为扩展LLM推理设定了新标准，具有更低的训练需求和计算成本。

Conclusion: SpecFormer通过创新的架构设计，在保持加速效果的同时降低了资源需求，为LLM推理提供了更高效的解决方案。

Abstract: Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.

</details>


### [23] [The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models](https://arxiv.org/abs/2511.20344)
*Taewhoo Lee,Minju Song,Chanwoong Yoon,Jungwoo Park,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在类比推理中的能力，发现LLMs能够编码类比实体间的关系，但在应用关系信息到新实体时存在困难，且成功推理需要结构对齐。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能够编码高层次关系概念并将其应用于新情境，这是人类认知的核心能力。

Method: 使用比例类比和故事类比测试LLMs，分析隐藏层表示，并通过策略性修补隐藏表示来促进信息传递。

Result: LLMs能有效编码类比关系，但在应用关系信息时存在困难；成功案例中关系信息在中上层传播，失败案例中关系信息缺失；结构对齐是成功推理的关键。

Conclusion: LLMs在编码和应用高层次关系概念方面表现出初步但有限的能力，与人类认知存在相似性和差距。

Abstract: Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.

</details>


### [24] [BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali](https://arxiv.org/abs/2511.20399)
*Abdullah Al Sefat*

Main category: cs.CL

TL;DR: 本文介绍了BengaliFig，一个针对孟加拉语的低资源文化推理挑战数据集，包含435个来自孟加拉口头和文学传统的谜语，用于评估LLM在比喻和文化推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多语言基准测试中表现出色，但在低资源语境下的比喻和文化推理评估不足，特别是在孟加拉语这样的低资源语言中。

Method: 创建了包含435个独特谜语的BengaliFig数据集，每个项目沿五个正交维度进行标注，并通过约束感知的AI辅助流程自动转换为多项选择格式。评估了八个前沿LLM在零样本和少样本思维链提示下的表现。

Result: 评估揭示了LLM在隐喻和文化特定推理方面存在一致的弱点。

Conclusion: BengaliFig为评估LLM在低资源文化语境中的鲁棒性提供了诊断工具，并朝着包容性和遗产感知的NLP评估迈出了一步。

Abstract: Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.

</details>


### [25] [A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines](https://arxiv.org/abs/2511.20409)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的任务导向型词干提取方法评估框架，包含词干提取有效性评分(SES)、下游任务性能变化(MPD)和语义相似度(ANLD)三个维度，用于全面评估词干提取方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前词干提取方法的评估方法有限，无法捕捉过度词干提取可能造成的危害，因此需要开发新的评估方法来全面评估词干提取方法。

Method: 提出包含三个方面的任务导向型评估框架：SES衡量词干提取效用，MPD评估对下游任务的影响，ANLD测量词干化词与原始词的语义相似度。

Result: 应用该框架比较孟加拉语(BNLTK)和英语(Snowball)词干提取器，发现孟加拉语词干提取器虽然SES最高(1.67)，但ANLD显示存在有害的过度词干提取(0.26)，导致下游性能下降；而英语词干提取器SES适中(1.31)且ANLD安全(0.14)，能对下游性能产生积极贡献。

Conclusion: 该研究提供了一个有价值的工具，能够区分潜在效率增益(高SES)和语义保持(低ANLD)，帮助选择更可靠的词干提取方法。

Abstract: Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 本文提出了一个基于部分信息分解（PID）的框架，用于量化多模态模型中各模态的贡献，通过将内部嵌入中的预测信息分解为独特、冗余和协同成分，提供比基于准确性的方法更清晰和可解释的见解。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于准确性的方法，将移除模态后的性能下降解释为其影响力，但这种方法无法区分模态是本身具有信息价值，还是仅通过与其他模态交互才产生价值。在跨注意力架构中，这种区分尤为重要。

Method: 开发了基于迭代比例拟合程序（IPFP）的算法，计算层和数据集级别的贡献，无需重新训练即可进行可扩展的仅推理分析。

Result: 该框架提供了原则性的、表示级别的多模态行为视图，能够量化模态贡献中的独特、冗余和协同成分。

Conclusion: 提出的PID框架为多模态模型提供了比基于结果指标更清晰和可解释的见解，能够区分模态的固有信息价值与交互价值。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [27] [PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer](https://arxiv.org/abs/2511.19472)
*Ruogu Ding,Xin Ning,Ulf Schlichtmann,Weikang Qian*

Main category: cs.LG

TL;DR: PrefixGPT使用GPT模型从零生成优化的前缀加法器，通过二维坐标序列表示拓扑结构，确保生成的设计合法且优化。


<details>
  <summary>Details</summary>
Motivation: 前缀加法器在计算密集型应用中广泛使用，但设计优化困难，因为设计规则严格且设计空间指数级增长。

Method: 将加法器拓扑表示为二维坐标序列，使用GPT模型预训练学习设计规则，然后微调优化设计质量。

Result: 找到新的最优设计，面积-延迟乘积(ADP)提升7.7%，平均ADP降低高达79.1%。

Conclusion: GPT风格模型能够掌握复杂硬件设计原理并应用于更高效的设计优化。

Abstract: Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.

</details>


### [28] [WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning](https://arxiv.org/abs/2511.19473)
*Haojin Yang,Rui Hu,Zequn Sun,Rui Zhou,Yujun Cai,Yiwei Wang*

Main category: cs.LG

TL;DR: WavefrontDiffusion是一种动态解码方法，通过从已确定位置向外扩展活动标记波前，在保持与基于块的方法相同计算成本的同时，实现更连贯和高效的文本生成。


<details>
  <summary>Details</summary>
Motivation: 主流去噪策略存在局限性：标准扩散会导致过早结束序列预测，块扩散会破坏连贯语义单元和推理过程。需要一种能够遵循语义结构自然流动的自适应调度方法。

Method: 提出WavefrontDiffusion动态解码方法，从已确定位置向外扩展活动标记波前，实现自适应调度，同时保持与基于块的方法相同的计算成本。

Result: 在推理和代码生成的四个基准测试中，WavefrontDiffusion实现了最先进的性能，产生具有更高语义保真度的输出。

Conclusion: 自适应调度对于实现更连贯和高效的生成具有重要价值，WavefrontDiffusion展示了在扩散语言模型中动态解码策略的优势。

Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.

</details>


### [29] [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 本文系统研究了MoE-LLMs在任务特定使用下的可剪枝性，开发了专家归因框架识别关键专家，评估了剪枝和重新对齐的性能权衡，并提出了防御策略以防止未经授权的模型压缩和微调。


<details>
  <summary>Details</summary>
Motivation: MoE架构在大语言模型中的广泛应用带来了新的安全漏洞：攻击者可以通过剪枝专家和廉价微调来压缩或重新利用模型，绕过许可和安全约束。

Method: 开发专家归因框架识别任务关键专家，评估剪枝和重新对齐的性能权衡，使用主动学习驱动的微调方法，并提出防御策略包括纠缠专家训练和选择性微调协议。

Result: 研究发现存在关键的知识损失-恢复权衡：虽然可以隔离某些专家来保持任务准确性，但如果没有针对性的重新对齐，会出现显著性能下降。

Conclusion: 这项工作将专家剪枝定位为威胁向量和防御目标，突出了MoE模块化的双重用途性质，并为MoE-LLMs的安全专业化提供了首个系统评估框架。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.

</details>


### [30] [Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms](https://arxiv.org/abs/2511.19481)
*Ruoxin Zhang,Zhizhao Wen,Chao Wang,Chenchen Tang,Puyang Xu,Yifan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于特征工程和粒子群优化的XGBoost机器学习回归模型，用于优化RAG系统中的检索质量，通过提高文档相关性来提升生成内容的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，检索增强生成技术被广泛应用，但系统性能高度依赖检索模块质量。现有模型在处理表格特征方面存在性能瓶颈，检索结果相关性低或包含噪声信息会导致生成内容失真。

Method: 提出基于特征工程和粒子群优化的XGBoost机器学习回归模型，通过相关性分析发现文档相关性与答案质量正相关(0.66)，语义相似度、冗余度与多样性负相关(-0.89, -0.88)。

Result: 实验结果表明，VMD PSO BiLSTM模型在所有评估指标上均优于决策树、AdaBoost等对比模型，MSE、RMSE、MAE和MAPE显著降低，R2值更高，预测精度、稳定性和数据解释能力更突出。

Conclusion: 该成果为优化RAG系统检索质量和改善生成效果提供了有效路径，对促进相关技术的实施和应用具有重要价值。

Abstract: With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a trade- off between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.

</details>


### [31] [OmniTFT: Omni Target Forecasting for Vital Signs and Laboratory Result Trajectories in Multi Center ICU Data](https://arxiv.org/abs/2511.19485)
*Wanzhe Xu,Yutong Dai,Yitao Yang,Martin Loza,Weihang Zhang,Yang Cui,Xin Zeng,Sung Joon Park,Kenta Nakai*

Main category: cs.LG

TL;DR: OmniTFT是一个基于Temporal Fusion Transformer的深度学习框架，用于联合预测ICU中的高频生命体征和稀疏采样的实验室结果，通过四种新策略提升性能，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: ICU中的生命体征存在噪声和快速波动，实验室测试存在缺失值、测量延迟和设备特定偏差等问题，使得综合预测极具挑战性。

Method: 提出OmniTFT框架，采用滑动窗口均衡采样、频率感知嵌入收缩、分层变量选择和影响对齐注意力校准四种策略，基于Temporal Fusion Transformer联合学习生命体征和实验室结果。

Result: 在MIMIC-III、MIMIC-IV和eICU数据集上，OmniTFT在生命体征和实验室结果的预测任务中均实现了显著的性能提升。

Conclusion: OmniTFT能够统一建模多个异质临床目标，保持跨机构泛化能力，其注意力模式可解释且与已知病理生理学一致，具有临床决策支持的潜在应用价值。

Abstract: Accurate multivariate time-series prediction of vital signs and laboratory results is crucial for early intervention and precision medicine in intensive care units (ICUs). However, vital signs are often noisy and exhibit rapid fluctuations, while laboratory tests suffer from missing values, measurement lags, and device-specific bias, making integrative forecasting highly challenging. To address these issues, we propose OmniTFT, a deep learning framework that jointly learns and forecasts high-frequency vital signs and sparsely sampled laboratory results based on the Temporal Fusion Transformer (TFT). Specifically, OmniTFT implements four novel strategies to enhance performance: sliding window equalized sampling to balance physiological states, frequency-aware embedding shrinkage to stabilize rare-class representations, hierarchical variable selection to guide model attention toward informative feature clusters, and influence-aligned attention calibration to enhance robustness during abrupt physiological changes. By reducing the reliance on target-specific architectures and extensive feature engineering, OmniTFT enables unified modeling of multiple heterogeneous clinical targets while preserving cross-institutional generalizability. Across forecasting tasks, OmniTFT achieves substantial performance improvement for both vital signs and laboratory results on the MIMIC-III, MIMIC-IV, and eICU datasets. Its attention patterns are interpretable and consistent with known pathophysiology, underscoring its potential utility for quantitative decision support in clinical care.

</details>


### [32] [Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification](https://arxiv.org/abs/2511.19486)
*Lei Wang,Zikun Ye,Jinglong Zhao*

Main category: cs.LG

TL;DR: 本文提出了一个结合微调和校正的框架，通过优化分配有限标记样本来提升大语言模型在市场研究和社科应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用微调或校正单独提升LLM性能，但缺乏将两者结合并优化样本分配的框架。

Method: 提出最小化预测误差方差作为微调目标，并基于经验缩放定律开发数据驱动方法，优化微调和校正阶段的样本分配。

Result: 实证分析验证了该框架相比单独使用微调或校正，在估计和推断性能方面均有提升。

Conclusion: 结合微调和校正并优化样本分配的框架能有效提升LLM在社科应用中的性能表现。

Abstract: Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.

</details>


### [33] [The Generalized Proximity Forest](https://arxiv.org/abs/2511.19487)
*Ben Shaw,Adam Rustad,Sofia Pelagalli Maia,Jake S. Rhodes,Kevin R. Moon*

Main category: cs.LG

TL;DR: 本文提出了广义邻近森林模型，将随机森林邻近度扩展到所有基于距离的监督机器学习场景，并引入了回归任务变体和元学习框架。


<details>
  <summary>Details</summary>
Motivation: 随机森林邻近度在监督学习任务中很有用，但其效用依赖于随机森林模型本身的成功，而随机森林并非在所有场景下都是理想模型。需要将邻近度方法扩展到更广泛的机器学习场景。

Method: 提出了广义邻近森林模型，将随机森林邻近度扩展到所有基于距离的监督机器学习场景；引入了回归任务的变体；提出了将广义邻近森林作为元学习框架，为任何预训练分类器扩展监督插补能力。

Result: 实验证明广义邻近森林模型相比随机森林模型和k近邻模型具有独特优势。

Conclusion: 广义邻近森林模型成功扩展了随机森林邻近度的应用范围，为各种监督距离学习场景提供了有效的邻近度计算方法。

Abstract: Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.

</details>


### [34] [OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally](https://arxiv.org/abs/2511.19491)
*Jitendra Parmar,Praveen Singh Thakur*

Main category: cs.LG

TL;DR: 该论文提出了一种开放世界机器学习模型，通过发现未知类别和增量学习新类别来实现持续学习，在开放世界学习和持续学习方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型遵循封闭世界假设，难以保留先前学到的知识来处理未来任务，而自动化智能系统需要学习新类别和已知任务。

Method: 模型包含两个相互连接的任务：首先发现数据中的未知类别并创建新类别，然后对每个新类别进行增量学习，从而实现持续学习。

Result: 该模型在开放世界学习中优于现有方法，在持续学习中表现优异，四个迭代的平均准确率最高达82.54%，最低准确率为65.87%。

Conclusion: 该模型能够在开放和持续学习环境中扩展对数据的理解并随时间改进，为开放世界机器学习提供了有效解决方案。

Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.

</details>


### [35] [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)
*Shivansh Chhawri,Rahul Mahadik,Suparna Rooj*

Main category: cs.LG

TL;DR: 本文系统研究了LLM压缩技术（知识蒸馏、结构化剪枝、低比特量化）在Qwen2.5 3B模型上的独立和组合效果，发现技术顺序对最终质量有显著影响，其中P-KD-Q序列在保持性能的同时实现3.68倍压缩比。


<details>
  <summary>Details</summary>
Motivation: LLM需要大量计算资源，在受限环境中部署需要模型压缩，但现有压缩技术的交互作用和最优顺序尚不明确。

Method: 在Qwen2.5 3B模型上评估多种压缩流程，包括单技术和三技术序列，使用困惑度、G-Eval、清晰度、提示对齐和压缩比作为指标。

Result: 量化提供最大独立压缩，剪枝引入中等质量下降。技术顺序显著影响最终质量：P-KD-Q序列表现最佳，实现3.68倍压缩比同时保持强指令跟随和语言理解能力。

Conclusion: 本研究为在资源受限环境中部署LLM提供了设计有效、顺序感知压缩流程的实用见解。

Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.

</details>


### [36] [Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data](https://arxiv.org/abs/2511.19498)
*Yi Zhang,Tianxiang Xu,Zijian Li,Chao Zhang,Kunyu Zhang,Zhan Gao,Meinuo Li,Xiaohan Zhang,Qichao Qi,Bing Chen*

Main category: cs.LG

TL;DR: 提出了一种分层双策略框架，用于选择性知识遗忘，在医疗领域精确移除专业知识同时保留基础医学能力，仅需修改0.1%参数即可实现82.7%遗忘率和88.5%知识保留。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在医疗场景中因训练数据记忆带来的隐私风险，特别是涉及不完善或隐私敏感的患者信息时，需要满足监管合规、可审计性和伦理标准。

Method: 采用分层双策略框架，结合几何约束梯度更新选择性调节目标参数，以及概念感知的令牌级干预，通过统一的四级医学概念层次区分保留关键和遗忘目标的令牌。

Result: 在MedMCQA（外科）和MHQA（焦虑、抑郁、创伤）数据集上评估，达到82.7%的遗忘率和88.5%的知识保留率，同时保持强大的隐私保证。

Conclusion: 该框架有效解决了医疗领域LLM的隐私风险问题，在保持基础医学能力的同时精确移除专业知识，满足临床研究的监管和伦理要求。

Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.

</details>


### [37] [Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection](https://arxiv.org/abs/2511.19499)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 本文提出TriDetect检测器，通过半监督方法发现假图像中的潜在架构模式，解决GAN和扩散模型检测器跨架构泛化问题。


<details>
  <summary>Details</summary>
Motivation: 当前检测器在跨生成器架构（如从GAN到扩散模型）时泛化能力不足，这源于不同架构产生的伪影存在根本差异。

Method: 提出TriDetect检测器，采用半监督方法，通过Sinkhorn-Knopp算法进行平衡聚类分配和跨视图一致性机制，学习基础的架构差异。

Result: 在两个标准基准和三个真实数据集上评估，与13个基线方法相比，证明了其对未见生成器的泛化能力。

Conclusion: TriDetect通过发现假图像中的潜在架构模式，有效提升了跨生成器架构的检测泛化性能。

Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.

</details>


### [38] [Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma](https://arxiv.org/abs/2511.19504)
*Subramanyam Sahoo,Aman Chadha,Vinija Jain,Divya Chaudhary*

Main category: cs.LG

TL;DR: 本文提出了对齐三元悖论：RLHF系统无法同时实现多样性代表性、多项式可扩展性和鲁棒性。通过复杂性理论分析证明，要实现全球规模的代表性和鲁棒性需要超多项式计算复杂度。


<details>
  <summary>Details</summary>
Motivation: RLHF在实践中面临安全性提升导致公平性下降、扩展到多样化群体计算不可行、鲁棒性增强放大多数偏见等问题，需要形式化这些根本性权衡。

Method: 通过整合统计学习理论和鲁棒优化的复杂性理论分析，证明同时实现代表性和鲁棒性的计算复杂度下界。

Result: 证明实现全球规模代表性(ε≤0.01)和鲁棒性(δ≤0.001)需要Ω(2^{d_context})操作，这是超多项式复杂度。当前RLHF实现通过牺牲代表性来解决这一悖论。

Conclusion: 框架统一解释了RLHF的病理现象，如偏好崩溃、谄媚行为和系统性偏见放大，并提出了通过战略性放宽对齐要求来应对这些根本性权衡的具体方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.

</details>


### [39] [Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning](https://arxiv.org/abs/2511.19513)
*Bing Liu,Boao Kong,Limin Lu,Kun Yuan,Chengcheng Zhao*

Main category: cs.LG

TL;DR: 本文重新审视了去中心化学习中处理异构节点权重的两种策略：将权重嵌入局部损失以保持均匀权重（双随机矩阵）和保持原始损失但使用λ诱导的行随机矩阵。通过建立加权Hilbert空间框架，发现行随机矩阵在该几何中成为自伴算子，而双随机矩阵则不会，这会产生额外的惩罚项放大共识误差从而减慢收敛。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究表明两种策略对全局损失产生相同的期望下降方向，但尚不清楚欧几里得空间保证是否紧致，以及它们行为的根本差异是什么。需要澄清这两种策略在收敛性能上的本质区别。

Method: 开发加权Hilbert空间框架L²(λ;ℝᵈ)，在该几何中分析两种策略的收敛性。使用Rayleigh商和Loewner序特征值比较来获得拓扑条件。

Result: 在加权Hilbert空间几何中，行随机矩阵成为自伴算子而双随机矩阵不会，产生额外的惩罚项放大共识误差。收敛差异不仅来自谱间隙，还来自这些惩罚项。推导了行随机设计即使具有较小谱间隙也能更快收敛的充分条件。

Conclusion: 获得了保证行随机策略优势的拓扑条件，并提供了实用的拓扑设计指南。收敛性能差异的根本原因在于加权Hilbert空间几何中矩阵的自伴性差异导致的额外惩罚项。

Abstract: Decentralized learning often involves a weighted global loss with heterogeneous node weights $λ$. We revisit two natural strategies for incorporating these weights: (i) embedding them into the local losses to retain a uniform weight (and thus a doubly stochastic matrix), and (ii) keeping the original losses while employing a $λ$-induced row-stochastic matrix. Although prior work shows that both strategies yield the same expected descent direction for the global loss, it remains unclear whether the Euclidean-space guarantees are tight and what fundamentally differentiates their behaviors. To clarify this, we develop a weighted Hilbert-space framework $L^2(λ;\mathbb{R}^d)$ and obtain convergence rates that are strictly tighter than those from Euclidean analysis. In this geometry, the row-stochastic matrix becomes self-adjoint whereas the doubly stochastic one does not, creating additional penalty terms that amplify consensus error, thereby slowing convergence. Consequently, the difference in convergence arises not only from spectral gaps but also from these penalty terms. We then derive sufficient conditions under which the row-stochastic design converges faster even with a smaller spectral gap. Finally, by using a Rayleigh-quotient and Loewner-order eigenvalue comparison, we further obtain topology conditions that guarantee this advantage and yield practical topology-design guidelines.

</details>


### [40] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文提出了一种自动化生成大规模心理驱动多轮越狱数据集的方法，基于Foot-in-the-Door心理原理构建了1500个场景，评估了7个LLM模型在单轮和多轮对话条件下的安全性表现。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用心理原理绕过LLM的安全对齐，现有防御方法依赖难以扩展的手动数据集创建，阻碍了防御进展。

Method: 开发自动化流水线，系统化地将FITD技术转化为可复现模板，创建包含非法活动和冒犯性内容的1500个场景基准数据集。

Result: GPT系列模型对对话历史显著脆弱，攻击成功率最多增加32个百分点；Gemini 2.5 Flash表现出卓越韧性，几乎免疫这些攻击；Claude 3 Haiku显示强大但不完美的抵抗能力。

Conclusion: 当前安全架构在处理对话上下文方面存在关键差异，需要能够抵抗基于叙事操纵的防御机制。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [41] [When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics](https://arxiv.org/abs/2511.19548)
*Yiven,Zhu*

Main category: cs.LG

TL;DR: 本文提出了一个基于模型的框架，将神经信号、计算决策模型和规范福利标准联系起来，分析神经数据在政策福利判断中的合法应用条件。


<details>
  <summary>Details</summary>
Motivation: 神经经济学承诺基于神经和计算证据进行福利分析，但需要明确神经数据何时能合法地用于政策福利判断，而不仅仅是描述行为。

Method: 开发了一个非经验性的、基于模型的框架，在actor-critic强化学习模型中形式化从神经活动到潜在价值和预测误差，再到福利主张的推理路径。

Result: 神经证据只有在神经-计算映射得到充分验证、决策模型识别出"真实"利益与情境依赖错误、福利标准明确指定和辩护时，才能约束福利判断。

Conclusion: 提出了神经经济学福利推理检查表，强调内部奖励信号（无论是生物的还是人工的）都是计算量，没有明确的规范模型不能被视为福利衡量标准。

Abstract: Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, "brain-based" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies "true" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.

</details>


### [42] [Online Sparse Feature Selection in Data Streams via Differential Evolution](https://arxiv.org/abs/2511.19555)
*Ruiyang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的在线差分进化稀疏特征选择方法(ODESFS)，通过潜在因子分析模型进行缺失值填补，并使用差分进化评估特征重要性，在六个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的在线稀疏流特征选择方法在特征评估方面存在显著局限性，导致性能下降，需要改进特征选择效果。

Method: 使用潜在因子分析模型进行缺失值填补，并通过差分进化算法评估特征重要性，实现最优特征子集选择。

Result: 在六个真实数据集上的综合实验表明，ODESFS始终优于最先进的OSFS和OS2FS方法，能够选择最优特征子集并获得更高的准确性。

Conclusion: ODESFS方法通过结合缺失值填补和差分进化特征评估，有效解决了高维流数据特征选择问题，显著提升了性能表现。

Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.

</details>


### [43] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: OTMF提出了一种基于最优传输理论的模型融合框架，通过发现任务向量的共同掩码来对齐任务特定模型的语义几何，解决参数插值引起的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法主要依赖权重空间的参数插值，这会导致特征空间的显著分布偏移并削弱任务特定知识。

Method: OTMF使用最优传输计划发现应用于任务向量的共同掩码，选择性地提取可转移和任务无关的组件，同时保留每个任务的独特结构特征。支持持续融合范式，增量集成新任务向量。

Result: 在多个视觉和语言基准测试上的综合实验表明，OTMF在准确性和效率方面都达到了最先进的性能。

Conclusion: 该方法在模型融合方面具有实践和理论价值，能够有效解决分布偏移问题并实现高效的多任务融合。

Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.

</details>


### [44] [ModHiFi: Identifying High Fidelity predictive components for Model Modification](https://arxiv.org/abs/2511.19566)
*Dhruva Kashyap,Chaitanya Murti,Pranav K Nayak,Tanay Narshana,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练数据或损失函数访问的模型修改方法，通过分析组件子集的局部重构行为来量化其全局重要性，应用于模型剪枝和类别遗忘任务。


<details>
  <summary>Details</summary>
Motivation: 现有的模型修改技术通常需要梯度或真实标签，这在计算资源有限的环境中不可行。本文旨在识别对模型预测性能至关重要的组件，而无需访问梯度或损失函数。

Method: 提出Subset Fidelity指标，利用组件子集的局部重构行为量化其全局重要性。基于此开发ModHiFi算法，包括用于结构化剪枝的ModHiFi-P和用于类别遗忘的ModHiFi-U。

Result: ModHiFi-P在ImageNet模型上比当前最优方法提速11%，在语言模型上表现有竞争力；ModHiFi-U在CIFAR-10上实现完全遗忘，在Swin Transformers上表现良好。

Conclusion: 该方法证明了无需训练数据或损失函数即可有效修改模型，为资源受限环境下的模型优化提供了新思路。

Abstract: Open weight models, which are ubiquitous, rarely provide access to their training data or loss function. This makes modifying such models for tasks such as pruning or unlearning constrained by this unavailability an active area of research. Existing techniques typically require gradients or ground-truth labels, rendering them infeasible in settings with limited computational resources. In this work, we investigate the fundamental question of identifying components that are critical to the model's predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term Subset Fidelity. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose ModHiFi, an algorithm for model modification that requires no training data or loss function access. ModHiFi-P, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. ModHiFi-U, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers.

</details>


### [45] [An Invariant Latent Space Perspective on Language Model Inversion](https://arxiv.org/abs/2511.19569)
*Wentao Ye,Jiaqi Hu,Haobo Wang,Xinpeng Ti,Zhiqing Xiao,Hao Chen,Liyao Li,Lei Feng,Sai Wu,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出语言模型逆向攻击（LMI）的威胁，并基于不变潜在空间假设提出Inv^2A方法，通过轻量级逆编码器从输出恢复隐藏提示，在9个数据集上优于基线方法4.77% BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 语言模型逆向攻击（LMI）对用户隐私和系统安全构成威胁，需要开发有效的逆向方法来揭示隐藏提示。

Method: 提出不变潜在空间假设，构建Inv^2A方法：将LLM视为不变解码器，学习轻量级逆编码器将输出映射到去噪伪表示；采用对比对齐和监督强化两阶段训练；可选无训练邻域搜索优化局部性能。

Result: 在9个数据集上，Inv^2A平均比基线方法提升4.77% BLEU分数，同时减少对大型逆向语料库的依赖；分析显示现有防御措施保护有限。

Conclusion: Inv^2A方法在语言模型逆向攻击方面表现优异，凸显了需要更强防御策略的必要性。

Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.

</details>


### [46] [Neural Tractability via Structure: Learning-Augmented Algorithms for Graph Combinatorial Optimization](https://arxiv.org/abs/2511.19573)
*Jialiang Li,Weitong Chen,Mingyu Guo*

Main category: cs.LG

TL;DR: 提出一种结合神经模型推理效率和参数化算法最优性保证的新框架，用于解决NP难图组合优化问题。该框架识别实例的结构困难部分由神经模型处理，简单部分由参数化算法搜索，从而获得更优解并改善分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经模型在组合优化问题中虽然推理速度快，但解质量不如经典搜索算法；而搜索算法虽然能提供最优性保证但速度较慢。需要结合两者的优势。

Method: 使用参数化算法作为搜索组件识别简单实例结构，神经模型处理困难部分生成指导信号，参数化搜索组件整合这些信号进行高效系统搜索。

Result: 在多个组合优化任务上，该框架实现了与商业求解器相竞争的解质量，并改善了神经求解器的分布外泛化能力。

Conclusion: 该框架不依赖于特定神经模型选择，能严格优于单独使用神经求解器，成功结合了神经模型的推理效率和搜索算法的解质量保证。

Abstract: Neural models have shown promise in solving NP-hard graph combinatorial optimization (CO) problems. Once trained, they offer fast inference and reasonably high-quality solutions for in-distribution testing instances, but they generally fall short in terms of absolute solution quality compared to classical search-based algorithms that are admittedly slower but offer optimality guarantee once search finishes.
  We propose a novel framework that combines the inference efficiency and exploratory power of neural models with the solution quality guarantee of search-based algorithms. In particular, we use parameterized algorithms (PAs) as the search component. PAs are dedicated to identifying easy instances of generally NP-hard problems, and allow for practically efficient search by exploiting structural simplicity (of the identified easy instances). Under our framework, we use parameterized analysis to identify the structurally hard parts of a CO instance. The neural model handles the hard parts by generating advisory signals based on its data-driven understanding. The PA-based search component then integrates the advisory signals to systematically and efficiently searches through the remaining structurally easy parts. Notably, our framework is agnostic to the choice of neural model and produces strictly better solutions than neural solvers alone.
  We examine our framework on multiple CO tasks. Empirical results show that it achieves superior solution quality, competitive with that of commercial solvers. Furthermore, by using the neural model only for exploratory advisory signals, our framework exhibits improved out-of-distribution generalization, addressing a key limitation of existing neural CO solvers.

</details>


### [47] [Learning Massively Multitask World Models for Continuous Control](https://arxiv.org/abs/2511.19584)
*Nicklas Hansen,Hao Su,Xiaolong Wang*

Main category: cs.LG

TL;DR: 本文提出了一个包含200个多样化任务的新基准，并介绍了Newt——一种语言条件化的多任务世界模型，通过演示预训练获取任务感知表示和动作先验，然后在所有任务上进行在线交互联合优化。


<details>
  <summary>Details</summary>
Motivation: 通用控制需要能够跨越多个任务和具体实现的智能体，但目前连续控制领域的强化学习研究仍以单任务或离线模式为主，这强化了在线强化学习无法扩展的观点。受基础模型方法（大规模预训练+轻量强化学习）的启发，本文探索是否可以通过在线交互训练单一智能体处理数百个任务。

Method: 首先在演示数据上进行预训练以获取任务感知表示和动作先验，然后通过在线交互在所有任务上进行联合优化。Newt是一种语言条件化的多任务世界模型。

Result: 实验表明，Newt相比强基线方法在多任务性能和数据效率方面表现更好，展现出强大的开环控制能力，并能快速适应未见过的任务。

Conclusion: 本文证明了通过适当的预训练和在线交互联合优化，单一智能体可以在数百个多样化任务上实现有效的多任务学习和快速适应能力。

Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.

</details>


### [48] [Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles](https://arxiv.org/abs/2511.19656)
*Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文针对双层优化问题，在光滑非凸-强凸设置下，建立了确定性一阶oracle模型下Ω(κ^{3/2}ε^{-2})和随机一阶oracle模型下Ω(κ^{5/2}ε^{-4})的下界，揭示了当前上下界之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 由于双层结构的复杂性，双层优化的下界研究进展有限。本文旨在填补这一空白，为光滑非凸-强凸设置下的双层优化建立新的下界。

Method: 开发新的困难实例，在确定性和随机一阶oracle模型下分析下界。使用零尊重算法框架来证明下界。

Result: 在确定性情况下，任何一阶零尊重算法至少需要Ω(κ^{3/2}ε^{-2})次oracle调用才能找到ε-精确的稳定点；在随机情况下，至少需要Ω(κ^{5/2}ε^{-4})次随机oracle调用。

Conclusion: 研究结果揭示了当前双层优化上下界之间的显著差距，表明即使是简化设置（如二次下层目标）也值得进一步研究，以理解标准一阶oracle下双层优化的最优复杂度。

Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Ω(κ^{3/2}ε^{-2})$ oracle calls to find an $ε$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Ω(κ^{5/2}ε^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.

</details>


### [49] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于认证式分块提取Transformer机制并支持认证式局部编辑的框架，通过提取结构化替代实现并提供机器可检查的证书来约束近似误差。


<details>
  <summary>Details</summary>
Motivation: 解决机制可解释性和模型编辑领域缺乏形式化保证的问题，为提取或编辑后的模型在相关输入上的偏差提供明确界限。

Method: 基于预训练Transformer和提示分布，提取残差块的结构化替代实现，提供包含近似误差边界、覆盖度指标和基础工件哈希的机器可检查证书，并利用Lipschitz组合定理将局部保证提升为全局偏差边界。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上获得高分块覆盖度和小残差误差，在TinyLlama设置中完全拼接模型在压力提示上的困惑度与基线相差约6e-5。

Conclusion: 分块提取配合显式证书对于真实Transformer语言模型是可行的，为机制可解释性和模型行为的形式化推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [50] [Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds](https://arxiv.org/abs/2511.19664)
*Jiaxin Shi,Michalis K. Titsias*

Main category: cs.LG

TL;DR: 本文提出了对扩散模型中广泛使用的重加权损失的新理论解释，通过构建时间相关的变分下界级联来改进标准证据下界，从而减少数据模型KL散度。


<details>
  <summary>Details</summary>
Motivation: 为扩散模型中广泛使用的重加权损失提供理论依据，改进标准证据下界，提升生成模型性能。

Method: 构建时间相关的变分下界级联，推导出可应用于任何生成扩散模型（包括连续高斯扩散和掩码扩散模型）的重加权目标。

Result: 在掩码扩散中应用该框架，在像素空间图像建模中显著优于先前训练损失，接近连续扩散模型的样本质量。

Conclusion: 该框架为掩码图像模型中广泛使用的简单加权方案提供了理论依据，并显著提升了扩散模型的性能。

Abstract: We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.

</details>


### [51] [TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification](https://arxiv.org/abs/2511.19694)
*Chin-Chia Michael Yeh,Uday Singh Saini,Junpeng Wang,Xin Dai,Xiran Fan,Jiarui Sun,Yujie Fan,Yan Zheng*

Main category: cs.LG

TL;DR: TiCT是一个基于Transformer的时间序列基础模型，仅使用合成数据预训练，能够通过上下文学习进行时间序列分类，无需微调即可达到与监督方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据普遍存在，但标注数据成本高昂。现有的大规模时间序列模型主要关注预测任务，缺乏能够进行免微调分类的通用基础模型。

Method: 提出TiCT模型，采用基于比特的标签编码和特殊输出注意力机制来处理任意数量的类别，并通过Mixup启发的合成预训练框架结合数据增强来促进泛化和噪声不变性。

Result: 在UCR Archive上的广泛评估显示，TiCT仅使用上下文示例在推理时就能达到与最先进监督方法竞争的性能，且无需更新任何模型权重。

Conclusion: TiCT证明了仅使用合成数据预训练的模型能够通过上下文学习有效处理时间序列分类任务，为开发免微调的时间序列基础模型提供了可行路径。

Abstract: The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.

</details>


### [52] [EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning](https://arxiv.org/abs/2511.19935)
*Songlin Zhao,Michael Pitts,Zhuwei Qin*

Main category: cs.LG

TL;DR: EfficientXpert是一个轻量级领域剪枝框架，通过传播感知的剪枝标准和高效的适配器更新算法，在LoRA微调过程中将通用预训练模型一步转换为稀疏的领域适配专家模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定领域的需求增加，但其大尺寸限制了在资源受限环境中的部署。现有压缩方法要么跨领域泛化能力差，要么开销过高。

Method: 结合传播感知剪枝标准（Foresight Mask）和高效适配器更新算法（Partial Brain Surgeon），集成到LoRA微调过程中。

Result: 在医疗和法律任务中，在40%稀疏度下保持高达98%的密集模型性能，优于现有最先进方法。

Conclusion: 分析显示领域依赖的结构变化会降低通用剪枝掩码的有效性，强调需要针对每个领域的自适应、领域感知剪枝策略。

Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.

</details>


### [53] [Training-Free Active Learning Framework in Materials Science with Large Language Models](https://arxiv.org/abs/2511.19730)
*Hongchen Wang,Rafael Espinosa Castañeda,Jay R. Werber,Yao Fehlis,Edward Kim,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 提出基于大语言模型的主动学习框架LLM-AL，在四个材料科学数据集上相比传统机器学习模型减少70%以上实验次数，实现更高效的实验选择。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在主动学习中存在冷启动问题和领域特定特征工程的限制，影响其泛化能力。大语言模型凭借预训练知识和通用表示能力，可以直接从文本描述中提出实验建议。

Method: 开发LLM-AL框架，采用迭代少样本设置，探索两种提示策略：简洁数值输入（适用于结构化特征数据集）和扩展描述文本（适用于实验过程特征数据集）。

Result: 在所有数据集上，LLM-AL能将达到最佳候选所需的实验次数减少70%以上，始终优于传统机器学习模型，执行更广泛和探索性搜索，同时以更少迭代达到最优。

Conclusion: LLM-AL可作为传统主动学习管道的通用替代方案，实现更高效和可解释的实验选择，具有LLM驱动自主发现的潜力。

Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.

</details>


### [54] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文提出了一种更细粒度的视角，将Transformer中的注意力头和MLP层分解为正交奇异方向，揭示了单个组件内部叠加的独立计算。


<details>
  <summary>Details</summary>
Motivation: 现有的机制可解释性方法通常将注意力头和MLP层视为不可分割单元，忽视了它们内部可能学习到的功能子结构。

Method: 将Transformer组件分解为正交奇异方向，在IOI、GP、GT等标准任务上验证该方法，分析计算图中节点在特定低秩方向上的激活模式。

Result: 发现先前识别的典型功能头（如名称移动器）编码了与不同奇异方向对齐的多个重叠子功能，有意义的计算存在于紧凑子空间中。

Conclusion: Transformer计算比先前假设的更加分布式、结构化和组合化，这为细粒度机制可解释性和模型内部理解开辟了新途径。

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [55] [DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning](https://arxiv.org/abs/2511.19750)
*Julien T. T. Vignoud,Valérian Rousset,Hugo El Guedj,Ignacio Aleman,Walid Bennaceur,Batuhan Faik Derinbay,Eduard Ďurech,Damien Gengler,Lucas Giordano,Felix Grimberg,Franziska Lippoldt,Christina Kopidaki,Jiafan Liu,Lauris Lopata,Nathan Maire,Paul Mansat,Martin Milenkoski,Emmanuel Omont,Güneş Özgün,Mina Petrović,Francesco Posa,Morgan Ridel,Giorgio Savini,Marcel Torne,Lucas Trognon,Alyssa Unell,Olena Zavertiaieva,Sai Praneeth Karimireddy,Tahseen Rabbani,Mary-Anne Hartley,Martin Jaggi*

Main category: cs.LG

TL;DR: DISCO是一个开源分布式协作学习平台，允许非技术用户在不共享原始数据的情况下协作构建机器学习模型，支持联邦学习和去中心化范式，提供隐私保护和模型个性化功能。


<details>
  <summary>Details</summary>
Motivation: 解决数据共享的隐私、知识产权和法律限制问题，避免数据碎片化导致的统计能力下降和可访问性偏见，使机器学习模型能够更公平地分布。

Method: 开发基于浏览器的Web应用程序，在本地训练模型，采用模块化设计支持联邦学习和去中心化范式，提供不同级别的隐私保证和权重聚合策略。

Result: 创建了开源的DISCO平台，代码库和展示界面已公开，支持跨平台使用（包括智能手机），无需编程知识即可使用。

Conclusion: DISCO为非技术用户提供了一种安全、可访问的协作机器学习解决方案，能够在不共享原始数据的情况下构建模型，促进更公平的AI发展。

Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai

</details>


### [56] [Geometry of Decision Making in Language Models](https://arxiv.org/abs/2511.20315)
*Abhinav Joshi,Divyanshu Bhatt,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文通过内在维度分析LLM在多项选择题任务中的决策过程，发现模型在早期层使用低维流形，中间层扩展维度，后期层压缩维度并收敛到决策相关表示。


<details>
  <summary>Details</summary>
Motivation: 研究LLM内部决策过程的不透明性，通过几何视角理解其泛化和推理能力。

Method: 使用28个开源transformer模型，通过多个估计器估计各层的内在维度，并量化每层在MCQA任务上的性能。

Result: 发现一致的ID模式：早期层低维操作，中间层维度扩展，后期层维度压缩并收敛到决策相关表示。

Conclusion: LLM隐式学习将语言输入投影到与任务决策对齐的结构化低维流形上，为语言模型的泛化和推理能力提供了新的几何视角。

Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.

</details>


### [57] [When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements](https://arxiv.org/abs/2511.19794)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 论文提出了一种基于配对多轮运行、BCa自助法置信区间和符号翻转置换检验的保守评估协议，用于在有限计算预算下准确评估机器学习模型1-2%的性能提升是否具有统计显著性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习论文中报告的1-2%性能提升通常缺乏不确定性估计和显著性检验，这些增益对随机种子、数据排序和实现细节高度敏感，难以区分真实算法改进与噪声。

Method: 提出配对多轮运行评估协议，使用BCa自助法置信区间和符号翻转置换检验对每轮种子差异进行统计检验，该协议设计为保守型，旨在防止过度声称。

Result: 在CIFAR-10、CIFAR-10N和AG News数据集上的实验显示，单次运行和非配对t检验经常错误地将0.6-2.0点的改进报告为显著，而使用三个种子的配对协议在这些设置下从未声明显著性。

Conclusion: 对于有限计算预算下的小幅性能提升，这种保守评估方法是更安全的默认选择，能够有效防止过度声称虚假改进。

Abstract: Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.
  We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.
  We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.

</details>


### [58] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: SAPO是一种软自适应策略优化方法，通过温度控制的平滑门机制替代硬裁剪，在保持序列级一致性的同时自适应地衰减离策略更新，提高训练稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GSPO和GRPO）使用硬裁剪来解决token级重要性比率高方差问题，但难以同时保持稳定性和有效学习。

Method: 提出Soft Adaptive Policy Optimization (SAPO)，用平滑的温度控制门替代硬裁剪，形成连续信任区域，自适应地衰减离策略更新同时保留有用学习信号。

Result: 在数学推理基准测试中，SAPO在相同训练预算下表现出更好的训练稳定性和更高的Pass@1性能。应用于Qwen3-VL模型系列时，在不同任务和模型大小上均获得一致性能提升。

Conclusion: SAPO为LLMs的强化学习训练提供了更可靠、可扩展和有效的优化策略。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [59] [Terminal Velocity Matching](https://arxiv.org/abs/2511.19797)
*Linqi Zhou,Mathias Parger,Ayaan Haque,Jiaming Song*

Main category: cs.LG

TL;DR: TVM是一种流匹配的泛化方法，支持高保真度的单步和少步生成建模，通过在终端时间而非初始时间进行正则化，实现了在ImageNet数据集上的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统流匹配方法在初始时间进行正则化，TVM旨在通过终端时间正则化改进生成模型性能，特别是在单步和少步生成场景下。

Method: TVM建模任意两个扩散时间步之间的转换，引入最小架构变更实现稳定单阶段训练，并开发了支持Jacobian-Vector Products反向传播的融合注意力核。

Result: 在ImageNet-256x256上，TVM单步生成FID为3.29，4步生成FID为1.99；在ImageNet-512x512上，单步FID为4.32，4步FID为2.94，均达到最先进水平。

Conclusion: TVM通过终端速度匹配和架构优化，实现了单步和少步生成建模的突破性性能，为高效生成模型提供了新思路。

Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.

</details>


### [60] [Cisco Time Series Model Technical Report](https://arxiv.org/abs/2511.19841)
*Liang Gou,Archit Khare,Praneet Pabolu,Prachi Patel,Joseph Ross,Hercy Shen,Yuhan,Song,Jingze Sun,Kristal Curtis,Vedant Dharnidharka,Abhinav Mathur,Hao Yang*

Main category: cs.LG

TL;DR: Cisco Time Series Model是一个单变量零样本预测器，通过多分辨率输入架构创新，在超过300B数据点上训练，在可观测性数据集上表现优异，同时保持通用预测基准的相似性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多分辨率输入的时间序列基础模型，特别是在可观测性领域提供更好的预测性能。

Method: 对流行的仅解码器时间序列模型(TimesFM)进行架构创新，使其能够接受多分辨率输入，并在超过300B个独特数据点上训练，其中一半以上来自可观测性领域。

Result: 模型在可观测性数据集上实现卓越性能，在通用预测基准(GIFT-Eval)上保持相似性能，多分辨率结构使模型在长上下文输入上做出更准确的预测。

Conclusion: 多分辨率仅解码器模型架构创新成功创建了一个强大的时间序列基础模型，在保持通用性能的同时，在可观测性领域表现优异。

Abstract: We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.

</details>


### [61] [Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization](https://arxiv.org/abs/2511.19851)
*Kun Guo,Xuefei Li,Xijun Wang,Howard H. Yang,Wei Feng,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出了一种混合分割与联邦学习（HSFL）框架，通过联合优化学习模式选择、批处理大小以及通信和计算资源，显著加速分布式学习收敛速度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）支持低延迟并行训练但模型精度较低，而分割学习（SL）精度高但延迟大。为了结合两者的优势，需要开发能够智能选择学习模式并优化相关参数的混合方法。

Method: 首先分析收敛性，揭示学习模式与批处理大小之间的相互作用；然后制定延迟最小化问题，提出两阶段解决方案：使用块坐标下降法求解松弛问题获得局部最优解，再通过舍入算法恢复整数批处理大小。

Result: 实验结果表明，该方法相比现有方法能显著加速达到目标精度的收敛过程。

Conclusion: HSFL框架通过联合优化学习模式、批处理大小和资源分配，有效平衡了精度和延迟，实现了更快的分布式学习性能。

Abstract: Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.

</details>


### [62] [Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms](https://arxiv.org/abs/2511.19893)
*Shuoyan Xu,Yu Zhang,Eric J. Miller*

Main category: cs.LG

TL;DR: 本文提出FACT框架，使用Transformer建模网约车司机的空闲行为，结合因果掩码和司机特定嵌入，在生存分析中表现优于传统和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 网约车平台具有高频、行为驱动的特点，但生存分析在司机行为建模中的应用尚未充分探索，需要开发能捕捉长期时间依赖性和潜在异质性的方法。

Method: 将空闲行为建模为循环生存过程，提出基于Transformer的FACT框架，使用因果掩码捕捉长期时间依赖，并整合司机特定嵌入来建模潜在异质性。

Result: 在多伦多网约车数据上的实验表明，FACT在时间相关C指数和Brier分数上均优于经典和深度学习生存模型。

Conclusion: 该方法能提供更准确的风险估计，支持平台留存策略，并为政策制定提供相关见解。

Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.

</details>


### [63] [Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning](https://arxiv.org/abs/2511.19941)
*Shenjun Zhong,Zhifeng Chen,Zhaolin Chen*

Main category: cs.LG

TL;DR: 本文提出了一种强化学习框架来优化磁共振指纹成像中的翻转角调度，通过非周期性模式增强指纹可分离性，并可能减少重复时间以加速采集。


<details>
  <summary>Details</summary>
Motivation: 磁共振指纹成像依赖于可调采集参数产生的瞬态信号动态，而优化翻转角等关键参数是一个复杂的高维序列决策问题。强化学习为自动参数选择提供了有前景的方法。

Method: 引入强化学习框架来优化磁共振指纹成像中的翻转角调度，通过RL算法自动选择参数以最大化参数空间中指纹的可区分性。

Result: 学习到的调度表现出非周期性模式，增强了指纹可分离性，并且RL优化的调度可能减少重复时间，从而加速磁共振指纹成像采集。

Conclusion: 强化学习框架成功优化了磁共振指纹成像中的翻转角调度，展示了非周期性模式的优势，并为加速采集提供了潜在可能性。

Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.

</details>


### [64] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文针对强化学习微调大语言模型导致的多样性崩溃问题，提出了理论分析和解决方案。通过证明RL微调中的选择和强化偏置导致多样性崩溃，作者提出了差分平滑方法，在正确轨迹上应用奖励修正，在理论和实验上都优于传统方法和基于熵的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有解决RL微调多样性崩溃的启发式方法存在随意性，经常在正确性和多样性之间权衡取舍，效果因任务而异，有时甚至相互矛盾。需要建立理论基础并提出更原则性的解决方案。

Method: 首先形式化证明RL微调导致多样性崩溃的原因（选择和强化偏置），然后提出差分平滑方法，仅在正确轨迹上应用奖励修正来同时提升正确性和多样性。

Result: 在1B到7B参数的模型上，在CountDown和真实世界数学推理等多个领域进行广泛实验，差分平滑方法在Pass@1和Pass@k指标上均取得一致提升，在AIME24数据集上最高提升6.7%。

Conclusion: 差分平滑方法在理论和实验上都优于传统RL方法和基于熵的启发式方法，能够同时提升正确性和多样性，解决了RL微调中的多样性崩溃问题。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [65] [Prompt Fairness: Sub-group Disparities in LLMs](https://arxiv.org/abs/2511.19956)
*Meiyu Zhong,Noel Teku,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型中的提示公平性问题，发现不同用户/风格的提示语即使询问相同问题，也会引发LLM的不同响应。作者提出信息论指标量化这种偏差，并通过多数投票和提示中性化等干预措施来减轻差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同用户/风格的提示下会产生质量差异显著的响应，这反映了模型行为中的结构性不平等问题，需要量化并解决这种提示公平性问题。

Method: 使用信息论指标捕获偏差的两个维度：子组敏感性和跨组一致性；提出多数投票和提示中性化等实际干预措施来减轻差异。

Result: 实验显示人口统计子组间存在明显的提示敏感性差异：缓解前跨组差异值达0.28，通常在0.14-0.22范围内；应用缓解策略后，差异持续减小，最大差距降至0.22，许多距离降至0.17或以下。

Conclusion: 通过提出的缓解策略，可以显著改善模型在不同用户群体间的响应稳定性和公平性，使输出在不同子组间更加稳定和一致。

Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.

</details>


### [66] [ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models](https://arxiv.org/abs/2511.19959)
*Yujia Wang,Yuanpu Cao,Jinghui Chen*

Main category: cs.LG

TL;DR: ParaBlock是一种用于联邦学习大语言模型的高效通信方法，通过并行通信和计算线程提升通信效率，同时保持与标准联邦块坐标下降方法相同的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，联邦学习中的单个模型块可能包含大量参数，导致通信延迟显著，特别是对于资源受限的客户端。需要解决联邦训练/微调大语言模型时的通信效率挑战。

Method: 提出ParaBlock方法，建立两个并行线程进行通信和计算，以增强通信效率。该方法在理论上证明能达到与标准联邦块坐标下降方法相同的收敛速率。

Result: 在通用指令跟随和数学推理任务上对大语言模型进行微调的实证评估表明，ParaBlock不仅保持了强大的性能，还显著提高了通信效率。

Conclusion: ParaBlock是一种有效的解决方案，能够在联邦学习环境中高效训练大语言模型，同时保持模型性能并显著降低通信开销。

Abstract: Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.

</details>


### [67] [Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.19966)
*Yujia Wang,Fenglong Ma,Jinghui Chen*

Main category: cs.LG

TL;DR: FedEcho是一个新颖的异步联邦学习框架，通过不确定性感知蒸馏来应对异步延迟和数据异构性带来的挑战，有效缓解过时更新和客户端偏差问题。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习虽然提高了效率和可扩展性，但面临过时更新和快速客户端主导学习过程的偏差问题，现有方法通常只能解决其中一个问题，造成冲突。

Method: 提出FedEcho框架，采用不确定性感知蒸馏技术，服务器评估滞后客户端预测的可靠性，根据估计的不确定性动态调整这些预测的影响。

Result: 通过广泛实验证明，FedEcho在异步联邦学习基准测试中持续优于现有方法，实现了稳健性能，且无需访问私有客户端数据。

Conclusion: FedEcho通过不确定性感知蒸馏有效缓解了异步联邦学习中的过时更新和数据异构性问题，为异步联邦学习提供了更稳健的解决方案。

Abstract: Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.

</details>


### [68] [Operator Learning at Machine Precision](https://arxiv.org/abs/2511.19980)
*Aras Bacho,Aleksei G. Sorokin,Xianjin Yang,Théo Bourdais,Edoardo Calvello,Matthieu Darcy,Alexander Hsu,Bamdad Hosseini,Houman Owhadi*

Main category: cs.LG

TL;DR: CHONKNORIS是一种新的算子学习方法，通过回归Tikhonov正则化牛顿-康托罗维奇更新的椭圆算子的Cholesky因子，实现了机器精度的非线性PDE求解。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子学习方法在增加复杂度时无法显著提高精度，与更简单的核方法和降阶模型相当，需要解决这一局限性。

Method: 基于数值分析中的牛顿类方法，回归Cholesky因子而非直接回归解算子，通过展开迭代获得神经架构，实现收缩映射。

Result: 在非线性椭圆方程、Burgers方程、非线性达西流问题、Calderón问题、逆波散射问题和地震成像等多个问题上进行了基准测试。

Conclusion: CHONKNORIS能够实现机器精度，并提出了基础模型变体FONKNORIS，能够准确求解未见过的非线性PDE如Klein-Gordon和Sine-Gordon方程。

Abstract: Neural operator learning methods have garnered significant attention in scientific computing for their ability to approximate infinite-dimensional operators. However, increasing their complexity often fails to substantially improve their accuracy, leaving them on par with much simpler approaches such as kernel methods and more traditional reduced-order models. In this article, we set out to address this shortcoming and introduce CHONKNORIS (Cholesky Newton--Kantorovich Neural Operator Residual Iterative System), an operator learning paradigm that can achieve machine precision. CHONKNORIS draws on numerical analysis: many nonlinear forward and inverse PDE problems are solvable by Newton-type methods. Rather than regressing the solution operator itself, our method regresses the Cholesky factors of the elliptic operator associated with Tikhonov-regularized Newton--Kantorovich updates. The resulting unrolled iteration yields a neural architecture whose machine-precision behavior follows from achieving a contractive map, requiring far lower accuracy than end-to-end approximation of the solution operator. We benchmark CHONKNORIS on a range of nonlinear forward and inverse problems, including a nonlinear elliptic equation, Burgers' equation, a nonlinear Darcy flow problem, the Calderón problem, an inverse wave scattering problem, and a problem from seismic imaging. We also present theoretical guarantees for the convergence of CHONKNORIS in terms of the accuracy of the emulated Cholesky factors. Additionally, we introduce a foundation model variant, FONKNORIS (Foundation Newton--Kantorovich Neural Operator Residual Iterative System), which aggregates multiple pre-trained CHONKNORIS experts for diverse PDEs to emulate the solution map of a novel nonlinear PDE. Our FONKNORIS model is able to accurately solve unseen nonlinear PDEs such as the Klein--Gordon and Sine--Gordon equations.

</details>


### [69] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 提出一种按需多任务稀疏框架，通过最大化参数重用最小化任务切换开销，在真实自动驾驶平台上实现6.6倍的任务切换加速


<details>
  <summary>Details</summary>
Motivation: 传统稀疏方法在单独优化各任务时忽略了频繁任务切换带来的显著I/O开销，需要专门设计来最小化切换成本

Method: 将权重分解为可重用的块粒度单元，跨任务对齐稀疏结构以最大化重叠，动态加载下一任务所需的小差分块集

Result: 在真实自动驾驶平台上，相比现有稀疏方法平均加速任务切换超过6.6倍

Conclusion: 该框架通过参数重用和动态块加载有效缓解了传统整体方法的冷启动延迟，实现了卓越的切换效率

Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.

</details>


### [70] [RankOOD - Class Ranking-based Out-of-Distribution Detection](https://arxiv.org/abs/2511.19996)
*Dishanika Denipitiyage,Naveen Karunanayake,Suranga Seneviratne,Sanjay Chawla*

Main category: cs.LG

TL;DR: RankOOD是一种基于排序的离群检测方法，通过Placket-Luce损失训练模型，利用ID类预测中的排序模式来检测OOD样本。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习中交叉熵损失训练的模型，在分布内类预测中会诱导出特定的排序模式，可以利用这种模式来区分分布外样本。

Method: 首先使用初始分类器为每个类提取排序列表，然后使用Plackett-Luce损失进行第二轮训练，将类排序作为预测变量。

Result: 在近OOD TinyImageNet评估基准上达到最先进性能，将FPR95降低了4.3%。

Conclusion: RankOOD框架通过利用类预测中的排序模式，有效提升了OOD检测性能。

Abstract: We propose RankOOD, a rank-based Out-of-Distribution (OOD) detection approach based on training a model with the Placket-Luce loss, which is now extensively used for preference alignment tasks in foundational models. Our approach is based on the insight that with a deep learning model trained using the Cross Entropy Loss, in-distribution (ID) class prediction induces a ranking pattern for each ID class prediction. The RankOOD framework formalizes the insight by first extracting a rank list for each class using an initial classifier and then uses another round of training with the Plackett-Luce loss, where the class rank, a fixed permutation for each class, is the predicted variable. An OOD example may get assigned with high probability to an ID example, but the probability of it respecting the ranking classification is likely to be small. RankOOD, achieves SOTA performance on the near-ODD TinyImageNet evaluation benchmark, reducing FPR95 by 4.3%.

</details>


### [71] [REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems](https://arxiv.org/abs/2511.19998)
*Nikit Phadke*

Main category: cs.LG

TL;DR: REWA提出了一种基于见证重叠结构的通用相似性理论，证明只要概念间的相似性可以表示为单调的见证重叠，就能通过紧凑编码实现可证明的排名保持。


<details>
  <summary>Details</summary>
Motivation: 统一各种相似性度量方法（如图邻域、因果关系、时间结构、拓扑特征等），为相似性系统提供理论基础，减少对哈希函数工程的依赖。

Method: 构建REWA系统：有限见证集、半随机比特分配、重叠单调性；在重叠间隙条件下，使用O(log(|V|/δ))比特保持top-k排名。

Result: 证明了在见证重叠条件下，相似性排名可以被紧凑编码保持；该框架统一了Bloom过滤器、minhash、LSH等多种方法。

Conclusion: REWA为相似性系统提供了基于见证重叠的模块化设计原则，支持从可重用原语组合构建相似性定义，具有广泛的应用前景。

Abstract: REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Δ(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/δ))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.

</details>


### [72] [Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting](https://arxiv.org/abs/2511.20004)
*Peining Zhang,Hongchen Qin,Haochen Zhang,Ziqi Guo,Guiling Wang,Jinbo Bi*

Main category: cs.LG

TL;DR: 研究发现，在零样本设置下，Sundial基础模型在输入上下文窗口足够长（覆盖超过1-2个完整季节周期）时，能够超越完全训练的LSTM模型，这是首次证明通用基础模型无需任务特定调优即可在遥感时间序列预测上超越专门监督模型。


<details>
  <summary>Details</summary>
Motivation: 研究时间序列基础模型在农业监测中叶片面积指数（LAI）预测的零样本预测能力，探索预训练模型在农业和环境应用中的潜力。

Method: 使用HiQ数据集（美国，2000-2022），系统比较统计基线、完全监督的LSTM和Sundial基础模型，采用多种评估协议进行对比分析。

Result: Sundial在零样本设置下，当输入上下文窗口覆盖超过1-2个完整季节周期时，能够超越完全训练的LSTM模型。

Conclusion: 预训练的时间序列基础模型在农业和环境应用中具有作为即插即用预测器的强大潜力，无需任务特定调优即可实现优异的预测性能。

Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.

</details>


### [73] [iRadioDiff: Physics-Informed Diffusion Model for Indoor Radio Map Construction and Localization](https://arxiv.org/abs/2511.20015)
*Xiucheng Wang,Tingwei Yuan,Yang Cao,Nan Cheng,Ruijin Sun,Weihua Zhuang*

Main category: cs.LG

TL;DR: iRadioDiff是一个基于扩散模型的室内无线电地图构建框架，通过物理信息提示和多路径关键先验来生成高保真度的电磁场分布，在室内定位任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统电磁求解器构建高保真室内无线电地图延迟过高，而基于学习的方法依赖稀疏测量或同质材料假设，无法准确建模室内环境的异质性和多路径特性。

Method: 提出采样自由的扩散框架iRadioDiff，通过接入点位置、材料反射和透射系数等物理信息提示，结合衍射点、强透射边界和视距轮廓等多路径关键先验，使用条件通道和边界加权目标指导生成过程。

Result: 实验表明iRadioDiff在室内无线电地图构建和基于接收信号强度的室内定位任务中达到最先进性能，并在不同布局和材料配置下具有良好的泛化能力。

Conclusion: iRadioDiff能够准确建模非平稳场不连续性，高效构建物理一致的无线电地图，为室内环境感知提供有效解决方案。

Abstract: Radio maps (RMs) serve as environment-aware electromagnetic (EM) representations that connect scenario geometry and material properties to the spatial distribution of signal strength, enabling localization without costly in-situ measurements. However, constructing high-fidelity indoor RMs remains challenging due to the prohibitive latency of EM solvers and the limitations of learning-based methods, which often rely on sparse measurements or assumptions of homogeneous material, which are misaligned with the heterogeneous and multipath-rich nature of indoor environments. To overcome these challenges, we propose iRadioDiff, a sampling-free diffusion-based framework for indoor RM construction. iRadioDiff is conditioned on access point (AP) positions, and physics-informed prompt encoded by material reflection and transmission coefficients. It further incorporates multipath-critical priors, including diffraction points, strong transmission boundaries, and line-of-sight (LoS) contours, to guide the generative process via conditional channels and boundary-weighted objectives. This design enables accurate modeling of nonstationary field discontinuities and efficient construction of physically consistent RMs. Experiments demonstrate that iRadioDiff achieves state-of-the-art performance in indoor RM construction and received signal strength based indoor localization, which offers effective generalization across layouts and material configurations. Code is available at https://github.com/UNIC-Lab/iRadioDiff.

</details>


### [74] [Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering](https://arxiv.org/abs/2511.20030)
*Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种针对多模态属性图（MMAGs）的聚类方法DGF，通过双重图滤波和三交叉对比学习策略，有效解决了现有多视图聚类方法在处理大预训练模型输出的多模态属性时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法过度依赖视图间的高相关性，忽视了多模态属性中存在的低模态相关性和强烈特征噪声问题，导致聚类性能不佳。

Method: 提出双重图滤波（DGF）方案，包含特征级去噪组件和三交叉对比训练策略，通过跨模态、跨邻域和跨社区的实例级对比学习来学习鲁棒且可区分的节点表示。

Result: 在8个基准MMAG数据集上的综合实验表明，DGF在聚类质量方面能够一致且显著地优于多种最先进的基线方法。

Conclusion: DGF方法通过创新的图滤波和对比学习机制，有效克服了传统多视图图聚类方法的局限性，为多模态属性图聚类提供了更优的解决方案。

Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.

</details>


### [75] [RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction](https://arxiv.org/abs/2511.20044)
*PengYu Chen,Xiaohou Shi,Yuan Chang,Yan Sun,Sajal K. Das*

Main category: cs.LG

TL;DR: RED-F是一个基于重构消除的双流对比预测框架，用于多变量时间序列中的异常预测。它通过重构消除模型生成纯净的正常模式基线，然后通过双流对比预测模型对比原始序列和纯净基线的预测轨迹来放大微弱的前兆信号。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法在训练时仅使用正常数据，倾向于重构正常模式，导致在面对微弱异常前兆时，预测被正常模式主导，淹没了用于预测的关键信号。

Method: 提出RED-F框架，包含重构消除模型（REM）和双流对比预测模型（DFM）。REM使用时频混合机制消除前兆信号，生成纯净基线；DFM接收原始序列和纯净基线作为并行输入，通过对比两个预测流的差异来放大前兆信号，并使用多序列预测目标增强预测敏感性。

Result: 在六个真实世界数据集上的广泛实验表明，RED-F在异常预测任务中具有卓越的能力。

Conclusion: RED-F通过重构消除和双流对比预测机制，成功地将困难的绝对信号检测任务转化为更简单、更鲁棒的相对轨迹比较任务，有效放大了微弱的前兆信号，提升了异常预测性能。

Abstract: The proactive prediction of anomalies (AP) in mul- tivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction- Elimination based Dual-stream Contrastive Forecasting frame- work, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future con- text to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.

</details>


### [76] [SOMBRL: Scalable and Optimistic Model-Based RL](https://arxiv.org/abs/2511.20066)
*Bhavya Sukhija,Lenart Treven,Carmelo Sferrazza,Florian Dörfler,Pieter Abbeel,Andreas Krause*

Main category: cs.LG

TL;DR: 提出了SOMBRL方法，一种基于乐观不确定性原则的模型强化学习算法，通过结合外部奖励和认知不确定性进行探索，在多种设置下具有次线性遗憾性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习中未知系统动态下的高效探索挑战，需要直接从在线交互中学习。

Method: 学习不确定性感知的动态模型，贪婪地最大化外部奖励和智能体认知不确定性的加权和，兼容任何策略优化器或规划器。

Result: 在状态和视觉控制环境中表现出色，在动态RC汽车硬件上优于现有最优方法，展示了基于原则探索的优势。

Conclusion: SOMBRL为基于模型的强化学习提供了灵活、可扩展且基于原则的探索解决方案，在理论和实践中都表现出色。

Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

</details>


### [77] [QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression](https://arxiv.org/abs/2511.20099)
*Lei Huang,Rui Zhang,Jiaming Guo,Yang Zhang,Di Huang,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 本文提出CRUX结构化中间表示空间，通过两阶段训练框架CRUX-V，将模糊的自然语言描述转化为精确的Verilog代码生成，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言描述的硬件描述语言生成方法存在模糊、冗余和非结构化问题，难以实现精确的Verilog代码生成。

Method: 引入CRUX结构化中间空间，设计包含联合表达建模和双空间优化的两阶段训练框架CRUX-V。

Result: 在多个Verilog生成基准测试中达到最先进性能，特别是在复杂设计任务下表现优异，且CRUX空间对其他代码模型具有可迁移性。

Conclusion: CRUX空间有效缩小了自由形式自然语言描述与精确Verilog生成之间的差距，为硬件代码生成提供了结构化解决方案。

Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.

</details>


### [78] [Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives](https://arxiv.org/abs/2511.20105)
*Grzegorz Dudek,Mateusz Kasprzyk,Paweł Pełka*

Main category: cs.LG

TL;DR: 本研究应用LightGBM模型对比特币实现波动率进行确定性和概率性预测，使用69个市场、行为和宏观经济指标作为预测因子，并与计量经济学和机器学习基准模型进行比较。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场具有高度非线性和高方差特性，传统模型难以有效捕捉其波动规律，需要探索更先进的机器学习方法来提高波动率预测精度。

Method: 采用LightGBM模型进行预测，对于概率性预测探索了两种分位数方法：直接使用pinball损失函数的分位数回归，以及将点预测转换为预测分布的残差模拟方法。使用增益和排列特征重要性技术识别主要驱动因素。

Result: LGBM模型在捕捉加密货币市场的非线性和高方差特性方面表现优异，交易量、滞后波动率指标、投资者关注度和市值被识别为波动率的主要驱动因素。

Conclusion: LGBM模型能够有效预测比特币波动率，同时提供对潜在波动动态的可解释性见解，为加密货币市场风险管理提供有力工具。

Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.

</details>


### [79] [On the Limits of Momentum in Decentralized and Federated Optimization](https://arxiv.org/abs/2511.20168)
*Riccardo Zaccone,Sai Praneeth Karimireddy,Carlo Masone*

Main category: cs.LG

TL;DR: 本文分析了联邦学习中动量方法在循环客户端参与模式下的表现，理论上证明了动量仍然无法避免统计异质性的影响，与SGD类似，任何比Θ(1/t)更快的步长衰减都会导致收敛到依赖于初始化和异质性边界的常数值。


<details>
  <summary>Details</summary>
Motivation: 探索动量方法在联邦学习中的使用，特别是为了缓解统计异质性的影响，研究在去中心化场景下动量是否能保证在无界异质性下的收敛性。

Method: 分析动量在循环客户端参与模式下的表现，进行理论证明和数值实验验证，包括深度学习实验。

Result: 动量方法仍然受到统计异质性的影响，与SGD类似，步长衰减无法解决异质性问题，收敛结果依赖于初始化和异质性边界。

Conclusion: 动量方法在联邦学习中无法完全克服统计异质性的挑战，需要进一步研究其他方法来处理异质性问题。

Abstract: Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Θ\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.

</details>


### [80] [Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling](https://arxiv.org/abs/2511.20257)
*Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger*

Main category: cs.LG

TL;DR: 提出了一种物理引导、可解释的时空学习框架，用于空气污染预测，该模型将污染物浓度的时空行为分解为两个透明可加的模块，在斯德哥尔摩地区数据集上表现优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 准确的空气污染预测对公共健康至关重要，但大多数模型在性能和可解释性之间存在权衡。本研究旨在开发一个既高性能又可解释的预测框架。

Method: 模型将污染物浓度的时空行为分解为两个模块：物理引导的传输核（基于风和地理条件的定向权重）和可解释的注意力机制（学习局部响应并将未来浓度归因于特定历史滞后和外生驱动因素）。

Result: 在斯德哥尔摩地区综合数据集上的评估显示，该模型在多个预测时间范围内始终优于最先进的基准方法。

Conclusion: 该模型将高预测性能与时空可解释性相结合，为实际应用中的空气质量管理提供了更可靠的基础。

Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.

</details>


### [81] [Learning Subgroups with Maximum Treatment Effects without Causal Heuristics](https://arxiv.org/abs/2511.20189)
*Lincen Yang,Zhong Li,Matthijs van Leeuwen,Saber Salehkaleybar*

Main category: cs.LG

TL;DR: 本文提出在结构因果模型框架下直接解决最优亚组发现问题，证明在基于分区的模型假设下，最优亚组发现可简化为标准监督学习问题，并基于CART方法实现，避免了传统方法中的因果启发式规则。


<details>
  <summary>Details</summary>
Motivation: 现有亚组发现方法存在两个主要问题：一是需要准确估计点状条件处理效应，将亚组发现问题转化为更难的准确点状估计问题；二是使用缺乏严格理论依据的因果启发式规则。本文旨在直接在结构因果模型框架下解决这些问题。

Method: 在基于分区的结构因果模型假设下，将最优亚组发现问题转化为数据生成模型恢复问题，从而简化为标准监督学习问题。具体采用CART树方法学习具有最大处理效应的亚组。

Result: 在大量合成和半合成数据集上的实验表明，该方法比多种基线方法更准确地识别出具有最大处理效应的亚组，避免了因果启发式规则的使用。

Conclusion: 在结构因果模型框架下，最优亚组发现问题可以简化为标准监督学习问题，基于CART的方法能够有效识别最大处理效应亚组，且优于传统启发式方法。

Abstract: Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.

</details>


### [82] [In-Context Compositional Learning via Sparse Coding Transformer](https://arxiv.org/abs/2511.20194)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏编码原理的注意力机制改进方法，通过将输入映射到编码字典和输出映射到解码字典，增强Transformer在组合学习任务中的能力。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理组合学习任务时存在局限性，因为它们并非专门为处理组合任务而设计，且提供的结构归纳偏置有限。本文旨在通过稀疏编码原理改进注意力机制，提升模型在组合任务中的表现。

Method: 重新解释注意力块为通过投影到两组学习字典原子（编码字典和解码字典）将输入映射到输出。编码字典将输入分解为一组系数，表示输入的组合结构，并对这些系数施加稀疏性。稀疏系数用于线性组合解码字典原子以生成输出。

Result: 在S-RAVEN和RAVEN数据集上的实验表明，该方法在组合泛化任务中有效。对于某些组合泛化任务，即使标准Transformer失败，该方法仍能保持性能。

Conclusion: 基于稀疏编码的注意力改进能够增强Transformer学习并应用组合规则的能力，在组合学习任务中表现出色。

Abstract: Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.

</details>


### [83] [HVAdam: A Full-Dimension Adaptive Optimizer](https://arxiv.org/abs/2511.20277)
*Yiheng Zhang,Shaowu Wu,Yuanzhuo Xu,Jiajun Wu,Shang Xu,Steve Drew,Xiaoguang Niu*

Main category: cs.LG

TL;DR: 本文提出了Anon优化器，通过可调节的自适应机制在SGD和Adam之间插值甚至超越两者，解决了自适应优化器泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器（如Adam）在大规模模型训练中表现出色，但在经典架构（如CNN）上泛化能力不如非自适应方法（如SGD）。研究发现自适应预调节器限制了优化器适应不同优化环境的能力。

Method: 提出Anon优化器，具有连续可调的自适应特性，能够插值SGD和Adam的行为。引入增量延迟更新（IDU）机制，比AMSGrad的硬最大跟踪策略更灵活，增强对梯度噪声的鲁棒性。

Result: 理论上在凸和非凸设置下建立了收敛保证。实验表明Anon在图像分类、扩散模型和语言建模任务上持续优于最先进的优化器。

Conclusion: 自适应可以作为有价值的可调设计原则，Anon提供了第一个统一可靠的框架，能够弥合经典和现代优化器之间的差距，并超越它们的优势特性。

Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.

</details>


### [84] [Communication-Efficient Learning for Satellite Constellations](https://arxiv.org/abs/2511.20220)
*Ruxandra-Stefania Tudose,Moritz H. W. Grüss,Grace Ra Kim,Karl H. Johansson,Nicola Bastianello*

Main category: cs.LG

TL;DR: 本文提出了一种用于低地球轨道卫星星座的联邦学习方法，通过本地训练、压缩和误差反馈机制来减少通信开销，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座的普及，需要解决在这些卫星上执行学习任务的问题，特别是如何在地面站和卫星之间进行高效的联邦学习。

Method: 采用联邦学习方法，卫星收集并本地处理数据，地面站聚合本地模型。设计了通信效率高的算法，包括本地训练减少通信次数、压缩减少通信大小，以及误差反馈机制提高准确性。

Result: 通过仿真实验验证了所提算法在真实空间场景中的优越性能，相比现有技术表现更好。

Conclusion: 提出的联邦学习算法能够在卫星星座环境中有效减少通信开销，同时保持模型准确性，误差反馈机制还可广泛应用于其他算法。

Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.

</details>


### [85] [Short-Range Oversquashing](https://arxiv.org/abs/2511.20406)
*Yaaqov Mishayev,Yonatan Sverdlov,Tal Amir,Nadav Dym*

Main category: cs.LG

TL;DR: 本文发现MPNN中的过度压缩问题不仅出现在长距离任务中，也会出现在短距离任务中，揭示了过度压缩的两个不同机制：瓶颈现象和梯度消失现象。研究表明虚拟节点无法解决短距离瓶颈问题，而Transformer是更有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: MPNN在图学习中广泛应用，但其处理长距离信息的能力受到过度压缩现象的限制。虽然有些研究者提倡使用图Transformer作为替代方案，但其他人认为可以通过虚拟节点等技术在MPNN框架内缓解这一问题。本文旨在深入理解过度压缩现象的本质。

Method: 通过分析短距离任务中的过度压缩现象，区分了过度压缩的两个不同机制：瓶颈现象（即使在低范围设置中也会出现）和梯度消失现象（与长距离任务密切相关）。

Result: 研究发现短距离瓶颈效应无法被现有的过度压缩解释所捕捉，添加虚拟节点也无法解决这一问题。相比之下，Transformer在此类任务中表现成功。

Conclusion: Transformer相比专门的MPNN是解决过度压缩问题的更有力方案，因为其能够有效处理短距离瓶颈问题，而现有MPNN改进方法对此无效。

Abstract: Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.
  In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.
  We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.

</details>


### [86] [MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490)
*Kiril Vasilev,Alexandre Misrahi,Eeshaan Jain,Phil F Cheng,Petros Liakopoulos,Olivier Michielin,Michael Moor,Charlotte Bunne*

Main category: cs.LG

TL;DR: MTBBench是一个模拟分子肿瘤委员会决策的基准测试，评估多模态大语言模型在复杂临床环境中的表现，发现现有模型存在幻觉、时序推理困难等问题，并提出增强框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法捕捉真实临床工作流程的复杂性，特别是多智能体决策环境如分子肿瘤委员会，需要整合异构数据和随时间演变的洞察。

Method: 引入MTBBench基准，通过临床挑战性、多模态和纵向肿瘤学问题模拟MTB式决策，使用临床验证的真实标注，并提供基于基础模型的工具框架增强多模态和纵向推理。

Result: 基准测试显示即使大规模LLM也缺乏可靠性，经常产生幻觉，难以处理时序数据推理，无法协调冲突证据或不同模态。提出的框架分别带来9.0%和11.2%的任务级性能提升。

Conclusion: MTBBench为推进多模态LLM推理、可靠性和工具使用提供了一个具有挑战性和现实性的测试平台，特别关注精准肿瘤学中的MTB环境。

Abstract: Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.

</details>


### [87] [MXtalTools: A Toolkit for Machine Learning on Molecular Crystals](https://arxiv.org/abs/2511.20327)
*Michael Kilgour,Mark E. Tuckerman,Jutta Rogal*

Main category: cs.LG

TL;DR: MXtalTools是一个用于分子晶体数据驱动建模的Python软件包，支持机器学习研究，提供数据集合成、模型训练、晶体参数化、结构采样等模块化功能，并利用CUDA加速实现高通量晶体建模。


<details>
  <summary>Details</summary>
Motivation: 为分子固体的机器学习研究提供一个灵活的工具包，解决现有工具在数据驱动建模方面的不足，促进分子晶体研究的发展。

Method: 开发包含多个功能模块的Python软件包：数据集合成与整理、集成工作流、晶体参数化与表示、结构采样与优化、端到端可微分晶体处理，并利用CUDA加速技术。

Result: 成功开发了MXtalTools软件包，提供了模块化的晶体建模功能，支持高通量计算，代码已在GitHub开源并提供详细文档。

Conclusion: MXtalTools为分子晶体的数据驱动建模提供了一个全面且灵活的工具集，其模块化设计便于集成到现有工作流或构建新的建模流程，有望推动分子固体研究的发展。

Abstract: We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.

</details>


### [88] [Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning](https://arxiv.org/abs/2511.20349)
*M. E. A. Kherchouche,F. Galpin,T. Dumas,F. Schnitzler,D. Menard,L. Zhang*

Main category: cs.LG

TL;DR: 本文研究了VVC帧内分区的复杂度问题，提出了两种机器学习方法来加速RDO过程中的穷举搜索。第一种是基于回归的方法预测CU的归一化RD成本，第二种是基于强化学习的方法使用DQN算法学习CU决策轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决VVC帧内分区在RDO过程中涉及的穷举搜索带来的高计算复杂度问题，加速编码过程。

Method: 1. 基于回归的方法：预测CU的归一化RD成本，利用相邻块的RD成本作为输入特征；2. 基于强化学习的方法：将分区决策建模为MDP问题，使用DQN算法学习CU决策轨迹。两种方法都应用预定义阈值来选择合适的分割。

Result: 提出的两种方法都是尺寸无关的，并集成了相邻块的RD成本作为输入特征，相比现有方法具有更好的适应性。

Conclusion: 通过机器学习和强化学习技术可以有效加速VVC帧内分区的RDO过程，提出的两种方法为视频编码优化提供了新的解决方案。

Abstract: In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.

</details>


### [89] [The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting](https://arxiv.org/abs/2511.20601)
*Heman Shakeri*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.

</details>


### [90] [MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers](https://arxiv.org/abs/2511.20382)
*Audrey Pei-Hsuan Chen*

Main category: cs.LG

TL;DR: MoRE是一个多组学表示学习框架，通过重用预训练的transformer模型，将异质多组学数据对齐到共享潜在空间中，使用参数高效微调策略实现跨样本和跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 多组学数据存在极端维度、模态异质性和批次效应等挑战，而预训练transformer在生物序列建模中表现出泛化能力，但在多组学整合中的应用尚未充分探索。

Method: MoRE在冻结的预训练transformer骨干上附加轻量级模态特定适配器和任务自适应融合层，联合优化掩码建模目标与监督对比学习和批次不变对齐损失。

Result: MoRE在批次鲁棒性和生物保守性方面表现优异，同时显著减少了可训练参数数量，在整合保真度、罕见群体检测和模态转移方面优于现有基线。

Conclusion: MoRE为实现通用组学基础模型提供了实用步骤，展示了重用预训练transformer在多组学整合中的潜力。

Abstract: Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.

</details>


### [91] [Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI](https://arxiv.org/abs/2511.20395)
*M. C. Schoppema,B. H. M. van der Velden,A. Hürriyetoğlu,M. D. Klijnstra,E. J. Faassen,A. Gerssen,H. J. van der Fels-Klerx*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习模型来预测荷兰Zeeland河口双壳类软体动物中的河豚毒素污染，识别出日照时间、全球辐射、水温和氯化物浓度是关键环境驱动因素。


<details>
  <summary>Details</summary>
Motivation: 自2012年以来，欧洲温带水域的双壳类软体动物中发现河豚毒素污染，这带来了食品安全风险和经济损失，因此早期预测TTX污染对食品行业和监管机构至关重要。

Method: 开发了一个基于深度学习的可解释模型，输入气象和水文特征，输出TTX污染的存在或缺失。

Result: 结果显示日出时间、日落时间、全球辐射、水温和氯化物浓度对TTX污染贡献最大，表明有效日照时间（由日长和全球辐射代表）是TTX污染的重要驱动因素。

Conclusion: 可解释的深度学习模型识别出日照时间、全球辐射、水温和水氯化物浓度等环境因素与双壳类软体动物中的河豚毒素污染相关，该方法成为减轻海洋毒素风险的有价值工具。

Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.

</details>


### [92] [DiFR: Inference Verification Despite Nondeterminism](https://arxiv.org/abs/2511.20621)
*Adam Karvonen,Daniel Reuter,Roy Rinberg,Luke Marks,Adrià Garriga-Alonso,Keri Warr*

Main category: cs.LG

TL;DR: Token-DiFR是一种通过比较生成token与可信参考实现预测来验证LLM推理输出的方法，使用采样种子同步约束有效输出，能以零额外成本检测推理错误。Activation-DiFR则通过随机正交投影压缩激活值进行高效验证。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理需求增长，需要验证推理过程是否正确执行且未被篡改，但由于数值噪声导致重复运行结果不同，难以区分合法变化与实际问题。

Method: Token-DiFR：使用可信参考实现和相同随机种子进行token级比较；Activation-DiFR：使用随机正交投影压缩激活值为紧凑指纹进行验证。

Result: Token-DiFR在300个输出token内检测4位量化的AUC>0.999；Activation-DiFR仅用2个输出token检测4位量化的AUC>0.999，通信开销比现有方法减少25-75%。

Conclusion: 提出的方法能可靠识别采样错误、模拟bug和模型量化，为可验证推理提供了实用解决方案，并发布了vLLM开源集成以加速部署。

Abstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.

</details>


### [93] [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)
*Wei He,Kai Han,Hang Zhou,Hanting Chen,Zhicheng Liu,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: ROOT是一种鲁棒正交化优化器，通过维度鲁棒的正交化方案和优化鲁棒的近端优化框架，解决了大语言模型训练中的正交化精度维度脆弱性和异常值诱导噪声问题，在噪声和非凸场景下实现了更快的收敛和更好的最终性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型优化面临关键挑战，模型规模扩大加剧了对算法不精确性和训练不稳定性的敏感性。现有基于动量正交化的优化器存在两个关键鲁棒性限制：正交化精度的维度脆弱性和异常值诱导噪声的脆弱性。

Method: 1. 使用自适应牛顿迭代和针对特定矩阵尺寸的细粒度系数开发维度鲁棒的正交化方案，确保在不同架构配置下保持一致的精度；2. 通过近端优化引入优化鲁棒框架，抑制异常值噪声同时保留有意义的梯度方向。

Result: 大量实验表明，ROOT实现了显著改进的鲁棒性，与Muon和基于Adam的优化器相比，具有更快的收敛速度和更优的最终性能，特别是在噪声和非凸场景下。

Conclusion: 这项工作为开发能够处理现代大规模模型训练复杂性的鲁棒精确优化器建立了新范式。

Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.

</details>


### [94] [Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets](https://arxiv.org/abs/2511.20407)
*Kasper Green Larsen,Natascha Schalburg*

Main category: cs.LG

TL;DR: 本文证明了首个基于边界的投票分类器泛化界，该界在假设集大小、边界、具有给定边界的训练点比例、训练样本数量和失败概率之间的权衡上是渐近紧的。


<details>
  <summary>Details</summary>
Motivation: 为投票分类器建立更精确的泛化理论，填补现有理论在边界分析方面的不足。

Method: 采用基于边界的分析方法，推导投票分类器的泛化误差上界。

Result: 得到了一个在多个关键参数（假设集大小、边界值、边界训练点比例、样本数、失败概率）权衡下渐近紧的泛化界。

Conclusion: 该工作为投票分类器提供了理论上更严格的泛化保证，在理论分析和实际应用中具有重要意义。

Abstract: We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.

</details>


### [95] [Diffusion for Fusion: Designing Stellarators with Generative AI](https://arxiv.org/abs/2511.20445)
*Misha Padidar,Teresa Huang,Andrew Giuliani,Marina Spivak*

Main category: cs.LG

TL;DR: 本文提出使用条件扩散模型快速生成具有理想特性的准对称仿星器设计，作为机器学习在仿星器设计中的案例研究。


<details>
  <summary>Details</summary>
Motivation: 仿星器设计通常需要大量计算时间，而机器学习方法可以利用大型优化仿星器数据集来加速设计过程。

Method: 在QUASR数据库上训练条件扩散模型，生成具有特定纵横比和平均旋转变换的准对称仿星器设计。

Result: 生成的仿星器表现出良好性能：准对称性偏差小于5%，且能设计训练中未见特性的仿星器。

Conclusion: 生成模型在仿星器设计中具有广阔应用前景，当前结果距离1%准对称性目标还有提升空间。

Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.

</details>


### [96] [Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2511.20456)
*Shreevanth Krishnaa Gopalakrishnan,Stephen Hailes*

Main category: cs.LG

TL;DR: 本文系统评估了基于CSI的深度学习模型在对抗扰动下的鲁棒性，比较了不同规模模型在多种威胁模型下的表现，发现小模型虽然效率高但在鲁棒性上明显不足，同时验证了物理可实现扰动相比无约束攻击的成功率更低，对抗训练能有效提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着CSI传感系统在人类活动识别和身份检测等应用中的普及，模型决策容易受到细微扰动影响，这给无处不在的传感应用带来了安全和可靠性问题。因此，在无线传感部署到真实环境前，量化理解模型的鲁棒性至关重要。

Method: 建立了一个系统评估框架，比较紧凑的时间自编码器模型与更大的深度架构在三个公共数据集上的表现，评估模型规模、训练机制和物理约束对鲁棒性的影响，涵盖白盒、黑盒/迁移和通用扰动等多种威胁模型。

Result: 实验表明，小模型虽然在干净数据上效率高且性能相当，但鲁棒性明显较差；物理可实现的信号空间扰动相比无约束特征空间攻击显著降低了攻击成功率；对抗训练能缓解这些漏洞，在保持清洁性能的同时提高平均鲁棒准确率。

Conclusion: 这些发现为无线传感向可靠、跨域操作发展提供了鲁棒性评估的量化基准，并为设计安全可信的人类中心传感系统提供了指导原则。

Abstract: Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.
  This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.

</details>


### [97] [NVIDIA Nemotron Parse 1.1](https://arxiv.org/abs/2511.20478)
*Kateryna Chumachenko,Amala Sanjay Deshmukh,Jarno Seppanen,Ilia Karmanov,Chia-Chih Chen,Lukas Voegtle,Philipp Fischer,Marek Wawrzos,Saeid Motiian,Roman Ageev,Kedi Wu,Alexandre Milesi,Maryam Moosaei,Krzysztof Pawelec,Padmavathy Subramanian,Mehrzad Samadi,Xin Yu,Celina Dear,Sarah Stoddard,Jenna Diamond,Jesse Oliver,Leanna Chraghchian,Patrick Skelly,Tom Balough,Yao Xu,Jane Polak Scowcroft,Daniel Korzekwa,Darragh Hanley,Sandip Bhaskar,Timo Roman,Karan Sapra,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron-Parse-1.1是一个轻量级文档解析和OCR模型，相比前代在OCR、Markdown格式、表格解析和图像文本提取方面有显著提升，支持更长输出序列，采用编码器-解码器架构，参数量885M，在公开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 改进前代模型Nemoretriever-Parse-1.0的能力，提供更强大的文档解析功能，包括通用OCR、Markdown格式化、结构化表格解析以及从图片、图表和图表中提取文本的能力。

Method: 采用编码器-解码器架构，包含885M参数，其中语言解码器为256M参数，支持更长的输出序列长度以适应视觉密集文档。

Result: 在公开基准测试中达到竞争性准确率，成为强大的轻量级OCR解决方案。同时发布了Nemotron-Parse-1.1-TC版本，在减少视觉标记长度的情况下提供20%速度提升且质量损失最小。

Conclusion: Nemotron-Parse-1.1是一个高效的轻量级文档解析模型，在多个文档解析任务上表现出色，模型权重已在Huggingface公开发布，并提供了优化的NIM容器和部分训练数据。

Abstract: We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.

</details>


### [98] [DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning](https://arxiv.org/abs/2511.20509)
*Mihaela Hudişteanu,Edwige Cyffers,Nikita P. Kalinin*

Main category: cs.LG

TL;DR: 提出了DP-MicroAdam，一种内存高效且稀疏感知的自适应差分隐私优化器，在多个基准测试中优于现有自适应DP优化器，并与DP-SGD竞争或超越其性能。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器在非隐私训练中是事实标准，但差分隐私训练仍主要使用DP-SGD，需要大量计算和超参数调优。

Method: 开发DP-MicroAdam优化器，具有内存效率和稀疏感知特性，证明其在随机非凸优化中以最优速率收敛。

Result: 在CIFAR-10、大规模ImageNet训练和预训练transformer的私有微调等基准测试中，DP-MicroAdam表现优于现有自适应DP优化器，与DP-SGD竞争或超越。

Conclusion: 自适应优化可以在差分隐私下同时提高性能和稳定性。

Abstract: Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.

</details>


### [99] [Adam Simplified: Bias Correction Simplified](https://arxiv.org/abs/2511.20516)
*Sam Laing,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文对Adam优化器中的偏差校正功能进行了系统研究，发现在最优超参数配置下，偏差校正不会改善最终测试性能，有时甚至有害，除非配合适当的学习率调度。


<details>
  <summary>Details</summary>
Motivation: Adam优化器是现代深度学习的基石，但其各个组件的经验必要性往往被视为理所当然。本文旨在深入探究偏差校正功能的作用，这一功能的贡献至今仍未被充分理解。

Method: 通过在视觉和语言建模任务上进行一系列系统消融实验，重新解释偏差校正为一种隐式学习率调度形式，其行为强烈依赖于平滑超参数β1、β2的选择。

Result: 实验表明，在最优超参数配置下，包含偏差校正不会改善最终测试性能。除非实施适当的学习率调度，否则偏差校正有时会对性能产生负面影响。

Conclusion: 研究结果挑战了偏差校正组件普遍包含的必要性，表明这一功能在特定条件下可能并非必需。

Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

</details>


### [100] [Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.20543)
*Alhasan Abdellatif,Hannah P. Menke,Ahmed H. Elsheikh,Florian Doster,Kamaljit Singh*

Main category: cs.LG

TL;DR: UFNO-FiLM通过引入FiLM层解耦标量输入和空间特征，并使用空间加权损失函数，相比UFNO在预测精度上有显著提升，特别是在地下多相流应用中气体饱和度MAE降低了21%。


<details>
  <summary>Details</summary>
Motivation: UFNO虽然通过并行UNet路径保留了高低频分量，但效率低下地将标量输入作为空间分布场处理，在频域中处理冗余的恒定信号，且标准损失函数未考虑误差敏感性的空间变化。

Method: 提出UFNO-FiLM架构，包含两个关键创新：1）使用FiLM层解耦标量输入和空间特征，避免将恒定信号引入傅里叶变换；2）采用空间加权损失函数，优先学习关键区域。

Result: 在地下多相流实验中，相比UFNO实现了21%的气体饱和度MAE降低，证明了该方法在提高预测精度方面的有效性。

Conclusion: UFNO-FiLM通过解耦标量输入和空间特征，结合空间加权损失函数，显著提升了模型预测性能，为处理复杂物理系统提供了更有效的解决方案。

Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.

</details>


### [101] [E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems](https://arxiv.org/abs/2511.20564)
*Rui Xue,Shichao Zhu,Liang Qin,Guangmou Pan,Yang Song,Tianfu Wu*

Main category: cs.LG

TL;DR: 提出了E2E-GRec端到端训练框架，将GNN训练与推荐系统统一，解决了传统两阶段方法的高计算开销和缺乏联合优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统工业部署采用两阶段流水线：GNN离线预训练生成节点嵌入，然后作为静态特征用于下游推荐系统。这种解耦范式导致高计算开销和缺乏联合优化。

Method: 包含三个关键组件：1) 高效子图采样确保训练可扩展性；2) 图特征自编码器作为辅助自监督任务；3) 两级特征融合机制结合基于Gradnorm的动态损失平衡。

Result: 在离线评估和在线A/B测试中表现优异，相对停留时长提升0.133%，用户跳过视频数量减少0.3171%。

Conclusion: E2E-GRec在多个推荐指标上显著超越传统方法，证明了端到端训练框架的有效性。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.

</details>


### [102] [A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent](https://arxiv.org/abs/2511.20584)
*Shuo Xie,Tianhao Wang,Beining Wu,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文揭示了自适应优化器与归一化最速下降(NSD)之间的紧密联系，分析了它们在凸和非凸设置下的收敛特性，并证明了自适应平滑性条件能够实现Nesterov动量加速，这是标准平滑性在非欧几何中无法实现的。


<details>
  <summary>Details</summary>
Motivation: 研究自适应优化器与归一化最速下降之间的理论联系，探索自适应平滑性在优化算法分析中的重要作用，特别是在非欧几何设置下的优势。

Method: 将自适应平滑性理论扩展到非凸设置，建立自适应平滑性与自适应优化器收敛性的精确对应关系，并引入自适应梯度方差概念进行随机优化分析。

Result: 证明了自适应平滑性能够精确刻画自适应优化器的收敛性，在凸设置下可实现Nesterov动量加速，且自适应梯度方差能够获得维度无关的收敛保证。

Conclusion: 自适应平滑性是理解自适应优化器性能的关键几何条件，在非欧几何设置下提供了优于标准平滑性的理论保证，为优化算法设计提供了新的理论框架。

Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.

</details>


### [103] [Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models](https://arxiv.org/abs/2511.20587)
*Karim Kadry,Abdallah Abdelwahed,Shoaib Goraya,Ajay Manicka,Naravich Chutisilp,Farhad Nezami,Elazer Edelman*

Main category: cs.LG

TL;DR: Anatomica是一个推理时框架，用于生成具有局部几何拓扑控制的多类解剖体素图。通过使用不同维度、位置和形状的立方体控制域来提取相关子结构，并计算可微惩罚函数来引导样本满足目标约束。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够灵活控制解剖结构几何和拓扑特征的框架，以支持合成数据集的合理设计，用于虚拟试验或机器学习工作流程。

Method: 使用立方体控制域提取局部子结构，通过体素矩控制几何特征（大小、形状、位置），通过持久同调控制拓扑特征（连通分量、环、空洞），并在潜在扩散模型中实现神经场解码器部分提取子结构。

Result: Anatomica能够灵活应用于不同的解剖系统，在任意维度和坐标系下组合约束来控制复杂结构。

Conclusion: 该框架实现了对解剖体素图的精确几何和拓扑控制，为合成数据集的生成提供了有效工具。

Abstract: We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.

</details>


### [104] [Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning](https://arxiv.org/abs/2511.20591)
*Charlotte Beylier,Hannah Selder,Arthur Fleig,Simon M. Hofmann,Nico Scherf*

Main category: cs.LG

TL;DR: 本文介绍了注意力导向指标（ATOMs）来研究强化学习智能体在训练过程中注意力的发展，通过在三个不同版本的Pong游戏中测试，发现ATOMs能有效区分不同游戏变体的注意力模式，并观察到注意力发展呈阶段性。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的学习过程除了数学公式外仍缺乏深入理解，需要新的方法来研究其注意力发展过程。

Method: 引入注意力导向指标（ATOMs），在三个设计用于教授不同行为的Pong游戏变体上进行受控实验，并辅以行为评估。

Result: ATOMs成功区分了在不同游戏变体上训练的智能体的注意力模式，这些注意力模式的差异转化为行为差异，且注意力发展呈现一致的阶段性特征。

Conclusion: ATOMs有助于提高对强化学习智能体学习过程的理解，并更好地理解注意力与学习之间的关系。

Abstract: The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.

</details>


### [105] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 本文研究了潜在扩散模型中的记忆效应，发现扩散模型在潜在代码上表现出非均匀记忆，倾向于过拟合解码器回拉度量高失真区域的样本。作者提出了一种基于维度的记忆贡献排序方法，通过移除较少记忆维度来显著提升成员推理攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型反转技术主要关注数据域扩散模型，而忽略了潜在空间生成模型中的编码器/解码器对和相应潜在代码。本文旨在研究潜在扩散模型中记忆效应的非均匀特性及其对隐私风险的影响。

Method: 引入一种原则性方法来按维度对解码器回拉度量的贡献进行排序，识别对记忆贡献最大的维度。在计算基于分数的成员推理攻击统计量时，移除较少记忆的维度。

Result: 实验表明，该方法在CIFAR-10、CelebA、ImageNet-1K等多个数据集上显著提升成员推理攻击性能，平均AUROC增益达2.7%，TPR@1%FPR提升6.42%。

Conclusion: 研究结果强调了自编码器几何结构对潜在扩散模型记忆效应的被忽视影响，为分析基于扩散的生成模型的隐私风险提供了新视角。

Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

</details>


### [106] [Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model](https://arxiv.org/abs/2511.20636)
*Ziyue Wang,Yayati Jadhav,Peter Pak,Amir Barati Farimani*

Main category: cs.LG

TL;DR: Image2Gcode是一个端到端的数据驱动框架，直接从图像生成3D打印机可执行的G代码，绕过了传统的CAD建模阶段。


<details>
  <summary>Details</summary>
Motivation: 传统机械设计和制造流程依赖CAD建模，这成为快速原型设计的主要瓶颈。即使小的设计变更也需要在CAD软件中手动更新，迭代耗时且难以扩展。

Method: 使用去噪扩散概率模型(DDPM)处理G代码序列，从图像中提取切片结构线索，通过迭代去噪将高斯噪声转换为可执行的打印移动轨迹和挤出参数。

Result: 建立了从视觉输入到原生工具路径的直接映射，无需CAD或STL中间文件，降低了增材制造的门槛，加速了从设计到制造的周期。

Conclusion: 该框架支持从简单草图或视觉参考进行按需原型制作，并与上游2D到3D重建模块集成，实现了从概念到物理工件的自动化流程，提高了设计迭代、修复工作流和分布式制造的可访问性。

Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [107] [Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.19562)
*Abraham Itzhak Weinberg*

Main category: cs.MA

TL;DR: TSLEC框架通过信任机制实现多智能体间的显式社会学习，相比独立学习将收敛速度提升23.9%，并产生组合式通信协议，在动态目标下保持稳健。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的涌现通信通常通过独立学习实现，导致收敛速度慢且可能产生次优协议。

Method: 引入TSLEC框架，智能体通过学习的信任关系调节知识转移，显式地向同伴传授成功策略。

Result: 信任机制将收敛所需回合数减少23.9%，产生组合式协议，在动态目标下解码准确率超过86.7%，信任分数与教学质量强相关。

Conclusion: 显式社会学习从根本上加速了多智能体协调中的涌现通信。

Abstract: Emergent communication in multi-agent systems typically occurs through independent learning, resulting in slow convergence and potentially suboptimal protocols. We introduce TSLEC (Trust-Based Social Learning with Emergent Communication), a framework where agents explicitly teach successful strategies to peers, with knowledge transfer modulated by learned trust relationships. Through experiments with 100 episodes across 30 random seeds, we demonstrate that trust-based social learning reduces episodes-to-convergence by 23.9% (p < 0.001, Cohen's d = 1.98) compared to independent emergence, while producing compositional protocols (C = 0.38) that remain robust under dynamic objectives (Phi > 0.867 decoding accuracy). Trust scores strongly correlate with teaching quality (r = 0.743, p < 0.001), enabling effective knowledge filtering. Our results establish that explicit social learning fundamentally accelerates emergent communication in multi-agent coordination.

</details>


### [108] [An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design](https://arxiv.org/abs/2511.19726)
*Roberto Garrone*

Main category: cs.MA

TL;DR: 本文提出了一个通用的自适应多智能体学习框架，整合了动态机制、信息论诊断、结构因果模型、先验生成方法和无监督行为识别，用于分析学习智能体和自适应控制如何共同影响系统轨迹。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统通常在反馈、适应和非平稳性下运行，但许多仿真研究仍采用静态决策规则和固定控制参数，需要更全面的分析框架。

Method: 集成四种动态机制区分静态与自适应智能体、固定与自适应系统参数；使用信息论诊断评估可预测性和结构；采用结构因果模型明确干预语义；开发从聚合数据生成智能体先验的程序；应用无监督方法识别涌现行为机制。

Result: 提供了一个领域无关的架构，用于系统比较非平衡、振荡或漂移动力学中的稳定性、性能和可解释性。

Conclusion: 该框架为开发可解释和可争议的多智能体决策过程提供了结构化方法，包括数学定义、计算算子和实验设计模板。

Abstract: Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.

</details>


### [109] [Complex Instruction Following with Diverse Style Policies in Football Games](https://arxiv.org/abs/2511.19885)
*Chenglu Sun,Shuo Shen,Haonan Hu,Wei Zhou,Chen Chen*

Main category: cs.MA

TL;DR: 本文提出了一种用于复杂多智能体环境的语言控制强化学习新范式LCDSP，包含多样化风格训练方法和风格解释器，能够在足球等复杂场景中理解和执行高级战术指令。


<details>
  <summary>Details</summary>
Motivation: 当前语言控制强化学习主要针对基础领域和简单指令，难以在复杂多智能体环境中理解和执行高级抽象指令，如足球比赛中的战术指令。

Method: 提出LCDSP框架，包含多样化风格训练方法（DST）和风格解释器（SI）。DST通过风格参数训练能够展现多样化行为的单一策略，SI将高级语言指令快速准确地转换为相应的风格参数。

Result: 在复杂的5v5足球环境中进行广泛实验，证明LCDSP能够有效理解抽象战术指令并准确执行所需的多样化行为风格。

Conclusion: LCDSP展示了在复杂现实世界应用中处理高级语言指令的潜力，为复杂多智能体环境中的语言控制强化学习提供了有效解决方案。

Abstract: Despite advancements in language-controlled reinforcement learning (LC-RL) for basic domains and straightforward commands (e.g., object manipulation and navigation), effectively extending LC-RL to comprehend and execute high-level or abstract instructions in complex, multi-agent environments, such as football games, remains a significant challenge. To address this gap, we introduce Language-Controlled Diverse Style Policies (LCDSP), a novel LC-RL paradigm specifically designed for complex scenarios. LCDSP comprises two key components: a Diverse Style Training (DST) method and a Style Interpreter (SI). The DST method efficiently trains a single policy capable of exhibiting a wide range of diverse behaviors by modulating agent actions through style parameters (SP). The SI is designed to accurately and rapidly translate high-level language instructions into these corresponding SP. Through extensive experiments in a complex 5v5 football environment, we demonstrate that LCDSP effectively comprehends abstract tactical instructions and accurately executes the desired diverse behavioral styles, showcasing its potential for complex, real-world applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探讨了使用可穿戴设备和AI方法预测慢性疼痛和鸦片使用障碍患者的疼痛峰值，发现机器学习模型预测准确率较高，而大语言模型在此领域表现有限。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛和鸦片使用障碍是相互关联的常见疾病，目前缺乏基于证据的综合治疗方法。可穿戴设备有潜力监测复杂患者信息，但大语言模型在理解疼痛峰值方面的应用尚未探索。

Method: 使用可穿戴设备收集数据，采用多种AI方法（包括机器学习模型和大语言模型）来预测疼痛峰值。

Result: 机器学习模型在预测疼痛峰值方面取得了相对较高的准确率（>0.7），而大语言模型在提供疼痛峰值洞察方面表现有限。

Conclusion: 可穿戴设备的实时监测结合先进AI模型可以促进疼痛峰值的早期检测，支持个性化干预，但需要开发能够提供可操作洞察的大语言模型。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [111] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: HeaRT是一个用于自动化设计循环的基础推理引擎，在40个电路基准测试中实现了>97%的推理准确率和>98%的Pass@1性能，同时运行成本仅为SOTA基线的<0.5倍实时token预算。


<details>
  <summary>Details</summary>
Motivation: 传统AI驱动的AMS设计自动化算法受限于对高质量数据集的依赖、跨架构可移植性差以及缺乏自适应机制，需要开发更智能、自适应、类人风格的设计优化方法。

Method: 提出了HeaRT基础推理引擎，作为实现智能自适应设计优化的第一步，该引擎能够在电路复杂度增加时保持高性能。

Result: HeaRT在尺寸和拓扑设计自适应任务中实现了>3倍的收敛速度提升，同时保留了先前的设计意图。

Conclusion: HeaRT代表了向智能自适应设计优化的重要进展，在保持高性能的同时显著提高了设计效率。

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [112] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: FISCAL框架通过生成金融事实核查的合成数据，训练出轻量级验证器MiniCheck-FISCAL，在多个金融数据集上超越GPT-3.5 Turbo等模型，接近更大模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在金融应用中存在的事实可靠性不足和计算效率低下的问题，当前系统容易产生幻觉细节且依赖过大的模型。

Method: 提出FISCAL框架生成金融事实核查的合成数据，使用该数据训练轻量级验证器MiniCheck-FISCAL。

Result: MiniCheck-FISCAL在多个金融数据集上表现优异，超越GPT-3.5 Turbo和类似规模的开源模型，接近Mixtral-8x22B等大20倍模型的准确性，在外部数据集上可与GPT-4o和Claude-3.5媲美。

Conclusion: 领域特定的合成数据结合高效微调，能使紧凑模型在金融AI应用中实现最先进的准确性、鲁棒性和可扩展性。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [113] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLMs）在教育评估项目与内容标准对齐过程中的应用潜力，测试了三种LLM模型在识别错位项目、选择正确技能和预筛选候选技能等任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的人工对齐审查虽然准确但耗时耗力，特别是在大型项目库中。研究旨在探索LLMs是否能加速这一过程而不牺牲准确性。

Method: 使用超过12,000个K-5年级的项目-技能对，测试了GPT-3.5 Turbo、GPT-4o-mini和GPT-4o三种LLM模型在三个实际任务中的表现：识别错位项目、从完整标准集中选择正确技能、以及在分类前缩小候选列表。

Result: GPT-4o-mini在约83-94%的情况下正确识别了对齐状态；数学领域表现强劲，阅读领域因标准语义重叠而表现较低；预筛选候选技能显著改善结果，正确技能出现在前五建议中的概率超过95%。

Conclusion: LLMs，特别是与候选筛选策略结合使用时，可以显著减少项目审查的人工负担，同时保持对齐准确性。建议开发混合流程，将基于LLM的筛选与在模糊情况下的人工审查相结合，为持续的项目验证和教学对齐提供可扩展解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [114] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: KOM是一个多智能体系统，用于自动化膝骨关节炎的评估、风险预测和治疗处方，在资源有限环境中提升护理效率。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎影响全球6亿多人，个性化多学科干预虽能延缓疾病进展，但在资源有限环境中实施困难。

Method: 开发KOM多智能体系统，自动化KOA评估、风险预测和治疗处方，基于患者个体特征生成定制管理计划。

Result: 基准实验显示KOM在影像分析和处方生成方面优于通用大语言模型；随机三臂模拟研究表明KOM与临床医生协作可将诊断和规划时间减少38.5%，并提升治疗质量。

Conclusion: KOM有助于促进自动化KOA管理，整合到临床工作流程中可提高护理效率，其模块化架构为开发其他慢性病AI辅助管理系统提供参考。

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [115] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于评估指导的提示优化方法，通过建立系统化的提示评估框架和训练执行无关的评估器，实现可解释的、查询依赖的提示优化，在多个数据集和模型上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法主要优化静态模板，在复杂动态用户场景中效果有限；现有查询依赖方法依赖不稳定的文本反馈或黑盒奖励模型，提供弱且不可解释的优化信号；提示质量本身缺乏统一系统定义，导致评估信号碎片化不可靠。

Method: 首先建立面向性能的系统化提示评估框架；开发并微调执行无关的评估器，直接从文本预测多维度质量分数；评估器指导度量感知的优化器，以可解释、查询依赖的方式诊断失败模式并重写提示。

Result: 评估器在预测提示性能方面达到最强准确率；评估指导的优化在8个数据集和3个骨干模型上持续超越静态模板和查询依赖基线方法。

Conclusion: 提出了统一的、基于度量的提示质量视角，证明了评估指导的优化管道能够在多样化任务中提供稳定、可解释且模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [116] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 本文提出了一种结合ω-正则目标与显式约束的强化学习方法，解决了传统标量奖励表达能力有限和安全性-性能权衡的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，表达能力有限且容易导致奖励黑客行为，同时单一性能指标掩盖了安全性-性能的权衡问题。

Method: 开发了基于线性规划的模型强化学习算法，将ω-正则目标与约束分离处理，并建立了到约束极限平均问题的转换方法。

Result: 算法在极限情况下能够产生最大化满足ω-正则目标概率的策略，同时满足指定阈值的ω-正则约束。

Conclusion: 该方法有效解决了强化学习中目标表达能力和安全性权衡的双重挑战，为复杂行为规范提供了理论保证。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [117] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: MicroSims是一个基于AI的轻量级教育模拟框架，能够快速生成可嵌入任何数字学习平台的交互式模拟，无需编程知识即可定制，解决了传统教育模拟成本高、技术复杂和平台依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 传统教育模拟创建需要大量资源和技术专长，限制了其在教育中的广泛应用。MicroSims旨在通过AI辅助生成、通用嵌入和无代码定制，降低教育模拟的创建门槛，促进教育公平。

Method: 采用标准化设计模式实现AI辅助生成，iframe架构确保通用嵌入和沙盒安全，透明可修改代码支持定制和教学透明度。包含设计原则、技术架构、元数据标准和开发流程的完整框架。

Result: 基于物理教育研究和STEM学科元分析，交互式模拟可将概念理解提高30-40%。MicroSims在保持这些优势的同时，解决了成本、技术复杂性和平台依赖等长期障碍。

Conclusion: MicroSims为教育公平和低成本智能交互式教科书开辟了新途径，使全球教育工作者能够按需创建定制化、与课程对齐的模拟，并为基于AI的自适应学习系统奠定基础。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [118] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 本研究将通用自我效能感量表(GSES)应用于10个大语言模型，在四种条件下评估其模拟自我评估能力。结果显示模型自我评估在不同条件下差异显著，但自我评估与真实能力不匹配，且心理测量提示能提供结构化洞察但无法校准性能估计。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型的评估主要关注任务准确性，而忽视了自我评估这一可靠智能的关键方面。本研究旨在探索LLMs的模拟自我评估能力及其与真实表现的关系。

Method: 将10项通用自我效能感量表(GSES)应用于10个LLMs，在四种条件(无任务、计算推理、社会推理、摘要)下进行测试，分析自我评估的稳定性、与任务表现的关联性，并进行定性分析。

Result: GSES响应在不同重复测试和随机项目顺序下高度稳定；模型在不同条件下自我效能感水平差异显著，总体得分低于人类标准；所有模型在计算和社会问题上准确率完美，但摘要表现差异大；自我评估与能力不匹配；后续置信度提示导致适度向下修正。

Conclusion: 心理测量提示能提供LLM沟通行为的结构化洞察，但无法提供校准的性能估计；自我效能感较高的模型表现出更自信、拟人化的推理风格，而得分较低的模型则反映谨慎、去拟人化的解释方式。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [119] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型来评估中间算法步骤，无需复杂训练。该方法使用相似性过滤去除冗余节点，利用沙箱执行反馈定位错误步骤，并在生成过程中及时纠正。实验表明，该方法在四个公共代码生成基准测试中优于现有方法，同时减少约15%的token消耗。


<details>
  <summary>Details</summary>
Motivation: 基于树搜索的方法在提升大语言模型代码生成能力方面取得进展，但由于难以有效评估中间算法步骤以及无法定位和及时纠正错误步骤，这些方法常常生成错误代码并增加计算成本。

Method: 提出RPM-MCTS方法，利用知识检索作为过程奖励模型来评估中间算法步骤，避免复杂训练过程。在扩展阶段使用相似性过滤去除冗余节点，确保推理路径多样性。同时利用沙箱执行反馈定位错误算法步骤，实现及时针对性纠正。

Result: 在四个公共代码生成基准测试上的广泛实验表明，RPM-MCTS优于当前最先进方法，同时实现约15%的token消耗减少。使用RPM-MCTS构建的数据对基础模型进行全微调显著提升了其代码能力。

Conclusion: RPM-MCTS是一种有效的代码生成方法，通过知识检索和沙箱反馈机制解决了中间步骤评估和错误定位问题，在性能和效率方面均优于现有方法。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [120] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱生成基准数据集的新方法，用于评估LLM输出的语义相似性方法，解决了现有基准生成成本高、领域适用性有限等问题。


<details>
  <summary>Details</summary>
Motivation: 当前语义相似性方法可能更关注句法或词汇形式而非语义内容，现有基准存在生成成本高、领域适用性有限、等价定义不明确等问题。

Method: 利用知识图谱生成语义相似或不同的自然语言陈述对，将不同对分为四种子类型，在四个不同领域生成基准数据集。

Result: 语义变化子类型和基准领域都会影响语义相似性方法的性能，没有方法始终表现最优。

Conclusion: 研究结果对使用LLM作为评判者检测文本语义内容具有重要启示。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [121] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了一个系统级分类法，识别了15种在真实世界LLM应用中出现的隐藏故障模式，分析了当前评估与监控实践的差距，并提出了构建可靠、可维护、成本感知的LLM系统的高层设计原则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正被快速集成到决策支持工具和AI软件系统中，但其在生产环境中的行为仍未被充分理解，且故障模式与传统机器学习模型有根本差异。

Method: 提出系统级分类法识别15种隐藏故障模式，分析评估与监控实践差距，研究部署挑战，并制定高层设计原则。

Result: 建立了涵盖多步推理漂移、潜在不一致性、上下文边界退化等故障模式的分类法，揭示了现有基准在稳定性、可重复性等方面的不足。

Conclusion: 通过将LLM可靠性框架化为系统工程问题而非纯模型中心问题，为未来评估方法学、AI系统鲁棒性和可靠LLM部署研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [122] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 团队MSRA_SC在CPDC 2025竞赛中提出统一框架，通过上下文工程和GRPO训练在API和GPU赛道取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决常识人物对话中的工具调用稳定性、执行可靠性和角色扮演指导问题，同时缓解小样本过拟合。

Method: 采用上下文工程（动态工具剪枝、人物信息裁剪、参数规范化、函数合并）和GRPO强化学习训练。

Result: 在最终评估中排名：Task 2 API第1名，Task 1 API第2名，Task 3 API和GPU赛道第3名。

Conclusion: 提出的框架简单有效，在多个任务中表现优异，证明了方法的有效性。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [123] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个将导航研究指标与商业可行性进行定量分析的微导航经济测试平台，通过成本-收益分析评估自主配送机器人的经济可行性，揭示任务成功率优化与经济部署优化的根本差异。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准只关注任务成功率指标，忽视了经济可行性这一对自主配送机器人商业部署至关重要的因素。

Method: CostNav模拟完整的经济生命周期，包括硬件、训练、能源、维护成本和配送收入，使用行业衍生参数，从缩减规模模拟投影到现实配送场景。

Result: 基准模型实现43.0%的SLA合规率，但商业上不可行：每次运行损失30.009美元，无盈亏平衡点，因为运营成本主要由碰撞引起的维护成本主导（占每次运行成本的99.7%）。

Conclusion: CostNav填补了导航研究与商业部署之间的差距，为评估基于规则的导航、模仿学习和成本感知RL训练奠定了基础，使能够基于数据做出跨导航范式的经济权衡决策。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [124] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW是一个通过构建和精化经验知识库来优化LLM智能体的框架，在保持计算效率的同时显著提升任务精度和减少API调用。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的智能体训练方法计算开销大且策略难以解释、适应和改进，需要一种更实用、可解释的优化方法。

Method: 引入BREW框架，通过任务评估和行为准则学习洞察，利用状态空间搜索确保鲁棒性，并采用有效的记忆分区方法提高检索和精化效率。

Result: 在OSWorld、τ²Bench和SpreadsheetBench等基准测试中，BREW实现了10-20%的任务精度提升，10-15%的API/工具调用减少，执行时间更快，同时保持与基础模型相当的计算效率。

Conclusion: BREW将知识库确立为模块化、可控的智能体优化基础，提供了一种透明、可解释和可扩展的行为塑造方式，与将记忆视为静态上下文的先前工作形成对比。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [125] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 本文旨在澄清主动推理概念，将其与自由能原理分离，提出在离散状态空间中实现主动推理的优化问题可以表述为约束散度最小化问题，可通过标准平均场方法求解。


<details>
  <summary>Details</summary>
Motivation: 澄清主动推理概念，将其从自由能原理中分离出来，提供更清晰的理论框架。

Method: 将主动推理在离散状态空间中的优化问题重新表述为约束散度最小化问题，使用标准平均场方法求解，不依赖期望自由能概念。

Result: 提出的感知/行动散度准则在建模感知时与变分自由能一致，在建模行动时与期望自由能泛函相差一个熵正则化项。

Conclusion: 主动推理可以在不依赖自由能原理的情况下实现，通过约束散度最小化方法为主动推理提供了替代的理论基础。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [126] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse是一个大规模几何-声学对齐数据集，通过显式连接3D几何→物理属性→模态参数→声学信号的因果链，实现物理一致的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉和语言的多模态学习框架缺乏物理一致性，忽视了物体几何、材料、振动模态和产生声音之间的内在因果关系。

Method: 引入VibraVerse数据集，每个3D模型具有明确的物理属性和体积几何，计算模态特征频率和特征向量用于冲击声合成；提出CLASP对比学习框架进行跨模态对齐。

Result: 在几何到声音预测、声音引导形状重建和跨模态表示学习等基准任务上，基于VibraVerse训练的模型展现出更高的准确性、可解释性和跨模态泛化能力。

Conclusion: VibraVerse为物理一致且因果可解释的多模态学习建立了基准，为声音引导的具身感知和物理世界理解提供了基础。

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>
